{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import RegexParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from getpass import getpass\n",
    "from langchain.agents.agent_toolkits import FileManagementToolkit\n",
    "import os\n",
    "\n",
    "python_script = os.path.join(os.getcwd() , \"langchain\" ,\"generated_script.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers import RegexParser\n",
    "\n",
    "template = \"\"\"You're an AI assistant specializing in python development. Based on the input provided, you must update the python code that is compatible with python 3.9. Additionally, offer a brief explanation about how you arrived at the python code.\n",
    "If the input is a question, you must explain the code only and additionnaly propose some code. Do not halucinate or make up information. If you do not know the answer, just say \"I don't know\".\n",
    "\n",
    "The python code you must update is the following:\n",
    "```python\n",
    "{python_code}\n",
    "```\n",
    "\n",
    "Do not forget about importing the necessary libraries.\n",
    "\n",
    "Write your anwser in the following format: the code and explanation in markdown format\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\", \"python_code\"])\n",
    "python_assistant_chain = LLMChain(llm=llm, prompt=prompt_template, verbose=True)\n",
    "\n",
    "def parse(output):\n",
    "    import re\n",
    "    python_code_match = re.search(r\"```python\\n(.*?)\\n```\\n(.*)\", output, re.DOTALL)\n",
    "    if python_code_match:\n",
    "        python_code = python_code_match.group(1)\n",
    "        explain_code = python_code_match.group(2)\n",
    "        print(python_code)\n",
    "        print(explain_code)\n",
    "    else:\n",
    "        print(\"Python code not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou're an AI assistant specializing in python development. Based on the input provided, you must update the python code that is compatible with python 3.9. Additionally, offer a brief explanation about how you arrived at the python code.\n",
      "If the input is a question, you must explain the code only and additionnaly propose some code. Do not halucinate or make up information. If you do not know the answer, just say \"I don't know\".\n",
      "\n",
      "The python code you must update is the following:\n",
      "```python\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(\"Heeey!\")\n",
      "```\n",
      "\n",
      "Do not forget about importing the necessary libraries.\n",
      "\n",
      "Write your anwser in the following format: the code and explanation in markdown format\n",
      "\n",
      "Question: replace the hey by hello world\n",
      "\n",
      "Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = python_assistant_chain.predict(question='replace the hey by hello world', python_code=open(python_script).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(\"Hello World!\")\n",
      "\n",
      "I updated the code to print \"Hello World!\" instead of \"Heeey!\" by importing the `sys` library and replacing the `print` statement with the desired output.\n"
     ]
    }
   ],
   "source": [
    "parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def add_and_multiply(a, b):\n",
      "    \"\"\"This function adds two variables and multiplies the sum by 3\n",
      "    \n",
      "    Args:\n",
      "        a (int): first number\n",
      "        b (int): second number\n",
      "    \n",
      "    Returns:\n",
      "        int: the sum of a and b multiplied by 3\n",
      "    \"\"\"\n",
      "    return (a + b) * 3\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    print(\"Heeey!\")\n",
      "\n",
      "I added a function called `add_and_multiply` that takes two arguments, `a` and `b`, and returns the sum of the two multiplied by 3. The function is documented with a docstring that explains what it does.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[python_assistant_chain, review_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/workspace/ai_projects/CustomerCareAI/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:171: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/mnt/data/workspace/ai_projects/CustomerCareAI/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:739: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LLMChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m      3\u001b[0m q_llm \u001b[39m=\u001b[39m OpenAI(\n\u001b[1;32m      4\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m      5\u001b[0m     openai_api_key\u001b[39m=\u001b[39mopenai_api_key,\n\u001b[1;32m      6\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-16k\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     max_tokens\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m llm \u001b[39m=\u001b[39m OpenAI(\n\u001b[1;32m     11\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-16k\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     13\u001b[0m     openai_api_key\u001b[39m=\u001b[39mopenai_api_key,\n\u001b[1;32m     14\u001b[0m     max_tokens\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m question_generator \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39mq_llm, prompt\u001b[39m=\u001b[39mcondense_question_prompt)\n\u001b[1;32m     19\u001b[0m doc_chain \u001b[39m=\u001b[39m load_qa_chain(llm\u001b[39m=\u001b[39mllm, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m, prompt\u001b[39m=\u001b[39mQA_PROMPT)\n\u001b[1;32m     20\u001b[0m chain \u001b[39m=\u001b[39m ConversationalRetrievalChain(\n\u001b[1;32m     21\u001b[0m     retriever\u001b[39m=\u001b[39mvectorstore\u001b[39m.\u001b[39mas_retriever(),\n\u001b[1;32m     22\u001b[0m     combine_docs_chain\u001b[39m=\u001b[39mdoc_chain,\n\u001b[1;32m     23\u001b[0m     question_generator\u001b[39m=\u001b[39mquestion_generator,\n\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LLMChain' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "q_llm = OpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key,\n",
    "    model_name=\"gpt-3.5-turbo-16k\",\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-16k\",\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "python_assistant_prompt = LLMChain(llm=q_llm, prompt=condense_question_prompt)\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=q_llm,\n",
    "    prompt=PROMPT,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain, python_assistant_prompt],\n",
    "    input_variables=[\"era\", \"title\"],\n",
    "    # Here we return multiple variables\n",
    "    output_variables=[\"social_post_text\"],\n",
    "    verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heeey!\n"
     ]
    }
   ],
   "source": [
    "exec(open(\"langchain/generated_script.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
