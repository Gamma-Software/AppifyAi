{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "openai_api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'langchain'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "if os.path.exists(\"langchain\"):\n",
    "    shutil.rmtree(\"langchain\")\n",
    "os.system(\"git clone https://github.com/hwchase17/langchain.git --branch master --single-branch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Union\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import PythonLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers.language import LanguageParser\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/865 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 865/865 [00:03<00:00, 242.57it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    \"langchain/langchain/\",\n",
    "    glob=\"**/*.py\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON),\n",
    "    show_progress=True\n",
    ")\n",
    "source_code_doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'langchain/langchain/cache.py',\n",
       " 'content_type': 'functions_classes',\n",
       " 'language': <Language.PYTHON: 'python'>}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_code_doc[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/334 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 334/334 [00:00<00:00, 2551.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='By default, `PromptTemplate` will validate the `template` string by checking whether the `input_variables` match the variables defined in `template`. You can disable this behavior by setting `validate_template` to `False`  \\n```python\\ntemplate = \"I am learning langchain because {reason}.\"  \\nprompt_template = PromptTemplate(template=template,\\ninput_variables=[\"reason\", \"foo\"]) # ValueError due to extra variables\\nprompt_template = PromptTemplate(template=template,\\ninput_variables=[\"reason\", \"foo\"],\\nvalidate_template=False) # No error\\n```', metadata={'Header 1': 'Validate template'}),\n",
       " Document(page_content='LangChain provides different types of `MessagePromptTemplate`. The most commonly used are `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, which create an AI message, system message and human message respectively.  \\nHowever, in cases where the chat model supports taking chat message with arbitrary role, you can use `ChatMessagePromptTemplate`, which allows user to specify the role name.  \\n```python\\nfrom langchain.prompts import ChatMessagePromptTemplate  \\nprompt = \"May the {subject} be with you\"  \\nchat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"Jedi\", template=prompt)\\nchat_message_prompt.format(subject=\"force\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nChatMessage(content=\\'May the force be with you\\', additional_kwargs={}, role=\\'Jedi\\')\\n```  \\n</CodeOutputBlock>  \\nLangChain also provides `MessagesPlaceholder`, which gives you full control of what messages to be rendered during formatting. This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting.  \\n```python\\nfrom langchain.prompts import MessagesPlaceholder  \\nhuman_prompt = \"Summarize our conversation so far in {word_count} words.\"\\nhuman_message_template = HumanMessagePromptTemplate.from_template(human_prompt)  \\nchat_prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=\"conversation\"), human_message_template])\\n```  \\n```python\\nhuman_message = HumanMessage(content=\"What is the best way to learn programming?\")\\nai_message = AIMessage(content=\"\"\"\\\\\\n1. Choose a programming language: Decide on a programming language that you want to learn.  \\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.  \\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\\\\n\"\"\")  \\nchat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=\"10\").to_messages()\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[HumanMessage(content=\\'What is the best way to learn programming?\\', additional_kwargs={}),\\nAIMessage(content=\\'1. Choose a programming language: Decide on a programming language that you want to learn. \\\\n\\\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\\\n\\\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\', additional_kwargs={}),\\nHumanMessage(content=\\'Summarize our conversation so far in 10 words.\\', additional_kwargs={})]\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Types of `MessagePromptTemplate`'}),\n",
       " Document(page_content='By default, `PromptTemplate` will treat the provided template as a Python f-string. You can specify other template format through `template_format` argument:  \\n```python', metadata={'Header 1': 'Template formats'}),\n",
       " Document(page_content='jinja2_template = \"Tell me a {{ adjective }} joke about {{ content }}\"\\nprompt_template = PromptTemplate.from_template(template=jinja2_template, template_format=\"jinja2\")  \\nprompt_template.format(adjective=\"funny\", content=\"chickens\")', metadata={'Header 1': 'Make sure jinja2 is installed before running this'}),\n",
       " Document(page_content='```  \\nCurrently, `PromptTemplate` only supports `jinja2` and `f-string` templating format. If there is any other templating format that you would like to use, feel free to open an issue in the [Github](https://github.com/hwchase17/langchain/issues) page.', metadata={'Header 1': '-> Tell me a funny joke about chickens.'}),\n",
       " Document(page_content='When you create a custom chain you can easily set it up to use the same callback system as all the built-in chains.\\n`_call`, `_generate`, `_run`, and equivalent async methods on Chains / LLMs / Chat Models / Agents / Tools now receive a 2nd argument called `run_manager` which is bound to that run, and contains the logging methods that can be used by that object (i.e. `on_llm_new_token`). This is useful when constructing a custom chain. See this guide for more information on how to [create custom chains and use callbacks inside them](/docs/modules/chains/how_to/custom_chain.html).', metadata={'Header 1': 'Callbacks for custom chains'}),\n",
       " Document(page_content='You can add tags to your callbacks by passing a `tags` argument to the `call()`/`run()`/`apply()` methods. This is useful for filtering your logs, eg. if you want to log all requests made to a specific LLMChain, you can add a tag, and then filter your logs by that tag. You can pass tags to both constructor and request callbacks, see the examples above for details. These tags are then passed to the `tags` argument of the \"start\" callback methods, ie. `on_llm_start`, `on_chat_model_start`, `on_chain_start`, `on_tool_start`.', metadata={'Header 1': 'Tags'}),\n",
       " Document(page_content='Dependents stats for `hwchase17/langchain`  \\n[![](https://img.shields.io/static/v1?label=Used%20by&message=5152&color=informational&logo=slickpic)](https://github.com/hwchase17/langchain/network/dependents)\\n[![](https://img.shields.io/static/v1?label=Used%20by%20(public)&message=172&color=informational&logo=slickpic)](https://github.com/hwchase17/langchain/network/dependents)\\n[![](https://img.shields.io/static/v1?label=Used%20by%20(private)&message=4980&color=informational&logo=slickpic)](https://github.com/hwchase17/langchain/network/dependents)\\n[![](https://img.shields.io/static/v1?label=Used%20by%20(stars)&message=17239&color=informational&logo=slickpic)](https://github.com/hwchase17/langchain/network/dependents)  \\n[update: 2023-05-17; only dependent repositories with Stars > 100]  \\n| Repository | Stars  |\\n| :--------  | -----: |\\n|[openai/openai-cookbook](https://github.com/openai/openai-cookbook) | 35401 |\\n|[LAION-AI/Open-Assistant](https://github.com/LAION-AI/Open-Assistant) | 32861 |\\n|[microsoft/TaskMatrix](https://github.com/microsoft/TaskMatrix) | 32766 |\\n|[hpcaitech/ColossalAI](https://github.com/hpcaitech/ColossalAI) | 29560 |\\n|[reworkd/AgentGPT](https://github.com/reworkd/AgentGPT) | 22315 |\\n|[imartinez/privateGPT](https://github.com/imartinez/privateGPT) | 17474 |\\n|[openai/chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin) | 16923 |\\n|[mindsdb/mindsdb](https://github.com/mindsdb/mindsdb) | 16112 |\\n|[jerryjliu/llama_index](https://github.com/jerryjliu/llama_index) | 15407 |\\n|[mlflow/mlflow](https://github.com/mlflow/mlflow) | 14345 |\\n|[GaiZhenbiao/ChuanhuChatGPT](https://github.com/GaiZhenbiao/ChuanhuChatGPT) | 10372 |\\n|[databrickslabs/dolly](https://github.com/databrickslabs/dolly) | 9919 |\\n|[AIGC-Audio/AudioGPT](https://github.com/AIGC-Audio/AudioGPT) | 8177 |\\n|[logspace-ai/langflow](https://github.com/logspace-ai/langflow) | 6807 |\\n|[imClumsyPanda/langchain-ChatGLM](https://github.com/imClumsyPanda/langchain-ChatGLM) | 6087 |\\n|[arc53/DocsGPT](https://github.com/arc53/DocsGPT) | 5292 |\\n|[e2b-dev/e2b](https://github.com/e2b-dev/e2b) | 4622 |\\n|[nsarrazin/serge](https://github.com/nsarrazin/serge) | 4076 |\\n|[madawei2699/myGPTReader](https://github.com/madawei2699/myGPTReader) | 3952 |\\n|[zauberzeug/nicegui](https://github.com/zauberzeug/nicegui) | 3952 |\\n|[go-skynet/LocalAI](https://github.com/go-skynet/LocalAI) | 3762 |\\n|[GreyDGL/PentestGPT](https://github.com/GreyDGL/PentestGPT) | 3388 |\\n|[mmabrouk/chatgpt-wrapper](https://github.com/mmabrouk/chatgpt-wrapper) | 3243 |\\n|[zilliztech/GPTCache](https://github.com/zilliztech/GPTCache) | 3189 |\\n|[wenda-LLM/wenda](https://github.com/wenda-LLM/wenda) | 3050 |\\n|[marqo-ai/marqo](https://github.com/marqo-ai/marqo) | 2930 |\\n|[gkamradt/langchain-tutorials](https://github.com/gkamradt/langchain-tutorials) | 2710 |\\n|[PrefectHQ/marvin](https://github.com/PrefectHQ/marvin) | 2545 |\\n|[project-baize/baize-chatbot](https://github.com/project-baize/baize-chatbot) | 2479 |\\n|[whitead/paper-qa](https://github.com/whitead/paper-qa) | 2399 |\\n|[langgenius/dify](https://github.com/langgenius/dify) | 2344 |\\n|[GerevAI/gerev](https://github.com/GerevAI/gerev) | 2283 |\\n|[hwchase17/chat-langchain](https://github.com/hwchase17/chat-langchain) | 2266 |\\n|[guangzhengli/ChatFiles](https://github.com/guangzhengli/ChatFiles) | 1903 |\\n|[Azure-Samples/azure-search-openai-demo](https://github.com/Azure-Samples/azure-search-openai-demo) | 1884 |\\n|[OpenBMB/BMTools](https://github.com/OpenBMB/BMTools) | 1860 |\\n|[Farama-Foundation/PettingZoo](https://github.com/Farama-Foundation/PettingZoo) | 1813 |\\n|[OpenGVLab/Ask-Anything](https://github.com/OpenGVLab/Ask-Anything) | 1571 |\\n|[IntelligenzaArtificiale/Free-Auto-GPT](https://github.com/IntelligenzaArtificiale/Free-Auto-GPT) | 1480 |\\n|[hwchase17/notion-qa](https://github.com/hwchase17/notion-qa) | 1464 |\\n|[NVIDIA/NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) | 1419 |\\n|[Unstructured-IO/unstructured](https://github.com/Unstructured-IO/unstructured) | 1410 |\\n|[Kav-K/GPTDiscord](https://github.com/Kav-K/GPTDiscord) | 1363 |\\n|[paulpierre/RasaGPT](https://github.com/paulpierre/RasaGPT) | 1344 |\\n|[StanGirard/quivr](https://github.com/StanGirard/quivr) | 1330 |\\n|[lunasec-io/lunasec](https://github.com/lunasec-io/lunasec) | 1318 |\\n|[vocodedev/vocode-python](https://github.com/vocodedev/vocode-python) | 1286 |\\n|[agiresearch/OpenAGI](https://github.com/agiresearch/OpenAGI) | 1156 |\\n|[h2oai/h2ogpt](https://github.com/h2oai/h2ogpt) | 1141 |\\n|[jina-ai/thinkgpt](https://github.com/jina-ai/thinkgpt) | 1106 |\\n|[yanqiangmiffy/Chinese-LangChain](https://github.com/yanqiangmiffy/Chinese-LangChain) | 1072 |\\n|[ttengwang/Caption-Anything](https://github.com/ttengwang/Caption-Anything) | 1064 |\\n|[jina-ai/dev-gpt](https://github.com/jina-ai/dev-gpt) | 1057 |\\n|[juncongmoo/chatllama](https://github.com/juncongmoo/chatllama) | 1003 |\\n|[greshake/llm-security](https://github.com/greshake/llm-security) | 1002 |\\n|[visual-openllm/visual-openllm](https://github.com/visual-openllm/visual-openllm) | 957 |\\n|[richardyc/Chrome-GPT](https://github.com/richardyc/Chrome-GPT) | 918 |\\n|[irgolic/AutoPR](https://github.com/irgolic/AutoPR) | 886 |\\n|[mmz-001/knowledge_gpt](https://github.com/mmz-001/knowledge_gpt) | 867 |\\n|[thomas-yanxin/LangChain-ChatGLM-Webui](https://github.com/thomas-yanxin/LangChain-ChatGLM-Webui) | 850 |\\n|[microsoft/X-Decoder](https://github.com/microsoft/X-Decoder) | 837 |\\n|[peterw/Chat-with-Github-Repo](https://github.com/peterw/Chat-with-Github-Repo) | 826 |\\n|[cirediatpl/FigmaChain](https://github.com/cirediatpl/FigmaChain) | 782 |\\n|[hashintel/hash](https://github.com/hashintel/hash) | 778 |\\n|[seanpixel/Teenage-AGI](https://github.com/seanpixel/Teenage-AGI) | 773 |\\n|[jina-ai/langchain-serve](https://github.com/jina-ai/langchain-serve) | 738 |\\n|[corca-ai/EVAL](https://github.com/corca-ai/EVAL) | 737 |\\n|[ai-sidekick/sidekick](https://github.com/ai-sidekick/sidekick) | 717 |\\n|[rlancemartin/auto-evaluator](https://github.com/rlancemartin/auto-evaluator) | 703 |\\n|[poe-platform/api-bot-tutorial](https://github.com/poe-platform/api-bot-tutorial) | 689 |\\n|[SamurAIGPT/Camel-AutoGPT](https://github.com/SamurAIGPT/Camel-AutoGPT) | 666 |\\n|[eyurtsev/kor](https://github.com/eyurtsev/kor) | 608 |\\n|[run-llama/llama-lab](https://github.com/run-llama/llama-lab) | 559 |\\n|[namuan/dr-doc-search](https://github.com/namuan/dr-doc-search) | 544 |\\n|[pieroit/cheshire-cat](https://github.com/pieroit/cheshire-cat) | 520 |\\n|[griptape-ai/griptape](https://github.com/griptape-ai/griptape) | 514 |\\n|[getmetal/motorhead](https://github.com/getmetal/motorhead) | 481 |\\n|[hwchase17/chat-your-data](https://github.com/hwchase17/chat-your-data) | 462 |\\n|[langchain-ai/langchain-aiplugin](https://github.com/langchain-ai/langchain-aiplugin) | 452 |\\n|[jina-ai/agentchain](https://github.com/jina-ai/agentchain) | 439 |\\n|[SamurAIGPT/ChatGPT-Developer-Plugins](https://github.com/SamurAIGPT/ChatGPT-Developer-Plugins) | 437 |\\n|[alexanderatallah/window.ai](https://github.com/alexanderatallah/window.ai) | 433 |\\n|[michaelthwan/searchGPT](https://github.com/michaelthwan/searchGPT) | 427 |\\n|[mpaepper/content-chatbot](https://github.com/mpaepper/content-chatbot) | 425 |\\n|[mckaywrigley/repo-chat](https://github.com/mckaywrigley/repo-chat) | 422 |\\n|[whyiyhw/chatgpt-wechat](https://github.com/whyiyhw/chatgpt-wechat) | 421 |\\n|[freddyaboulton/gradio-tools](https://github.com/freddyaboulton/gradio-tools) | 407 |\\n|[jonra1993/fastapi-alembic-sqlmodel-async](https://github.com/jonra1993/fastapi-alembic-sqlmodel-async) | 395 |\\n|[yeagerai/yeagerai-agent](https://github.com/yeagerai/yeagerai-agent) | 383 |\\n|[akshata29/chatpdf](https://github.com/akshata29/chatpdf) | 374 |\\n|[OpenGVLab/InternGPT](https://github.com/OpenGVLab/InternGPT) | 368 |\\n|[ruoccofabrizio/azure-open-ai-embeddings-qna](https://github.com/ruoccofabrizio/azure-open-ai-embeddings-qna) | 358 |\\n|[101dotxyz/GPTeam](https://github.com/101dotxyz/GPTeam) | 357 |\\n|[mtenenholtz/chat-twitter](https://github.com/mtenenholtz/chat-twitter) | 354 |\\n|[amosjyng/langchain-visualizer](https://github.com/amosjyng/langchain-visualizer) | 343 |\\n|[msoedov/langcorn](https://github.com/msoedov/langcorn) | 334 |\\n|[showlab/VLog](https://github.com/showlab/VLog) | 330 |\\n|[continuum-llms/chatgpt-memory](https://github.com/continuum-llms/chatgpt-memory) | 324 |\\n|[steamship-core/steamship-langchain](https://github.com/steamship-core/steamship-langchain) | 323 |\\n|[daodao97/chatdoc](https://github.com/daodao97/chatdoc) | 320 |\\n|[xuwenhao/geektime-ai-course](https://github.com/xuwenhao/geektime-ai-course) | 308 |\\n|[StevenGrove/GPT4Tools](https://github.com/StevenGrove/GPT4Tools) | 301 |\\n|[logan-markewich/llama_index_starter_pack](https://github.com/logan-markewich/llama_index_starter_pack) | 300 |\\n|[andylokandy/gpt-4-search](https://github.com/andylokandy/gpt-4-search) | 299 |\\n|[Anil-matcha/ChatPDF](https://github.com/Anil-matcha/ChatPDF) | 287 |\\n|[itamargol/openai](https://github.com/itamargol/openai) | 273 |\\n|[BlackHC/llm-strategy](https://github.com/BlackHC/llm-strategy) | 267 |\\n|[momegas/megabots](https://github.com/momegas/megabots) | 259 |\\n|[bborn/howdoi.ai](https://github.com/bborn/howdoi.ai) | 238 |\\n|[Cheems-Seminar/grounded-segment-any-parts](https://github.com/Cheems-Seminar/grounded-segment-any-parts) | 232 |\\n|[ur-whitelab/exmol](https://github.com/ur-whitelab/exmol) | 227 |\\n|[sullivan-sean/chat-langchainjs](https://github.com/sullivan-sean/chat-langchainjs) | 227 |\\n|[explosion/spacy-llm](https://github.com/explosion/spacy-llm) | 226 |\\n|[recalign/RecAlign](https://github.com/recalign/RecAlign) | 218 |\\n|[jupyterlab/jupyter-ai](https://github.com/jupyterlab/jupyter-ai) | 218 |\\n|[alvarosevilla95/autolang](https://github.com/alvarosevilla95/autolang) | 215 |\\n|[conceptofmind/toolformer](https://github.com/conceptofmind/toolformer) | 213 |\\n|[MagnivOrg/prompt-layer-library](https://github.com/MagnivOrg/prompt-layer-library) | 209 |\\n|[JohnSnowLabs/nlptest](https://github.com/JohnSnowLabs/nlptest) | 208 |\\n|[airobotlab/KoChatGPT](https://github.com/airobotlab/KoChatGPT) | 197 |\\n|[langchain-ai/auto-evaluator](https://github.com/langchain-ai/auto-evaluator) | 195 |\\n|[yvann-hub/Robby-chatbot](https://github.com/yvann-hub/Robby-chatbot) | 195 |\\n|[alejandro-ao/langchain-ask-pdf](https://github.com/alejandro-ao/langchain-ask-pdf) | 192 |\\n|[daveebbelaar/langchain-experiments](https://github.com/daveebbelaar/langchain-experiments) | 189 |\\n|[NimbleBoxAI/ChainFury](https://github.com/NimbleBoxAI/ChainFury) | 187 |\\n|[kaleido-lab/dolphin](https://github.com/kaleido-lab/dolphin) | 184 |\\n|[Anil-matcha/Website-to-Chatbot](https://github.com/Anil-matcha/Website-to-Chatbot) | 183 |\\n|[plchld/InsightFlow](https://github.com/plchld/InsightFlow) | 180 |\\n|[OpenBMB/AgentVerse](https://github.com/OpenBMB/AgentVerse) | 166 |\\n|[benthecoder/ClassGPT](https://github.com/benthecoder/ClassGPT) | 166 |\\n|[jbrukh/gpt-jargon](https://github.com/jbrukh/gpt-jargon) | 161 |\\n|[hardbyte/qabot](https://github.com/hardbyte/qabot) | 160 |\\n|[shaman-ai/agent-actors](https://github.com/shaman-ai/agent-actors) | 153 |\\n|[radi-cho/datasetGPT](https://github.com/radi-cho/datasetGPT) | 153 |\\n|[poe-platform/poe-protocol](https://github.com/poe-platform/poe-protocol) | 152 |\\n|[paolorechia/learn-langchain](https://github.com/paolorechia/learn-langchain) | 149 |\\n|[ajndkr/lanarky](https://github.com/ajndkr/lanarky) | 149 |\\n|[fengyuli-dev/multimedia-gpt](https://github.com/fengyuli-dev/multimedia-gpt) | 147 |\\n|[yasyf/compress-gpt](https://github.com/yasyf/compress-gpt) | 144 |\\n|[homanp/superagent](https://github.com/homanp/superagent) | 143 |\\n|[realminchoi/babyagi-ui](https://github.com/realminchoi/babyagi-ui) | 141 |\\n|[ethanyanjiali/minChatGPT](https://github.com/ethanyanjiali/minChatGPT) | 141 |\\n|[ccurme/yolopandas](https://github.com/ccurme/yolopandas) | 139 |\\n|[hwchase17/langchain-streamlit-template](https://github.com/hwchase17/langchain-streamlit-template) | 138 |\\n|[Jaseci-Labs/jaseci](https://github.com/Jaseci-Labs/jaseci) | 136 |\\n|[hirokidaichi/wanna](https://github.com/hirokidaichi/wanna) | 135 |\\n|[Haste171/langchain-chatbot](https://github.com/Haste171/langchain-chatbot) | 134 |\\n|[jmpaz/promptlib](https://github.com/jmpaz/promptlib) | 130 |\\n|[Klingefjord/chatgpt-telegram](https://github.com/Klingefjord/chatgpt-telegram) | 130 |\\n|[filip-michalsky/SalesGPT](https://github.com/filip-michalsky/SalesGPT) | 128 |\\n|[handrew/browserpilot](https://github.com/handrew/browserpilot) | 128 |\\n|[shauryr/S2QA](https://github.com/shauryr/S2QA) | 127 |\\n|[steamship-core/vercel-examples](https://github.com/steamship-core/vercel-examples) | 127 |\\n|[yasyf/summ](https://github.com/yasyf/summ) | 127 |\\n|[gia-guar/JARVIS-ChatGPT](https://github.com/gia-guar/JARVIS-ChatGPT) | 126 |\\n|[jerlendds/osintbuddy](https://github.com/jerlendds/osintbuddy) | 125 |\\n|[ibiscp/LLM-IMDB](https://github.com/ibiscp/LLM-IMDB) | 124 |\\n|[Teahouse-Studios/akari-bot](https://github.com/Teahouse-Studios/akari-bot) | 124 |\\n|[hwchase17/chroma-langchain](https://github.com/hwchase17/chroma-langchain) | 124 |\\n|[menloparklab/langchain-cohere-qdrant-doc-retrieval](https://github.com/menloparklab/langchain-cohere-qdrant-doc-retrieval) | 123 |\\n|[peterw/StoryStorm](https://github.com/peterw/StoryStorm) | 123 |\\n|[chakkaradeep/pyCodeAGI](https://github.com/chakkaradeep/pyCodeAGI) | 123 |\\n|[petehunt/langchain-github-bot](https://github.com/petehunt/langchain-github-bot) | 115 |\\n|[su77ungr/CASALIOY](https://github.com/su77ungr/CASALIOY) | 113 |\\n|[eunomia-bpf/GPTtrace](https://github.com/eunomia-bpf/GPTtrace) | 113 |\\n|[zenml-io/zenml-projects](https://github.com/zenml-io/zenml-projects) | 112 |\\n|[pablomarin/GPT-Azure-Search-Engine](https://github.com/pablomarin/GPT-Azure-Search-Engine) | 111 |\\n|[shamspias/customizable-gpt-chatbot](https://github.com/shamspias/customizable-gpt-chatbot) | 109 |\\n|[WongSaang/chatgpt-ui-server](https://github.com/WongSaang/chatgpt-ui-server) | 108 |\\n|[davila7/file-gpt](https://github.com/davila7/file-gpt) | 104 |\\n|[enhancedocs/enhancedocs](https://github.com/enhancedocs/enhancedocs) | 102 |\\n|[aurelio-labs/arxiv-bot](https://github.com/aurelio-labs/arxiv-bot) | 101 |  \\n_Generated by [github-dependents-info](https://github.com/nvuillam/github-dependents-info)_  \\n[github-dependents-info --repo hwchase17/langchain --markdownfile dependents.md --minstars 100 --sort stars]', metadata={'Header 1': 'Dependents'}),\n",
       " Document(page_content='The output of the format method is available as string, list of messages and `ChatPromptValue`  \\nAs string:  \\n```python\\noutput = chat_prompt.format(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\\noutput\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'System: You are a helpful assistant that translates English to French.\\\\nHuman: I love programming.\\'\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'Format template output'}),\n",
       " Document(page_content='output_2 = chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_string()  \\nassert output == output_2\\n```  \\nAs `ChatPromptValue`  \\n```python\\nchat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nChatPromptValue(messages=[SystemMessage(content=\\'You are a helpful assistant that translates English to French.\\', additional_kwargs={}), HumanMessage(content=\\'I love programming.\\', additional_kwargs={})])\\n```  \\n</CodeOutputBlock>  \\nAs list of Message objects  \\n```python\\nchat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[SystemMessage(content=\\'You are a helpful assistant that translates English to French.\\', additional_kwargs={}),\\nHumanMessage(content=\\'I love programming.\\', additional_kwargs={})]\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'or alternatively'}),\n",
       " Document(page_content='This page covers how to use the modelscope ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific modelscope wrappers.', metadata={'Header 1': 'ModelScope'}),\n",
       " Document(page_content='* Install the Python SDK with `pip install modelscope`', metadata={'Header 1': 'ModelScope', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a modelscope Embeddings wrapper, which you can access with  \\n```python\\nfrom langchain.embeddings import ModelScopeEmbeddings\\n```  \\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/text_embedding/integrations/modelscope_hub.html)', metadata={'Header 1': 'ModelScope', 'Header 2': 'Wrappers', 'Header 3': 'Embeddings'}),\n",
       " Document(page_content='This page covers how to use [llama.cpp](https://github.com/ggerganov/llama.cpp) within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Llama-cpp wrappers.', metadata={'Header 1': 'Llama.cpp'}),\n",
       " Document(page_content='- Install the Python package with `pip install llama-cpp-python`\\n- Download one of the [supported models](https://github.com/ggerganov/llama.cpp#description) and convert them to the llama.cpp format per the [instructions](https://github.com/ggerganov/llama.cpp)', metadata={'Header 1': 'Llama.cpp', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a LlamaCpp LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import LlamaCpp\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/model_io/models/llms/integrations/llamacpp.html)', metadata={'Header 1': 'Llama.cpp', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='There exists a LlamaCpp Embeddings wrapper, which you can access with\\n```python\\nfrom langchain.embeddings import LlamaCppEmbeddings\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/text_embedding/integrations/llamacpp.html)', metadata={'Header 1': 'Llama.cpp', 'Header 2': 'Wrappers', 'Header 3': 'Embeddings'}),\n",
       " Document(page_content=\">[Momento Cache](https://docs.momentohq.com/) is the world's first truly serverless caching service. It provides instant elasticity, scale-to-zero\\n> capability, and blazing-fast performance.\\n> With Momento Cache, you grab the SDK, you get an end point, input a few lines into your code, and you're off and running.  \\nThis page covers how to use the [Momento](https://gomomento.com) ecosystem within LangChain.\", metadata={'Header 1': 'Momento'}),\n",
       " Document(page_content='- Sign up for a free account [here](https://docs.momentohq.com/getting-started) and get an auth token\\n- Install the Momento Python SDK with `pip install momento`', metadata={'Header 1': 'Momento', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='The Cache wrapper allows for [Momento](https://gomomento.com) to be used as a serverless, distributed, low-latency cache for LLM prompts and responses.  \\nThe standard cache is the go-to use case for [Momento](https://gomomento.com) users in any environment.  \\nImport the cache as follows:  \\n```python\\nfrom langchain.cache import MomentoCache\\n```  \\nAnd set up like so:  \\n```python\\nfrom datetime import timedelta\\nfrom momento import CacheClient, Configurations, CredentialProvider\\nimport langchain', metadata={'Header 1': 'Momento', 'Header 2': 'Cache'}),\n",
       " Document(page_content='cache_client = CacheClient(\\nConfigurations.Laptop.v1(),\\nCredentialProvider.from_environment_variable(\"MOMENTO_AUTH_TOKEN\"),\\ndefault_ttl=timedelta(days=1))', metadata={'Header 1': 'Instantiate the Momento client'}),\n",
       " Document(page_content='cache_name = \"langchain\"', metadata={'Header 1': 'Choose a Momento cache name of your choice'}),\n",
       " Document(page_content='langchain.llm_cache = MomentoCache(cache_client, cache_name)\\n```', metadata={'Header 1': 'Instantiate the LLM cache'}),\n",
       " Document(page_content='Momento can be used as a distributed memory store for LLMs.', metadata={'Header 1': 'Instantiate the LLM cache', 'Header 2': 'Memory'}),\n",
       " Document(page_content='See [this notebook](/docs/modules/memory/integrations/momento_chat_message_history.html) for a walkthrough of how to use Momento as a memory store for chat message history.', metadata={'Header 1': 'Instantiate the LLM cache', 'Header 2': 'Memory', 'Header 3': 'Chat Message History Memory'}),\n",
       " Document(page_content='>[Microsoft PowerPoint](https://en.wikipedia.org/wiki/Microsoft_PowerPoint) is a presentation program by Microsoft.', metadata={'Header 1': 'Microsoft PowerPoint'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'Microsoft PowerPoint', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/microsoft_powerpoint.html).  \\n```python\\nfrom langchain.document_loaders import UnstructuredPowerPointLoader\\n```', metadata={'Header 1': 'Microsoft PowerPoint', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the Google Search API within LangChain.\\nIt is broken into two parts: installation and setup, and then references to the specific Google Search wrapper.', metadata={'Header 1': 'Google Search'}),\n",
       " Document(page_content='- Install requirements with `pip install google-api-python-client`\\n- Set up a Custom Search Engine, following [these instructions](https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search)\\n- Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables `GOOGLE_API_KEY` and `GOOGLE_CSE_ID` respectively', metadata={'Header 1': 'Google Search', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a GoogleSearchAPIWrapper utility which wraps this API. To import this utility:  \\n```python\\nfrom langchain.utilities import GoogleSearchAPIWrapper\\n```  \\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/modules/agents/tools/integrations/google_search.html).', metadata={'Header 1': 'Google Search', 'Header 2': 'Wrappers', 'Header 3': 'Utility'}),\n",
       " Document(page_content='You can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"google-search\"])\\n```  \\nFor more information on tools, see [this page](/docs/modules/agents/tools/).', metadata={'Header 1': 'Google Search', 'Header 2': 'Wrappers', 'Header 3': 'Tool'}),\n",
       " Document(page_content='>[StarRocks](https://www.starrocks.io/) is a High-Performance Analytical Database.\\n`StarRocks` is a next-gen sub-second MPP database for full analytics scenarios, including multi-dimensional analytics, real-time analytics and ad-hoc query.  \\n>Usually `StarRocks` is categorized into OLAP, and it has showed excellent performance in [ClickBench — a Benchmark For Analytical DBMS](https://benchmark.clickhouse.com/). Since it has a super-fast vectorized execution engine, it could also be used as a fast vectordb.', metadata={'Header 1': 'StarRocks'}),\n",
       " Document(page_content='```bash\\npip install pymysql\\n```', metadata={'Header 1': 'StarRocks', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/vectorstores/integrations/starrocks.html).  \\n```python\\nfrom langchain.vectorstores import StarRocks\\n```', metadata={'Header 1': 'StarRocks', 'Header 2': 'Vector Store'}),\n",
       " Document(page_content='This page covers how to use the Modal ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Modal wrappers.', metadata={'Header 1': 'Modal'}),\n",
       " Document(page_content='- Install with `pip install modal-client`\\n- Run `modal token new`', metadata={'Header 1': 'Modal', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='You must include a prompt. There is a rigid response structure.  \\n```python\\nclass Item(BaseModel):\\nprompt: str  \\n@stub.webhook(method=\"POST\")\\ndef my_webhook(item: Item):\\nreturn {\"prompt\": my_function.call(item.prompt)}\\n```  \\nAn example with GPT2:  \\n```python\\nfrom pydantic import BaseModel  \\nimport modal  \\nstub = modal.Stub(\"example-get-started\")  \\nvolume = modal.SharedVolume().persist(\"gpt2_model_vol\")\\nCACHE_PATH = \"/root/model_cache\"  \\n@stub.function(\\ngpu=\"any\",\\nimage=modal.Image.debian_slim().pip_install(\\n\"tokenizers\", \"transformers\", \"torch\", \"accelerate\"\\n),\\nshared_volumes={CACHE_PATH: volume},\\nretries=3,\\n)\\ndef run_gpt2(text: str):\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\\ntokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2\\')\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2\\')\\nencoded_input = tokenizer(text, return_tensors=\\'pt\\').input_ids\\noutput = model.generate(encoded_input, max_length=50, do_sample=True)\\nreturn tokenizer.decode(output[0], skip_special_tokens=True)  \\nclass Item(BaseModel):\\nprompt: str  \\n@stub.webhook(method=\"POST\")\\ndef get_text(item: Item):\\nreturn {\"prompt\": run_gpt2.call(item.prompt)}\\n```', metadata={'Header 1': 'Modal', 'Header 2': 'Define your Modal Functions and Webhooks'}),\n",
       " Document(page_content='There exists an Modal LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import Modal\\n```', metadata={'Header 1': 'Modal', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='>[Hologres](https://www.alibabacloud.com/help/en/hologres/latest/introduction) is a unified real-time data warehousing service developed by Alibaba Cloud. You can use Hologres to write, update, process, and analyze large amounts of data in real time.\\n>`Hologres` supports standard `SQL` syntax, is compatible with `PostgreSQL`, and supports most PostgreSQL functions. Hologres supports online analytical processing (OLAP) and ad hoc analysis for up to petabytes of data, and provides high-concurrency and low-latency online data services.  \\n>`Hologres` provides **vector database** functionality by adopting [Proxima](https://www.alibabacloud.com/help/en/hologres/latest/vector-processing).\\n>`Proxima` is a high-performance software library developed by `Alibaba DAMO Academy`. It allows you to search for the nearest neighbors of vectors. Proxima provides higher stability and performance than similar open source software such as Faiss. Proxima allows you to search for similar text or image embeddings with high throughput and low latency. Hologres is deeply integrated with Proxima to provide a high-performance vector search service.', metadata={'Header 1': 'Hologres'}),\n",
       " Document(page_content='Click [here](https://www.alibabacloud.com/zh/product/hologres) to fast deploy a Hologres cloud instance.  \\n```bash\\npip install psycopg2\\n```', metadata={'Header 1': 'Hologres', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/vectorstores/integrations/hologres.html).  \\n```python\\nfrom langchain.vectorstores import Hologres\\n```', metadata={'Header 1': 'Hologres', 'Header 2': 'Vector Store'}),\n",
       " Document(page_content='>[SingleStoreDB](https://singlestore.com/) is a high-performance distributed SQL database that supports deployment both in the [cloud](https://www.singlestore.com/cloud/) and on-premises. It provides vector storage, and vector functions including [dot_product](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/dot_product.html) and [euclidean_distance](https://docs.singlestore.com/managed-service/en/reference/sql-reference/vector-functions/euclidean_distance.html), thereby supporting AI applications that require text similarity matching.', metadata={'Header 1': 'SingleStoreDB'}),\n",
       " Document(page_content='There are several ways to establish a [connection](https://singlestoredb-python.labs.singlestore.com/generated/singlestoredb.connect.html) to the database. You can either set up environment variables or pass named parameters to the `SingleStoreDB constructor`.\\nAlternatively, you may provide these parameters to the `from_documents` and `from_texts` methods.  \\n```bash\\npip install singlestoredb\\n```', metadata={'Header 1': 'SingleStoreDB', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/vectorstores/integrations/singlestoredb.html).  \\n```python\\nfrom langchain.vectorstores import SingleStoreDB\\n```', metadata={'Header 1': 'SingleStoreDB', 'Header 2': 'Vector Store'}),\n",
       " Document(page_content='>[Brave Search](https://en.wikipedia.org/wiki/Brave_Search) is a search engine developed by Brave Software.\\n> - `Brave Search` uses its own web index. As of May 2022, it covered over 10 billion pages and was used to serve 92%\\n> of search results without relying on any third-parties, with the remainder being retrieved\\n> server-side from the Bing API or (on an opt-in basis) client-side from Google. According\\n> to Brave, the index was kept \"intentionally smaller than that of Google or Bing\" in order to\\n> help avoid spam and other low-quality content, with the disadvantage that \"Brave Search is\\n> not yet as good as Google in recovering long-tail queries.\"\\n>- `Brave Search Premium`: As of April 2023 Brave Search is an ad-free website, but it will\\n> eventually switch to a new model that will include ads and premium users will get an ad-free experience.\\n> User data including IP addresses won\\'t be collected from its users by default. A premium account\\n> will be required for opt-in data-collection.', metadata={'Header 1': 'Brave Search'}),\n",
       " Document(page_content='To get access to the Brave Search API, you need to [create an account and get an API key](https://api.search.brave.com/app/dashboard).', metadata={'Header 1': 'Brave Search', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/brave_search.html).  \\n```python\\nfrom langchain.document_loaders import BraveSearchLoader\\n```', metadata={'Header 1': 'Brave Search', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/agents/tools/integrations/brave_search.html).  \\n```python\\nfrom langchain.tools import BraveSearch\\n```', metadata={'Header 1': 'Brave Search', 'Header 2': 'Tool'}),\n",
       " Document(page_content='>[Figma](https://www.figma.com/) is a collaborative web application for interface design.', metadata={'Header 1': 'Figma'}),\n",
       " Document(page_content=\"The Figma API requires an `access token`, `node_ids`, and a `file key`.  \\nThe `file key` can be pulled from the URL.  https://www.figma.com/file/{filekey}/sampleFilename  \\n`Node IDs` are also available in the URL. Click on anything and look for the '?node-id={node_id}' param.  \\n`Access token` [instructions](https://help.figma.com/hc/en-us/articles/8085703771159-Manage-personal-access-tokens).\", metadata={'Header 1': 'Figma', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/figma.html).  \\n```python\\nfrom langchain.document_loaders import FigmaFileLoader\\n```', metadata={'Header 1': 'Figma', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the PipelineAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific PipelineAI wrappers.', metadata={'Header 1': 'PipelineAI'}),\n",
       " Document(page_content='- Install with `pip install pipeline-ai`\\n- Get a Pipeline Cloud api key and set it as an environment variable (`PIPELINE_API_KEY`)', metadata={'Header 1': 'PipelineAI', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a PipelineAI LLM wrapper, which you can access with  \\n```python\\nfrom langchain.llms import PipelineAI\\n```', metadata={'Header 1': 'PipelineAI', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='This page covers how to use the Hugging Face ecosystem (including the [Hugging Face Hub](https://huggingface.co)) within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Hugging Face wrappers.', metadata={'Header 1': 'Hugging Face'}),\n",
       " Document(page_content=\"If you want to work with the Hugging Face Hub:\\n- Install the Hub client library with `pip install huggingface_hub`\\n- Create a Hugging Face account (it's free!)\\n- Create an [access token](https://huggingface.co/docs/hub/security-tokens) and set it as an environment variable (`HUGGINGFACEHUB_API_TOKEN`)  \\nIf you want work with the Hugging Face Python libraries:\\n- Install `pip install transformers` for working with models and tokenizers\\n- Install `pip install datasets` for working with datasets\", metadata={'Header 1': 'Hugging Face', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub.\\nNote that these wrappers only work for models that support the following tasks: [`text2text-generation`](https://huggingface.co/models?library=transformers&pipeline_tag=text2text-generation&sort=downloads), [`text-generation`](https://huggingface.co/models?library=transformers&pipeline_tag=text-classification&sort=downloads)  \\nTo use the local pipeline wrapper:\\n```python\\nfrom langchain.llms import HuggingFacePipeline\\n```  \\nTo use a the wrapper for a model hosted on Hugging Face Hub:\\n```python\\nfrom langchain.llms import HuggingFaceHub\\n```\\nFor a more detailed walkthrough of the Hugging Face Hub wrapper, see [this notebook](/docs/modules/model_io/models/llms/integrations/huggingface_hub.html)', metadata={'Header 1': 'Hugging Face', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='There exists two Hugging Face Embeddings wrappers, one for a local model and one for a model hosted on Hugging Face Hub.\\nNote that these wrappers only work for [`sentence-transformers` models](https://huggingface.co/models?library=sentence-transformers&sort=downloads).  \\nTo use the local pipeline wrapper:\\n```python\\nfrom langchain.embeddings import HuggingFaceEmbeddings\\n```  \\nTo use a the wrapper for a model hosted on Hugging Face Hub:\\n```python\\nfrom langchain.embeddings import HuggingFaceHubEmbeddings\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/text_embedding/integrations/huggingfacehub.html)', metadata={'Header 1': 'Hugging Face', 'Header 2': 'Wrappers', 'Header 3': 'Embeddings'}),\n",
       " Document(page_content='There are several places you can use tokenizers available through the `transformers` package.\\nBy default, it is used to count tokens for all LLMs.  \\nYou can also use it to count tokens when splitting documents with\\n```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\nCharacterTextSplitter.from_huggingface_tokenizer(...)\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/document_transformers/text_splitters/huggingface_length_function.html)', metadata={'Header 1': 'Hugging Face', 'Header 2': 'Wrappers', 'Header 3': 'Tokenizer'}),\n",
       " Document(page_content='The Hugging Face Hub has lots of great [datasets](https://huggingface.co/datasets) that can be used to evaluate your LLM chains.  \\nFor a detailed walkthrough of how to use them to do so, see [this notebook](/docs/use_cases/evaluation/huggingface_datasets.html)', metadata={'Header 1': 'Hugging Face', 'Header 2': 'Wrappers', 'Header 3': 'Datasets'}),\n",
       " Document(page_content='>[Modern Treasury](https://www.moderntreasury.com/) simplifies complex payment operations. It is a unified platform to power products and processes that move money.\\n>- Connect to banks and payment systems\\n>- Track transactions and balances in real-time\\n>- Automate payment operations for scale', metadata={'Header 1': 'Modern Treasury'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'Modern Treasury', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/modern_treasury.html).  \\n```python\\nfrom langchain.document_loaders import ModernTreasuryLoader\\n```', metadata={'Header 1': 'Modern Treasury', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the Tair ecosystem within LangChain.', metadata={'Header 1': 'Tair'}),\n",
       " Document(page_content='Install Tair Python SDK with `pip install tair`.', metadata={'Header 1': 'Tair', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around TairVector, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\nTo import this vectorstore:  \\n```python\\nfrom langchain.vectorstores import Tair\\n```  \\nFor a more detailed walkthrough of the Tair wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/tair.html)', metadata={'Header 1': 'Tair', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='>[WhatsApp](https://www.whatsapp.com/) (also called `WhatsApp Messenger`) is a freeware, cross-platform, centralized instant messaging (IM) and voice-over-IP (VoIP) service. It allows users to send text and voice messages, make voice and video calls, and share images, documents, user locations, and other content.', metadata={'Header 1': 'WhatsApp'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'WhatsApp', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/whatsapp_chat.html).  \\n```python\\nfrom langchain.document_loaders import WhatsAppChatLoader\\n```', metadata={'Header 1': 'WhatsApp', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page demonstrates how to use [OpenLLM](https://github.com/bentoml/OpenLLM)\\nwith LangChain.  \\n`OpenLLM` is an open platform for operating large language models (LLMs) in\\nproduction. It enables developers to easily run inference with any open-source\\nLLMs, deploy to the cloud or on-premises, and build powerful AI apps.', metadata={'Header 1': 'OpenLLM'}),\n",
       " Document(page_content='Install the OpenLLM package via PyPI:  \\n```bash\\npip install openllm\\n```', metadata={'Header 1': 'OpenLLM', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content=\"OpenLLM supports a wide range of open-source LLMs as well as serving users' own\\nfine-tuned LLMs. Use `openllm model` command to see all available models that\\nare pre-optimized for OpenLLM.\", metadata={'Header 1': 'OpenLLM', 'Header 2': 'LLM'}),\n",
       " Document(page_content='There is a OpenLLM Wrapper which supports loading LLM in-process or accessing a\\nremote OpenLLM server:  \\n```python\\nfrom langchain.llms import OpenLLM\\n```', metadata={'Header 1': 'OpenLLM', 'Header 2': 'Wrappers'}),\n",
       " Document(page_content='This wrapper supports connecting to an OpenLLM server via HTTP or gRPC. The\\nOpenLLM server can run either locally or on the cloud.  \\nTo try it out locally, start an OpenLLM server:  \\n```bash\\nopenllm start flan-t5\\n```  \\nWrapper usage:  \\n```python\\nfrom langchain.llms import OpenLLM  \\nllm = OpenLLM(server_url=\\'http://localhost:3000\\')  \\nllm(\"What is the difference between a duck and a goose? And why there are so many Goose in Canada?\")\\n```', metadata={'Header 1': 'OpenLLM', 'Header 2': 'Wrappers', 'Header 3': 'Wrapper for OpenLLM server'}),\n",
       " Document(page_content='You can also use the OpenLLM wrapper to load LLM in current Python process for\\nrunning inference.  \\n```python\\nfrom langchain.llms import OpenLLM  \\nllm = OpenLLM(model_name=\"dolly-v2\", model_id=\\'databricks/dolly-v2-7b\\')  \\nllm(\"What is the difference between a duck and a goose? And why there are so many Goose in Canada?\")\\n```', metadata={'Header 1': 'OpenLLM', 'Header 2': 'Wrappers', 'Header 3': 'Wrapper for Local Inference'}),\n",
       " Document(page_content='For a more detailed walkthrough of the OpenLLM Wrapper, see the\\n[example notebook](/docs/modules/model_io/models/llms/integrations/openllm.html)', metadata={'Header 1': 'OpenLLM', 'Header 2': 'Wrappers', 'Header 3': 'Usage'}),\n",
       " Document(page_content='This page covers how to use the Pinecone ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Pinecone wrappers.', metadata={'Header 1': 'Pinecone'}),\n",
       " Document(page_content='Install the Python SDK:\\n```bash\\npip install pinecone-client\\n```', metadata={'Header 1': 'Pinecone', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around Pinecone indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\n```python\\nfrom langchain.vectorstores import Pinecone\\n```  \\nFor a more detailed walkthrough of the Pinecone vectorstore, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/pinecone.html)', metadata={'Header 1': 'Pinecone', 'Header 2': 'Vectorstore'}),\n",
       " Document(page_content='>[MediaWiki XML Dumps](https://www.mediawiki.org/wiki/Manual:Importing_XML_dumps) contain the content of a wiki\\n> (wiki pages with all their revisions), without the site-related data. A XML dump does not create a full backup\\n> of the wiki database, the dump does not contain user accounts, images, edit logs, etc.', metadata={'Header 1': 'MediaWikiDump'}),\n",
       " Document(page_content='We need to install several python packages.  \\nThe `mediawiki-utilities` supports XML schema 0.11 in unmerged branches.\\n```bash\\npip install -qU git+https://github.com/mediawiki-utilities/python-mwtypes@updates_schema_0.11\\n```  \\nThe `mediawiki-utilities mwxml` has a bug, fix PR pending.  \\n```bash\\npip install -qU git+https://github.com/gdedrouas/python-mwxml@xml_format_0.11\\npip install -qU mwparserfromhell\\n```', metadata={'Header 1': 'MediaWikiDump', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/mediawikidump.html).  \\n```python\\nfrom langchain.document_loaders import MWDumpLoader\\n```', metadata={'Header 1': 'MediaWikiDump', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use Beam within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Beam wrappers.', metadata={'Header 1': 'Beam'}),\n",
       " Document(page_content='- [Create an account](https://www.beam.cloud/)\\n- Install the Beam CLI with `curl https://raw.githubusercontent.com/slai-labs/get-beam/main/get-beam.sh -sSfL | sh`\\n- Register API keys with `beam configure`\\n- Set environment variables (`BEAM_CLIENT_ID`) and (`BEAM_CLIENT_SECRET`)\\n- Install the Beam SDK `pip install beam-sdk`', metadata={'Header 1': 'Beam', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a Beam LLM wrapper, which you can access with  \\n```python\\nfrom langchain.llms.beam import Beam\\n```', metadata={'Header 1': 'Beam', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='This is the environment you’ll be developing against once you start the app.\\nIt\\'s also used to define the maximum response length from the model.\\n```python\\nllm = Beam(model_name=\"gpt2\",\\nname=\"langchain-gpt2-test\",\\ncpu=8,\\nmemory=\"32Gi\",\\ngpu=\"A10G\",\\npython_version=\"python3.8\",\\npython_packages=[\\n\"diffusers[torch]>=0.10\",\\n\"transformers\",\\n\"torch\",\\n\"pillow\",\\n\"accelerate\",\\n\"safetensors\",\\n\"xformers\",],\\nmax_length=\"50\",\\nverbose=False)\\n```', metadata={'Header 1': 'Beam', 'Header 2': 'Define your Beam app.'}),\n",
       " Document(page_content=\"Once defined, you can deploy your Beam app by calling your model's `_deploy()` method.  \\n```python\\nllm._deploy()\\n```\", metadata={'Header 1': 'Beam', 'Header 2': 'Deploy your Beam app'}),\n",
       " Document(page_content='Once a beam model is deployed, it can be called by callying your model\\'s `_call()` method.\\nThis returns the GPT2 text response to your prompt.  \\n```python\\nresponse = llm._call(\"Running machine learning on a remote GPU\")\\n```  \\nAn example script which deploys the model and calls it would be:  \\n```python\\nfrom langchain.llms.beam import Beam\\nimport time  \\nllm = Beam(model_name=\"gpt2\",\\nname=\"langchain-gpt2-test\",\\ncpu=8,\\nmemory=\"32Gi\",\\ngpu=\"A10G\",\\npython_version=\"python3.8\",\\npython_packages=[\\n\"diffusers[torch]>=0.10\",\\n\"transformers\",\\n\"torch\",\\n\"pillow\",\\n\"accelerate\",\\n\"safetensors\",\\n\"xformers\",],\\nmax_length=\"50\",\\nverbose=False)  \\nllm._deploy()  \\nresponse = llm._call(\"Running machine learning on a remote GPU\")  \\nprint(response)\\n```', metadata={'Header 1': 'Beam', 'Header 2': 'Call your Beam app'}),\n",
       " Document(page_content='>[Microsoft Azure](https://en.wikipedia.org/wiki/Microsoft_Azure), often referred to as `Azure` is a cloud computing platform run by `Microsoft`, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). `Microsoft Azure` supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.  \\n>[Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/) is an `Azure` service with powerful language models from `OpenAI` including the `GPT-3`, `Codex` and `Embeddings model` series for content generation, summarization, semantic search, and natural language to code translation.', metadata={'Header 1': 'Azure OpenAI'}),\n",
       " Document(page_content='```bash\\npip install openai\\npip install tiktoken\\n```  \\nSet the environment variables to get access to the `Azure OpenAI` service.  \\n```python\\nimport os  \\nos.environ[\"OPENAI_API_TYPE\"] = \"azure\"\\nos.environ[\"OPENAI_API_BASE\"] = \"https://<your-endpoint.openai.azure.com/\"\\nos.environ[\"OPENAI_API_KEY\"] = \"your AzureOpenAI key\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\\n```', metadata={'Header 1': 'Azure OpenAI', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/model_io/models/llms/integrations/azure_openai_example.html).  \\n```python\\nfrom langchain.llms import AzureOpenAI\\n```', metadata={'Header 1': 'Azure OpenAI', 'Header 2': 'LLM'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/text_embedding/integrations/azureopenai.html)  \\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\n```', metadata={'Header 1': 'Azure OpenAI', 'Header 2': 'Text Embedding Models'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/model_io/models/chat/integrations/azure_chat_openai.html)  \\n```python\\nfrom langchain.chat_models import AzureChatOpenAI\\n```', metadata={'Header 1': 'Azure OpenAI', 'Header 2': 'Chat Models'}),\n",
       " Document(page_content='>[Obsidian](https://obsidian.md/) is a powerful and extensible knowledge base\\nthat works on top of your local folder of plain text files.', metadata={'Header 1': 'Obsidian'}),\n",
       " Document(page_content='All instructions are in examples below.', metadata={'Header 1': 'Obsidian', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/obsidian.html).  \\n```python\\nfrom langchain.document_loaders import ObsidianLoader\\n```', metadata={'Header 1': 'Obsidian', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Blackboard Learn](https://en.wikipedia.org/wiki/Blackboard_Learn) (previously the `Blackboard Learning Management System`)\\n> is a web-based virtual learning environment and learning management system developed by Blackboard Inc.\\n> The software features course management, customizable open architecture, and scalable design that allows\\n> integration with student information systems and authentication protocols. It may be installed on local servers,\\n> hosted by `Blackboard ASP Solutions`, or provided as Software as a Service hosted on Amazon Web Services.\\n> Its main purposes are stated to include the addition of online elements to courses traditionally delivered\\n> face-to-face and development of completely online courses with few or no face-to-face meetings.', metadata={'Header 1': 'Blackboard'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'Blackboard', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/blackboard.html).  \\n```python\\nfrom langchain.document_loaders import BlackboardLoader  \\n```', metadata={'Header 1': 'Blackboard', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='> [Typesense](https://typesense.org) is an open source, in-memory search engine, that you can either\\n> [self-host](https://typesense.org/docs/guide/install-typesense.html#option-2-local-machine-self-hosting) or run\\n> on [Typesense Cloud](https://cloud.typesense.org/).\\n> `Typesense` focuses on performance by storing the entire index in RAM (with a backup on disk) and also\\n> focuses on providing an out-of-the-box developer experience by simplifying available options and setting good defaults.', metadata={'Header 1': 'Typesense'}),\n",
       " Document(page_content='```bash\\npip install typesense openapi-schema-pydantic openai tiktoken\\n```', metadata={'Header 1': 'Typesense', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/vectorstores/integrations/typesense.html).  \\n```python\\nfrom langchain.vectorstores import Typesense\\n```', metadata={'Header 1': 'Typesense', 'Header 2': 'Vector Store'}),\n",
       " Document(page_content='>[Zilliz Cloud](https://zilliz.com/doc/quick_start) is a fully managed service on cloud for `LF AI Milvus®`,', metadata={'Header 1': 'Zilliz'}),\n",
       " Document(page_content='Install the Python SDK:\\n```bash\\npip install pymilvus\\n```', metadata={'Header 1': 'Zilliz', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='A wrapper around Zilliz indexes allows you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\n```python\\nfrom langchain.vectorstores import Milvus\\n```  \\nFor a more detailed walkthrough of the Miluvs wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/zilliz.html)', metadata={'Header 1': 'Zilliz', 'Header 2': 'Vectorstore'}),\n",
       " Document(page_content='>[Reddit](www.reddit.com) is an American social news aggregation, content rating, and discussion website.', metadata={'Header 1': 'Reddit'}),\n",
       " Document(page_content='First, you need to install a python package.  \\n```bash\\npip install praw\\n```  \\nMake a [Reddit Application](https://www.reddit.com/prefs/apps/) and initialize the loader with with your Reddit API credentials.', metadata={'Header 1': 'Reddit', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/reddit.html).  \\n```python\\nfrom langchain.document_loaders import RedditPostsLoader\\n```', metadata={'Header 1': 'Reddit', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>The `unstructured` package from\\n[Unstructured.IO](https://www.unstructured.io/) extracts clean text from raw source documents like\\nPDFs and Word documents.\\nThis page covers how to use the [`unstructured`](https://github.com/Unstructured-IO/unstructured)\\necosystem within LangChain.', metadata={'Header 1': 'Unstructured'}),\n",
       " Document(page_content='If you are using a loader that runs locally, use the following steps to get `unstructured` and\\nits dependencies running locally.  \\n- Install the Python SDK with `pip install \"unstructured[local-inference]\"`\\n- Install the following system dependencies if they are not already available on your system.\\nDepending on what document types you\\'re parsing, you may not need all of these.\\n- `libmagic-dev` (filetype detection)\\n- `poppler-utils` (images and PDFs)\\n- `tesseract-ocr`(images and PDFs)\\n- `libreoffice` (MS Office docs)\\n- `pandoc` (EPUBs)  \\nIf you want to get up and running with less set up, you can\\nsimply run `pip install unstructured` and use `UnstructuredAPIFileLoader` or\\n`UnstructuredAPIFileIOLoader`. That will process your document using the hosted Unstructured API.  \\nThe Unstructured API requires API keys to make requests.\\nYou can generate a free API key [here](https://www.unstructured.io/api-key) and start using it today!\\nCheckout the README [here](https://github.com/Unstructured-IO/unstructured-api) here to get started making API calls.\\nWe\\'d love to hear your feedback, let us know how it goes in our [community slack](https://join.slack.com/t/unstructuredw-kbe4326/shared_invite/zt-1x7cgo0pg-PTptXWylzPQF9xZolzCnwQ).\\nAnd stay tuned for improvements to both quality and performance!\\nCheck out the instructions\\n[here](https://github.com/Unstructured-IO/unstructured-api#dizzy-instructions-for-using-the-docker-image) if you\\'d like to self-host the Unstructured API or run it locally.', metadata={'Header 1': 'Unstructured', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='The primary `unstructured` wrappers within `langchain` are data loaders. The following\\nshows how to use the most basic unstructured data loader. There are other file-specific\\ndata loaders available in the `langchain.document_loaders` module.  \\n```python\\nfrom langchain.document_loaders import UnstructuredFileLoader  \\nloader = UnstructuredFileLoader(\"state_of_the_union.txt\")\\nloader.load()\\n```  \\nIf you instantiate the loader with `UnstructuredFileLoader(mode=\"elements\")`, the loader\\nwill track additional metadata like the page number and text type (i.e. title, narrative text)\\nwhen that information is available.', metadata={'Header 1': 'Unstructured', 'Header 2': 'Wrappers', 'Header 3': 'Data Loaders'}),\n",
       " Document(page_content=\">[Apache Cassandra®](https://cassandra.apache.org/) is a free and open-source, distributed, wide-column\\n> store, NoSQL database management system designed to handle large amounts of data across many commodity servers,\\n> providing high availability with no single point of failure. Cassandra offers support for clusters spanning\\n> multiple datacenters, with asynchronous masterless replication allowing low latency operations for all clients.\\n> Cassandra was designed to implement a combination of _Amazon's Dynamo_ distributed storage and replication\\n> techniques combined with _Google's Bigtable_ data and storage engine model.\", metadata={'Header 1': 'Cassandra'}),\n",
       " Document(page_content='```bash\\npip install cassandra-driver\\npip install cassio\\n```', metadata={'Header 1': 'Cassandra', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/vectorstores/integrations/cassandra.html).  \\n```python\\nfrom langchain.memory import CassandraChatMessageHistory\\n```', metadata={'Header 1': 'Cassandra', 'Header 2': 'Vector Store'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/memory/integrations/cassandra_chat_message_history.html).  \\n```python\\nfrom langchain.memory import CassandraChatMessageHistory\\n```', metadata={'Header 1': 'Cassandra', 'Header 2': 'Memory'}),\n",
       " Document(page_content='This page covers how to use the AnalyticDB ecosystem within LangChain.', metadata={'Header 1': 'AnalyticDB'}),\n",
       " Document(page_content='There exists a wrapper around AnalyticDB, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import AnalyticDB\\n```  \\nFor a more detailed walkthrough of the AnalyticDB wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/analyticdb.html)', metadata={'Header 1': 'AnalyticDB', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='>[Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.', metadata={'Header 1': 'Microsoft OneDrive'}),\n",
       " Document(page_content='First, you need to install a python package.  \\n```bash\\npip install o365\\n```  \\nThen follow instructions [here](/docs/modules/data_connection/document_loaders/integrations/microsoft_onedrive.html).', metadata={'Header 1': 'Microsoft OneDrive', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/microsoft_onedrive.html).  \\n```python\\nfrom langchain.document_loaders import OneDriveLoader\\n```', metadata={'Header 1': 'Microsoft OneDrive', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Wikipedia](https://wikipedia.org/) is a multilingual free online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and using a wiki-based editing system called MediaWiki. `Wikipedia` is the largest and most-read reference work in history.', metadata={'Header 1': 'Wikipedia'}),\n",
       " Document(page_content='```bash\\npip install wikipedia\\n```', metadata={'Header 1': 'Wikipedia', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/wikipedia.html).  \\n```python\\nfrom langchain.document_loaders import WikipediaLoader\\n```', metadata={'Header 1': 'Wikipedia', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/retrievers/integrations/wikipedia.html).  \\n```python\\nfrom langchain.retrievers import WikipediaRetriever\\n```', metadata={'Header 1': 'Wikipedia', 'Header 2': 'Retriever'}),\n",
       " Document(page_content='>[Trello](https://www.atlassian.com/software/trello) is a web-based project management and collaboration tool that allows individuals and teams to organize and track their tasks and projects. It provides a visual interface known as a \"board\" where users can create lists and cards to represent their tasks and activities.\\n>The TrelloLoader allows us to load cards from a `Trello` board.', metadata={'Header 1': 'Trello'}),\n",
       " Document(page_content='```bash\\npip install py-trello beautifulsoup4\\n```  \\nSee [setup instructions](/docs/modules/data_connection/document_loaders/integrations/trello.html).', metadata={'Header 1': 'Trello', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/trello.html).  \\n```python\\nfrom langchain.document_loaders import TrelloLoader\\n```', metadata={'Header 1': 'Trello', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Vespa](https://vespa.ai/) is a fully featured search engine and vector database.\\n> It supports vector search (ANN), lexical search, and search in structured data, all in the same query.', metadata={'Header 1': 'Vespa'}),\n",
       " Document(page_content='```bash\\npip install pyvespa\\n```', metadata={'Header 1': 'Vespa', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/retrievers/integrations/vespa.html).  \\n```python\\nfrom langchain.retrievers import VespaRetriever\\n```', metadata={'Header 1': 'Vespa', 'Header 2': 'Retriever'}),\n",
       " Document(page_content='>[Zep](https://docs.getzep.com/) - A long-term memory store for LLM applications.  \\n>`Zep` stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, and exposes them via simple, low-latency APIs.\\n>- Long-term memory persistence, with access to historical messages irrespective of your summarization strategy.\\n>- Auto-summarization of memory messages based on a configurable message window. A series of summaries are stored, providing flexibility for future summarization strategies.\\n>- Vector search over memories, with messages automatically embedded on creation.\\n>- Auto-token counting of memories and summaries, allowing finer-grained control over prompt assembly.\\n>- Python and JavaScript SDKs.  \\n`Zep` [project](https://github.com/getzep/zep)', metadata={'Header 1': 'Zep'}),\n",
       " Document(page_content='```bash\\npip install zep_python\\n```', metadata={'Header 1': 'Zep', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/retrievers/integrations/zep_memorystore.html).  \\n```python\\nfrom langchain.retrievers import ZepRetriever\\n```', metadata={'Header 1': 'Zep', 'Header 2': 'Retriever'}),\n",
       " Document(page_content=\"This page covers how to use Nomic's Atlas ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Atlas wrappers.\", metadata={'Header 1': 'AtlasDB'}),\n",
       " Document(page_content='- Install the Python package with `pip install nomic`\\n- Nomic is also included in langchains poetry extras `poetry install -E all`', metadata={'Header 1': 'AtlasDB', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around the Atlas neural database, allowing you to use it as a vectorstore.\\nThis vectorstore also gives you full access to the underlying AtlasProject object, which will allow you to use the full range of Atlas map interactions, such as bulk tagging and automatic topic modeling.\\nPlease see [the Atlas docs](https://docs.nomic.ai/atlas_api.html) for more detailed information.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import AtlasDB\\n```  \\nFor a more detailed walkthrough of the AtlasDB wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/atlas.html)', metadata={'Header 1': 'AtlasDB', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='>[Hacker News](https://en.wikipedia.org/wiki/Hacker_News) (sometimes abbreviated as `HN`) is a social news\\n> website focusing on computer science and entrepreneurship. It is run by the investment fund and startup\\n> incubator `Y Combinator`. In general, content that can be submitted is defined as \"anything that gratifies\\n> one\\'s intellectual curiosity.\"', metadata={'Header 1': 'Hacker News'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'Hacker News', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/hacker_news.html).  \\n```python\\nfrom langchain.document_loaders import HNLoader\\n```', metadata={'Header 1': 'Hacker News', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Rockset](https://rockset.com/product/) is a real-time analytics database service for serving low latency, high concurrency analytical queries at scale. It builds a Converged Index™ on structured and semi-structured data with an efficient store for vector embeddings. Its support for running SQL on schemaless data makes it a perfect choice for running vector search with metadata filters.', metadata={'Header 1': 'Rockset'}),\n",
       " Document(page_content='Make sure you have Rockset account and go to the web console to get the API key. Details can be found on [the website](https://rockset.com/docs/rest-api/).  \\n```bash\\npip install rockset\\n```', metadata={'Header 1': 'Rockset', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/vectorstores/integrations/rockset.html).  \\n```python\\nfrom langchain.vectorstores import RocksetDB\\n```', metadata={'Header 1': 'Rockset', 'Header 2': 'Vector Store'}),\n",
       " Document(page_content='>[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows.  \\nWe use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.', metadata={'Header 1': 'SageMaker Endpoint'}),\n",
       " Document(page_content='```bash\\npip install boto3\\n```  \\nFor instructions on how to expose model as a `SageMaker Endpoint`, please see [here](https://www.philschmid.de/custom-inference-huggingface-sagemaker).  \\n**Note**: In order to handle batched requests, we need to adjust the return line in the `predict_fn()` function within the custom `inference.py` script:  \\nChange from  \\n```\\nreturn {\"vectors\": sentence_embeddings[0].tolist()}\\n```  \\nto:  \\n```\\nreturn {\"vectors\": sentence_embeddings.tolist()}\\n```  \\nWe have to set up following required parameters of the `SagemakerEndpoint` call:\\n- `endpoint_name`: The name of the endpoint from the deployed Sagemaker model.\\nMust be unique within an AWS Region.\\n- `credentials_profile_name`: The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which\\nhas either access keys or role information specified.\\nIf not specified, the default credential profile or, if on an EC2 instance,\\ncredentials from IMDS will be used.\\nSee [this guide](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html).', metadata={'Header 1': 'SageMaker Endpoint', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/model_io/models/llms/integrations/sagemaker.html).  \\n```python\\nfrom langchain import SagemakerEndpoint\\nfrom langchain.llms.sagemaker_endpoint import LLMContentHandler\\n```', metadata={'Header 1': 'SageMaker Endpoint', 'Header 2': 'LLM'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/text_embedding/integrations/sagemaker-endpoint.html).\\n```python\\nfrom langchain.embeddings import SagemakerEndpointEmbeddings\\nfrom langchain.llms.sagemaker_endpoint import ContentHandlerBase\\n```', metadata={'Header 1': 'SageMaker Endpoint', 'Header 2': 'Text Embedding Models'}),\n",
       " Document(page_content='>[YouTube](https://www.youtube.com/) is an online video sharing and social media platform created by Google.\\n> We download the `YouTube` transcripts and video information.', metadata={'Header 1': 'YouTube'}),\n",
       " Document(page_content='```bash\\npip install youtube-transcript-api\\npip install pytube\\n```\\nSee a [usage example](/docs/modules/data_connection/document_loaders/integrations/youtube_transcript.html).', metadata={'Header 1': 'YouTube', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/youtube_transcript.html).  \\n```python\\nfrom langchain.document_loaders import YoutubeLoader\\nfrom langchain.document_loaders import GoogleApiYoutubeLoader\\n```', metadata={'Header 1': 'YouTube', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the GooseAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific GooseAI wrappers.', metadata={'Header 1': 'GooseAI'}),\n",
       " Document(page_content='- Install the Python SDK with `pip install openai`\\n- Get your GooseAI api key from this link [here](https://goose.ai/).\\n- Set the environment variable (`GOOSEAI_API_KEY`).  \\n```python\\nimport os\\nos.environ[\"GOOSEAI_API_KEY\"] = \"YOUR_API_KEY\"\\n```', metadata={'Header 1': 'GooseAI', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an GooseAI LLM wrapper, which you can access with:\\n```python\\nfrom langchain.llms import GooseAI\\n```', metadata={'Header 1': 'GooseAI', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content=\">[Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction) is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.  \\n>[Azure Files](https://learn.microsoft.com/en-us/azure/storage/files/storage-files-introduction) offers fully managed\\n> file shares in the cloud that are accessible via the industry standard Server Message Block (`SMB`) protocol,\\n> Network File System (`NFS`) protocol, and `Azure Files REST API`. `Azure Files` are based on the `Azure Blob Storage`.  \\n`Azure Blob Storage` is designed for:\\n- Serving images or documents directly to a browser.\\n- Storing files for distributed access.\\n- Streaming video and audio.\\n- Writing to log files.\\n- Storing data for backup and restore, disaster recovery, and archiving.\\n- Storing data for analysis by an on-premises or Azure-hosted service.\", metadata={'Header 1': 'Azure Blob Storage'}),\n",
       " Document(page_content='```bash\\npip install azure-storage-blob\\n```', metadata={'Header 1': 'Azure Blob Storage', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example for the Azure Blob Storage](/docs/modules/data_connection/document_loaders/integrations/azure_blob_storage_container.html).  \\n```python\\nfrom langchain.document_loaders import AzureBlobStorageContainerLoader\\n```  \\nSee a [usage example for the Azure Files](/docs/modules/data_connection/document_loaders/integrations/azure_blob_storage_file.html).  \\n```python\\nfrom langchain.document_loaders import AzureBlobStorageFileLoader\\n```', metadata={'Header 1': 'Azure Blob Storage', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[GitBook](https://docs.gitbook.com/) is a modern documentation platform where teams can document everything from products to internal knowledge bases and APIs.', metadata={'Header 1': 'GitBook'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'GitBook', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/gitbook.html).  \\n```python\\nfrom langchain.document_loaders import GitbookLoader\\n```', metadata={'Header 1': 'GitBook', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Notion](https://www.notion.so/) is a collaboration platform with modified Markdown support that integrates kanban\\n> boards, tasks, wikis and databases. It is an all-in-one workspace for notetaking, knowledge and data management,\\n> and project and task management.', metadata={'Header 1': 'Notion DB'}),\n",
       " Document(page_content='All instructions are in examples below.', metadata={'Header 1': 'Notion DB', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='We have two different loaders: `NotionDirectoryLoader` and `NotionDBLoader`.  \\nSee a [usage example for the NotionDirectoryLoader](/docs/modules/data_connection/document_loaders/integrations/notion.html).  \\n```python\\nfrom langchain.document_loaders import NotionDirectoryLoader\\n```  \\nSee a [usage example for the NotionDBLoader](/docs/modules/data_connection/document_loaders/integrations/notiondb.html).  \\n```python\\nfrom langchain.document_loaders import NotionDBLoader\\n```', metadata={'Header 1': 'Notion DB', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the OpenSearch ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific OpenSearch wrappers.', metadata={'Header 1': 'OpenSearch'}),\n",
       " Document(page_content='- Install the Python package with `pip install opensearch-py`', metadata={'Header 1': 'OpenSearch', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around OpenSearch vector databases, allowing you to use it as a vectorstore\\nfor semantic search using approximate vector search powered by lucene, nmslib and faiss engines\\nor using painless scripting and script scoring functions for bruteforce vector search.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import OpenSearchVectorSearch\\n```  \\nFor a more detailed walkthrough of the OpenSearch wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/opensearch.html)', metadata={'Header 1': 'OpenSearch', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='>[2markdown](https://2markdown.com/) service transforms website content into structured markdown files.', metadata={'Header 1': '2Markdown'}),\n",
       " Document(page_content='We need the `API key`. See [instructions how to get it](https://2markdown.com/login).', metadata={'Header 1': '2Markdown', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/tomarkdown.html).  \\n```python\\nfrom langchain.document_loaders import ToMarkdownLoader\\n```', metadata={'Header 1': '2Markdown', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the Petals ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Petals wrappers.', metadata={'Header 1': 'Petals'}),\n",
       " Document(page_content='- Install with `pip install petals`\\n- Get a Hugging Face api key and set it as an environment variable (`HUGGINGFACE_API_KEY`)', metadata={'Header 1': 'Petals', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an Petals LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import Petals\\n```', metadata={'Header 1': 'Petals', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='>[Spreedly](https://docs.spreedly.com/) is a service that allows you to securely store credit cards and use them to transact against any number of payment gateways and third party APIs. It does this by simultaneously providing a card tokenization/vault service as well as a gateway and receiver integration service. Payment methods tokenized by Spreedly are stored at `Spreedly`, allowing you to independently store a card and then pass that card to different end points based on your business requirements.', metadata={'Header 1': 'Spreedly'}),\n",
       " Document(page_content='See [setup instructions](/docs/modules/data_connection/document_loaders/integrations/spreedly.html).', metadata={'Header 1': 'Spreedly', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/spreedly.html).  \\n```python\\nfrom langchain.document_loaders import SpreedlyLoader\\n```', metadata={'Header 1': 'Spreedly', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case.', metadata={'Header 1': 'Bedrock'}),\n",
       " Document(page_content='```bash\\npip install boto3\\n```', metadata={'Header 1': 'Bedrock', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/model_io/models/llms/integrations/bedrock.html).  \\n```python\\nfrom langchain import Bedrock\\n```', metadata={'Header 1': 'Bedrock', 'Header 2': 'LLM'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/text_embedding/integrations/bedrock.html).\\n```python\\nfrom langchain.embeddings import BedrockEmbeddings\\n```', metadata={'Header 1': 'Bedrock', 'Header 2': 'Text Embedding Models'}),\n",
       " Document(page_content='>[Aleph Alpha](https://docs.aleph-alpha.com/) was founded in 2019 with the mission to research and build the foundational technology for an era of strong AI. The team of international scientists, engineers, and innovators researches, develops, and deploys transformative AI like large language and multimodal models and runs the fastest European commercial AI cluster.  \\n>[The Luminous series](https://docs.aleph-alpha.com/docs/introduction/luminous/) is a family of large language models.', metadata={'Header 1': 'Aleph Alpha'}),\n",
       " Document(page_content='```bash\\npip install aleph-alpha-client\\n```  \\nYou have to create a new token. Please, see [instructions](https://docs.aleph-alpha.com/docs/account/#create-a-new-token).  \\n```python\\nfrom getpass import getpass  \\nALEPH_ALPHA_API_KEY = getpass()\\n```', metadata={'Header 1': 'Aleph Alpha', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/model_io/models/llms/integrations/aleph_alpha.html).  \\n```python\\nfrom langchain.llms import AlephAlpha\\n```', metadata={'Header 1': 'Aleph Alpha', 'Header 2': 'LLM'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/text_embedding/integrations/aleph_alpha.html).  \\n```python\\nfrom langchain.embeddings import AlephAlphaSymmetricSemanticEmbedding, AlephAlphaAsymmetricSemanticEmbedding\\n```', metadata={'Header 1': 'Aleph Alpha', 'Header 2': 'Text Embedding Models'}),\n",
       " Document(page_content='This page covers how to use the Hazy Research ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Hazy Research wrappers.', metadata={'Header 1': 'Hazy Research'}),\n",
       " Document(page_content='- To use the `manifest`, install it with `pip install manifest-ml`', metadata={'Header 1': 'Hazy Research', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content=\"There exists an LLM wrapper around Hazy Research's `manifest` library.\\n`manifest` is a python library which is itself a wrapper around many model providers, and adds in caching, history, and more.  \\nTo use this wrapper:\\n```python\\nfrom langchain.llms.manifest import ManifestWrapper\\n```\", metadata={'Header 1': 'Hazy Research', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='>[Motherduck](https://motherduck.com/) is a managed DuckDB-in-the-cloud service.', metadata={'Header 1': 'Motherduck'}),\n",
       " Document(page_content='First, you need to install `duckdb` python package.  \\n```bash\\npip install duckdb\\n```  \\nYou will also need to sign up for an account at [Motherduck](https://motherduck.com/)  \\nAfter that, you should set up a connection string - we mostly integrate with Motherduck through SQLAlchemy.\\nThe connection string is likely in the form:  \\n```\\ntoken=\"...\"  \\nconn_str = f\"duckdb:///md:{token}@my_db\"\\n```', metadata={'Header 1': 'Motherduck', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='You can use the SQLChain to query data in your Motherduck instance in natural language.  \\n```\\nfrom langchain import OpenAI, SQLDatabase, SQLDatabaseChain\\ndb = SQLDatabase.from_uri(conn_str)\\ndb_chain = SQLDatabaseChain.from_llm(OpenAI(temperature=0), db, verbose=True)\\n```  \\nFrom here, see the [SQL Chain](/docs/modules/chains/popular/sqlite.html) documentation on how to use.', metadata={'Header 1': 'Motherduck', 'Header 2': 'SQLChain'}),\n",
       " Document(page_content='You can also easily use Motherduck to cache LLM requests.\\nOnce again this is done through the SQLAlchemy wrapper.  \\n```\\nimport sqlalchemy\\neng = sqlalchemy.create_engine(conn_str)\\nlangchain.llm_cache = SQLAlchemyCache(engine=eng)\\n```  \\nFrom here, see the [LLM Caching](/docs/modules/model_io/models/llms/how_to/llm_caching) documentation on how to use.', metadata={'Header 1': 'Motherduck', 'Header 2': 'LLMCache'}),\n",
       " Document(page_content='>[Google BigQuery](https://cloud.google.com/bigquery) is a serverless and cost-effective enterprise data warehouse that works across clouds and scales with your data.\\n`BigQuery` is a part of the `Google Cloud Platform`.', metadata={'Header 1': 'Google BigQuery'}),\n",
       " Document(page_content='First, you need to install `google-cloud-bigquery` python package.  \\n```bash\\npip install google-cloud-bigquery\\n```', metadata={'Header 1': 'Google BigQuery', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/google_bigquery.html).  \\n```python\\nfrom langchain.document_loaders import BigQueryLoader\\n```', metadata={'Header 1': 'Google BigQuery', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the Anyscale ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Anyscale wrappers.', metadata={'Header 1': 'Anyscale'}),\n",
       " Document(page_content='- Get an Anyscale Service URL, route and API key and set them as environment variables (`ANYSCALE_SERVICE_URL`,`ANYSCALE_SERVICE_ROUTE`, `ANYSCALE_SERVICE_TOKEN`).\\n- Please see [the Anyscale docs](https://docs.anyscale.com/productionize/services-v2/get-started) for more details.', metadata={'Header 1': 'Anyscale', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an Anyscale LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import Anyscale\\n```', metadata={'Header 1': 'Anyscale', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='>[Psychic](https://www.psychic.dev/) is a platform for integrating with SaaS tools like `Notion`, `Zendesk`,\\n> `Confluence`, and `Google Drive` via OAuth and syncing documents from these applications to your SQL or vector\\n> database. You can think of it like Plaid for unstructured data.', metadata={'Header 1': 'Psychic'}),\n",
       " Document(page_content='```bash\\npip install psychicapi\\n```  \\nPsychic is easy to set up - you import the `react` library and configure it with your `Sidekick API` key, which you get\\nfrom the [Psychic dashboard](https://dashboard.psychic.dev/). When you connect the applications, you\\nview these connections from the dashboard and retrieve data using the server-side libraries.  \\n1. Create an account in the [dashboard](https://dashboard.psychic.dev/).\\n2. Use the [react library](https://docs.psychic.dev/sidekick-link) to add the Psychic link modal to your frontend react app. You will use this to connect the SaaS apps.\\n3. Once you have created a connection, you can use the `PsychicLoader` by following the [example notebook](/docs/modules/data_connection/document_loaders/integrations/psychic.html)', metadata={'Header 1': 'Psychic', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content=\"1.\\t**Universal API:** Instead of building OAuth flows and learning the APIs for every SaaS app, you integrate Psychic once and leverage our universal API to retrieve data.\\n2.\\t**Data Syncs:** Data in your customers' SaaS apps can get stale fast. With Psychic you can configure webhooks to keep your documents up to date on a daily or realtime basis.\\n3.\\t**Simplified OAuth:** Psychic handles OAuth end-to-end so that you don't have to spend time creating OAuth clients for each integration, keeping access tokens fresh, and handling OAuth redirect logic.\", metadata={'Header 1': 'Psychic', 'Header 2': 'Advantages vs Other Document Loaders'}),\n",
       " Document(page_content='>[iFixit](https://www.ifixit.com) is the largest, open repair community on the web. The site contains nearly 100k\\n> repair manuals, 200k Questions & Answers on 42k devices, and all the data is licensed under `CC-BY-NC-SA 3.0`.', metadata={'Header 1': 'iFixit'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'iFixit', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/ifixit.html).  \\n```python\\nfrom langchain.document_loaders import IFixitLoader\\n```', metadata={'Header 1': 'iFixit', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the Banana ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Banana wrappers.', metadata={'Header 1': 'Banana'}),\n",
       " Document(page_content='- Install with `pip install banana-dev`\\n- Get an Banana api key and set it as an environment variable (`BANANA_API_KEY`)', metadata={'Header 1': 'Banana', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='If you want to use an available language model template you can find one [here](https://app.banana.dev/templates/conceptofmind/serverless-template-palmyra-base).\\nThis template uses the Palmyra-Base model by [Writer](https://writer.com/product/api/).\\nYou can check out an example Banana repository [here](https://github.com/conceptofmind/serverless-template-palmyra-base).', metadata={'Header 1': 'Banana', 'Header 2': 'Define your Banana Template'}),\n",
       " Document(page_content='Banana Apps must include the \"output\" key in the return json.\\nThere is a rigid response structure.  \\n```python', metadata={'Header 1': 'Banana', 'Header 2': 'Build the Banana app'}),\n",
       " Document(page_content=\"result = {'output': result}\\n```  \\nAn example inference function would be:  \\n```python\\ndef inference(model_inputs:dict) -> dict:\\nglobal model\\nglobal tokenizer\", metadata={'Header 1': 'Return the results as a dictionary'}),\n",
       " Document(page_content='prompt = model_inputs.get(\\'prompt\\', None)\\nif prompt == None:\\nreturn {\\'message\\': \"No prompt provided\"}', metadata={'Header 1': 'Parse out your arguments'}),\n",
       " Document(page_content=\"input_ids = tokenizer.encode(prompt, return_tensors='pt').cuda()\\noutput = model.generate(\\ninput_ids,\\nmax_length=100,\\ndo_sample=True,\\ntop_k=50,\\ntop_p=0.95,\\nnum_return_sequences=1,\\ntemperature=0.9,\\nearly_stopping=True,\\nno_repeat_ngram_size=3,\\nnum_beams=5,\\nlength_penalty=1.5,\\nrepetition_penalty=1.5,\\nbad_words_ids=[[tokenizer.encode(' ', add_prefix_space=True)[0]]]\\n)  \\nresult = tokenizer.decode(output[0], skip_special_tokens=True)\", metadata={'Header 1': 'Run the model'}),\n",
       " Document(page_content=\"result = {'output': result}\\nreturn result\\n```  \\nYou can find a full example of a Banana app [here](https://github.com/conceptofmind/serverless-template-palmyra-base/blob/main/app.py).\", metadata={'Header 1': 'Return the results as a dictionary'}),\n",
       " Document(page_content='There exists an Banana LLM wrapper, which you can access with  \\n```python\\nfrom langchain.llms import Banana\\n```  \\nYou need to provide a model key located in the dashboard:  \\n```python\\nllm = Banana(model_key=\"YOUR_MODEL_KEY\")\\n```', metadata={'Header 1': 'Return the results as a dictionary', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content=\">[Azure Cognitive Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) (formerly known as `Azure Search`) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.  \\n>Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities:\\n>- A search engine for full text search over a search index containing user-owned content\\n>- Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation\\n>- Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more\\n>- Programmability through REST APIs and client libraries in Azure SDKs\\n>- Azure integration at the data layer, machine learning layer, and AI (Cognitive Services)\", metadata={'Header 1': 'Azure Cognitive Search'}),\n",
       " Document(page_content='See [set up instructions](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal).', metadata={'Header 1': 'Azure Cognitive Search', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/retrievers/integrations/azure_cognitive_search.html).  \\n```python\\nfrom langchain.retrievers import AzureCognitiveSearchRetriever\\n```', metadata={'Header 1': 'Azure Cognitive Search', 'Header 2': 'Retriever'}),\n",
       " Document(page_content='>[Cohere](https://cohere.ai/about) is a Canadian startup that provides natural language processing models\\n> that help companies improve human-machine interactions.', metadata={'Header 1': 'Cohere'}),\n",
       " Document(page_content='- Install the Python SDK :\\n```bash\\npip install cohere\\n```  \\nGet a [Cohere api key](https://dashboard.cohere.ai/) and set it as an environment variable (`COHERE_API_KEY`)', metadata={'Header 1': 'Cohere', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an Cohere LLM wrapper, which you can access with\\nSee a [usage example](/docs/modules/model_io/models/llms/integrations/cohere.html).  \\n```python\\nfrom langchain.llms import Cohere\\n```', metadata={'Header 1': 'Cohere', 'Header 2': 'LLM'}),\n",
       " Document(page_content='There exists an Cohere Embedding model, which you can access with\\n```python\\nfrom langchain.embeddings import CohereEmbeddings\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/text_embedding/integrations/cohere.html)', metadata={'Header 1': 'Cohere', 'Header 2': 'Text Embedding Model'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/retrievers/integrations/cohere-reranker.html).  \\n```python\\nfrom langchain.retrievers.document_compressors import CohereRerank\\n```', metadata={'Header 1': 'Cohere', 'Header 2': 'Retriever'}),\n",
       " Document(page_content='This page covers how to use [PromptLayer](https://www.promptlayer.com) within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific PromptLayer wrappers.', metadata={'Header 1': 'PromptLayer'}),\n",
       " Document(page_content='If you want to work with PromptLayer:\\n- Install the promptlayer python library `pip install promptlayer`\\n- Create a PromptLayer account\\n- Create an api token and set it as an environment variable (`PROMPTLAYER_API_KEY`)', metadata={'Header 1': 'PromptLayer', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an PromptLayer OpenAI LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import PromptLayerOpenAI\\n```  \\nTo tag your requests, use the argument `pl_tags` when instanializing the LLM\\n```python\\nfrom langchain.llms import PromptLayerOpenAI\\nllm = PromptLayerOpenAI(pl_tags=[\"langchain-requests\", \"chatbot\"])\\n```  \\nTo get the PromptLayer request id, use the argument `return_pl_id` when instanializing the LLM\\n```python\\nfrom langchain.llms import PromptLayerOpenAI\\nllm = PromptLayerOpenAI(return_pl_id=True)\\n```\\nThis will add the PromptLayer request ID in the `generation_info` field of the `Generation` returned when using `.generate` or `.agenerate`  \\nFor example:\\n```python\\nllm_results = llm.generate([\"hello world\"])\\nfor res in llm_results.generations:\\nprint(\"pl request id: \", res[0].generation_info[\"pl_request_id\"])\\n```\\nYou can use the PromptLayer request ID to add a prompt, score, or other metadata to your request. [Read more about it here](https://magniv.notion.site/Track-4deee1b1f7a34c1680d085f82567dab9).  \\nThis LLM is identical to the [OpenAI](/docs/ecosystem/integrations/openai.html) LLM, except that\\n- all your requests will be logged to your PromptLayer account\\n- you can add `pl_tags` when instantializing to tag your requests on PromptLayer\\n- you can add `return_pl_id` when instantializing to return a PromptLayer request id to use [while tracking requests](https://magniv.notion.site/Track-4deee1b1f7a34c1680d085f82567dab9).  \\nPromptLayer also provides native wrappers for [`PromptLayerChatOpenAI`](/docs/modules/model_io/models/chat/integrations/promptlayer_chatopenai.html) and `PromptLayerOpenAIChat`', metadata={'Header 1': 'PromptLayer', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='[Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.  \\nAPI Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. API Gateway has no minimum fees or startup costs. You pay for the API calls you receive and the amount of data transferred out and, with the API Gateway tiered pricing model, you can reduce your cost as your API usage scales.', metadata={'Header 1': 'Amazon API Gateway'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/model_io/models/llms/integrations/amazon_api_gateway_example.html).  \\n```python\\nfrom langchain.llms import AmazonAPIGateway  \\napi_url = \"https://<api_gateway_id>.execute-api.<region>.amazonaws.com/LATEST/HF\"\\nllm = AmazonAPIGateway(api_url=api_url)', metadata={'Header 1': 'Amazon API Gateway', 'Header 2': 'LLM'}),\n",
       " Document(page_content='parameters = {\\n\"max_new_tokens\": 100,\\n\"num_return_sequences\": 1,\\n\"top_k\": 50,\\n\"top_p\": 0.95,\\n\"do_sample\": False,\\n\"return_full_text\": True,\\n\"temperature\": 0.2,\\n}  \\nprompt = \"what day comes after Friday?\"\\nllm.model_kwargs = parameters\\nllm(prompt)\\n>>> \\'what day comes after Friday?\\\\nSaturday\\'\\n```', metadata={'Header 1': 'These are sample parameters for Falcon 40B Instruct Deployed from Amazon SageMaker JumpStart'}),\n",
       " Document(page_content='```python\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\nfrom langchain.llms import AmazonAPIGateway  \\napi_url = \"https://<api_gateway_id>.execute-api.<region>.amazonaws.com/LATEST/HF\"\\nllm = AmazonAPIGateway(api_url=api_url)  \\nparameters = {\\n\"max_new_tokens\": 50,\\n\"num_return_sequences\": 1,\\n\"top_k\": 250,\\n\"top_p\": 0.25,\\n\"do_sample\": False,\\n\"temperature\": 0.1,\\n}  \\nllm.model_kwargs = parameters', metadata={'Header 1': 'These are sample parameters for Falcon 40B Instruct Deployed from Amazon SageMaker JumpStart', 'Header 2': 'Agent'}),\n",
       " Document(page_content='tools = load_tools([\"python_repl\", \"llm-math\"], llm=llm)', metadata={'Header 1': \"Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\"}),\n",
       " Document(page_content='agent = initialize_agent(\\ntools,\\nllm,\\nagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\\nverbose=True,\\n)', metadata={'Header 1': \"Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\"}),\n",
       " Document(page_content='agent.run(\"\"\"\\nWrite a Python script that prints \"Hello, world!\"\\n\"\"\")  \\n>>> \\'Hello, world!\\'\\n```', metadata={'Header 1': \"Now let's test it out!\"}),\n",
       " Document(page_content='This page covers how to use the [Serper](https://serper.dev) Google Search API within LangChain. Serper is a low-cost Google Search API that can be used to add answer box, knowledge graph, and organic results data from Google Search.\\nIt is broken into two parts: setup, and then references to the specific Google Serper wrapper.', metadata={'Header 1': 'Google Serper'}),\n",
       " Document(page_content='- Go to [serper.dev](https://serper.dev) to sign up for a free account\\n- Get the api key and set it as an environment variable (`SERPER_API_KEY`)', metadata={'Header 1': 'Google Serper', 'Header 2': 'Setup'}),\n",
       " Document(page_content='There exists a GoogleSerperAPIWrapper utility which wraps this API. To import this utility:  \\n```python\\nfrom langchain.utilities import GoogleSerperAPIWrapper\\n```  \\nYou can use it as part of a Self Ask chain:  \\n```python\\nfrom langchain.utilities import GoogleSerperAPIWrapper\\nfrom langchain.llms.openai import OpenAI\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.agents import AgentType  \\nimport os  \\nos.environ[\"SERPER_API_KEY\"] = \"\"\\nos.environ[\\'OPENAI_API_KEY\\'] = \"\"  \\nllm = OpenAI(temperature=0)\\nsearch = GoogleSerperAPIWrapper()\\ntools = [\\nTool(\\nname=\"Intermediate Answer\",\\nfunc=search.run,\\ndescription=\"useful for when you need to ask with search\"\\n)\\n]  \\nself_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\\nself_ask_with_search.run(\"What is the hometown of the reigning men\\'s U.S. Open champion?\")\\n```  \\n#### Output\\n```\\nEntering new AgentExecutor chain...\\nYes.\\nFollow up: Who is the reigning men\\'s U.S. Open champion?\\nIntermediate answer: Current champions Carlos Alcaraz, 2022 men\\'s singles champion.\\nFollow up: Where is Carlos Alcaraz from?\\nIntermediate answer: El Palmar, Spain\\nSo the final answer is: El Palmar, Spain  \\n> Finished chain.  \\n\\'El Palmar, Spain\\'\\n```  \\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/modules/agents/tools/integrations/google_serper.html).', metadata={'Header 1': 'Google Serper', 'Header 2': 'Wrappers', 'Header 3': 'Utility'}),\n",
       " Document(page_content='You can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"google-serper\"])\\n```  \\nFor more information on tools, see [this page](/docs/modules/agents/tools/).', metadata={'Header 1': 'Google Serper', 'Header 2': 'Wrappers', 'Header 3': 'Tool'}),\n",
       " Document(page_content='This page covers how to use the NLPCloud ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific NLPCloud wrappers.', metadata={'Header 1': 'NLPCloud'}),\n",
       " Document(page_content='- Install the Python SDK with `pip install nlpcloud`\\n- Get an NLPCloud api key and set it as an environment variable (`NLPCLOUD_API_KEY`)', metadata={'Header 1': 'NLPCloud', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an NLPCloud LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import NLPCloud\\n```', metadata={'Header 1': 'NLPCloud', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='>[ROAM](https://roamresearch.com/) is a note-taking tool for networked thought, designed to create a personal knowledge base.', metadata={'Header 1': 'Roam'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'Roam', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/roam.html).  \\n```python\\nfrom langchain.document_loaders import RoamLoader\\n```', metadata={'Header 1': 'Roam', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the `GPT4All` wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example.', metadata={'Header 1': 'GPT4All'}),\n",
       " Document(page_content='- Install the Python package with `pip install pyllamacpp`\\n- Download a [GPT4All model](https://github.com/nomic-ai/pyllamacpp#supported-model) and place it in your desired directory', metadata={'Header 1': 'GPT4All', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content=\"To use the GPT4All wrapper, you need to provide the path to the pre-trained model file and the model's configuration.  \\n```python\\nfrom langchain.llms import GPT4All\", metadata={'Header 1': 'GPT4All', 'Header 2': 'Usage', 'Header 3': 'GPT4All'}),\n",
       " Document(page_content='model = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)', metadata={'Header 1': 'Instantiate the model. Callbacks support token-wise streaming'}),\n",
       " Document(page_content='response = model(\"Once upon a time, \")\\n```  \\nYou can also customize the generation parameters, such as n_predict, temp, top_p, top_k, and others.  \\nTo stream the model\\'s predictions, add in a CallbackManager.  \\n```python\\nfrom langchain.llms import GPT4All\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler', metadata={'Header 1': 'Generate text'}),\n",
       " Document(page_content='callbacks = [StreamingStdOutCallbackHandler()]\\nmodel = GPT4All(model=\"./models/gpt4all-model.bin\", n_ctx=512, n_threads=8)', metadata={'Header 1': 'from langchain.callbacks.streamlit import StreamlitCallbackHandler'}),\n",
       " Document(page_content='model(\"Once upon a time, \", callbacks=callbacks)\\n```', metadata={'Header 1': 'Generate text. Tokens are streamed through the callback manager.'}),\n",
       " Document(page_content='You can find links to model file downloads in the [pyllamacpp](https://github.com/nomic-ai/pyllamacpp) repository.  \\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/model_io/models/llms/integrations/gpt4all.html)', metadata={'Header 1': 'Generate text. Tokens are streamed through the callback manager.', 'Header 2': 'Model File'}),\n",
       " Document(page_content='>[Git](https://en.wikipedia.org/wiki/Git) is a distributed version control system that tracks changes in any set of computer files, usually used for coordinating work among programmers collaboratively developing source code during software development.', metadata={'Header 1': 'Git'}),\n",
       " Document(page_content='First, you need to install `GitPython` python package.  \\n```bash\\npip install GitPython\\n```', metadata={'Header 1': 'Git', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/git.html).  \\n```python\\nfrom langchain.document_loaders import GitLoader\\n```', metadata={'Header 1': 'Git', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Messenger](https://en.wikipedia.org/wiki/Messenger_(software)) is an American proprietary instant messaging app and\\n> platform developed by `Meta Platforms`. Originally developed as `Facebook Chat` in 2008, the company revamped its\\n> messaging service in 2010.', metadata={'Header 1': 'Facebook Chat'}),\n",
       " Document(page_content='First, you need to install `pandas` python package.  \\n```bash\\npip install pandas\\n```', metadata={'Header 1': 'Facebook Chat', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/facebook_chat.html).  \\n```python\\nfrom langchain.document_loaders import FacebookChatLoader\\n```', metadata={'Header 1': 'Facebook Chat', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use [Graphsignal](https://app.graphsignal.com) to trace and monitor LangChain. Graphsignal enables full visibility into your application. It provides latency breakdowns by chains and tools, exceptions with full context, data monitoring, compute/GPU utilization, OpenAI cost analytics, and more.', metadata={'Header 1': 'Graphsignal'}),\n",
       " Document(page_content='- Install the Python library with `pip install graphsignal`\\n- Create free Graphsignal account [here](https://graphsignal.com)\\n- Get an API key and set it as an environment variable (`GRAPHSIGNAL_API_KEY`)', metadata={'Header 1': 'Graphsignal', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='Graphsignal automatically instruments and starts tracing and monitoring chains. Traces and metrics are then available in your [Graphsignal dashboards](https://app.graphsignal.com).  \\nInitialize the tracer by providing a deployment name:  \\n```python\\nimport graphsignal  \\ngraphsignal.configure(deployment=\\'my-langchain-app-prod\\')\\n```  \\nTo additionally trace any function or code, you can use a decorator or a context manager:  \\n```python\\n@graphsignal.trace_function\\ndef handle_request():\\nchain.run(\"some initial text\")\\n```  \\n```python\\nwith graphsignal.start_trace(\\'my-chain\\'):\\nchain.run(\"some initial text\")\\n```  \\nOptionally, enable profiling to record function-level statistics for each trace.  \\n```python\\nwith graphsignal.start_trace(\\n\\'my-chain\\', options=graphsignal.TraceOptions(enable_profiling=True)):\\nchain.run(\"some initial text\")\\n```  \\nSee the [Quick Start](https://graphsignal.com/docs/guides/quick-start/) guide for complete setup instructions.', metadata={'Header 1': 'Graphsignal', 'Header 2': 'Tracing and Monitoring'}),\n",
       " Document(page_content=\"This page covers how to use MyScale vector database within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific MyScale wrappers.  \\nWith MyScale, you can manage both structured and unstructured (vectorized) data, and perform joint queries and analytics on both types of data using SQL. Plus, MyScale's cloud-native OLAP architecture, built on top of ClickHouse, enables lightning-fast data processing even on massive datasets.\", metadata={'Header 1': 'MyScale'}),\n",
       " Document(page_content='[Overview to MyScale and High performance vector search](https://docs.myscale.com/en/overview/)  \\nYou can now register on our SaaS and [start a cluster now!](https://docs.myscale.com/en/quickstart/)  \\nIf you are also interested in how we managed to integrate SQL and vector, please refer to [this document](https://docs.myscale.com/en/vector-reference/) for further syntax reference.  \\nWe also deliver with live demo on huggingface! Please checkout our [huggingface space](https://huggingface.co/myscale)! They search millions of vector within a blink!', metadata={'Header 1': 'MyScale', 'Header 2': 'Introduction'}),\n",
       " Document(page_content='- Install the Python SDK with `pip install clickhouse-connect`', metadata={'Header 1': 'MyScale', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There are two ways to set up parameters for myscale index.  \\n1. Environment Variables  \\nBefore you run the app, please set the environment variable with `export`:\\n`export MYSCALE_HOST=\\'<your-endpoints-url>\\' MYSCALE_PORT=<your-endpoints-port> MYSCALE_USERNAME=<your-username> MYSCALE_PASSWORD=<your-password> ...`  \\nYou can easily find your account, password and other info on our SaaS. For details please refer to [this document](https://docs.myscale.com/en/cluster-management/)\\nEvery attributes under `MyScaleSettings` can be set with prefix `MYSCALE_` and is case insensitive.  \\n2. Create `MyScaleSettings` object with parameters  \\n```python\\nfrom langchain.vectorstores import MyScale, MyScaleSettings\\nconfig = MyScaleSetting(host=\"<your-backend-url>\", port=8443, ...)\\nindex = MyScale(embedding_function, config)\\nindex.add_documents(...)\\n```', metadata={'Header 1': 'MyScale', 'Header 2': 'Installation and Setup', 'Header 3': 'Setting up envrionments'}),\n",
       " Document(page_content='supported functions:\\n- `add_texts`\\n- `add_documents`\\n- `from_texts`\\n- `from_documents`\\n- `similarity_search`\\n- `asimilarity_search`\\n- `similarity_search_by_vector`\\n- `asimilarity_search_by_vector`\\n- `similarity_search_with_relevance_scores`', metadata={'Header 1': 'MyScale', 'Header 2': 'Wrappers'}),\n",
       " Document(page_content='There exists a wrapper around MyScale database, allowing you to use it as a vectorstore,\\nwhether for semantic search or similar example retrieval.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import MyScale\\n```  \\nFor a more detailed walkthrough of the MyScale wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/myscale.html)', metadata={'Header 1': 'MyScale', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='This page covers how to use the DeepInfra ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific DeepInfra wrappers.', metadata={'Header 1': 'DeepInfra'}),\n",
       " Document(page_content='- Get your DeepInfra api key from this link [here](https://deepinfra.com/).\\n- Get an DeepInfra api key and set it as an environment variable (`DEEPINFRA_API_TOKEN`)', metadata={'Header 1': 'DeepInfra', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='DeepInfra provides a range of Open Source LLMs ready for deployment.\\nYou can list supported models [here](https://deepinfra.com/models?type=text-generation).\\ngoogle/flan\\\\* models can be viewed [here](https://deepinfra.com/models?type=text2text-generation).  \\nYou can view a list of request and response parameters [here](https://deepinfra.com/databricks/dolly-v2-12b#API)', metadata={'Header 1': 'DeepInfra', 'Header 2': 'Available Models'}),\n",
       " Document(page_content='There exists an DeepInfra LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import DeepInfra\\n```', metadata={'Header 1': 'DeepInfra', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='This page covers how to use the [Redis](https://redis.com) ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Redis wrappers.', metadata={'Header 1': 'Redis'}),\n",
       " Document(page_content='- Install the Redis Python SDK with `pip install redis`', metadata={'Header 1': 'Redis', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='The Cache wrapper allows for [Redis](https://redis.io) to be used as a remote, low-latency, in-memory cache for LLM prompts and responses.  \\n#### Standard Cache\\nThe standard cache is the Redis bread & butter of use case in production for both [open source](https://redis.io) and [enterprise](https://redis.com) users globally.  \\nTo import this cache:\\n```python\\nfrom langchain.cache import RedisCache\\n```  \\nTo use this cache with your LLMs:\\n```python\\nimport langchain\\nimport redis  \\nredis_client = redis.Redis.from_url(...)\\nlangchain.llm_cache = RedisCache(redis_client)\\n```  \\n#### Semantic Cache\\nSemantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends Redis as both a cache and a vectorstore.  \\nTo import this cache:\\n```python\\nfrom langchain.cache import RedisSemanticCache\\n```  \\nTo use this cache with your LLMs:\\n```python\\nimport langchain\\nimport redis', metadata={'Header 1': 'Redis', 'Header 2': 'Wrappers', 'Header 3': 'Cache'}),\n",
       " Document(page_content='from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings  \\nredis_url = \"redis://localhost:6379\"  \\nlangchain.llm_cache = RedisSemanticCache(\\nembedding=FakeEmbeddings(),\\nredis_url=redis_url\\n)\\n```', metadata={'Header 1': 'use any embedding provider...'}),\n",
       " Document(page_content='The vectorstore wrapper turns Redis into a low-latency [vector database](https://redis.com/solutions/use-cases/vector-database/) for semantic search or LLM content retrieval.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import Redis\\n```  \\nFor a more detailed walkthrough of the Redis vectorstore wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/redis.html).', metadata={'Header 1': 'use any embedding provider...', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='The Redis vector store retriever wrapper generalizes the vectorstore class to perform low-latency document retrieval. To create the retriever, simply call `.as_retriever()` on the base vectorstore class.', metadata={'Header 1': 'use any embedding provider...', 'Header 3': 'Retriever'}),\n",
       " Document(page_content='Redis can be used to persist LLM conversations.  \\n#### Vector Store Retriever Memory  \\nFor a more detailed walkthrough of the `VectorStoreRetrieverMemory` wrapper, see [this notebook](/docs/modules/memory/integrations/vectorstore_retriever_memory.html).  \\n#### Chat Message History Memory\\nFor a detailed example of Redis to cache conversation message history, see [this notebook](/docs/modules/memory/integrations/redis_chat_message_history.html).', metadata={'Header 1': 'use any embedding provider...', 'Header 3': 'Memory'}),\n",
       " Document(page_content='>[Project Gutenberg](https://www.gutenberg.org/about/) is an online library of free eBooks.', metadata={'Header 1': 'Gutenberg'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'Gutenberg', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/gutenberg.html).  \\n```python\\nfrom langchain.document_loaders import GutenbergLoader\\n```', metadata={'Header 1': 'Gutenberg', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Confluence](https://www.atlassian.com/software/confluence) is a wiki collaboration platform that saves and organizes all of the project-related material. `Confluence` is a knowledge base that primarily handles content management activities.', metadata={'Header 1': 'Confluence'}),\n",
       " Document(page_content='```bash\\npip install atlassian-python-api\\n```  \\nWe need to set up `username/api_key` or `Oauth2 login`.\\nSee [instructions](https://support.atlassian.com/atlassian-account/docs/manage-api-tokens-for-your-atlassian-account/).', metadata={'Header 1': 'Confluence', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/confluence.html).  \\n```python\\nfrom langchain.document_loaders import ConfluenceLoader\\n```', metadata={'Header 1': 'Confluence', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Telegram Messenger](https://web.telegram.org/a/) is a globally accessible freemium, cross-platform, encrypted, cloud-based and centralized instant messaging service. The application also provides optional end-to-end encrypted chats and video calling, VoIP, file sharing and several other features.', metadata={'Header 1': 'Telegram'}),\n",
       " Document(page_content='See [setup instructions](/docs/modules/data_connection/document_loaders/integrations/telegram.html).', metadata={'Header 1': 'Telegram', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/telegram.html).  \\n```python\\nfrom langchain.document_loaders import TelegramChatFileLoader\\nfrom langchain.document_loaders import TelegramChatApiLoader\\n```', metadata={'Header 1': 'Telegram', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the AI21 ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific AI21 wrappers.', metadata={'Header 1': 'AI21 Labs'}),\n",
       " Document(page_content='- Get an AI21 api key and set it as an environment variable (`AI21_API_KEY`)', metadata={'Header 1': 'AI21 Labs', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an AI21 LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import AI21\\n```', metadata={'Header 1': 'AI21 Labs', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='>[OpenWeatherMap](https://openweathermap.org/) is an open source weather service provider.', metadata={'Header 1': 'Weather'}),\n",
       " Document(page_content='```bash\\npip install pyowm\\n```  \\nWe must set up the `OpenWeatherMap API token`.', metadata={'Header 1': 'Weather', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/weather.html).  \\n```python\\nfrom langchain.document_loaders import WeatherDataLoader\\n```', metadata={'Header 1': 'Weather', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[EverNote](https://evernote.com/) is intended for archiving and creating notes in which photos, audio and saved web content can be embedded. Notes are stored in virtual \"notebooks\" and can be tagged, annotated, edited, searched, and exported.', metadata={'Header 1': 'EverNote'}),\n",
       " Document(page_content='First, you need to install `lxml` and `html2text` python packages.  \\n```bash\\npip install lxml\\npip install html2text\\n```', metadata={'Header 1': 'EverNote', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/evernote.html).  \\n```python\\nfrom langchain.document_loaders import EverNoteLoader\\n```', metadata={'Header 1': 'EverNote', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the Prediction Guard ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Prediction Guard wrappers.', metadata={'Header 1': 'Prediction Guard'}),\n",
       " Document(page_content='- Install the Python SDK with `pip install predictionguard`\\n- Get an Prediction Guard access token (as described [here](https://docs.predictionguard.com/)) and set it as an environment variable (`PREDICTIONGUARD_TOKEN`)', metadata={'Header 1': 'Prediction Guard', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a Prediction Guard LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import PredictionGuard\\n```  \\nYou can provide the name of the Prediction Guard model as an argument when initializing the LLM:\\n```python\\npgllm = PredictionGuard(model=\"MPT-7B-Instruct\")\\n```  \\nYou can also provide your access token directly as an argument:\\n```python\\npgllm = PredictionGuard(model=\"MPT-7B-Instruct\", token=\"<your access token>\")\\n```  \\nFinally, you can provide an \"output\" argument that is used to structure/ control the output of the LLM:\\n```python\\npgllm = PredictionGuard(model=\"MPT-7B-Instruct\", output={\"type\": \"boolean\"})\\n```', metadata={'Header 1': 'Prediction Guard', 'Header 2': 'LLM Wrapper'}),\n",
       " Document(page_content='Basic usage of the controlled or guarded LLM wrapper:\\n```python\\nimport os  \\nimport predictionguard as pg\\nfrom langchain.llms import PredictionGuard\\nfrom langchain import PromptTemplate, LLMChain', metadata={'Header 1': 'Prediction Guard', 'Header 2': 'Example usage'}),\n",
       " Document(page_content='os.environ[\"PREDICTIONGUARD_TOKEN\"] = \"<your Prediction Guard access token>\"', metadata={'Header 1': 'Your Prediction Guard API key. Get one at predictionguard.com'}),\n",
       " Document(page_content='template = \"\"\"Respond to the following query based on the context.  \\nContext: EVERY comment, DM + email suggestion has led us to this EXCITING announcement! 🎉 We have officially added TWO new candle subscription box options! 📦\\nExclusive Candle Box - $80\\nMonthly Candle Box - $45 (NEW!)\\nScent of The Month Box - $28 (NEW!)\\nHead to stories to get ALLL the deets on each box! 👆 BONUS: Save 50% on your first box with code 50OFF! 🎉  \\nQuery: {query}  \\nResult: \"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"query\"])', metadata={'Header 1': 'Define a prompt template'}),\n",
       " Document(page_content='pgllm = PredictionGuard(model=\"MPT-7B-Instruct\",\\noutput={\\n\"type\": \"categorical\",\\n\"categories\": [\\n\"product announcement\",\\n\"apology\",\\n\"relational\"\\n]\\n})\\npgllm(prompt.format(query=\"What kind of post is this?\"))\\n```  \\nBasic LLM Chaining with the Prediction Guard wrapper:\\n```python\\nimport os  \\nfrom langchain import PromptTemplate, LLMChain\\nfrom langchain.llms import PredictionGuard', metadata={'Header 1': 'structures.'}),\n",
       " Document(page_content='os.environ[\"OPENAI_API_KEY\"] = \"<your OpenAI api key>\"', metadata={'Header 1': 'you to access all the latest open access models (see https://docs.predictionguard.com)'}),\n",
       " Document(page_content='os.environ[\"PREDICTIONGUARD_TOKEN\"] = \"<your Prediction Guard access token>\"  \\npgllm = PredictionGuard(model=\"OpenAI-text-davinci-003\")  \\ntemplate = \"\"\"Question: {question}  \\nAnswer: Let\\'s think step by step.\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\\nllm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)  \\nquestion = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"  \\nllm_chain.predict(question=question)\\n```', metadata={'Header 1': 'Your Prediction Guard API key. Get one at predictionguard.com'}),\n",
       " Document(page_content='>[College Confidential](https://www.collegeconfidential.com/) gives information on 3,800+ colleges and universities.', metadata={'Header 1': 'College Confidential'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'College Confidential', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/college_confidential.html).  \\n```python\\nfrom langchain.document_loaders import CollegeConfidentialLoader\\n```', metadata={'Header 1': 'College Confidential', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is American artificial intelligence (AI) research laboratory\\n> consisting of the non-profit `OpenAI Incorporated`\\n> and its for-profit subsidiary corporation `OpenAI Limited Partnership`.\\n> `OpenAI` conducts AI research with the declared intention of promoting and developing a friendly AI.\\n> `OpenAI` systems run on an `Azure`-based supercomputing platform from `Microsoft`.  \\n>The [OpenAI API](https://platform.openai.com/docs/models) is powered by a diverse set of models with different capabilities and price points.\\n>\\n>[ChatGPT](https://chat.openai.com) is the Artificial Intelligence (AI) chatbot developed by `OpenAI`.', metadata={'Header 1': 'OpenAI'}),\n",
       " Document(page_content=\"- Install the Python SDK with\\n```bash\\npip install openai\\n```\\n- Get an OpenAI api key and set it as an environment variable (`OPENAI_API_KEY`)\\n- If you want to use OpenAI's tokenizer (only available for Python 3.9+), install it\\n```bash\\npip install tiktoken\\n```\", metadata={'Header 1': 'OpenAI', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='```python\\nfrom langchain.llms import OpenAI\\n```  \\nIf you are using a model hosted on `Azure`, you should use different wrapper for that:\\n```python\\nfrom langchain.llms import AzureOpenAI\\n```\\nFor a more detailed walkthrough of the `Azure` wrapper, see [this notebook](/docs/modules/model_io/models/llms/integrations/azure_openai_example.html)', metadata={'Header 1': 'OpenAI', 'Header 2': 'LLM'}),\n",
       " Document(page_content='```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/text_embedding/integrations/openai.html)', metadata={'Header 1': 'OpenAI', 'Header 2': 'Text Embedding Model'}),\n",
       " Document(page_content='There are several places you can use the `tiktoken` tokenizer. By default, it is used to count tokens\\nfor OpenAI LLMs.  \\nYou can also use it to count tokens when splitting documents with\\n```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\nCharacterTextSplitter.from_tiktoken_encoder(...)\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/document_transformers/text_splitters/tiktoken.html)', metadata={'Header 1': 'OpenAI', 'Header 2': 'Tokenizer'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/chains/additional/moderation.html).  \\n```python\\nfrom langchain.chains import OpenAIModerationChain\\n```', metadata={'Header 1': 'OpenAI', 'Header 2': 'Chain'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/chatgpt_loader.html).  \\n```python\\nfrom langchain.document_loaders.chatgpt import ChatGPTLoader\\n```', metadata={'Header 1': 'OpenAI', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/retrievers/integrations/chatgpt-plugin.html).  \\n```python\\nfrom langchain.retrievers import ChatGPTPluginRetriever\\n```', metadata={'Header 1': 'OpenAI', 'Header 2': 'Retriever'}),\n",
       " Document(page_content='This page covers how to use the `RWKV-4` wrapper within LangChain.\\nIt is broken into two parts: installation and setup, and then usage with an example.', metadata={'Header 1': 'RWKV-4'}),\n",
       " Document(page_content='- Install the Python package with `pip install rwkv`\\n- Install the tokenizer Python package with `pip install tokenizer`\\n- Download a [RWKV model](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) and place it in your desired directory\\n- Download the [tokens file](https://raw.githubusercontent.com/BlinkDL/ChatRWKV/main/20B_tokenizer.json)', metadata={'Header 1': 'RWKV-4', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content=\"To use the RWKV wrapper, you need to provide the path to the pre-trained model file and the tokenizer's configuration.\\n```python\\nfrom langchain.llms import RWKV\", metadata={'Header 1': 'RWKV-4', 'Header 2': 'Usage', 'Header 3': 'RWKV'}),\n",
       " Document(page_content='```python  \\ndef generate_prompt(instruction, input=None):\\nif input:\\nreturn f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.', metadata={'Header 1': 'Test the model'}),\n",
       " Document(page_content='{instruction}', metadata={'Header 1': 'Instruction:'}),\n",
       " Document(page_content='{input}', metadata={'Header 1': 'Input:'}),\n",
       " Document(page_content='\"\"\"\\nelse:\\nreturn f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.', metadata={'Header 1': 'Response:'}),\n",
       " Document(page_content='{instruction}', metadata={'Header 1': 'Instruction:'}),\n",
       " Document(page_content='\"\"\"  \\nmodel = RWKV(model=\"./models/RWKV-4-Raven-3B-v7-Eng-20230404-ctx4096.pth\", strategy=\"cpu fp32\", tokens_path=\"./rwkv/20B_tokenizer.json\")\\nresponse = model(generate_prompt(\"Once upon a time, \"))\\n```', metadata={'Header 1': 'Response:'}),\n",
       " Document(page_content='You can find links to model file downloads at the [RWKV-4-Raven](https://huggingface.co/BlinkDL/rwkv-4-raven/tree/main) repository.', metadata={'Header 1': 'Response:', 'Header 2': 'Model File'}),\n",
       " Document(page_content='```\\nRWKV VRAM\\nModel | 8bit | bf16/fp16 | fp32\\n14B   | 16GB | 28GB      | >50GB\\n7B    | 8GB  | 14GB      | 28GB\\n3B    | 2.8GB| 6GB       | 12GB\\n1b5   | 1.3GB| 3GB       | 6GB\\n```  \\nSee the [rwkv pip](https://pypi.org/project/rwkv/) page for more information about strategies, including streaming and cuda support.', metadata={'Header 1': 'Response:', 'Header 2': 'Model File', 'Header 3': 'Rwkv-4 models -> recommended VRAM'}),\n",
       " Document(page_content='>[Slack](https://slack.com/) is an instant messaging program.', metadata={'Header 1': 'Slack'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'Slack', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/slack.html).  \\n```python\\nfrom langchain.document_loaders import SlackDirectoryLoader\\n```', metadata={'Header 1': 'Slack', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the CerebriumAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific CerebriumAI wrappers.', metadata={'Header 1': 'CerebriumAI'}),\n",
       " Document(page_content='- Install with `pip install cerebrium`\\n- Get an CerebriumAI api key and set it as an environment variable (`CEREBRIUMAI_API_KEY`)', metadata={'Header 1': 'CerebriumAI', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an CerebriumAI LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import CerebriumAI\\n```', metadata={'Header 1': 'CerebriumAI', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='> [Flyte](https://github.com/flyteorg/flyte) is an open-source orchestrator that facilitates building production-grade data and ML pipelines.\\n> It is built for scalability and reproducibility, leveraging Kubernetes as its underlying platform.  \\nThe purpose of this notebook is to demonstrate the integration of a `FlyteCallback` into your Flyte task, enabling you to effectively monitor and track your LangChain experiments.', metadata={'Header 1': 'Flyte'}),\n",
       " Document(page_content='- Install the Flytekit library by running the command `pip install flytekit`.\\n- Install the Flytekit-Envd plugin by running the command `pip install flytekitplugins-envd`.\\n- Install LangChain by running the command `pip install langchain`.\\n- Install [Docker](https://docs.docker.com/engine/install/) on your system.', metadata={'Header 1': 'Flyte', 'Header 2': 'Installation & Setup'}),\n",
       " Document(page_content='A Flyte [task](https://docs.flyte.org/projects/cookbook/en/latest/auto/core/flyte_basics/task.html) serves as the foundational building block of Flyte.\\nTo execute LangChain experiments, you need to write Flyte tasks that define the specific steps and operations involved.  \\nNOTE: The [getting started guide](https://docs.flyte.org/projects/cookbook/en/latest/index.html) offers detailed, step-by-step instructions on installing Flyte locally and running your initial Flyte pipeline.  \\nFirst, import the necessary dependencies to support your LangChain experiments.  \\n```python\\nimport os  \\nfrom flytekit import ImageSpec, task\\nfrom langchain.agents import AgentType, initialize_agent, load_tools\\nfrom langchain.callbacks import FlyteCallbackHandler\\nfrom langchain.chains import LLMChain\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.schema import HumanMessage\\n```  \\nSet up the necessary environment variables to utilize the OpenAI API and Serp API:  \\n```python', metadata={'Header 1': 'Flyte', 'Header 2': 'Flyte Tasks'}),\n",
       " Document(page_content='os.environ[\"OPENAI_API_KEY\"] = \"<your_openai_api_key>\"', metadata={'Header 1': 'Set OpenAI API key'}),\n",
       " Document(page_content='os.environ[\"SERPAPI_API_KEY\"] = \"<your_serp_api_key>\"\\n```  \\nReplace `<your_openai_api_key>` and `<your_serp_api_key>` with your respective API keys obtained from OpenAI and Serp API.  \\nTo guarantee reproducibility of your pipelines, Flyte tasks are containerized.\\nEach Flyte task must be associated with an image, which can either be shared across the entire Flyte [workflow](https://docs.flyte.org/projects/cookbook/en/latest/auto/core/flyte_basics/basic_workflow.html) or provided separately for each task.  \\nTo streamline the process of supplying the required dependencies for each Flyte task, you can initialize an [`ImageSpec`](https://docs.flyte.org/projects/cookbook/en/latest/auto/core/image_spec/image_spec.html) object.\\nThis approach automatically triggers a Docker build, alleviating the need for users to manually create a Docker image.  \\n```python\\ncustom_image = ImageSpec(\\nname=\"langchain-flyte\",\\npackages=[\\n\"langchain\",\\n\"openai\",\\n\"spacy\",\\n\"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0.tar.gz\",\\n\"textstat\",\\n\"google-search-results\",\\n],\\nregistry=\"<your-registry>\",\\n)\\n```  \\nYou have the flexibility to push the Docker image to a registry of your preference.\\n[Docker Hub](https://hub.docker.com/) or [GitHub Container Registry (GHCR)](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) is a convenient option to begin with.  \\nOnce you have selected a registry, you can proceed to create Flyte tasks that log the LangChain metrics to Flyte Deck.  \\nThe following examples demonstrate tasks related to OpenAI LLM, chains and agent with tools:', metadata={'Header 1': 'Set Serp API key'}),\n",
       " Document(page_content='```python\\n@task(disable_deck=False, container_image=custom_image)\\ndef langchain_llm() -> str:\\nllm = ChatOpenAI(\\nmodel_name=\"gpt-3.5-turbo\",\\ntemperature=0.2,\\ncallbacks=[FlyteCallbackHandler()],\\n)\\nreturn llm([HumanMessage(content=\"Tell me a joke\")]).content\\n```', metadata={'Header 1': 'Set Serp API key', 'Header 3': 'LLM'}),\n",
       " Document(page_content='```python\\n@task(disable_deck=False, container_image=custom_image)\\ndef langchain_chain() -> list[dict[str, str]]:\\ntemplate = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\\nTitle: {title}\\nPlaywright: This is a synopsis for the above play:\"\"\"\\nllm = ChatOpenAI(\\nmodel_name=\"gpt-3.5-turbo\",\\ntemperature=0,\\ncallbacks=[FlyteCallbackHandler()],\\n)\\nprompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\\nsynopsis_chain = LLMChain(\\nllm=llm, prompt=prompt_template, callbacks=[FlyteCallbackHandler()]\\n)\\ntest_prompts = [\\n{\\n\"title\": \"documentary about good video games that push the boundary of game design\"\\n},\\n]\\nreturn synopsis_chain.apply(test_prompts)\\n```', metadata={'Header 1': 'Set Serp API key', 'Header 3': 'Chain'}),\n",
       " Document(page_content='```python\\n@task(disable_deck=False, container_image=custom_image)\\ndef langchain_agent() -> str:\\nllm = OpenAI(\\nmodel_name=\"gpt-3.5-turbo\",\\ntemperature=0,\\ncallbacks=[FlyteCallbackHandler()],\\n)\\ntools = load_tools(\\n[\"serpapi\", \"llm-math\"], llm=llm, callbacks=[FlyteCallbackHandler()]\\n)\\nagent = initialize_agent(\\ntools,\\nllm,\\nagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\\ncallbacks=[FlyteCallbackHandler()],\\nverbose=True,\\n)\\nreturn agent.run(\\n\"Who is Leonardo DiCaprio\\'s girlfriend? Could you calculate her current age and raise it to the power of 0.43?\"\\n)\\n```  \\nThese tasks serve as a starting point for running your LangChain experiments within Flyte.', metadata={'Header 1': 'Set Serp API key', 'Header 3': 'Agent'}),\n",
       " Document(page_content='To execute the Flyte tasks on the configured Flyte backend, use the following command:  \\n```bash\\npyflyte run --image <your-image> langchain_flyte.py langchain_llm\\n```  \\nThis command will initiate the execution of the `langchain_llm` task on the Flyte backend. You can trigger the remaining two tasks in a similar manner.  \\nThe metrics will be displayed on the Flyte UI as follows:  \\n![LangChain LLM](https://ik.imagekit.io/c8zl7irwkdda/Screenshot_2023-06-20_at_1.23.29_PM_MZYeG0dKa.png?updatedAt=1687247642993)', metadata={'Header 1': 'Set Serp API key', 'Header 2': 'Execute the Flyte Tasks on Kubernetes'}),\n",
       " Document(page_content='This page covers how to use the SerpAPI search APIs within LangChain.\\nIt is broken into two parts: installation and setup, and then references to the specific SerpAPI wrapper.', metadata={'Header 1': 'SerpAPI'}),\n",
       " Document(page_content='- Install requirements with `pip install google-search-results`\\n- Get a SerpAPI api key and either set it as an environment variable (`SERPAPI_API_KEY`)', metadata={'Header 1': 'SerpAPI', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a SerpAPI utility which wraps this API. To import this utility:  \\n```python\\nfrom langchain.utilities import SerpAPIWrapper\\n```  \\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/modules/agents/tools/integrations/serpapi.html).', metadata={'Header 1': 'SerpAPI', 'Header 2': 'Wrappers', 'Header 3': 'Utility'}),\n",
       " Document(page_content='You can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"serpapi\"])\\n```  \\nFor more information on this, see [this page](/docs/modules/agents/tools)', metadata={'Header 1': 'SerpAPI', 'Header 2': 'Wrappers', 'Header 3': 'Tool'}),\n",
       " Document(page_content='>[Google Cloud Storage](https://en.wikipedia.org/wiki/Google_Cloud_Storage) is a managed service for storing unstructured data.', metadata={'Header 1': 'Google Cloud Storage'}),\n",
       " Document(page_content='First, you need to install `google-cloud-bigquery` python package.  \\n```bash\\npip install google-cloud-storage\\n```', metadata={'Header 1': 'Google Cloud Storage', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There are two loaders for the `Google Cloud Storage`: the `Directory` and the `File` loaders.  \\nSee a [usage example](/docs/modules/data_connection/document_loaders/integrations/google_cloud_storage_directory.html).  \\n```python\\nfrom langchain.document_loaders import GCSDirectoryLoader\\n```\\nSee a [usage example](/docs/modules/data_connection/document_loaders/integrations/google_cloud_storage_file.html).  \\n```python\\nfrom langchain.document_loaders import GCSFileLoader\\n```', metadata={'Header 1': 'Google Cloud Storage', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Microsoft Word](https://www.microsoft.com/en-us/microsoft-365/word) is a word processor developed by Microsoft.', metadata={'Header 1': 'Microsoft Word'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'Microsoft Word', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/microsoft_word.html).  \\n```python\\nfrom langchain.document_loaders import UnstructuredWordDocumentLoader\\n```', metadata={'Header 1': 'Microsoft Word', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the [Helicone](https://helicone.ai) ecosystem within LangChain.', metadata={'Header 1': 'Helicone'}),\n",
       " Document(page_content='Helicone is an [open source](https://github.com/Helicone/helicone) observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.  \\n![Helicone](/img/HeliconeDashboard.png)', metadata={'Header 1': 'Helicone', 'Header 2': 'What is Helicone?'}),\n",
       " Document(page_content='With your LangChain environment you can just add the following parameter.  \\n```bash\\nexport OPENAI_API_BASE=\"https://oai.hconeai.com/v1\"\\n```  \\nNow head over to [helicone.ai](https://helicone.ai/onboarding?step=2) to create your account, and add your OpenAI API key within our dashboard to view your logs.  \\n![Helicone](/img/HeliconeKeys.png)', metadata={'Header 1': 'Helicone', 'Header 2': 'Quick start'}),\n",
       " Document(page_content='```python\\nfrom langchain.llms import OpenAI\\nimport openai\\nopenai.api_base = \"https://oai.hconeai.com/v1\"  \\nllm = OpenAI(temperature=0.9, headers={\"Helicone-Cache-Enabled\": \"true\"})\\ntext = \"What is a helicone?\"\\nprint(llm(text))\\n```  \\n[Helicone caching docs](https://docs.helicone.ai/advanced-usage/caching)', metadata={'Header 1': 'Helicone', 'Header 2': 'How to enable Helicone caching'}),\n",
       " Document(page_content='```python\\nfrom langchain.llms import OpenAI\\nimport openai\\nopenai.api_base = \"https://oai.hconeai.com/v1\"  \\nllm = OpenAI(temperature=0.9, headers={\\n\"Helicone-Property-Session\": \"24\",\\n\"Helicone-Property-Conversation\": \"support_issue_2\",\\n\"Helicone-Property-App\": \"mobile\",\\n})\\ntext = \"What is a helicone?\"\\nprint(llm(text))\\n```  \\n[Helicone property docs](https://docs.helicone.ai/advanced-usage/custom-properties)', metadata={'Header 1': 'Helicone', 'Header 2': 'How to use Helicone custom properties'}),\n",
       " Document(page_content='>[Chroma](https://docs.trychroma.com/getting-started) is a database for building AI applications with embeddings.', metadata={'Header 1': 'Chroma'}),\n",
       " Document(page_content='```bash\\npip install chromadb\\n```', metadata={'Header 1': 'Chroma', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around Chroma vector databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\n```python\\nfrom langchain.vectorstores import Chroma\\n```  \\nFor a more detailed walkthrough of the Chroma wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/chroma.html)', metadata={'Header 1': 'Chroma', 'Header 2': 'VectorStore'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/retrievers/how_to/self_query/chroma_self_query.html).  \\n```python\\nfrom langchain.retrievers import SelfQueryRetriever\\n```', metadata={'Header 1': 'Chroma', 'Header 2': 'Retriever'}),\n",
       " Document(page_content='This page covers how to use the Weaviate ecosystem within LangChain.  \\nWhat is Weaviate?  \\n**Weaviate in a nutshell:**\\n- Weaviate is an open-source \\u200bdatabase of the type \\u200bvector search engine.\\n- Weaviate allows you to store JSON documents in a class property-like fashion while attaching machine learning vectors to these documents to represent them in vector space.\\n- Weaviate can be used stand-alone (aka bring your vectors) or with a variety of modules that can do the vectorization for you and extend the core capabilities.\\n- Weaviate has a GraphQL-API to access your data easily.\\n- We aim to bring your vector search set up to production to query in mere milliseconds (check our [open source benchmarks](https://weaviate.io/developers/weaviate/current/benchmarks/) to see if Weaviate fits your use case).\\n- Get to know Weaviate in the [basics getting started guide](https://weaviate.io/developers/weaviate/current/core-knowledge/basics.html) in under five minutes.  \\n**Weaviate in detail:**  \\nWeaviate is a low-latency vector search engine with out-of-the-box support for different media types (text, images, etc.). It offers Semantic Search, Question-Answer Extraction, Classification, Customizable Models (PyTorch/TensorFlow/Keras), etc. Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering and the fault tolerance of a cloud-native database. It is all accessible through GraphQL, REST, and various client-side programming languages.', metadata={'Header 1': 'Weaviate'}),\n",
       " Document(page_content='- Install the Python SDK with `pip install weaviate-client`', metadata={'Header 1': 'Weaviate', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around Weaviate indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import Weaviate\\n```  \\nFor a more detailed walkthrough of the Weaviate wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/weaviate.html)', metadata={'Header 1': 'Weaviate', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='>[Twitter](https://twitter.com/) is an online social media and social networking service.', metadata={'Header 1': 'Twitter'}),\n",
       " Document(page_content='```bash\\npip install tweepy\\n```  \\nWe must initialize the loader with the `Twitter API` token, and we need to set up the Twitter `username`.', metadata={'Header 1': 'Twitter', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/twitter.html).  \\n```python\\nfrom langchain.document_loaders import TwitterTweetLoader\\n```', metadata={'Header 1': 'Twitter', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='> [Tigris](htttps://tigrisdata.com) is an open source Serverless NoSQL Database and Search Platform designed to simplify building high-performance vector search applications.\\n> `Tigris` eliminates the infrastructure complexity of managing, operating, and synchronizing multiple tools, allowing you to focus on building great applications instead.', metadata={'Header 1': 'Tigris'}),\n",
       " Document(page_content='```bash\\npip install tigrisdb openapi-schema-pydantic openai tiktoken\\n```', metadata={'Header 1': 'Tigris', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/vectorstores/integrations/tigris.html).  \\n```python\\nfrom langchain.vectorstores import Tigris\\n```', metadata={'Header 1': 'Tigris', 'Header 2': 'Vector Store'}),\n",
       " Document(page_content='This page covers how to use the Milvus ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Milvus wrappers.', metadata={'Header 1': 'Milvus'}),\n",
       " Document(page_content='- Install the Python SDK with `pip install pymilvus`', metadata={'Header 1': 'Milvus', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around Milvus indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import Milvus\\n```  \\nFor a more detailed walkthrough of the Miluvs wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/milvus.html)', metadata={'Header 1': 'Milvus', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='>[OpenWeatherMap](https://openweathermap.org/api/) provides all essential weather data for a specific location:\\n>- Current weather\\n>- Minute forecast for 1 hour\\n>- Hourly forecast for 48 hours\\n>- Daily forecast for 8 days\\n>- National weather alerts\\n>- Historical weather data for 40+ years back  \\nThis page covers how to use the `OpenWeatherMap API` within LangChain.', metadata={'Header 1': 'OpenWeatherMap'}),\n",
       " Document(page_content='- Install requirements with\\n```bash\\npip install pyowm\\n```\\n- Go to OpenWeatherMap and sign up for an account to get your API key [here](https://openweathermap.org/api/)\\n- Set your API key as `OPENWEATHERMAP_API_KEY` environment variable', metadata={'Header 1': 'OpenWeatherMap', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a OpenWeatherMapAPIWrapper utility which wraps this API. To import this utility:  \\n```python\\nfrom langchain.utilities.openweathermap import OpenWeatherMapAPIWrapper\\n```  \\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/modules/agents/tools/integrations/openweathermap.html).', metadata={'Header 1': 'OpenWeatherMap', 'Header 2': 'Wrappers', 'Header 3': 'Utility'}),\n",
       " Document(page_content='You can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:  \\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"openweathermap-api\"])\\n```  \\nFor more information on tools, see [this page](/docs/modules/agents/tools/).', metadata={'Header 1': 'OpenWeatherMap', 'Header 2': 'Wrappers', 'Header 3': 'Tool'}),\n",
       " Document(page_content='>[Bilibili](https://www.bilibili.tv/) is one of the most beloved long-form video sites in China.', metadata={'Header 1': 'BiliBili'}),\n",
       " Document(page_content='```bash\\npip install bilibili-api-python\\n```', metadata={'Header 1': 'BiliBili', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/bilibili.html).  \\n```python\\nfrom langchain.document_loaders import BiliBiliLoader\\n```', metadata={'Header 1': 'BiliBili', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the Jina ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Jina wrappers.', metadata={'Header 1': 'Jina'}),\n",
       " Document(page_content='- Install the Python SDK with `pip install jina`\\n- Get a Jina AI Cloud auth token from [here](https://cloud.jina.ai/settings/tokens) and set it as an environment variable (`JINA_AUTH_TOKEN`)', metadata={'Header 1': 'Jina', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a Jina Embeddings wrapper, which you can access with\\n```python\\nfrom langchain.embeddings import JinaEmbeddings\\n```\\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/data_connection/text_embedding/integrations/jina.html)', metadata={'Header 1': 'Jina', 'Header 2': 'Wrappers', 'Header 3': 'Embeddings'}),\n",
       " Document(page_content='This page covers how to use the SearxNG search API within LangChain.\\nIt is broken into two parts: installation and setup, and then references to the specific SearxNG API wrapper.', metadata={'Header 1': 'SearxNG Search API'}),\n",
       " Document(page_content='While it is possible to utilize the wrapper in conjunction with  [public searx\\ninstances](https://searx.space/) these instances frequently do not permit API\\naccess (see note on output format below) and have limitations on the frequency\\nof requests. It is recommended to opt for a self-hosted instance instead.', metadata={'Header 1': 'SearxNG Search API', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content=\"See [this page](https://searxng.github.io/searxng/admin/installation.html) for installation instructions.  \\nWhen you install SearxNG, the only active output format by default is the HTML format.\\nYou need to activate the `json` format to use the API. This can be done by adding the following line to the `settings.yml` file:\\n```yaml\\nsearch:\\nformats:\\n- html\\n- json\\n```\\nYou can make sure that the API is working by issuing a curl request to the API endpoint:  \\n`curl -kLX GET --data-urlencode q='langchain' -d format=json http://localhost:8888`  \\nThis should return a JSON object with the results.\", metadata={'Header 1': 'SearxNG Search API', 'Header 2': 'Installation and Setup', 'Header 3': 'Self Hosted Instance:'}),\n",
       " Document(page_content='To use the wrapper we need to pass the host of the SearxNG instance to the wrapper with:\\n1. the named parameter `searx_host` when creating the instance.\\n2. exporting the environment variable `SEARXNG_HOST`.  \\nYou can use the wrapper to get results from a SearxNG instance.  \\n```python\\nfrom langchain.utilities import SearxSearchWrapper\\ns = SearxSearchWrapper(searx_host=\"http://localhost:8888\")\\ns.run(\"what is a large language model?\")\\n```', metadata={'Header 1': 'SearxNG Search API', 'Header 2': 'Wrappers', 'Header 3': 'Utility'}),\n",
       " Document(page_content='You can also load this wrapper as a Tool (to use with an Agent).  \\nYou can do this with:  \\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"searx-search\"],\\nsearx_host=\"http://localhost:8888\",\\nengines=[\"github\"])\\n```  \\nNote that we could _optionally_ pass custom engines to use.  \\nIf you want to obtain results with metadata as *json* you can use:\\n```python\\ntools = load_tools([\"searx-search-results-json\"],\\nsearx_host=\"http://localhost:8888\",\\nnum_results=5)\\n```  \\n#### Quickly creating tools  \\nThis examples showcases a quick way to create multiple tools from the same\\nwrapper.  \\n```python\\nfrom langchain.tools.searx_search.tool import SearxSearchResults  \\nwrapper = SearxSearchWrapper(searx_host=\"**\")\\ngithub_tool = SearxSearchResults(name=\"Github\", wrapper=wrapper,\\nkwargs = {\\n\"engines\": [\"github\"],\\n})  \\narxiv_tool = SearxSearchResults(name=\"Arxiv\", wrapper=wrapper,\\nkwargs = {\\n\"engines\": [\"arxiv\"]\\n})\\n```  \\nFor more information on tools, see [this page](/docs/modules/agents/tools/).', metadata={'Header 1': 'SearxNG Search API', 'Header 2': 'Wrappers', 'Header 3': 'Tool'}),\n",
       " Document(page_content='This page covers how to use the ForefrontAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific ForefrontAI wrappers.', metadata={'Header 1': 'ForefrontAI'}),\n",
       " Document(page_content='- Get an ForefrontAI api key and set it as an environment variable (`FOREFRONTAI_API_KEY`)', metadata={'Header 1': 'ForefrontAI', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an ForefrontAI LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import ForefrontAI\\n```', metadata={'Header 1': 'ForefrontAI', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='>[Docugami](https://docugami.com) converts business documents into a Document XML Knowledge Graph, generating forests\\n> of XML semantic trees representing entire documents. This is a rich representation that includes the semantic and\\n> structural characteristics of various chunks in the document as an XML tree.', metadata={'Header 1': 'Docugami'}),\n",
       " Document(page_content='```bash\\npip install lxml\\n```', metadata={'Header 1': 'Docugami', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/docugami.html).  \\n```python\\nfrom langchain.document_loaders import DocugamiLoader\\n```', metadata={'Header 1': 'Docugami', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Databerry](https://databerry.ai) is an [open source](https://github.com/gmpetrov/databerry) document retrieval platform that helps to connect your personal data with Large Language Models.', metadata={'Header 1': 'Databerry'}),\n",
       " Document(page_content='We need to sign up for Databerry, create a datastore, add some data and get your datastore api endpoint url.\\nWe need the [API Key](https://docs.databerry.ai/api-reference/authentication).', metadata={'Header 1': 'Databerry', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/retrievers/integrations/databerry.html).  \\n```python\\nfrom langchain.retrievers import DataberryRetriever\\n```', metadata={'Header 1': 'Databerry', 'Header 2': 'Retriever'}),\n",
       " Document(page_content='>[IMSDb](https://imsdb.com/) is the `Internet Movie Script Database`.\\n>', metadata={'Header 1': 'IMSDb'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'IMSDb', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/imsdb.html).  \\n```python\\nfrom langchain.document_loaders import IMSDbLoader\\n```', metadata={'Header 1': 'IMSDb', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the Qdrant ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Qdrant wrappers.', metadata={'Header 1': 'Qdrant'}),\n",
       " Document(page_content='- Install the Python SDK with `pip install qdrant-client`', metadata={'Header 1': 'Qdrant', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around Qdrant indexes, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import Qdrant\\n```  \\nFor a more detailed walkthrough of the Qdrant wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/qdrant.html)', metadata={'Header 1': 'Qdrant', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='>[AZLyrics](https://www.azlyrics.com/) is a large, legal, every day growing collection of lyrics.', metadata={'Header 1': 'AZLyrics'}),\n",
       " Document(page_content=\"There isn't any special setup for it.\", metadata={'Header 1': 'AZLyrics', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/azlyrics.html).  \\n```python\\nfrom langchain.document_loaders import AZLyricsLoader\\n```', metadata={'Header 1': 'AZLyrics', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Elasticsearch](https://www.elastic.co/elasticsearch/) is a distributed, RESTful search and analytics engine.\\n> It provides a distributed, multi-tenant-capable full-text search engine with an HTTP web interface and schema-free\\n> JSON documents.', metadata={'Header 1': 'Elasticsearch'}),\n",
       " Document(page_content='```bash\\npip install elasticsearch\\n```', metadata={'Header 1': 'Elasticsearch', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content=\">In information retrieval, [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25) (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones, and others.  \\n>The name of the actual ranking function is BM25. The fuller name, Okapi BM25, includes the name of the first system to use it, which was the Okapi information retrieval system, implemented at London's City University in the 1980s and 1990s. BM25 and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent TF-IDF-like retrieval functions used in document retrieval.  \\nSee a [usage example](/docs/modules/data_connection/retrievers/integrations/elastic_search_bm25.html).  \\n```python\\nfrom langchain.retrievers import ElasticSearchBM25Retriever\\n```\", metadata={'Header 1': 'Elasticsearch', 'Header 2': 'Retriever'}),\n",
       " Document(page_content='> [Annoy](https://github.com/spotify/annoy) (`Approximate Nearest Neighbors Oh Yeah`) is a C++ library with Python bindings to search for points in space that are close to a given query point. It also creates large read-only file-based data structures that are mmapped into memory so that many processes may share the same data.', metadata={'Header 1': 'Annoy'}),\n",
       " Document(page_content='```bash\\npip install annoy\\n```', metadata={'Header 1': 'Annoy', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/vectorstores/integrations/annoy.html).  \\n```python\\nfrom langchain.vectorstores import Annoy\\n```', metadata={'Header 1': 'Annoy', 'Header 2': 'Vectorstore'}),\n",
       " Document(page_content='This page covers how to use the Deep Lake ecosystem within LangChain.', metadata={'Header 1': 'Deep Lake'}),\n",
       " Document(page_content=\"- More than just a (multi-modal) vector store. You can later use the dataset to fine-tune your own LLM models.\\n- Not only stores embeddings, but also the original data with automatic version control.\\n- Truly serverless. Doesn't require another service and can be used with major cloud providers (AWS S3, GCS, etc.)\", metadata={'Header 1': 'Deep Lake', 'Header 2': 'Why Deep Lake?'}),\n",
       " Document(page_content='1. [Ultimate Guide to LangChain & Deep Lake: Build ChatGPT to Answer Questions on Your Financial Data](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/)\\n2. [Twitter the-algorithm codebase analysis with Deep Lake](../use_cases/code/twitter-the-algorithm-analysis-deeplake.html)\\n3. Here is [whitepaper](https://www.deeplake.ai/whitepaper) and [academic paper](https://arxiv.org/pdf/2209.10785.pdf) for Deep Lake\\n4. Here is a set of additional resources available for review: [Deep Lake](https://github.com/activeloopai/deeplake), [Get started](https://docs.activeloop.ai/getting-started) and\\xa0[Tutorials](https://docs.activeloop.ai/hub-tutorials)', metadata={'Header 1': 'Deep Lake', 'Header 2': 'More Resources'}),\n",
       " Document(page_content='- Install the Python package with `pip install deeplake`', metadata={'Header 1': 'Deep Lake', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around Deep Lake, a data lake for Deep Learning applications, allowing you to use it as a vector store (for now), whether for semantic search or example selection.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import DeepLake\\n```  \\nFor a more detailed walkthrough of the Deep Lake wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/deeplake.html)', metadata={'Header 1': 'Deep Lake', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='This page covers how to run models on Replicate within LangChain.', metadata={'Header 1': 'Replicate'}),\n",
       " Document(page_content='- Create a [Replicate](https://replicate.com) account. Get your API key and set it as an environment variable (`REPLICATE_API_TOKEN`)\\n- Install the [Replicate python client](https://github.com/replicate/replicate-python) with `pip install replicate`', metadata={'Header 1': 'Replicate', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='Find a model on the [Replicate explore page](https://replicate.com/explore), and then paste in the model name and version in this format: `owner-name/model-name:version`  \\nFor example, for this [dolly model](https://replicate.com/replicate/dolly-v2-12b), click on the API tab. The model name/version would be: `\"replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5\"`  \\nOnly the `model` param is required, but any other model parameters can also be passed in with the format `input={model_param: value, ...}`  \\nFor example, if we were running stable diffusion and wanted to change the image dimensions:  \\n```\\nReplicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\", input={\\'image_dimensions\\': \\'512x512\\'})\\n```  \\n*Note that only the first output of a model will be returned.*\\nFrom here, we can initialize our model:  \\n```python\\nllm = Replicate(model=\"replicate/dolly-v2-12b:ef0e1aefc61f8e096ebe4db6b2bacc297daf2ef6899f0f7e001ec445893500e5\")\\n```  \\nAnd run it:  \\n```python\\nprompt = \"\"\"\\nAnswer the following yes/no question by reasoning step by step.\\nCan a dog drive a car?\\n\"\"\"\\nllm(prompt)\\n```  \\nWe can call any Replicate model (not just LLMs) using this syntax. For example, we can call [Stable Diffusion](https://replicate.com/stability-ai/stable-diffusion):  \\n```python\\ntext2image = Replicate(model=\"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\", input={\\'image_dimensions\\':\\'512x512\\'})  \\nimage_output = text2image(\"A cat riding a motorcycle by Picasso\")\\n```', metadata={'Header 1': 'Replicate', 'Header 2': 'Calling a model'}),\n",
       " Document(page_content='This page covers how to use the StochasticAI ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific StochasticAI wrappers.', metadata={'Header 1': 'StochasticAI'}),\n",
       " Document(page_content='- Install with `pip install stochasticx`\\n- Get an StochasticAI api key and set it as an environment variable (`STOCHASTICAI_API_KEY`)', metadata={'Header 1': 'StochasticAI', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an StochasticAI LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import StochasticAI\\n```', metadata={'Header 1': 'StochasticAI', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='This page covers how to use the Postgres [PGVector](https://github.com/pgvector/pgvector) ecosystem within LangChain\\nIt is broken into two parts: installation and setup, and then references to specific PGVector wrappers.', metadata={'Header 1': 'PGVector'}),\n",
       " Document(page_content='- Install the Python package with `pip install pgvector`', metadata={'Header 1': 'PGVector', 'Header 2': 'Installation'}),\n",
       " Document(page_content='1. The first step is to create a database with the `pgvector` extension installed.  \\nFollow the steps at [PGVector Installation Steps](https://github.com/pgvector/pgvector#installation) to install the database and the extension. The docker image is the easiest way to get started.', metadata={'Header 1': 'PGVector', 'Header 2': 'Setup'}),\n",
       " Document(page_content='There exists a wrapper around Postgres vector databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores.pgvector import PGVector\\n```', metadata={'Header 1': 'PGVector', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='For a more detailed walkthrough of the PGVector Wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/pgvector.html)', metadata={'Header 1': 'PGVector', 'Header 2': 'Wrappers', 'Header 3': 'Usage'}),\n",
       " Document(page_content='>[Airbyte](https://github.com/airbytehq/airbyte) is a data integration platform for ELT pipelines from APIs,\\n> databases & files to warehouses & lakes. It has the largest catalog of ELT connectors to data warehouses and databases.', metadata={'Header 1': 'Airbyte'}),\n",
       " Document(page_content=\"This instruction shows how to load any source from `Airbyte` into a local `JSON` file that can be read in as a document.  \\n**Prerequisites:**\\nHave `docker desktop` installed.  \\n**Steps:**\\n1. Clone Airbyte from GitHub - `git clone https://github.com/airbytehq/airbyte.git`.\\n2. Switch into Airbyte directory - `cd airbyte`.\\n3. Start Airbyte - `docker compose up`.\\n4. In your browser, just visit http://localhost:8000. You will be asked for a username and password. By default, that's username `airbyte` and password `password`.\\n5. Setup any source you wish.\\n6. Set destination as Local JSON, with specified destination path - lets say `/json_data`. Set up a manual sync.\\n7. Run the connection.\\n8. To see what files are created, navigate to: `file:///tmp/airbyte_local/`.\", metadata={'Header 1': 'Airbyte', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/airbyte_json.html).  \\n```python\\nfrom langchain.document_loaders import AirbyteJSONLoader\\n```', metadata={'Header 1': 'Airbyte', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Google Drive](https://en.wikipedia.org/wiki/Google_Drive) is a file storage and synchronization service developed by Google.  \\nCurrently, only `Google Docs` are supported.', metadata={'Header 1': 'Google Drive'}),\n",
       " Document(page_content='First, you need to install several python package.  \\n```bash\\npip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\\n```', metadata={'Header 1': 'Google Drive', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example and authorizing instructions](/docs/modules/data_connection/document_loaders/integrations/google_drive.html).  \\n```python\\nfrom langchain.document_loaders import GoogleDriveLoader\\n```', metadata={'Header 1': 'Google Drive', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='This page covers how to use the Grobid to parse articles for LangChain.\\nIt is seperated into two parts: installation and running the server', metadata={'Header 1': 'Grobid'}),\n",
       " Document(page_content='#Ensure You have Java installed\\n!apt-get install -y openjdk-11-jdk -q\\n!update-alternatives --set java /usr/lib/jvm/java-11-openjdk-amd64/bin/java  \\n#Clone and install the Grobid Repo\\nimport os\\n!git clone https://github.com/kermitt2/grobid.git\\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\\nos.chdir(\\'grobid\\')\\n!./gradlew clean install  \\n#Run the server,\\nget_ipython().system_raw(\\'nohup ./gradlew run > grobid.log 2>&1 &\\')  \\nYou can now use the GrobidParser to produce documents\\n```python\\nfrom langchain.document_loaders.parsers import GrobidParser\\nfrom langchain.document_loaders.generic import GenericLoader  \\n#Produce chunks from article paragraphs\\nloader = GenericLoader.from_filesystem(\\n\"/Users/31treehaus/Desktop/Papers/\",\\nglob=\"*\",\\nsuffixes=[\".pdf\"],\\nparser= GrobidParser(segment_sentences=False)\\n)\\ndocs = loader.load()  \\n#Produce chunks from article sentences\\nloader = GenericLoader.from_filesystem(\\n\"/Users/31treehaus/Desktop/Papers/\",\\nglob=\"*\",\\nsuffixes=[\".pdf\"],\\nparser= GrobidParser(segment_sentences=True)\\n)\\ndocs = loader.load()\\n```\\nChunk metadata will include bboxes although these are a bit funky to parse, see https://grobid.readthedocs.io/en/latest/Coordinates-in-PDF/', metadata={'Header 1': 'Grobid', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='>[Infino](https://github.com/infinohq/infino) is an open-source observability platform that stores both metrics and application logs together.  \\nKey features of infino include:\\n- Metrics Tracking: Capture time taken by LLM model to handle request, errors, number of tokens, and costing indication for the particular LLM.\\n- Data Tracking: Log and store prompt, request, and response data for each LangChain interaction.\\n- Graph Visualization: Generate basic graphs over time, depicting metrics such as request duration, error occurrences, token count, and cost.', metadata={'Header 1': 'Infino'}),\n",
       " Document(page_content=\"First, you'll need to install the  `infinopy` Python package as follows:  \\n```bash\\npip install infinopy\\n```  \\nIf you already have an Infino Server running, then you're good to go; but if\\nyou don't, follow the next steps to start it:  \\n- Make sure you have Docker installed\\n- Run the following in your terminal:\\n```\\ndocker run --rm --detach --name infino-example -p 3000:3000 infinohq/infino:latest\\n```\", metadata={'Header 1': 'Infino', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example of `InfinoCallbackHandler`](/docs/modules/callbacks/integrations/infino.html).  \\n```python\\nfrom langchain.callbacks import InfinoCallbackHandler\\n```', metadata={'Header 1': 'Infino', 'Header 2': 'Using Infino'}),\n",
       " Document(page_content='This page covers how to use [Apify](https://apify.com) within LangChain.', metadata={'Header 1': 'Apify'}),\n",
       " Document(page_content='Apify is a cloud platform for web scraping and data extraction,\\nwhich provides an [ecosystem](https://apify.com/store) of more than a thousand\\nready-made apps called *Actors* for various scraping, crawling, and extraction use cases.  \\n[![Apify Actors](/img/ApifyActors.png)](https://apify.com/store)  \\nThis integration enables you run Actors on the Apify platform and load their results into LangChain to feed your vector\\nindexes with documents and data from the web, e.g. to generate answers from websites with documentation,\\nblogs, or knowledge bases.', metadata={'Header 1': 'Apify', 'Header 2': 'Overview'}),\n",
       " Document(page_content='- Install the Apify API client for Python with `pip install apify-client`\\n- Get your [Apify API token](https://console.apify.com/account/integrations) and either set it as\\nan environment variable (`APIFY_API_TOKEN`) or pass it to the `ApifyWrapper` as `apify_api_token` in the constructor.', metadata={'Header 1': 'Apify', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='You can use the `ApifyWrapper` to run Actors on the Apify platform.  \\n```python\\nfrom langchain.utilities import ApifyWrapper\\n```  \\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/modules/agents/tools/integrations/apify.html).', metadata={'Header 1': 'Apify', 'Header 2': 'Wrappers', 'Header 3': 'Utility'}),\n",
       " Document(page_content='You can also use our `ApifyDatasetLoader` to get data from Apify dataset.  \\n```python\\nfrom langchain.document_loaders import ApifyDatasetLoader\\n```  \\nFor a more detailed walkthrough of this loader, see [this notebook](/docs/modules/data_connection/document_loaders/integrations/apify_dataset.html).', metadata={'Header 1': 'Apify', 'Header 2': 'Wrappers', 'Header 3': 'Loader'}),\n",
       " Document(page_content='This page covers how to use the [C Transformers](https://github.com/marella/ctransformers) library within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific C Transformers wrappers.', metadata={'Header 1': 'C Transformers'}),\n",
       " Document(page_content='- Install the Python package with `pip install ctransformers`\\n- Download a supported [GGML model](https://huggingface.co/TheBloke) (see [Supported Models](https://github.com/marella/ctransformers#supported-models))', metadata={'Header 1': 'C Transformers', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content=\"There exists a CTransformers LLM wrapper, which you can access with:  \\n```python\\nfrom langchain.llms import CTransformers\\n```  \\nIt provides a unified interface for all models:  \\n```python\\nllm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2')  \\nprint(llm('AI is going to'))\\n```  \\nIf you are getting `illegal instruction` error, try using `lib='avx'` or `lib='basic'`:  \\n```py\\nllm = CTransformers(model='/path/to/ggml-gpt-2.bin', model_type='gpt2', lib='avx')\\n```  \\nIt can be used with models hosted on the Hugging Face Hub:  \\n```py\\nllm = CTransformers(model='marella/gpt-2-ggml')\\n```  \\nIf a model repo has multiple model files (`.bin` files), specify a model file using:  \\n```py\\nllm = CTransformers(model='marella/gpt-2-ggml', model_file='ggml-model.bin')\\n```  \\nAdditional parameters can be passed using the `config` parameter:  \\n```py\\nconfig = {'max_new_tokens': 256, 'repetition_penalty': 1.1}  \\nllm = CTransformers(model='marella/gpt-2-ggml', config=config)\\n```  \\nSee [Documentation](https://github.com/marella/ctransformers#config) for a list of available parameters.  \\nFor a more detailed walkthrough of this, see [this notebook](/docs/modules/model_io/models/llms/integrations/ctransformers.html).\", metadata={'Header 1': 'C Transformers', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content='>[Discord](https://discord.com/) is a VoIP and instant messaging social platform. Users have the ability to communicate\\n> with voice calls, video calls, text messaging, media and files in private chats or as part of communities called\\n> \"servers\". A server is a collection of persistent chat rooms and voice channels which can be accessed via invite links.', metadata={'Header 1': 'Discord'}),\n",
       " Document(page_content=\"```bash\\npip install pandas\\n```  \\nFollow these steps to download your `Discord` data:  \\n1. Go to your **User Settings**\\n2. Then go to **Privacy and Safety**\\n3. Head over to the **Request all of my Data** and click on **Request Data** button  \\nIt might take 30 days for you to receive your data. You'll receive an email at the address which is registered\\nwith Discord. That email will have a download button using which you would be able to download your personal Discord data.\", metadata={'Header 1': 'Discord', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/discord.html).  \\n```python\\nfrom langchain.document_loaders import DiscordChatLoader\\n```', metadata={'Header 1': 'Discord', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Stripe](https://stripe.com/en-ca) is an Irish-American financial services and software as a service (SaaS) company. It offers payment-processing software and application programming interfaces for e-commerce websites and mobile applications.', metadata={'Header 1': 'Stripe'}),\n",
       " Document(page_content='See [setup instructions](/docs/modules/data_connection/document_loaders/integrations/stripe.html).', metadata={'Header 1': 'Stripe', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/stripe.html).  \\n```python\\nfrom langchain.document_loaders import StripeLoader\\n```', metadata={'Header 1': 'Stripe', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[scikit-learn](https://scikit-learn.org/stable/) is an open source collection of machine learning algorithms,\\n> including some implementations of the [k nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html). `SKLearnVectorStore` wraps this implementation and adds the possibility to persist the vector store in json, bson (binary json) or Apache Parquet format.', metadata={'Header 1': 'scikit-learn'}),\n",
       " Document(page_content='- Install the Python package with `pip install scikit-learn`', metadata={'Header 1': 'scikit-learn', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='`SKLearnVectorStore` provides a simple wrapper around the nearest neighbor implementation in the\\nscikit-learn package, allowing you to use it as a vectorstore.  \\nTo import this vectorstore:  \\n```python\\nfrom langchain.vectorstores import SKLearnVectorStore\\n```  \\nFor a more detailed walkthrough of the SKLearnVectorStore wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/sklearn.html).', metadata={'Header 1': 'scikit-learn', 'Header 2': 'Vector Store'}),\n",
       " Document(page_content='This page covers how to use [Yeager.ai](https://yeager.ai) to generate LangChain tools and agents.', metadata={'Header 1': 'Yeager.ai'}),\n",
       " Document(page_content='Yeager.ai is an ecosystem designed to simplify the process of creating AI agents and tools.  \\nIt features yAgents, a No-code LangChain Agent Builder, which enables users to build, test, and deploy AI solutions with ease. Leveraging the LangChain framework, yAgents allows seamless integration with various language models and resources, making it suitable for developers, researchers, and AI enthusiasts across diverse applications.', metadata={'Header 1': 'Yeager.ai', 'Header 2': 'What is Yeager.ai?'}),\n",
       " Document(page_content='Low code generative agent designed to help you build, prototype, and deploy Langchain tools with ease.', metadata={'Header 1': 'Yeager.ai', 'Header 2': 'yAgents'}),\n",
       " Document(page_content='```\\npip install yeagerai-agent\\nyeagerai-agent\\n```\\nGo to http://127.0.0.1:7860  \\nThis will install the necessary dependencies and set up yAgents on your system. After the first run, yAgents will create a .env file where you can input your OpenAI API key. You can do the same directly from the Gradio interface under the tab \"Settings\".  \\n`OPENAI_API_KEY=<your_openai_api_key_here>`  \\nWe recommend using GPT-4,. However, the tool can also work with GPT-3 if the problem is broken down sufficiently.', metadata={'Header 1': 'Yeager.ai', 'Header 2': 'yAgents', 'Header 3': 'How to use?'}),\n",
       " Document(page_content=\"yAgents makes it easy to create and execute AI-powered tools. Here's a brief overview of the process:\\n1. Create a tool: To create a tool, provide a natural language prompt to yAgents. The prompt should clearly describe the tool's purpose and functionality. For example:\\n`create a tool that returns the n-th prime number`  \\n2. Load the tool into the toolkit: To load a tool into yAgents, simply provide a command to yAgents that says so. For example:\\n`load the tool that you just created it into your toolkit`  \\n3. Execute the tool: To run a tool or agent, simply provide a command to yAgents that includes the name of the tool and any required parameters. For example:\\n`generate the 50th prime number`  \\nYou can see a video of how it works [here](https://www.youtube.com/watch?v=KA5hCM3RaWE).  \\nAs you become more familiar with yAgents, you can create more advanced tools and agents to automate your work and enhance your productivity.  \\nFor more information, see [yAgents' Github](https://github.com/yeagerai/yeagerai-agent) or our [docs](https://yeagerai.gitbook.io/docs/general/welcome-to-yeager.ai)\", metadata={'Header 1': 'Yeager.ai', 'Header 2': 'yAgents', 'Header 3': 'Creating and Executing Tools with yAgents'}),\n",
       " Document(page_content='This page covers how to use the [Runhouse](https://github.com/run-house/runhouse) ecosystem within LangChain.\\nIt is broken into three parts: installation and setup, LLMs, and Embeddings.', metadata={'Header 1': 'Runhouse'}),\n",
       " Document(page_content=\"- Install the Python SDK with `pip install runhouse`\\n- If you'd like to use on-demand cluster, check your cloud credentials with `sky check`\", metadata={'Header 1': 'Runhouse', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='For a basic self-hosted LLM, you can use the `SelfHostedHuggingFaceLLM` class. For more\\ncustom LLMs, you can use the `SelfHostedPipeline` parent class.  \\n```python\\nfrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\\n```  \\nFor a more detailed walkthrough of the Self-hosted LLMs, see [this notebook](/docs/modules/model_io/models/llms/integrations/runhouse.html)', metadata={'Header 1': 'Runhouse', 'Header 2': 'Self-hosted LLMs'}),\n",
       " Document(page_content='There are several ways to use self-hosted embeddings with LangChain via Runhouse.  \\nFor a basic self-hosted embedding from a Hugging Face Transformers model, you can use\\nthe `SelfHostedEmbedding` class.\\n```python\\nfrom langchain.llms import SelfHostedPipeline, SelfHostedHuggingFaceLLM\\n```  \\nFor a more detailed walkthrough of the Self-hosted Embeddings, see [this notebook](/docs/modules/data_connection/text_embedding/integrations/self-hosted.html)', metadata={'Header 1': 'Runhouse', 'Header 2': 'Self-hosted Embeddings'}),\n",
       " Document(page_content='>[spaCy](https://spacy.io/) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.', metadata={'Header 1': 'spaCy'}),\n",
       " Document(page_content='```bash\\npip install spacy\\n```', metadata={'Header 1': 'spaCy', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_transformers/text_splitters/split_by_token.html#spacy).  \\n```python\\nfrom langchain.llms import SpacyTextSplitter\\n```', metadata={'Header 1': 'spaCy', 'Header 2': 'Text Splitter'}),\n",
       " Document(page_content='This page covers how to use [Metal](https://getmetal.io) within LangChain.', metadata={'Header 1': 'Metal'}),\n",
       " Document(page_content='Metal is a  managed retrieval & memory platform built for production. Easily index your data into `Metal` and run semantic search and retrieval on it.  \\n![Metal](/img/MetalDash.png)', metadata={'Header 1': 'Metal', 'Header 2': 'What is Metal?'}),\n",
       " Document(page_content='Get started by [creating a Metal account](https://app.getmetal.io/signup).  \\nThen, you can easily take advantage of the `MetalRetriever` class to start retrieving your data for semantic search, prompting context, etc. This class takes a `Metal` instance and a dictionary of parameters to pass to the Metal API.  \\n```python\\nfrom langchain.retrievers import MetalRetriever\\nfrom metal_sdk.metal import Metal  \\nmetal = Metal(\"API_KEY\", \"CLIENT_ID\", \"INDEX_ID\");\\nretriever = MetalRetriever(metal, params={\"limit\": 2})  \\ndocs = retriever.get_relevant_documents(\"search term\")\\n```', metadata={'Header 1': 'Metal', 'Header 2': 'Quick start'}),\n",
       " Document(page_content='This page covers how to use the Writer ecosystem within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific Writer wrappers.', metadata={'Header 1': 'Writer'}),\n",
       " Document(page_content='- Get an Writer api key and set it as an environment variable (`WRITER_API_KEY`)', metadata={'Header 1': 'Writer', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists an Writer LLM wrapper, which you can access with\\n```python\\nfrom langchain.llms import Writer\\n```', metadata={'Header 1': 'Writer', 'Header 2': 'Wrappers', 'Header 3': 'LLM'}),\n",
       " Document(page_content=\">[Diffbot](https://docs.diffbot.com/docs) is a service to read web pages. Unlike traditional web scraping tools,\\n> `Diffbot` doesn't require any rules to read the content on a page.\\n>It starts with computer vision, which classifies a page into one of 20 possible types. Content is then interpreted by a machine learning model trained to identify the key attributes on a page based on its type.\\n>The result is a website transformed into clean-structured data (like JSON or CSV), ready for your application.\", metadata={'Header 1': 'Diffbot'}),\n",
       " Document(page_content='Read [instructions](https://docs.diffbot.com/reference/authentication) how to get the Diffbot API Token.', metadata={'Header 1': 'Diffbot', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/diffbot.html).  \\n```python\\nfrom langchain.document_loaders import DiffbotLoader\\n```', metadata={'Header 1': 'Diffbot', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='lanchchain decorators is a layer on the top of LangChain that provides syntactic sugar 🍭 for writing custom langchain prompts and chains  \\nFor Feedback, Issues, Contributions - please raise an issue here:\\n[ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators)  \\nMain principles and benefits:  \\n- more `pythonic` way of writing code\\n- write multiline prompts that wont break your code flow with indentation\\n- making use of IDE in-built support for **hinting**, **type checking** and **popup with docs** to quickly peek in the function to see the prompt, parameters it consumes etc.\\n- leverage all the power of 🦜🔗 LangChain ecosystem\\n- adding support for **optional parameters**\\n- easily share parameters between the prompts by binding them to one class  \\nHere is a simple example of a code written with **LangChain Decorators ✨**  \\n``` python  \\n@llm_prompt\\ndef write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\")->str:\\n\"\"\"\\nWrite me a short header for my post about {topic} for {platform} platform.\\nIt should be for {audience} audience.\\n(Max 15 words)\\n\"\"\"\\nreturn', metadata={'Header 1': 'LangChain Decorators ✨'}),\n",
       " Document(page_content='write_me_short_post(topic=\"starwars\")', metadata={'Header 1': 'run it naturaly'}),\n",
       " Document(page_content='write_me_short_post(topic=\"starwars\", platform=\"redit\")\\n```', metadata={'Header 1': 'or'}),\n",
       " Document(page_content='```bash\\npip install langchain_decorators\\n```', metadata={'Header 1': 'Quick start', 'Header 2': 'Installation'}),\n",
       " Document(page_content='Good idea on how to start is to review the examples here:\\n- [jupyter notebook](https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb)\\n- [colab notebook](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)', metadata={'Header 1': 'Quick start', 'Header 2': 'Examples'}),\n",
       " Document(page_content='Here we are just marking a function as a prompt with `llm_prompt` decorator, turning it effectively into a LLMChain. Instead of running it  \\nStandard LLMchain takes much more init parameter than just inputs_variables and prompt... here is this implementation detail hidden in the decorator.\\nHere is how it works:  \\n1. Using **Global settings**:  \\n``` python', metadata={'Header 1': 'Defining other parameters'}),\n",
       " Document(page_content='from langchain_decorators import GlobalSettings  \\nGlobalSettings.define_settings(\\ndefault_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally\\ndefault_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming\\n)\\n```  \\n2. Using predefined **prompt types**  \\n``` python\\n#You can change the default prompt types\\nfrom langchain_decorators import PromptTypes, PromptTypeSettings  \\nPromptTypes.AGENT_REASONING.llm = ChatOpenAI()', metadata={'Header 1': 'define global settings for all prompty (if not set - chatGPT is the current default)'}),\n",
       " Document(page_content='class MyCustomPromptTypes(PromptTypes):\\nGPT4=PromptTypeSettings(llm=ChatOpenAI(model=\"gpt-4\"))  \\n@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4)\\ndef write_a_complicated_code(app_idea:str)->str:\\n...  \\n```  \\n3.  Define the settings **directly in the decorator**  \\n``` python\\nfrom langchain.llms import OpenAI  \\n@llm_prompt(\\nllm=OpenAI(temperature=0.7),\\nstop_tokens=[\"\\\\nObservation\"],\\n...\\n)\\ndef creative_writer(book_title:str)->str:\\n...\\n```', metadata={'Header 1': 'Or you can just define your own ones:'}),\n",
       " Document(page_content='To pass any of these, just declare them in the function (or use kwargs to pass anything)  \\n```python  \\n@llm_prompt()\\nasync def write_me_short_post(topic:str, platform:str=\"twitter\", memory:SimpleMemory = None):\\n\"\"\"\\n{history_key}\\nWrite me a short header for my post about {topic} for {platform} platform.\\nIt should be for {audience} audience.\\n(Max 15 words)\\n\"\"\"\\npass  \\nawait write_me_short_post(topic=\"old movies\")  \\n```', metadata={'Header 1': 'Or you can just define your own ones:', 'Header 2': 'Passing a memory and/or callbacks:'}),\n",
       " Document(page_content=\"If we wan't to leverage streaming:\\n- we need to define prompt as async function\\n- turn on the streaming on the decorator, or we can define PromptType with streaming on\\n- capture the stream using StreamingContext  \\nThis way we just mark which prompt should be streamed, not needing to tinker with what LLM should we use, passing around the creating and distribute streaming handler into particular part of our chain... just turn the streaming on/off on prompt/prompt type...  \\nThe streaming will happen only if we call it in streaming context ... there we can define a simple function to handle the stream  \\n``` python\", metadata={'Header 1': 'Simplified streaming'}),\n",
       " Document(page_content='from langchain_decorators import StreamingContext, llm_prompt', metadata={'Header 1': 'this code example is complete and should run as it is'}),\n",
       " Document(page_content='@llm_prompt(capture_stream=True)\\nasync def write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\\n\"\"\"\\nWrite me a short header for my post about {topic} for {platform} platform.\\nIt should be for {audience} audience.\\n(Max 15 words)\\n\"\"\"\\npass', metadata={'Header 1': \"note that only async functions can be streamed (will get an error if it's not)\"}),\n",
       " Document(page_content='tokens=[]\\ndef capture_stream_func(new_token:str):\\ntokens.append(new_token)', metadata={'Header 1': 'just an arbitrary  function to demonstrate the streaming... wil be some websockets code in the real world'}),\n",
       " Document(page_content='with StreamingContext(stream_to_stdout=True, callback=capture_stream_func):\\nresult = await run_prompt()\\nprint(\"Stream finished ... we can distinguish tokens thanks to alternating colors\")  \\nprint(\"\\\\nWe\\'ve captured\",len(tokens),\"tokens🎉\\\\n\")\\nprint(\"Here is the result:\")\\nprint(result)\\n```', metadata={'Header 1': 'only the prompts marked with capture_stream will be captured here'}),\n",
       " Document(page_content='By default the prompt is is the whole function docs, unless you mark your prompt', metadata={'Header 1': 'Prompt declarations'}),\n",
       " Document(page_content='We can specify what part of our docs is the prompt definition, by specifying a code block with `<prompt>` language tag  \\n``` python\\n@llm_prompt\\ndef write_me_short_post(topic:str, platform:str=\"twitter\", audience:str = \"developers\"):\\n\"\"\"\\nHere is a good way to write a prompt as part of a function docstring, with additional documentation for devs.  \\nIt needs to be a code block, marked as a `<prompt>` language\\n```<prompt>\\nWrite me a short header for my post about {topic} for {platform} platform.\\nIt should be for {audience} audience.\\n(Max 15 words)\\n```  \\nNow only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\\n(It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\\n\"\"\"\\nreturn\\n```', metadata={'Header 1': 'Prompt declarations', 'Header 2': 'Documenting your prompt'}),\n",
       " Document(page_content='For chat models is very useful to define prompt as a set of message templates... here is how to do it:  \\n``` python\\n@llm_prompt\\ndef simulate_conversation(human_input:str, agent_role:str=\"a pirate\"):\\n\"\"\"', metadata={'Header 1': 'Prompt declarations', 'Header 2': 'Chat messages prompt'}),\n",
       " Document(page_content='- note the `:system` sufix inside the <prompt:_role_> tag  \\n```<prompt:system>\\nYou are a {agent_role} hacker. You mus act like one.\\nYou reply always in code, using python or javascript code block...\\nfor example:  \\n... do not reply with anything else.. just with code - respecting your role.\\n```', metadata={'Header 1': 'Prompt declarations', 'Header 2': 'System message'}),\n",
       " Document(page_content='(we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)\\n``` <prompt:user>\\nHelo, who are you\\n```\\na reply:  \\n``` <prompt:assistant>\\n\\\\``` python <<- escaping inner code block with \\\\ that should be part of the prompt\\ndef hello():\\nprint(\"Argh... hello you pesky pirate\")\\n\\\\```\\n```  \\nwe can also add some history using placeholder\\n```<prompt:placeholder>\\n{history}\\n```\\n```<prompt:user>\\n{human_input}\\n```  \\nNow only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.\\n(It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))\\n\"\"\"\\npass  \\n```  \\nthe roles here are model native roles (assistant, user, system for chatGPT)', metadata={'Header 1': 'human message'}),\n",
       " Document(page_content='- you can define a whole sections of your prompt that should be optional\\n- if any input in the section is missing, the whole section wont be rendered  \\nthe syntax for this is as follows:  \\n``` python\\n@llm_prompt\\ndef prompt_with_optional_partials():\\n\"\"\"\\nthis text will be rendered always, but  \\n{? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | \"\")   ?}  \\nyou can also place it in between the words\\nthis too will be rendered{? , but\\nthis  block will be rendered only if {this_value} and {this_value}\\nis not empty?} !\\n\"\"\"\\n```', metadata={'Header 1': 'Optional sections'}),\n",
       " Document(page_content='- llm_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string)\\n- list, dict and pydantic outputs are also supported natively (automaticaly)  \\n``` python', metadata={'Header 1': 'Output parsers'}),\n",
       " Document(page_content='from langchain_decorators import llm_prompt  \\n@llm_prompt\\ndef write_name_suggestions(company_business:str, count:int)->list:\\n\"\"\" Write me {count} good name suggestions for company that {company_business}\\n\"\"\"\\npass  \\nwrite_name_suggestions(company_business=\"sells cookies\", count=5)\\n```', metadata={'Header 1': 'this code example is complete and should run as it is'}),\n",
       " Document(page_content='for dict / pydantic you need to specify the formatting instructions...\\nthis can be tedious, that\\'s why you can let the output parser gegnerate you the instructions based on the model (pydantic)  \\n``` python\\nfrom langchain_decorators import llm_prompt\\nfrom pydantic import BaseModel, Field  \\nclass TheOutputStructureWeExpect(BaseModel):\\nname:str = Field (description=\"The name of the company\")\\nheadline:str = Field( description=\"The description of the company (for landing page)\")\\nemployees:list[str] = Field(description=\"5-8 fake employee names with their positions\")  \\n@llm_prompt()\\ndef fake_company_generator(company_business:str)->TheOutputStructureWeExpect:\\n\"\"\" Generate a fake company that {company_business}\\n{FORMAT_INSTRUCTIONS}\\n\"\"\"\\nreturn  \\ncompany = fake_company_generator(company_business=\"sells cookies\")', metadata={'Header 1': 'this code example is complete and should run as it is', 'Header 2': 'More complex structures'}),\n",
       " Document(page_content='print(\"Company name: \",company.name)\\nprint(\"company headline: \",company.headline)\\nprint(\"company employees: \",company.employees)  \\n```', metadata={'Header 1': 'print the result nicely formatted'}),\n",
       " Document(page_content='``` python\\nfrom pydantic import BaseModel\\nfrom langchain_decorators import llm_prompt  \\nclass AssistantPersonality(BaseModel):\\nassistant_name:str\\nassistant_role:str\\nfield:str  \\n@property\\ndef a_property(self):\\nreturn \"whatever\"  \\ndef hello_world(self, function_kwarg:str=None):\\n\"\"\"\\nWe can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method\\n\"\"\"  \\n@llm_prompt\\ndef introduce_your_self(self)->str:\\n\"\"\"\\n```\\xa0<prompt:system>\\nYou are an assistant named {assistant_name}.\\nYour role is to act as {assistant_role}\\n```\\n```<prompt:user>\\nIntroduce your self (in less than 20 words)\\n```\\n\"\"\"  \\npersonality = AssistantPersonality(assistant_name=\"John\", assistant_role=\"a pirate\")  \\nprint(personality.introduce_your_self(personality))\\n```', metadata={'Header 1': 'Binding the prompt to an object'}),\n",
       " Document(page_content='- these and few more examples are also available in the [colab notebook here](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)\\n- including the [ReAct Agent re-implementation](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp) using purely langchain decorators', metadata={'Header 1': 'More examples:'}),\n",
       " Document(page_content='![Argilla - Open-source data platform for LLMs](https://argilla.io/og.png)  \\n>[Argilla](https://argilla.io/) is an open-source data curation platform for LLMs.\\n> Using Argilla, everyone can build robust language models through faster data curation\\n> using both human and machine feedback. We provide support for each step in the MLOps cycle,\\n> from data labeling to model monitoring.', metadata={'Header 1': 'Argilla'}),\n",
       " Document(page_content=\"First, you'll need to install the  `argilla` Python package as follows:  \\n```bash\\npip install argilla --upgrade\\n```  \\nIf you already have an Argilla Server running, then you're good to go; but if\\nyou don't, follow the next steps to install it.  \\nIf you don't you can refer to [Argilla - 🚀 Quickstart](https://docs.argilla.io/en/latest/getting_started/quickstart.html#Running-Argilla-Quickstart) to deploy Argilla either on HuggingFace Spaces, locally, or on a server.\", metadata={'Header 1': 'Argilla', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example of `ArgillaCallbackHandler`](/docs/modules/callbacks/integrations/argilla.html).  \\n```python\\nfrom langchain.callbacks import ArgillaCallbackHandler\\n```', metadata={'Header 1': 'Argilla', 'Header 2': 'Tracking'}),\n",
       " Document(page_content='>[arXiv](https://arxiv.org/) is an open-access archive for 2 million scholarly articles in the fields of physics,\\n> mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and\\n> systems science, and economics.', metadata={'Header 1': 'Arxiv'}),\n",
       " Document(page_content='First, you need to install `arxiv` python package.  \\n```bash\\npip install arxiv\\n```  \\nSecond, you need to install `PyMuPDF` python package which transforms PDF files downloaded from the `arxiv.org` site into the text format.  \\n```bash\\npip install pymupdf\\n```', metadata={'Header 1': 'Arxiv', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/arxiv.html).  \\n```python\\nfrom langchain.document_loaders import ArxivLoader\\n```', metadata={'Header 1': 'Arxiv', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/retrievers/integrations/arxiv.html).  \\n```python\\nfrom langchain.retrievers import ArxivRetriever\\n```', metadata={'Header 1': 'Arxiv', 'Header 2': 'Retriever'}),\n",
       " Document(page_content='This page covers how to use [LanceDB](https://github.com/lancedb/lancedb) within LangChain.\\nIt is broken into two parts: installation and setup, and then references to specific LanceDB wrappers.', metadata={'Header 1': 'LanceDB'}),\n",
       " Document(page_content='- Install the Python SDK with `pip install lancedb`', metadata={'Header 1': 'LanceDB', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around LanceDB databases, allowing you to use it as a vectorstore,\\nwhether for semantic search or example selection.  \\nTo import this vectorstore:  \\n```python\\nfrom langchain.vectorstores import LanceDB\\n```  \\nFor a more detailed walkthrough of the LanceDB wrapper, see [this notebook](/docs/modules/data_connection/vectorstores/integrations/lancedb.html)', metadata={'Header 1': 'LanceDB', 'Header 2': 'Wrappers', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='>[WolframAlpha](https://en.wikipedia.org/wiki/WolframAlpha) is an answer engine developed by `Wolfram Research`.\\n> It answers factual queries by computing answers from externally sourced data.  \\nThis page covers how to use the `Wolfram Alpha API` within LangChain.', metadata={'Header 1': 'Wolfram Alpha'}),\n",
       " Document(page_content='- Install requirements with\\n```bash\\npip install wolframalpha\\n```\\n- Go to wolfram alpha and sign up for a developer account [here](https://developer.wolframalpha.com/)\\n- Create an app and get your `APP ID`\\n- Set your APP ID as an environment variable `WOLFRAM_ALPHA_APPID`', metadata={'Header 1': 'Wolfram Alpha', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a WolframAlphaAPIWrapper utility which wraps this API. To import this utility:  \\n```python\\nfrom langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\\n```  \\nFor a more detailed walkthrough of this wrapper, see [this notebook](/docs/modules/agents/tools/integrations/wolfram_alpha.html).', metadata={'Header 1': 'Wolfram Alpha', 'Header 2': 'Wrappers', 'Header 3': 'Utility'}),\n",
       " Document(page_content='You can also easily load this wrapper as a Tool (to use with an Agent).\\nYou can do this with:\\n```python\\nfrom langchain.agents import load_tools\\ntools = load_tools([\"wolfram-alpha\"])\\n```  \\nFor more information on tools, see [this page](/docs/modules/agents/tools/).', metadata={'Header 1': 'Wolfram Alpha', 'Header 2': 'Wrappers', 'Header 3': 'Tool'}),\n",
       " Document(page_content='>[DuckDB](https://duckdb.org/) is an in-process SQL OLAP database management system.', metadata={'Header 1': 'DuckDB'}),\n",
       " Document(page_content='First, you need to install `duckdb` python package.  \\n```bash\\npip install duckdb\\n```', metadata={'Header 1': 'DuckDB', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/duckdb.html).  \\n```python\\nfrom langchain.document_loaders import DuckDBLoader\\n```', metadata={'Header 1': 'DuckDB', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content='>[Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html) is an object storage service.  \\n>[AWS S3 Directory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)  \\n>[AWS S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html)', metadata={'Header 1': 'AWS S3 Directory'}),\n",
       " Document(page_content='```bash\\npip install boto3\\n```', metadata={'Header 1': 'AWS S3 Directory', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example for S3DirectoryLoader](/docs/modules/data_connection/document_loaders/integrations/aws_s3_directory.html).  \\nSee a [usage example for S3FileLoader](/docs/modules/data_connection/document_loaders/integrations/aws_s3_file.html).  \\n```python\\nfrom langchain.document_loaders import S3DirectoryLoader, S3FileLoader\\n```', metadata={'Header 1': 'AWS S3 Directory', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content=\"What is Vectara?  \\n**Vectara Overview:**\\n- Vectara is developer-first API platform for building GenAI applications\\n- To use Vectara - first [sign up](https://console.vectara.com/signup) and create an account. Then create a corpus and an API key for indexing and searching.\\n- You can use Vectara's [indexing API](https://docs.vectara.com/docs/indexing-apis/indexing) to add documents into Vectara's index\\n- You can use Vectara's [Search API](https://docs.vectara.com/docs/search-apis/search) to query Vectara's index (which also supports Hybrid search implicitly).\\n- You can use Vectara's integration with LangChain as a Vector store or using the Retriever abstraction.\", metadata={'Header 1': 'Vectara'}),\n",
       " Document(page_content='To use Vectara with LangChain no special installation steps are required. You just have to provide your customer_id, corpus ID, and an API key created within the Vectara console to enable indexing and searching.  \\nAlternatively these can be provided as environment variables\\n- export `VECTARA_CUSTOMER_ID`=\"your_customer_id\"\\n- export `VECTARA_CORPUS_ID`=\"your_corpus_id\"\\n- export `VECTARA_API_KEY`=\"your-vectara-api-key\"', metadata={'Header 1': 'Vectara', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='There exists a wrapper around the Vectara platform, allowing you to use it as a vectorstore, whether for semantic search or example selection.  \\nTo import this vectorstore:\\n```python\\nfrom langchain.vectorstores import Vectara\\n```  \\nTo create an instance of the Vectara vectorstore:\\n```python\\nvectara = Vectara(\\nvectara_customer_id=customer_id,\\nvectara_corpus_id=corpus_id,\\nvectara_api_key=api_key\\n)\\n```\\nThe customer_id, corpus_id and api_key are optional, and if they are not supplied will be read from the environment variables `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY`, respectively.  \\nAfer you have the vectorstore, you can `add_texts` or `add_documents` as per the standard `VectorStore` interface, for example:  \\n```python\\nvectara.add_texts([\"to be or not to be\", \"that is the question\"])\\n```  \\nSince Vectara supports file-upload, we also added the ability to upload files (PDF, TXT, HTML, PPT, DOC, etc) directly as file. When using this method, the file is uploaded directly to the Vectara backend, processed and chunked optimally there, so you don\\'t have to use the LangChain document loader or chunking mechanism.  \\nAs an example:  \\n```python\\nvectara.add_files([\"path/to/file1.pdf\", \"path/to/file2.pdf\",...])\\n```  \\nTo query the vectorstore, you can use the `similarity_search` method (or `similarity_search_with_score`), which takes a query string and returns a list of results:\\n```python\\nresults = vectara.similarity_score(\"what is LangChain?\")\\n```  \\n`similarity_search_with_score` also supports the following additional arguments:\\n- `k`: number of results to return (defaults to 5)\\n- `lambda_val`: the [lexical matching](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) factor for hybrid search (defaults to 0.025)\\n- `filter`: a [filter](https://docs.vectara.com/docs/common-use-cases/filtering-by-metadata/filter-overview) to apply to the results (default None)\\n- `n_sentence_context`: number of sentences to include before/after the actual matching segment when returning results. This defaults to 0 so as to return the exact text segment that matches, but can be used with other values e.g. 2 or 3 to return adjacent text segments.  \\nThe results are returned as a list of relevant documents, and a relevance score of each document.  \\nFor a more detailed examples of using the Vectara wrapper, see one of these two sample notebooks:\\n* [Chat Over Documents with Vectara](./vectara_chat.html)\\n* [Vectara Text Generation](./vectara_text_generation.html)', metadata={'Header 1': 'Vectara', 'Header 2': 'Usage', 'Header 3': 'VectorStore'}),\n",
       " Document(page_content='This section of documentation covers how we approach and think about evaluation in LangChain.\\nBoth evaluation of internal chains/agents, but also how we would recommend people building on top of LangChain approach evaluation.', metadata={'Header 1': 'Evaluation'}),\n",
       " Document(page_content=\"It can be really hard to evaluate LangChain chains and agents.\\nThere are two main reasons for this:  \\n**# 1: Lack of data**  \\nYou generally don't have a ton of data to evaluate your chains/agents over before starting a project.\\nThis is usually because Large Language Models (the core of most chains/agents) are terrific few-shot and zero shot learners,\\nmeaning you are almost always able to get started on a particular task (text-to-SQL, question answering, etc) without\\na large dataset of examples.\\nThis is in stark contrast to traditional machine learning where you had to first collect a bunch of datapoints\\nbefore even getting started using a model.  \\n**# 2: Lack of metrics**  \\nMost chains/agents are performing tasks for which there are not very good metrics to evaluate performance.\\nFor example, one of the most common use cases is generating text of some form.\\nEvaluating generated text is much more complicated than evaluating a classification prediction, or a numeric prediction.\", metadata={'Header 1': 'Evaluation', 'Header 2': 'The Problem'}),\n",
       " Document(page_content=\"LangChain attempts to tackle both of those issues.\\nWhat we have so far are initial passes at solutions - we do not think we have a perfect solution.\\nSo we very much welcome feedback, contributions, integrations, and thoughts on this.  \\nHere is what we have for each problem so far:  \\n**# 1: Lack of data**  \\nWe have started [LangChainDatasets](https://huggingface.co/LangChainDatasets) a Community space on Hugging Face.\\nWe intend this to be a collection of open source datasets for evaluating common chains and agents.\\nWe have contributed five datasets of our own to start, but we highly intend this to be a community effort.\\nIn order to contribute a dataset, you simply need to join the community and then you will be able to upload datasets.  \\nWe're also aiming to make it as easy as possible for people to create their own datasets.\\nAs a first pass at this, we've added a QAGenerationChain, which given a document comes up\\nwith question-answer pairs that can be used to evaluate question-answering tasks over that document down the line.\\nSee [this notebook](/docs/guides/evaluation/qa_generation.html) for an example of how to use this chain.  \\n**# 2: Lack of metrics**  \\nWe have two solutions to the lack of metrics.  \\nThe first solution is to use no metrics, and rather just rely on looking at results by eye to get a sense for how the chain/agent is performing.\\nTo assist in this, we have developed (and will continue to develop) [tracing](/docs/guides/tracing/), a UI-based visualizer of your chain and agent runs.  \\nThe second solution we recommend is to use Language Models themselves to evaluate outputs.\\nFor this we have a few different chains and prompts aimed at tackling this issue.\", metadata={'Header 1': 'Evaluation', 'Header 2': 'The Solution'}),\n",
       " Document(page_content=\"We have created a bunch of examples combining the above two solutions to show how we internally evaluate chains and agents when we are developing.\\nIn addition to the examples we've curated, we also highly welcome contributions here.\\nTo facilitate that, we've included a [template notebook](/docs/guides/evaluation/benchmarking_template.html) for community members to use to build their own examples.  \\nThe existing examples we have are:  \\n[Question Answering (State of Union)](/docs/guides/evaluation/qa_benchmarking_sota.html): A notebook showing evaluation of a question-answering task over a State-of-the-Union address.  \\n[Question Answering (Paul Graham Essay)](/docs/guides/evaluation/qa_benchmarking_pg.html): A notebook showing evaluation of a question-answering task over a Paul Graham essay.  \\n[SQL Question Answering (Chinook)](/docs/guides/evaluation/sql_qa_benchmarking_chinook.html): A notebook showing evaluation of a question-answering task over a SQL database (the Chinook database).  \\n[Agent Vectorstore](/docs/guides/evaluation/agent_vectordb_sota_pg.html): A notebook showing evaluation of an agent doing question answering while routing between two different vector databases.  \\n[Agent Search + Calculator](/docs/guides/evaluation/agent_benchmarking.html): A notebook showing evaluation of an agent doing question answering using a Search engine and a Calculator as tools.  \\n[Evaluating an OpenAPI Chain](/docs/guides/evaluation/openapi_eval.html): A notebook showing evaluation of an OpenAPI chain, including how to generate test data if you don't have any.\", metadata={'Header 1': 'Evaluation', 'Header 2': 'The Examples'}),\n",
       " Document(page_content='In addition, we also have some more generic resources for evaluation.  \\n[Question Answering](/docs/guides/evaluation/question_answering.html): An overview of LLMs aimed at evaluating question answering systems in general.  \\n[Data Augmented Question Answering](/docs/guides/evaluation/data_augmented_question_answering.html): An end-to-end example of evaluating a question answering system focused on a specific document (a RetrievalQAChain to be precise). This example highlights how to use LLMs to come up with question/answer examples to evaluate over, and then highlights how to use LLMs to evaluate performance on those generated examples.  \\n[Hugging Face Datasets](/docs/guides/evaluation/huggingface_datasets.html): Covers an example of loading and using a dataset from Hugging Face for evaluation.', metadata={'Header 1': 'Evaluation', 'Header 2': 'Other Examples'}),\n",
       " Document(page_content=\"In today's fast-paced technological landscape, the use of Large Language Models (LLMs) is rapidly expanding. As a result, it's crucial for developers to understand how to effectively deploy these models in production environments. LLM interfaces typically fall into two categories:  \\n- **Case 1: Utilizing External LLM Providers (OpenAI, Anthropic, etc.)**\\nIn this scenario, most of the computational burden is handled by the LLM providers, while LangChain simplifies the implementation of business logic around these services. This approach includes features such as prompt templating, chat message generation, caching, vector embedding database creation, preprocessing, etc.  \\n- **Case 2: Self-hosted Open-Source Models**\\nAlternatively, developers can opt to use smaller, yet comparably capable, self-hosted open-source LLM models. This approach can significantly decrease costs, latency, and privacy concerns associated with transferring data to external LLM providers.  \\nRegardless of the framework that forms the backbone of your product, deploying LLM applications comes with its own set of challenges. It's vital to understand the trade-offs and key considerations when evaluating serving frameworks.\", metadata={'Header 1': 'Deployment'}),\n",
       " Document(page_content='This guide aims to provide a comprehensive overview of the requirements for deploying LLMs in a production setting, focusing on:  \\n- **Designing a Robust LLM Application Service**\\n- **Maintaining Cost-Efficiency**\\n- **Ensuring Rapid Iteration**  \\nUnderstanding these components is crucial when assessing serving systems. LangChain integrates with several open-source projects designed to tackle these issues, providing a robust framework for productionizing your LLM applications. Some notable frameworks include:  \\n- [Ray Serve](/docs/ecosystem/integrations/ray_serve.html)\\n- [BentoML](https://github.com/bentoml/BentoML)\\n- [OpenLLM](/docs/ecosystem/integrations/openllm.html)\\n- [Modal](/docs/ecosystem/integrations/modal.html)  \\nThese links will provide further information on each ecosystem, assisting you in finding the best fit for your LLM deployment needs.', metadata={'Header 1': 'Deployment', 'Header 2': 'Outline'}),\n",
       " Document(page_content=\"When deploying an LLM service in production, it's imperative to provide a seamless user experience free from outages. Achieving 24/7 service availability involves creating and maintaining several sub-systems surrounding your application.\", metadata={'Header 1': 'Deployment', 'Header 2': 'Designing a Robust LLM Application Service'}),\n",
       " Document(page_content='Monitoring forms an integral part of any system running in a production environment. In the context of LLMs, it is essential to monitor both performance and quality metrics.  \\n**Performance Metrics:** These metrics provide insights into the efficiency and capacity of your model. Here are some key examples:  \\n- Query per second (QPS): This measures the number of queries your model processes in a second, offering insights into its utilization.\\n- Latency: This metric quantifies the delay from when your client sends a request to when they receive a response.\\n- Tokens Per Second (TPS): This represents the number of tokens your model can generate in a second.  \\n**Quality Metrics:** These metrics are typically customized according to the business use-case. For instance, how does the output of your system compare to a baseline, such as a previous version? Although these metrics can be calculated offline, you need to log the necessary data to use them later.', metadata={'Header 1': 'Deployment', 'Header 2': 'Designing a Robust LLM Application Service', 'Header 3': 'Monitoring'}),\n",
       " Document(page_content=\"Your application may encounter errors such as exceptions in your model inference or business logic code, causing failures and disrupting traffic. Other potential issues could arise from the machine running your application, such as unexpected hardware breakdowns or loss of spot-instances during high-demand periods. One way to mitigate these risks is by increasing redundancy through replica scaling and implementing recovery mechanisms for failed replicas. However, model replicas aren't the only potential points of failure. It's essential to build resilience against various failures that could occur at any point in your stack.\", metadata={'Header 1': 'Deployment', 'Header 2': 'Designing a Robust LLM Application Service', 'Header 3': 'Fault tolerance'}),\n",
       " Document(page_content='System upgrades are often necessary but can result in service disruptions if not handled correctly. One way to prevent downtime during upgrades is by implementing a smooth transition process from the old version to the new one. Ideally, the new version of your LLM service is deployed, and traffic gradually shifts from the old to the new version, maintaining a constant QPS throughout the process.', metadata={'Header 1': 'Deployment', 'Header 2': 'Designing a Robust LLM Application Service', 'Header 3': 'Zero down time upgrade'}),\n",
       " Document(page_content=\"Load balancing, in simple terms, is a technique to distribute work evenly across multiple computers, servers, or other resources to optimize the utilization of the system, maximize throughput, minimize response time, and avoid overload of any single resource. Think of it as a traffic officer directing cars (requests) to different roads (servers) so that no single road becomes too congested.  \\nThere are several strategies for load balancing. For example, one common method is the *Round Robin* strategy, where each request is sent to the next server in line, cycling back to the first when all servers have received a request. This works well when all servers are equally capable. However, if some servers are more powerful than others, you might use a *Weighted Round Robin* or *Least Connections* strategy, where more requests are sent to the more powerful servers, or to those currently handling the fewest active requests. Let's imagine you're running a LLM chain. If your application becomes popular, you could have hundreds or even thousands of users asking questions at the same time. If one server gets too busy (high load), the load balancer would direct new requests to another server that is less busy. This way, all your users get a timely response and the system remains stable.\", metadata={'Header 1': 'Deployment', 'Header 2': 'Designing a Robust LLM Application Service', 'Header 3': 'Load balancing'}),\n",
       " Document(page_content=\"Deploying LLM services can be costly, especially when you're handling a large volume of user interactions. Charges by LLM providers are usually based on tokens used, making a chat system inference on these models potentially expensive. However, several strategies can help manage these costs without compromising the quality of the service.\", metadata={'Header 1': 'Deployment', 'Header 2': 'Maintaining Cost-Efficiency and Scalability'}),\n",
       " Document(page_content='Several smaller and open-source LLMs are emerging to tackle the issue of reliance on LLM providers. Self-hosting allows you to maintain similar quality to LLM provider models while managing costs. The challenge lies in building a reliable, high-performing LLM serving system on your own machines.', metadata={'Header 1': 'Deployment', 'Header 2': 'Maintaining Cost-Efficiency and Scalability', 'Header 3': 'Self-hosting models'}),\n",
       " Document(page_content=\"Computational logic within your application requires precise resource allocation. For instance, if part of your traffic is served by an OpenAI endpoint and another part by a self-hosted model, it's crucial to allocate suitable resources for each. Auto-scaling—adjusting resource allocation based on traffic—can significantly impact the cost of running your application. This strategy requires a balance between cost and responsiveness, ensuring neither resource over-provisioning nor compromised application responsiveness.\", metadata={'Header 1': 'Deployment', 'Header 2': 'Maintaining Cost-Efficiency and Scalability', 'Header 3': 'Resource Management and Auto-Scaling'}),\n",
       " Document(page_content='On platforms like AWS, spot instances offer substantial cost savings, typically priced at about a third of on-demand instances. The trade-off is a higher crash rate, necessitating a robust fault-tolerance mechanism for effective use.', metadata={'Header 1': 'Deployment', 'Header 2': 'Maintaining Cost-Efficiency and Scalability', 'Header 3': 'Utilizing Spot Instances'}),\n",
       " Document(page_content='When self-hosting your models, you should consider independent scaling. For example, if you have two translation models, one fine-tuned for French and another for Spanish, incoming requests might necessitate different scaling requirements for each.', metadata={'Header 1': 'Deployment', 'Header 2': 'Maintaining Cost-Efficiency and Scalability', 'Header 3': 'Independent Scaling'}),\n",
       " Document(page_content=\"In the context of Large Language Models, batching requests can enhance efficiency by better utilizing your GPU resources. GPUs are inherently parallel processors, designed to handle multiple tasks simultaneously. If you send individual requests to the model, the GPU might not be fully utilized as it's only working on a single task at a time. On the other hand, by batching requests together, you're allowing the GPU to work on multiple tasks at once, maximizing its utilization and improving inference speed. This not only leads to cost savings but can also improve the overall latency of your LLM service.  \\nIn summary, managing costs while scaling your LLM services requires a strategic approach. Utilizing self-hosting models, managing resources effectively, employing auto-scaling, using spot instances, independently scaling models, and batching requests are key strategies to consider. Open-source libraries such as Ray Serve and BentoML are designed to deal with these complexities.\", metadata={'Header 1': 'Deployment', 'Header 2': 'Maintaining Cost-Efficiency and Scalability', 'Header 3': 'Batching requests'}),\n",
       " Document(page_content=\"The LLM landscape is evolving at an unprecedented pace, with new libraries and model architectures being introduced constantly. Consequently, it's crucial to avoid tying yourself to a solution specific to one particular framework. This is especially relevant in serving, where changes to your infrastructure can be time-consuming, expensive, and risky. Strive for infrastructure that is not locked into any specific machine learning library or framework, but instead offers a general-purpose, scalable serving layer. Here are some aspects where flexibility plays a key role:\", metadata={'Header 1': 'Deployment', 'Header 2': 'Ensuring Rapid Iteration'}),\n",
       " Document(page_content='Deploying systems like LangChain demands the ability to piece together different models and connect them via logic. Take the example of building a natural language input SQL query engine. Querying an LLM and obtaining the SQL command is only part of the system. You need to extract metadata from the connected database, construct a prompt for the LLM, run the SQL query on an engine, collect and feed back the response to the LLM as the query runs, and present the results to the user. This demonstrates the need to seamlessly integrate various complex components built in Python into a dynamic chain of logical blocks that can be served together.', metadata={'Header 1': 'Deployment', 'Header 2': 'Ensuring Rapid Iteration', 'Header 3': 'Model composition'}),\n",
       " Document(page_content=\"Many hosted solutions are restricted to a single cloud provider, which can limit your options in today's multi-cloud world. Depending on where your other infrastructure components are built, you might prefer to stick with your chosen cloud provider.\", metadata={'Header 1': 'Deployment', 'Header 2': 'Cloud providers'}),\n",
       " Document(page_content='Rapid iteration also involves the ability to recreate your infrastructure quickly and reliably. This is where Infrastructure as Code (IaC) tools like Terraform, CloudFormation, or Kubernetes YAML files come into play. They allow you to define your infrastructure in code files, which can be version controlled and quickly deployed, enabling faster and more reliable iterations.', metadata={'Header 1': 'Deployment', 'Header 2': 'Infrastructure as Code (IaC)'}),\n",
       " Document(page_content='In a fast-paced environment, implementing CI/CD pipelines can significantly speed up the iteration process. They help automate the testing and deployment of your LLM applications, reducing the risk of errors and enabling faster feedback and iteration.', metadata={'Header 1': 'Deployment', 'Header 2': 'CI/CD'}),\n",
       " Document(page_content=\"So, you've created a really cool chain - now what? How do you deploy it and make it easily shareable with the world?  \\nThis section covers several options for that. Note that these options are meant for quick deployment of prototypes and demos, not for production systems. If you need help with the deployment of a production system, please contact us directly.  \\nWhat follows is a list of template GitHub repositories designed to be easily forked and modified to use your chain. This list is far from exhaustive, and we are EXTREMELY open to contributions here.\", metadata={'Header 1': 'Template repos'}),\n",
       " Document(page_content='This repo serves as a template for how to deploy a LangChain with Streamlit.\\nIt implements a chatbot interface.\\nIt also contains instructions for how to deploy this app on the Streamlit platform.', metadata={'Header 1': 'Template repos', 'Header 2': '[Streamlit](https://github.com/hwchase17/langchain-streamlit-template)'}),\n",
       " Document(page_content='This repo serves as a template for how deploy a LangChain with Gradio.\\nIt implements a chatbot interface, with a \"Bring-Your-Own-Token\" approach (nice for not wracking up big bills).\\nIt also contains instructions for how to deploy this app on the Hugging Face platform.\\nThis is heavily influenced by James Weaver\\'s [excellent examples](https://huggingface.co/JavaFXpert).', metadata={'Header 1': 'Template repos', 'Header 2': '[Gradio (on Hugging Face)](https://github.com/hwchase17/langchain-gradio-template)'}),\n",
       " Document(page_content='This repo is a cookbook explaining how to visualize and deploy LangChain agents with Chainlit.\\nYou create ChatGPT-like UIs with Chainlit. Some of the key features include intermediary steps visualisation, element management & display (images, text, carousel, etc.) as well as cloud deployment.\\nChainlit [doc](https://docs.chainlit.io/langchain) on the integration with LangChain', metadata={'Header 1': 'Template repos', 'Header 2': '[Chainlit](https://github.com/Chainlit/cookbook)'}),\n",
       " Document(page_content='This repo serves as a template for how deploy a LangChain with [Beam](https://beam.cloud).  \\nIt implements a Question Answering app and contains instructions for deploying the app as a serverless REST API.', metadata={'Header 1': 'Template repos', 'Header 2': '[Beam](https://github.com/slai-labs/get-beam/tree/main/examples/langchain-question-answering)'}),\n",
       " Document(page_content='A minimal example on how to run LangChain on Vercel using Flask.', metadata={'Header 1': 'Template repos', 'Header 2': '[Vercel](https://github.com/homanp/vercel-langchain)'}),\n",
       " Document(page_content='A minimal example on how to run LangChain on Vercel using FastAPI and LangCorn/Uvicorn.', metadata={'Header 1': 'Template repos', 'Header 2': '[FastAPI + Vercel](https://github.com/msoedov/langcorn)'}),\n",
       " Document(page_content='A minimal example on how to deploy LangChain to [Kinsta](https://kinsta.com) using Flask.', metadata={'Header 1': 'Template repos', 'Header 2': '[Kinsta](https://github.com/kinsta/hello-world-langchain)'}),\n",
       " Document(page_content='A minimal example of how to deploy LangChain to [Fly.io](https://fly.io/) using Flask.', metadata={'Header 1': 'Template repos', 'Header 2': '[Fly.io](https://github.com/fly-apps/hello-fly-langchain)'}),\n",
       " Document(page_content='A minimal example on how to deploy LangChain to DigitalOcean App Platform.', metadata={'Header 1': 'Template repos', 'Header 2': '[Digitalocean App Platform](https://github.com/homanp/digitalocean-langchain)'}),\n",
       " Document(page_content='A minimal example on how to deploy LangChain to Google Cloud Run.', metadata={'Header 1': 'Template repos', 'Header 2': '[Google Cloud Run](https://github.com/homanp/gcp-langchain)'}),\n",
       " Document(page_content='This repository contains LangChain adapters for Steamship, enabling LangChain developers to rapidly deploy their apps on Steamship. This includes: production-ready endpoints, horizontal scaling across dependencies, persistent storage of app state, multi-tenancy support, etc.', metadata={'Header 1': 'Template repos', 'Header 2': '[SteamShip](https://github.com/steamship-core/steamship-langchain/)'}),\n",
       " Document(page_content='This repository allows users to serve local chains and agents as RESTful, gRPC, or WebSocket APIs, thanks to [Jina](https://docs.jina.ai/). Deploy your chains & agents with ease and enjoy independent scaling, serverless and autoscaling APIs, as well as a Streamlit playground on Jina AI Cloud.', metadata={'Header 1': 'Template repos', 'Header 2': '[Langchain-serve](https://github.com/jina-ai/langchain-serve)'}),\n",
       " Document(page_content='This repository provides an example of how to deploy a LangChain application with [BentoML](https://github.com/bentoml/BentoML). BentoML is a framework that enables the containerization of machine learning applications as standard OCI images. BentoML also allows for the automatic generation of OpenAPI and gRPC endpoints. With BentoML, you can integrate models from all popular ML frameworks and deploy them as microservices running on the most optimal hardware and scaling independently.', metadata={'Header 1': 'Template repos', 'Header 2': '[BentoML](https://github.com/ssheng/BentoChain)'}),\n",
       " Document(page_content=\"OpenLLM is a platform for operating large language models (LLMs) in production. With OpenLLM, you can run inference with any open-source LLM, deploy to the cloud or on-premises, and build powerful AI apps. It supports a wide range of open-source LLMs, offers flexible APIs, and first-class support for LangChain and BentoML.\\nSee OpenLLM's [integration doc](https://github.com/bentoml/OpenLLM#%EF%B8%8F-integrations) for usage with LangChain.\", metadata={'Header 1': 'Template repos', 'Header 2': '[OpenLLM](https://github.com/bentoml/OpenLLM)'}),\n",
       " Document(page_content='These templates serve as examples of how to build, deploy, and share LangChain applications using Databutton. You can create user interfaces with Streamlit, automate tasks by scheduling Python code, and store files and data in the built-in store. Examples include a Chatbot interface with conversational memory, a Personal search engine, and a starter template for LangChain apps. Deploying and sharing is just one click away.', metadata={'Header 1': 'Template repos', 'Header 2': '[Databutton](https://databutton.com/home?new-data-app=true)'}),\n",
       " Document(page_content='Lots of data and information is stored behind APIs.\\nThis page covers all resources available in LangChain for working with APIs.', metadata={'Header 1': 'Interacting with APIs'}),\n",
       " Document(page_content='If you are just getting started, and you have relatively simple apis, you should get started with chains.\\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\\nunderstand what is happening better.  \\n- [API Chain](/docs/modules/chains/popular/api.html)', metadata={'Header 1': 'Interacting with APIs', 'Header 2': 'Chains'}),\n",
       " Document(page_content='Agents are more complex, and involve multiple queries to the LLM to understand what to do.\\nThe downside of agents are that you have less control. The upside is that they are more powerful,\\nwhich allows you to use them on larger and more complex schemas.  \\n- [OpenAPI Agent](/docs/modules/agents/toolkits/openapi.html)', metadata={'Header 1': 'Interacting with APIs', 'Header 2': 'Agents'}),\n",
       " Document(page_content='Most APIs and databases still deal with structured information.\\nTherefore, in order to better work with those, it can be useful to extract structured information from text.\\nExamples of this include:  \\n- Extracting a structured row to insert into a database from a sentence\\n- Extracting multiple rows to insert into a database from a long document\\n- Extracting the correct API parameters from a user query  \\nThis work is extremely related to [output parsing](/docs/modules/model_io/output_parsers/).\\nOutput parsers are responsible for instructing the LLM to respond in a specific format.\\nIn this case, the output parsers specify the format of the data you would like to extract from the document.\\nThen, in addition to the output format instructions, the prompt should also contain the data you would like to extract information from.  \\nWhile normal output parsers are good enough for basic structuring of response data,\\nwhen doing extraction you often want to extract more complicated or nested structures.\\nFor a deep dive on extraction, we recommend checking out [`kor`](https://eyurtsev.github.io/kor/),\\na library that uses the existing LangChain chain and OutputParser abstractions\\nbut deep dives on allowing extraction of more complicated schemas.', metadata={'Header 1': 'Extraction'}),\n",
       " Document(page_content='Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.\\nThis page covers all resources available in LangChain for working with data in this format.', metadata={'Header 1': 'Analyzing structured data'}),\n",
       " Document(page_content='If you have text data stored in a tabular format, you may want to load the data into a Document and then index it as you would\\nother text/unstructured data. For this, you should use a document loader like the [CSVLoader](/docs/modules/data_connection/document_loaders/how_to/csv.html)\\nand then you should [create an index](/docs/modules/data_connection) over that data, and [query it that way](/docs/modules/chains/popular/vector_db_qa.html).', metadata={'Header 1': 'Analyzing structured data', 'Header 2': 'Document loading'}),\n",
       " Document(page_content=\"If you have more numeric tabular data, or have a large amount of data and don't want to index it, you should get started\\nby looking at various chains and agents we have for dealing with this data.\", metadata={'Header 1': 'Analyzing structured data', 'Header 2': 'Querying'}),\n",
       " Document(page_content='If you are just getting started, and you have relatively small/simple tabular data, you should get started with chains.\\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\\nunderstand what is happening better.  \\n- [SQL Database Chain](/docs/modules/chains/popular/sqlite.html)', metadata={'Header 1': 'Analyzing structured data', 'Header 2': 'Querying', 'Header 3': 'Chains'}),\n",
       " Document(page_content='Agents are more complex, and involve multiple queries to the LLM to understand what to do.\\nThe downside of agents are that you have less control. The upside is that they are more powerful,\\nwhich allows you to use them on larger databases and more complex schemas.  \\n- [SQL Agent](/docs/modules/agents/toolkits/sql_database.html)\\n- [Pandas Agent](/docs/modules/agents/toolkits/pandas.html)\\n- [CSV Agent](/docs/modules/agents/toolkits/csv.html)', metadata={'Header 1': 'Analyzing structured data', 'Header 2': 'Querying', 'Header 3': 'Agents'}),\n",
       " Document(page_content='Summarization involves creating a smaller summary of multiple longer documents.\\nThis can be useful for distilling long documents into the core pieces of information.  \\nThe recommended way to get started using a summarization chain is:  \\n```python\\nfrom langchain.chains.summarize import load_summarize_chain\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\nchain.run(docs)\\n```  \\nThe following resources exist:\\n- [Summarization notebook](/docs/modules/chains/popular/summarize.html): A notebook walking through how to accomplish this task.  \\nAdditional related resources include:\\n- [Modules for working with documents](/docs/modules/data_connection): Core components for working with documents.', metadata={'Header 1': 'Summarization'}),\n",
       " Document(page_content=\"Autonomous Agents are agents that designed to be more long running.\\nYou give them one or multiple long term goals, and they independently execute towards those goals.\\nThe applications combine tool usage and long term memory.  \\nAt the moment, Autonomous Agents are fairly experimental and based off of other open-source projects.\\nBy implementing these open source projects in LangChain primitives we can get the benefits of LangChain -\\neasy switching and experimenting with multiple LLMs, usage of different vectorstores as memory,\\nusage of LangChain's collection of tools.\", metadata={'Header 1': 'Autonomous (long-running) agents'}),\n",
       " Document(page_content='- [Baby AGI](/docs/use_cases/autonomous_agents/aby_agi.html): a notebook implementing BabyAGI as LLM Chains\\n- [Baby AGI with Tools](/docs/use_cases/autonomous_agents/baby_agi_with_agent.html): building off the above notebook, this example substitutes in an agent with tools as the execution tools, allowing it to actually take actions.', metadata={'Header 1': 'Autonomous (long-running) agents', 'Header 2': 'Baby AGI ([Original Repo](https://github.com/yoheinakajima/babyagi))'}),\n",
       " Document(page_content='- [AutoGPT](/docs/use_cases/autonomous_agents/autogpt.html): a notebook implementing AutoGPT in LangChain primitives\\n- [WebSearch Research Assistant](/docs/use_cases/autonomous_agents/marathon_times.html): a notebook showing how to use AutoGPT plus specific tools to act as research assistant that can use the web.', metadata={'Header 1': 'Autonomous (long-running) agents', 'Header 2': 'AutoGPT ([Original Repo](https://github.com/Significant-Gravitas/Auto-GPT))'}),\n",
       " Document(page_content='- [Meta-Prompt](/docs/use_cases/autonomous_agents/meta_prompt.html): a notebook implementing Meta-Prompt in LangChain primitives', metadata={'Header 1': 'Autonomous (long-running) agents', 'Header 2': 'MetaPrompt ([Original Repo](https://github.com/ngoodman/metaprompt))'}),\n",
       " Document(page_content='Question answering in this context refers to question answering over your document data.\\nFor question answering over other types of data, please see other sources documentation like [SQL database Question Answering](/docs/use_cases/tabular.html) or [Interacting with APIs](/docs/use_cases/apis.html).  \\nFor question answering over many documents, you almost always want to create an index over the data.\\nThis can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money).  \\n**Load Your Documents**  \\n```python\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\\'../../modules/state_of_the_union.txt\\')\\n```  \\nSee [here](/docs/modules/data_connection/document_loaders/) for more information on how to get started with document loading.  \\n**Create Your Index**  \\n```python\\nfrom langchain.indexes import VectorstoreIndexCreator\\nindex = VectorstoreIndexCreator().from_loaders([loader])\\n```  \\nThe best and most popular index by far at the moment is the VectorStore index.  \\n**Query Your Index**  \\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nindex.query(query)\\n```  \\nAlternatively, use `query_with_sources` to also get back the sources involved  \\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nindex.query_with_sources(query)\\n```  \\nAgain, these high level interfaces obfuscate a lot of what is going on under the hood, so please see [this notebook](/docs/modules/data_connection/) for a more thorough introduction to data modules.', metadata={'Header 1': 'Question answering over documents'}),\n",
       " Document(page_content='Question answering involves fetching multiple documents, and then asking a question of them.\\nThe LLM response will contain the answer to your question, based on the content of the documents.  \\nThe recommended way to get started using a question answering chain is:  \\n```python\\nfrom langchain.chains.question_answering import load_qa_chain\\nchain = load_qa_chain(llm, chain_type=\"stuff\")\\nchain.run(input_documents=docs, question=query)\\n```  \\nThe following resources exist:  \\n- [Question Answering Notebook](/docs/modules/chains/additional/question_answering.html): A notebook walking through how to accomplish this task.\\n- [VectorDB Question Answering Notebook](/docs/modules/chains/popular/vector_db_qa.html): A notebook walking through how to do question answering over a vector database. This can often be useful for when you have a LOT of documents, and you don\\'t want to pass them all to the LLM, but rather first want to do some semantic search over embeddings.', metadata={'Header 1': 'Question answering over documents', 'Header 2': 'Document Question Answering'}),\n",
       " Document(page_content='There is also a variant of this, where in addition to responding with the answer the language model will also cite its sources (eg which of the documents passed in it used).  \\nThe recommended way to get started using a question answering with sources chain is:  \\n```python\\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\\nchain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```', metadata={'Header 1': 'Question answering over documents', 'Header 2': 'Adding in sources'}),\n",
       " Document(page_content='Additional related resources include:  \\n- [Building blocks for working with Documents](/docs/modules/data_connection/): Guides on how to use several of the utilities which will prove helpful for this task, including Text Splitters (for splitting up long documents) and Embeddings & Vectorstores (useful for the above Vector DB example).\\n- [CombineDocuments Chains](/docs/modules/chains/document/): A conceptual overview of specific types of chains by which you can accomplish this task.', metadata={'Header 1': 'Question answering over documents', 'Header 2': 'Additional Related Resources'}),\n",
       " Document(page_content='For examples to this done in an end-to-end manner, please see the following resources:  \\n- [Semantic search over a group chat with Sources Notebook](/docs/use_cases/question_answering/semantic-search-over-chat.html): A notebook that semantically searches over a group chat conversation.\\n- [Document context aware text splitting and QA](/docs/use_cases/question_answering/document-context-aware-QA.html): A notebook that shows context aware splitting on markdown files and SelfQueryRetriever for QA using the resulting metadata.', metadata={'Header 1': 'Question answering over documents', 'Header 2': 'End-to-end examples'}),\n",
       " Document(page_content='Agent simulations involve interacting one of more agents with each other.\\nAgent simulations generally involve two main components:  \\n- Long Term Memory\\n- Simulation Environment  \\nSpecific implementations of agent simulations (or parts of agent simulations) include:', metadata={'Header 1': 'Agent simulations'}),\n",
       " Document(page_content='- [Simulated Environment: Gymnasium](./gymnasium.html): an example of how to create a simple agent-environment interaction loop with [Gymnasium](https://gymnasium.farama.org/) (formerly [OpenAI Gym](https://github.com/openai/gym)).', metadata={'Header 1': 'Agent simulations', 'Header 2': 'Simulations with One Agent'}),\n",
       " Document(page_content='- [CAMEL](./camel_role_playing.html): an implementation of the CAMEL (Communicative Agents for “Mind” Exploration of Large Scale Language Model Society) paper, where two agents communicate with each other.\\n- [Two Player D&D](./two_player_dnd.html): an example of how to use a generic simulator for two agents to implement a variant of the popular Dungeons & Dragons role playing game.\\n- [Agent Debates with Tools](./two_agent_debate_tools.html): an example of how to enable Dialogue Agents to use tools to inform their responses.', metadata={'Header 1': 'Agent simulations', 'Header 2': 'Simulations with Two Agents'}),\n",
       " Document(page_content='- [Multi-Player D&D](./multi_player_dnd.html): an example of how to use a generic dialogue simulator for multiple dialogue agents with a custom speaker-ordering, illustrated with a variant of the popular Dungeons & Dragons role playing game.\\n- [Decentralized Speaker Selection](./multiagent_bidding.html): an example of how to implement a multi-agent dialogue without a fixed schedule for who speaks when. Instead the agents decide for themselves who speaks by outputting bids to speak. This example shows how to do this in the context of a fictitious presidential debate.\\n- [Authoritarian Speaker Selection](./multiagent_authoritarian.html): an example of how to implement a multi-agent dialogue, where a privileged agent directs who speaks what. This example also showcases how to enable the privileged agent to determine when the conversation terminates. This example shows how to do this in the context of a fictitious news show.\\n- [Simulated Environment: PettingZoo](./petting_zoo.html): an example of how to create a agent-environment interaction loop for multiple agents with [PettingZoo](https://pettingzoo.farama.org/) (a multi-agent version of [Gymnasium](https://gymnasium.farama.org/)).\\n- [Generative Agents](./characters.html): This notebook implements a generative agent based on the paper [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442) by Park, et. al.', metadata={'Header 1': 'Agent simulations', 'Header 2': 'Simulations with Multiple Agents'}),\n",
       " Document(page_content='Agents can be used for a variety of tasks.\\nAgents combine the decision making ability of a language model with tools in order to create a system\\nthat can execute and implement solutions on your behalf. Before reading any more, it is highly\\nrecommended that you read the documentation in the `agent` module to understand the concepts associated with agents more.\\nSpecifically, you should be familiar with what the `agent`, `tool`, and `agent executor` abstractions are before reading more.  \\n- [Agent documentation](/docs/modules/agents.html) (for interacting with the outside world)', metadata={'Header 1': 'Agents'}),\n",
       " Document(page_content=\"Once you have read that documentation, you should be prepared to create your own agent.\\nWhat exactly does that involve?\\nHere's how we recommend getting started with creating your own agent:\", metadata={'Header 1': 'Agents', 'Header 2': 'Create Your Own Agent'}),\n",
       " Document(page_content='Agents are largely defined by the tools they can use.\\nIf you have a specific task you want the agent to accomplish, you have to give it access to the right tools.\\nWe have many tools natively in LangChain, so you should first look to see if any of them meet your needs.\\nBut we also make it easy to define a custom tool, so if you need custom tools you should absolutely do that.', metadata={'Header 1': 'Agents', 'Header 2': 'Create Your Own Agent', 'Header 3': 'Step 1: Create Tools'}),\n",
       " Document(page_content='The built-in LangChain agent types are designed to work well in generic situations,\\nbut you may be able to improve performance by modifying the agent implementation.\\nThere are several ways you could do this:  \\n1. Modify the base prompt. This can be used to give the agent more context on how it should behave, etc.\\n2. Modify the output parser. This is necessary if the agent is having trouble parsing the language model output.', metadata={'Header 1': 'Agents', 'Header 2': 'Create Your Own Agent', 'Header 3': '(Optional) Step 2: Modify Agent'}),\n",
       " Document(page_content='This step is usually not necessary, as this is pretty general logic.\\nPossible reasons you would want to modify this include adding different stopping conditions, or handling errors', metadata={'Header 1': 'Agents', 'Header 2': 'Create Your Own Agent', 'Header 3': '(Optional) Step 3: Modify Agent Executor'}),\n",
       " Document(page_content='Specific examples of agents include:  \\n- [AI Plugins](./custom_agent_with_plugin_retrieval.html): an implementation of an agent that is designed to be able to use all AI Plugins.\\n- [Plug-and-PlAI (Plugins Database)](./custom_agent_with_plugin_retrieval_using_plugnplai.html): an implementation of an agent that is designed to be able to use all AI Plugins retrieved from PlugNPlAI.\\n- [Wikibase Agent](./wikibase_agent.html): an implementation of an agent that is designed to interact with Wikibase.\\n- [Sales GPT](./sales_agent_with_context.html): This notebook demonstrates an implementation of a Context-Aware AI Sales agent.\\n- [Multi-Modal Output Agent](./multi_modal_output_agent.html): an implementation of a multi-modal output agent that can generate text and images.', metadata={'Header 1': 'Agents', 'Header 2': 'Examples'}),\n",
       " Document(page_content='[comment: Please, a reference example here \"docs/integrations/arxiv.md\"]::\\n[comment: Use this template to create a new .md file in \"docs/integrations/\"]::', metadata={}),\n",
       " Document(page_content='[comment: Only one Tile/H1 is allowed!]::  \\n>  \\n[comment: Description: After reading this description, a reader should decide if this integration is good enough to try/follow reading OR]::\\n[comment: go to read the next integration doc. ]::\\n[comment: Description should include a link to the source for follow reading.]::', metadata={'Header 1': 'Title_REPLACE_ME'}),\n",
       " Document(page_content='[comment: Installation and Setup: All necessary additional package installations and set ups for Tokens, etc]::  \\n```bash\\npip install package_name_REPLACE_ME\\n```  \\n[comment: OR this text:]::\\nThere isn\\'t any special setup for it.  \\n[comment: The next H2/## sections with names of the integration modules, like \"LLM\", \"Text Embedding Models\", etc]::\\n[comment: see \"Modules\" in the \"index.html\" page]::\\n[comment: Each H2 section should include a link to an example(s) and a python code with import of the integration class]::\\n[comment: Below are several example sections. Remove all unnecessary sections. Add all necessary sections not provided here.]::', metadata={'Header 1': 'Title_REPLACE_ME', 'Header 2': 'Installation and Setup'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/model_io/models/llms/integrations/INCLUDE_REAL_NAME.html).  \\n```python\\nfrom langchain.llms import integration_class_REPLACE_ME\\n```', metadata={'Header 1': 'Title_REPLACE_ME', 'Header 2': 'LLM'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/text_embedding/integrations/INCLUDE_REAL_NAME.html)  \\n```python\\nfrom langchain.embeddings import integration_class_REPLACE_ME\\n```', metadata={'Header 1': 'Title_REPLACE_ME', 'Header 2': 'Text Embedding Models'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/model_io/models/chat/integrations/INCLUDE_REAL_NAME.html)  \\n```python\\nfrom langchain.chat_models import integration_class_REPLACE_ME\\n```', metadata={'Header 1': 'Title_REPLACE_ME', 'Header 2': 'Chat Models'}),\n",
       " Document(page_content='See a [usage example](/docs/modules/data_connection/document_loaders/integrations/INCLUDE_REAL_NAME.html).  \\n```python\\nfrom langchain.document_loaders import integration_class_REPLACE_ME\\n```', metadata={'Header 1': 'Title_REPLACE_ME', 'Header 2': 'Document Loader'}),\n",
       " Document(page_content=\"Since language models are good at producing text, that makes them ideal for creating chatbots.\\nAside from the base prompts/LLMs, an important concept to know for Chatbots is `memory`.\\nMost chat based applications rely on remembering what happened in previous interactions, which `memory` is designed to help with.  \\nThe following resources exist:\\n- [ChatGPT Clone](/docs/modules/agents/how_to/chatgpt_clone.html): A notebook walking through how to recreate a ChatGPT-like experience with LangChain.\\n- [Conversation Agent](/docs/modules/agents/agent_types/chat_conversation_agent.html): A notebook walking through how to create an agent optimized for conversation.  \\nAdditional related resources include:\\n- [Memory concepts and examples](/docs/modules/memory/): Explanation of key concepts related to memory along with how-to's and examples.  \\nMore end-to-end examples include:\\n- [Voice Assistant](./voice_assistant.html): A notebook walking through how to create a voice assistant using LangChain.\", metadata={'Header 1': 'Chatbots'}),\n",
       " Document(page_content='Overview  \\nLangChain is a useful tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code. This documentation page outlines the essential components of the system and guides using LangChain for better code comprehension, contextual question answering, and code generation in GitHub repositories.', metadata={'Header 1': 'Code Understanding'}),\n",
       " Document(page_content='Conversational RetrieverChain is a retrieval-focused system that interacts with the data stored in a VectorStore. Utilizing advanced techniques, like context-aware filtering and ranking, it retrieves the most relevant code snippets and information for a given user query. Conversational RetrieverChain is engineered to deliver high-quality, pertinent results while considering conversation history and context.  \\nLangChain Workflow for Code Understanding and Generation  \\n1. Index the code base: Clone the target repository, load all files within, chunk the files, and execute the indexing process. Optionally, you can skip this step and use an already indexed dataset.  \\n2. Embedding and Code Store: Code snippets are embedded using a code-aware embedding model and stored in a VectorStore.\\nQuery Understanding: GPT-4 processes user queries, grasping the context and extracting relevant details.  \\n3. Construct the Retriever: Conversational RetrieverChain searches the VectorStore to identify the most relevant code snippets for a given query.  \\n4. Build the Conversational Chain: Customize the retriever settings and define any user-defined filters as needed.  \\n5. Ask questions: Define a list of questions to ask about the codebase, and then use the ConversationalRetrievalChain to generate context-aware answers. The LLM (GPT-4) generates comprehensive, context-aware answers based on retrieved code snippets and conversation history.  \\nThe full tutorial is available below.\\n- [Twitter the-algorithm codebase analysis with Deep Lake](./twitter-the-algorithm-analysis-deeplake.html): A notebook walking through how to parse github source code and run queries conversation.\\n- [LangChain codebase analysis with Deep Lake](./code-analysis-deeplake.html): A notebook walking through how to analyze and do question answering over THIS code base.', metadata={'Header 1': 'Code Understanding', 'Header 2': 'Conversational Retriever Chain'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\nsidebar_custom_props:\\ndescription: Interface with language models\\n---', metadata={}),\n",
       " Document(page_content='The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.  \\n- [Prompts](/docs/modules/model_io/prompts/): Templatize, dynamically select, and manage model inputs\\n- [Language models](/docs/modules/model_io/models/): Make calls to language models through common interfaces\\n- [Output parsers](/docs/modules/model_io/output_parsers/): Extract information from model outputs  \\n![model_io_diagram](/img/model_io.jpg)', metadata={'Header 1': 'Model I/O'}),\n",
       " Document(page_content='---\\nsidebar_class_name: hidden\\n---', metadata={}),\n",
       " Document(page_content='LangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:  \\n#### [Model I/O](/docs/modules/model_io/)\\nInterface with language models\\n#### [Data connection](/docs/modules/data_connection/)\\nInterface with application-specific data\\n#### [Chains](/docs/modules/chains/)\\nConstruct sequences of calls\\n#### [Agents](/docs/modules/agents/)\\nLet chains choose which tools to use given high-level directives\\n#### [Memory](/docs/modules/memory/)\\nPersist application state between runs of a chain\\n#### [Callbacks](/docs/modules/callbacks/)\\nLog and stream intermediate steps of any chain', metadata={'Header 1': 'Modules'}),\n",
       " Document(page_content='Some LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it\\'s available. This is useful if you want to display the response to the user as it\\'s being generated, or if you want to process the response as it\\'s being generated.  \\nimport StreamingLLM from \"@snippets/modules/model_io/models/llms/how_to/streaming_llm.mdx\"  \\n<StreamingLLM/>', metadata={'Header 1': 'Streaming'}),\n",
       " Document(page_content='---\\nsidebar_position: 1\\n---', metadata={}),\n",
       " Document(page_content='LangChain provides interfaces and integrations for two types of models:  \\n- [LLMs](/docs/modules/model_io/models/llms/): Models that take a text string as input and return a text string\\n- [Chat models](/docs/modules/model_io/models/chat/): Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message', metadata={'Header 1': 'Language models'}),\n",
       " Document(page_content='LLMs and Chat Models are subtly but importantly different. LLMs in LangChain refer to pure text completion models.\\nThe APIs they wrap take a string prompt as input and output a string completion. OpenAI\\'s GPT-3 is implemented as an LLM.\\nChat models are often backed by LLMs but tuned specifically for having conversations.\\nAnd, crucially, their provider APIs expose a different interface than pure text completion models. Instead of a single string,\\nthey take a list of chat messages as input. Usually these messages are labeled with the speaker (usually one of \"System\",\\n\"AI\", and \"Human\"). And they return a (\"AI\") chat message as output. GPT-4 and Anthropic\\'s Claude are both implemented as Chat Models.  \\nTo make it possible to swap LLMs and Chat Models, both implement the Base Language Model interface. This exposes common\\nmethods \"predict\", which takes a string and returns a string, and \"predict messages\", which takes messages and returns a message.\\nIf you are using a specific model it\\'s recommended you use the methods specific to that model class (i.e., \"predict\" for LLMs and \"predict messages\" for Chat Models),\\nbut if you\\'re creating an application that should work with different types of models the shared interface can be helpful.', metadata={'Header 1': 'Language models', 'Header 2': 'LLMs vs Chat Models'}),\n",
       " Document(page_content='This is a collection of `LangChain` videos on `YouTube`.', metadata={'Header 1': 'YouTube tutorials'}),\n",
       " Document(page_content='- [Building the Future with LLMs, `LangChain`, & `Pinecone`](https://youtu.be/nMniwlGyX-c) by [Pinecone](https://www.youtube.com/@pinecone-io)\\n- [LangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36](https://youtu.be/lhby7Ql7hbk) by [Weaviate • Vector Database](https://www.youtube.com/@Weaviate)\\n- [LangChain Demo + Q&A with Harrison Chase](https://youtu.be/zaYTXQFR0_s?t=788) by [Full Stack Deep Learning](https://www.youtube.com/@FullStackDeepLearning)\\n- [LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin)](https://youtu.be/gVkF8cwfBLI) by [Chat with data](https://www.youtube.com/@chatwithdata)\\n- ⛓️ [LangChain \"Agents in Production\" Webinar](https://youtu.be/k8GNCCs16F4) by [LangChain](https://www.youtube.com/@LangChain)', metadata={'Header 1': 'YouTube tutorials', 'Header 3': 'Introduction to LangChain with Harrison Chase, creator of LangChain'}),\n",
       " Document(page_content=\"- [Building AI LLM Apps with LangChain (and more?) - LIVE STREAM](https://www.youtube.com/live/M-2Cj_2fzWI?feature=share) by [Nicholas Renotte](https://www.youtube.com/@NicholasRenotte)\\n- [First look - `ChatGPT` + `WolframAlpha` (`GPT-3.5` and Wolfram|Alpha via LangChain by James Weaver)](https://youtu.be/wYGbY811oMo) by [Dr Alan D. Thompson](https://www.youtube.com/@DrAlanDThompson)\\n- [LangChain explained - The hottest new Python framework](https://youtu.be/RoR4XJw8wIc) by [AssemblyAI](https://www.youtube.com/@AssemblyAI)\\n- [Chatbot with INFINITE MEMORY using `OpenAI` & `Pinecone` - `GPT-3`, `Embeddings`, `ADA`, `Vector DB`, `Semantic`](https://youtu.be/2xNzB7xq8nk) by [David Shapiro ~ AI](https://www.youtube.com/@DavidShapiroAutomator)\\n- [LangChain for LLMs is... basically just an Ansible playbook](https://youtu.be/X51N9C-OhlE) by [David Shapiro ~ AI](https://www.youtube.com/@DavidShapiroAutomator)\\n- [Build your own LLM Apps with LangChain & `GPT-Index`](https://youtu.be/-75p09zFUJY) by [1littlecoder](https://www.youtube.com/@1littlecoder)\\n- [`BabyAGI` - New System of Autonomous AI Agents with LangChain](https://youtu.be/lg3kJvf1kXo) by [1littlecoder](https://www.youtube.com/@1littlecoder)\\n- [Run `BabyAGI` with Langchain Agents (with Python Code)](https://youtu.be/WosPGHPObx8) by [1littlecoder](https://www.youtube.com/@1littlecoder)\\n- [How to Use Langchain With `Zapier` | Write and Send Email with GPT-3 | OpenAI API Tutorial](https://youtu.be/p9v2-xEa9A0) by [StarMorph AI](https://www.youtube.com/@starmorph)\\n- [Use Your Locally Stored Files To Get Response From GPT - `OpenAI` | Langchain | Python](https://youtu.be/NC1Ni9KS-rk) by [Shweta Lodha](https://www.youtube.com/@shweta-lodha)\\n- [`Langchain JS` | How to Use GPT-3, GPT-4 to Reference your own Data | `OpenAI Embeddings` Intro](https://youtu.be/veV2I-NEjaM) by [StarMorph AI](https://www.youtube.com/@starmorph)\\n- [The easiest way to work with large language models | Learn LangChain in 10min](https://youtu.be/kmbS6FDQh7c) by [Sophia Yang](https://www.youtube.com/@SophiaYangDS)\\n- [4 Autonomous AI Agents: “Westworld” simulation `BabyAGI`, `AutoGPT`, `Camel`, `LangChain`](https://youtu.be/yWbnH6inT_U) by [Sophia Yang](https://www.youtube.com/@SophiaYangDS)\\n- [AI CAN SEARCH THE INTERNET? Langchain Agents + OpenAI ChatGPT](https://youtu.be/J-GL0htqda8) by [tylerwhatsgood](https://www.youtube.com/@tylerwhatsgood)\\n- [Query Your Data with GPT-4 | Embeddings, Vector Databases | Langchain JS Knowledgebase](https://youtu.be/jRnUPUTkZmU) by [StarMorph AI](https://www.youtube.com/@starmorph)\\n- [`Weaviate` + LangChain for LLM apps presented by Erika Cardenas](https://youtu.be/7AGj4Td5Lgw) by [`Weaviate` • Vector Database](https://www.youtube.com/@Weaviate)\\n- [Langchain Overview — How to Use Langchain & `ChatGPT`](https://youtu.be/oYVYIq0lOtI) by [Python In Office](https://www.youtube.com/@pythoninoffice6568)\\n- [Langchain Overview - How to Use Langchain & `ChatGPT`](https://youtu.be/oYVYIq0lOtI) by [Python In Office](https://www.youtube.com/@pythoninoffice6568)\\n- [Custom langchain Agent & Tools with memory. Turn any `Python function` into langchain tool with Gpt 3](https://youtu.be/NIG8lXk0ULg) by [echohive](https://www.youtube.com/@echohive)\\n- [LangChain: Run Language Models Locally - `Hugging Face Models`](https://youtu.be/Xxxuw4_iCzw) by [Prompt Engineering](https://www.youtube.com/@engineerprompt)\\n- [`ChatGPT` with any `YouTube` video using langchain and `chromadb`](https://youtu.be/TQZfB2bzVwU) by [echohive](https://www.youtube.com/@echohive)\\n- [How to Talk to a `PDF` using LangChain and `ChatGPT`](https://youtu.be/v2i1YDtrIwk) by [Automata Learning Lab](https://www.youtube.com/@automatalearninglab)\\n- [Langchain Document Loaders Part 1: Unstructured Files](https://youtu.be/O5C0wfsen98) by [Merk](https://www.youtube.com/@merksworld)\\n- [LangChain - Prompt Templates (what all the best prompt engineers use)](https://youtu.be/1aRu8b0XNOQ) by [Nick Daigler](https://www.youtube.com/@nick_daigs)\\n- [LangChain. Crear aplicaciones Python impulsadas por GPT](https://youtu.be/DkW_rDndts8) by [Jesús Conde](https://www.youtube.com/@0utKast)\\n- [Easiest Way to Use GPT In Your Products | LangChain Basics Tutorial](https://youtu.be/fLy0VenZyGc) by [Rachel Woods](https://www.youtube.com/@therachelwoods)\\n- [`BabyAGI` + `GPT-4` Langchain Agent with Internet Access](https://youtu.be/wx1z_hs5P6E) by [tylerwhatsgood](https://www.youtube.com/@tylerwhatsgood)\\n- [Learning LLM Agents. How does it actually work? LangChain, AutoGPT & OpenAI](https://youtu.be/mb_YAABSplk) by [Arnoldas Kemeklis](https://www.youtube.com/@processusAI)\\n- [Get Started with LangChain in `Node.js`](https://youtu.be/Wxx1KUWJFv4) by [Developers Digest](https://www.youtube.com/@DevelopersDigest)\\n- [LangChain + `OpenAI` tutorial: Building a Q&A system w/ own text data](https://youtu.be/DYOU_Z0hAwo) by [Samuel Chan](https://www.youtube.com/@SamuelChan)\\n- [Langchain + `Zapier` Agent](https://youtu.be/yribLAb-pxA) by [Merk](https://www.youtube.com/@merksworld)\\n- [Connecting the Internet with `ChatGPT` (LLMs) using Langchain And Answers Your Questions](https://youtu.be/9Y0TBC63yZg) by [Kamalraj M M](https://www.youtube.com/@insightbuilder)\\n- [Build More Powerful LLM Applications for Business’s with LangChain (Beginners Guide)](https://youtu.be/sp3-WLKEcBg) by[ No Code Blackbox](https://www.youtube.com/@nocodeblackbox)\\n- ⛓️ [LangFlow LLM Agent Demo for 🦜🔗LangChain](https://youtu.be/zJxDHaWt-6o) by [Cobus Greyling](https://www.youtube.com/@CobusGreylingZA)\\n- ⛓️ [Chatbot Factory: Streamline Python Chatbot Creation with LLMs and Langchain](https://youtu.be/eYer3uzrcuM) by [Finxter](https://www.youtube.com/@CobusGreylingZA)\\n- ⛓️ [LangChain Tutorial - ChatGPT mit eigenen Daten](https://youtu.be/0XDLyY90E2c) by [Coding Crashkurse](https://www.youtube.com/@codingcrashkurse6429)\\n- ⛓️ [Chat with a `CSV` | LangChain Agents Tutorial (Beginners)](https://youtu.be/tjeti5vXWOU) by [GoDataProf](https://www.youtube.com/@godataprof)\\n- ⛓️ [Introdução ao Langchain - #Cortes - Live DataHackers](https://youtu.be/fw8y5VRei5Y) by [Prof. João Gabriel Lima](https://www.youtube.com/@profjoaogabriellima)\\n- ⛓️ [LangChain: Level up `ChatGPT` !? | LangChain Tutorial Part 1](https://youtu.be/vxUGx8aZpDE) by [Code Affinity](https://www.youtube.com/@codeaffinitydev)\\n- ⛓️ [KI schreibt krasses Youtube Skript 😲😳 | LangChain Tutorial Deutsch](https://youtu.be/QpTiXyK1jus) by [SimpleKI](https://www.youtube.com/@simpleki)\\n- ⛓️ [Chat with Audio: Langchain, `Chroma DB`, OpenAI, and `Assembly AI`](https://youtu.be/Kjy7cx1r75g) by [AI Anytime](https://www.youtube.com/@AIAnytime)\\n- ⛓️ [QA over documents with Auto vector index selection with Langchain router chains](https://youtu.be/9G05qybShv8) by [echohive](https://www.youtube.com/@echohive)\\n- ⛓️ [Build your own custom LLM application with `Bubble.io` & Langchain (No Code & Beginner friendly)](https://youtu.be/O7NhQGu1m6c) by [No Code Blackbox](https://www.youtube.com/@nocodeblackbox)\\n- ⛓️ [Simple App to Question Your Docs: Leveraging `Streamlit`, `Hugging Face Spaces`, LangChain, and `Claude`!](https://youtu.be/X4YbNECRr7o) by [Chris Alexiuk](https://www.youtube.com/@chrisalexiuk)\\n- ⛓️ [LANGCHAIN AI- `ConstitutionalChainAI` + Databutton AI ASSISTANT Web App](https://youtu.be/5zIU6_rdJCU) by [Avra](https://www.youtube.com/@Avra_b)\\n- ⛓️ [LANGCHAIN AI AUTONOMOUS AGENT WEB APP - 👶 `BABY AGI` 🤖 with EMAIL AUTOMATION using `DATABUTTON`](https://youtu.be/cvAwOGfeHgw) by [Avra](https://www.youtube.com/@Avra_b)\\n- ⛓️ [The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain)](https://youtu.be/v_LIcVyg5dk) by [Absent Data](https://www.youtube.com/@absentdata)\\n- ⛓️ [Memory in LangChain | Deep dive (python)](https://youtu.be/70lqvTFh_Yg) by [Eden Marco](https://www.youtube.com/@EdenMarco)\\n- ⛓️ [9 LangChain UseCases | Beginner's Guide | 2023](https://youtu.be/zS8_qosHNMw) by [Data Science Basics](https://www.youtube.com/@datasciencebasics)\\n- ⛓️ [Use Large Language Models in Jupyter Notebook | LangChain | Agents & Indexes](https://youtu.be/JSe11L1a_QQ) by [Abhinaw Tiwari](https://www.youtube.com/@AbhinawTiwariAT)\\n- ⛓️ [How to Talk to Your Langchain Agent | `11 Labs` + `Whisper`](https://youtu.be/N4k459Zw2PU) by [VRSEN](https://www.youtube.com/@vrsen)\\n- ⛓️ [LangChain Deep Dive: 5 FUN AI App Ideas To Build Quickly and Easily](https://youtu.be/mPYEPzLkeks) by [James NoCode](https://www.youtube.com/@jamesnocode)\\n- ⛓️ [BEST OPEN Alternative to OPENAI's EMBEDDINGs for Retrieval QA: LangChain](https://youtu.be/ogEalPMUCSY) by [Prompt Engineering](https://www.youtube.com/@engineerprompt)\\n- ⛓️ [LangChain 101: Models](https://youtu.be/T6c_XsyaNSQ) by [Mckay Wrigley](https://www.youtube.com/@realmckaywrigley)\\n- ⛓️ [LangChain with JavaScript Tutorial #1 | Setup & Using LLMs](https://youtu.be/W3AoeMrg27o) by [Leon van Zyl](https://www.youtube.com/@leonvanzyl)\\n- ⛓️ [LangChain Overview & Tutorial for Beginners: Build Powerful AI Apps Quickly & Easily (ZERO CODE)](https://youtu.be/iI84yym473Q) by [James NoCode](https://www.youtube.com/@jamesnocode)\\n- ⛓️ [LangChain In Action: Real-World Use Case With Step-by-Step Tutorial](https://youtu.be/UO699Szp82M) by [Rabbitmetrics](https://www.youtube.com/@rabbitmetrics)\\n- ⛓️ [Summarizing and Querying Multiple Papers with LangChain](https://youtu.be/p_MQRWH5Y6k) by [Automata Learning Lab](https://www.youtube.com/@automatalearninglab)\\n- ⛓️ [Using Langchain (and `Replit`) through `Tana`, ask `Google`/`Wikipedia`/`Wolfram Alpha` to fill out a table](https://youtu.be/Webau9lEzoI) by [Stian Håklev](https://www.youtube.com/@StianHaklev)\\n- ⛓️ [Langchain PDF App (GUI) | Create a ChatGPT For Your `PDF` in Python](https://youtu.be/wUAUdEw5oxM) by [Alejandro AO - Software & Ai](https://www.youtube.com/@alejandro_ao)\\n- ⛓️ [Auto-GPT with LangChain 🔥 | Create Your Own Personal AI Assistant](https://youtu.be/imDfPmMKEjM) by [Data Science Basics](https://www.youtube.com/@datasciencebasics)\\n- ⛓️ [Create Your OWN Slack AI Assistant with Python & LangChain](https://youtu.be/3jFXRNn2Bu8) by [Dave Ebbelaar](https://www.youtube.com/@daveebbelaar)\\n- ⛓️ [How to Create LOCAL Chatbots with GPT4All and LangChain [Full Guide]](https://youtu.be/4p1Fojur8Zw) by [Liam Ottley](https://www.youtube.com/@LiamOttley)\\n- ⛓️ [Build a `Multilingual PDF` Search App with LangChain, `Cohere` and `Bubble`](https://youtu.be/hOrtuumOrv8) by [Menlo Park Lab](https://www.youtube.com/@menloparklab)\\n- ⛓️ [Building a LangChain Agent (code-free!) Using `Bubble` and `Flowise`](https://youtu.be/jDJIIVWTZDE) by [Menlo Park Lab](https://www.youtube.com/@menloparklab)\\n- ⛓️ [Build a LangChain-based Semantic PDF Search App with No-Code Tools Bubble and Flowise](https://youtu.be/s33v5cIeqA4) by [Menlo Park Lab](https://www.youtube.com/@menloparklab)\\n- ⛓️ [LangChain Memory Tutorial | Building a ChatGPT Clone in Python](https://youtu.be/Cwq91cj2Pnc) by [Alejandro AO - Software & Ai](https://www.youtube.com/@alejandro_ao)\\n- ⛓️ [ChatGPT For Your DATA | Chat with Multiple Documents Using LangChain](https://youtu.be/TeDgIDqQmzs) by [Data Science Basics](https://www.youtube.com/@datasciencebasics)\\n- ⛓️ [`Llama Index`: Chat with Documentation using URL Loader](https://youtu.be/XJRoDEctAwA) by [Merk](https://www.youtube.com/@merksworld)\\n- ⛓️ [Using OpenAI, LangChain, and `Gradio` to Build Custom GenAI Applications](https://youtu.be/1MsmqMg3yUc) by [David Hundley](https://www.youtube.com/@dkhundley)\\n- ⛓️ [LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF](https://youtu.be/FuqdVNB_8c0)\\n- [LangChain Crash Course: Build an AutoGPT app in 25 minutes](https://youtu.be/MlK6SIjcjE8) by [Nicholas Renotte](https://www.youtube.com/@NicholasRenotte)\\n- [LangChain Crash Course - Build apps with language models](https://youtu.be/LbT1yp6quS8) by [Patrick Loeber](https://www.youtube.com/@patloeber)\\n- [LangChain Explained in 13 Minutes | QuickStart Tutorial for Beginners](https://youtu.be/aywZrzNaKjs) by [Rabbitmetrics](https://www.youtube.com/@rabbitmetrics)\", metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Videos (sorted by views)'}),\n",
       " Document(page_content='⛓ icon marks a new addition [last update 2023-05-15]', metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series'}),\n",
       " Document(page_content='⛓[LangChain for LLM Application Development](https://learn.deeplearning.ai/langchain) by Harrison Chase presented by [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng)', metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series', 'Header 3': 'DeepLearning.AI course'}),\n",
       " Document(page_content='[LangChain AI Handbook](https://www.pinecone.io/learn/langchain/) By **James Briggs** and **Francisco Ingham**', metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series', 'Header 3': 'Handbook'}),\n",
       " Document(page_content=\"[LangChain Tutorials](https://www.youtube.com/watch?v=FuqdVNB_8c0&list=PL9V0lbeJ69brU-ojMpU1Y7Ic58Tap0Cw6) by [Edrick](https://www.youtube.com/@edrickdch):\\n- ⛓ [LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF](https://youtu.be/FuqdVNB_8c0)\\n- ⛓ [LangChain 101: The Complete Beginner's Guide](https://youtu.be/P3MAbZ2eMUI)  \\n[LangChain Crash Course: Build an AutoGPT app in 25 minutes](https://youtu.be/MlK6SIjcjE8) by [Nicholas Renotte](https://www.youtube.com/@NicholasRenotte)  \\n[LangChain Crash Course - Build apps with language models](https://youtu.be/LbT1yp6quS8) by [Patrick Loeber](https://www.youtube.com/@patloeber)  \\n[LangChain Explained in 13 Minutes | QuickStart Tutorial for Beginners](https://youtu.be/aywZrzNaKjs) by [Rabbitmetrics](https://www.youtube.com/@rabbitmetrics)\", metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series', 'Header 3': 'Tutorials'}),\n",
       " Document(page_content='- #1 [Getting Started with `GPT-3` vs. Open Source LLMs](https://youtu.be/nE2skSRWTTs)\\n- #2 [Prompt Templates for `GPT 3.5` and other LLMs](https://youtu.be/RflBcK0oDH0)\\n- #3 [LLM Chains using `GPT 3.5` and other LLMs](https://youtu.be/S8j9Tk0lZHU)\\n- #4 [Chatbot Memory for `Chat-GPT`, `Davinci` + other LLMs](https://youtu.be/X05uK0TZozM)\\n- #5 [Chat with OpenAI in LangChain](https://youtu.be/CnAgB3A5OlU)\\n- ⛓ #6 [Fixing LLM Hallucinations with Retrieval Augmentation in LangChain](https://youtu.be/kvdVduIJsc8)\\n- ⛓ #7 [LangChain Agents Deep Dive with GPT 3.5](https://youtu.be/jSP-gSEyVeI)\\n- ⛓ #8 [Create Custom Tools for Chatbots in LangChain](https://youtu.be/q-HNphrWsDE)\\n- ⛓ #9 [Build Conversational Agents with Vector DBs](https://youtu.be/H6bCqqw9xyI)', metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series', 'Header 3': '[LangChain for Gen AI and LLMs](https://www.youtube.com/playlist?list=PLIUOU7oqGTLieV9uTIFMm6_4PXg-hlN6F) by [James Briggs](https://www.youtube.com/@jamesbriggs):'}),\n",
       " Document(page_content=\"- [What Is LangChain? - LangChain + `ChatGPT` Overview](https://youtu.be/_v_fgW2SkkQ)\\n- [Quickstart Guide](https://youtu.be/kYRB-vJFy38)\\n- [Beginner Guide To 7 Essential Concepts](https://youtu.be/2xxziIWmaSA)\\n- [`OpenAI` + `Wolfram Alpha`](https://youtu.be/UijbzCIJ99g)\\n- [Ask Questions On Your Custom (or Private) Files](https://youtu.be/EnT-ZTrcPrg)\\n- [Connect `Google Drive Files` To `OpenAI`](https://youtu.be/IqqHqDcXLww)\\n- [`YouTube Transcripts` + `OpenAI`](https://youtu.be/pNcQ5XXMgH4)\\n- [Question A 300 Page Book (w/ `OpenAI` + `Pinecone`)](https://youtu.be/h0DHDp1FbmQ)\\n- [Workaround `OpenAI's` Token Limit With Chain Types](https://youtu.be/f9_BWhCI4Zo)\\n- [Build Your Own OpenAI + LangChain Web App in 23 Minutes](https://youtu.be/U_eV8wfMkXU)\\n- [Working With The New `ChatGPT API`](https://youtu.be/e9P7FLi5Zy8)\\n- [OpenAI + LangChain Wrote Me 100 Custom Sales Emails](https://youtu.be/y1pyAQM-3Bo)\\n- [Structured Output From `OpenAI` (Clean Dirty Data)](https://youtu.be/KwAXfey-xQk)\\n- [Connect `OpenAI` To +5,000 Tools (LangChain + `Zapier`)](https://youtu.be/7tNm0yiDigU)\\n- [Use LLMs To Extract Data From Text (Expert Mode)](https://youtu.be/xZzvwR9jdPA)\\n- ⛓ [Extract Insights From Interview Transcripts Using LLMs](https://youtu.be/shkMOHwJ4SM)\\n- ⛓ [5 Levels Of LLM Summarizing: Novice to Expert](https://youtu.be/qaPMdcCqtWk)\", metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series', 'Header 3': '[LangChain 101](https://www.youtube.com/playlist?list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5) by [Data Independent](https://www.youtube.com/@DataIndependent):'}),\n",
       " Document(page_content=\"- [LangChain Basics - LLMs & PromptTemplates with Colab](https://youtu.be/J_0qvRt4LNk)\\n- [LangChain Basics - Tools and Chains](https://youtu.be/hI2BY7yl_Ac)\\n- [`ChatGPT API` Announcement & Code Walkthrough with LangChain](https://youtu.be/phHqvLHCwH4)\\n- [Conversations with Memory (explanation & code walkthrough)](https://youtu.be/X550Zbz_ROE)\\n- [Chat with `Flan20B`](https://youtu.be/VW5LBavIfY4)\\n- [Using `Hugging Face Models` locally (code walkthrough)](https://youtu.be/Kn7SX2Mx_Jk)\\n- [`PAL` : Program-aided Language Models with LangChain code](https://youtu.be/dy7-LvDu-3s)\\n- [Building a Summarization System with LangChain and `GPT-3` - Part 1](https://youtu.be/LNq_2s_H01Y)\\n- [Building a Summarization System with LangChain and `GPT-3` - Part 2](https://youtu.be/d-yeHDLgKHw)\\n- [Microsoft's `Visual ChatGPT` using LangChain](https://youtu.be/7YEiEyfPF5U)\\n- [LangChain Agents - Joining Tools and Chains with Decisions](https://youtu.be/ziu87EXZVUE)\\n- [Comparing LLMs with LangChain](https://youtu.be/rFNG0MIEuW0)\\n- [Using `Constitutional AI` in LangChain](https://youtu.be/uoVqNFDwpX4)\\n- [Talking to `Alpaca` with LangChain - Creating an Alpaca Chatbot](https://youtu.be/v6sF8Ed3nTE)\\n- [Talk to your `CSV` & `Excel` with LangChain](https://youtu.be/xQ3mZhw69bc)\\n- [`BabyAGI`: Discover the Power of Task-Driven Autonomous Agents!](https://youtu.be/QBcDLSE2ERA)\\n- [Improve your `BabyAGI` with LangChain](https://youtu.be/DRgPyOXZ-oE)\\n- ⛓ [Master `PDF` Chat with LangChain - Your essential guide to queries on documents](https://youtu.be/ZzgUqFtxgXI)\\n- ⛓ [Using LangChain with `DuckDuckGO` `Wikipedia` & `PythonREPL` Tools](https://youtu.be/KerHlb8nuVc)\\n- ⛓ [Building Custom Tools and Agents with LangChain (gpt-3.5-turbo)](https://youtu.be/biS8G8x8DdA)\\n- ⛓ [LangChain Retrieval QA Over Multiple Files with `ChromaDB`](https://youtu.be/3yPBVii7Ct0)\\n- ⛓ [LangChain Retrieval QA with Instructor Embeddings & `ChromaDB` for PDFs](https://youtu.be/cFCGUjc33aU)\\n- ⛓ [LangChain + Retrieval Local LLMs for Retrieval QA - No OpenAI!!!](https://youtu.be/9ISVjh8mdlA)\", metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series', 'Header 3': '[LangChain How to and guides](https://www.youtube.com/playlist?list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ) by [Sam Witteveen](https://www.youtube.com/@samwitteveenai):'}),\n",
       " Document(page_content='- [LangChain Crash Course — All You Need to Know to Build Powerful Apps with LLMs](https://youtu.be/5-fc4Tlgmro)\\n- [Working with MULTIPLE `PDF` Files in LangChain: `ChatGPT` for your Data](https://youtu.be/s5LhRdh5fu4)\\n- [`ChatGPT` for YOUR OWN `PDF` files with LangChain](https://youtu.be/TLf90ipMzfE)\\n- [Talk to YOUR DATA without OpenAI APIs: LangChain](https://youtu.be/wrD-fZvT6UI)\\n- ⛓️ [CHATGPT For WEBSITES: Custom ChatBOT](https://youtu.be/RBnuhhmD21U)', metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series', 'Header 3': '[LangChain](https://www.youtube.com/playlist?list=PLVEEucA9MYhOu89CX8H3MBZqayTbcCTMr) by [Prompt Engineering](https://www.youtube.com/@engineerprompt):'}),\n",
       " Document(page_content=\"- [LangChain Beginner's Tutorial for `Typescript`/`Javascript`](https://youtu.be/bH722QgRlhQ)\\n- [`GPT-4` Tutorial: How to Chat With Multiple `PDF` Files (~1000 pages of Tesla's 10-K Annual Reports)](https://youtu.be/Ix9WIZpArm0)\\n- [`GPT-4` & LangChain Tutorial: How to Chat With A 56-Page `PDF` Document (w/`Pinecone`)](https://youtu.be/ih9PBGVVOO4)\\n- ⛓ [LangChain & Supabase Tutorial: How to Build a ChatGPT Chatbot For Your Website](https://youtu.be/R2FMzcsmQY8)\", metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series', 'Header 3': 'LangChain by [Chat with data](https://www.youtube.com/@chatwithdata)'}),\n",
       " Document(page_content='- [Getting Started with LangChain: Load Custom Data, Run OpenAI Models, Embeddings and `ChatGPT`](https://www.youtube.com/watch?v=muXbPpG_ys4)\\n- [Loaders, Indexes & Vectorstores in LangChain: Question Answering on `PDF` files with `ChatGPT`](https://www.youtube.com/watch?v=FQnvfR8Dmr0)\\n- [LangChain Models: `ChatGPT`, `Flan Alpaca`, `OpenAI Embeddings`, Prompt Templates & Streaming](https://www.youtube.com/watch?v=zy6LiK5F5-s)\\n- [LangChain Chains: Use `ChatGPT` to Build Conversational Agents, Summaries and Q&A on Text With LLMs](https://www.youtube.com/watch?v=h1tJZQPcimM)\\n- [Analyze Custom CSV Data with `GPT-4` using Langchain](https://www.youtube.com/watch?v=Ew3sGdX8at4)\\n- ⛓ [Build ChatGPT Chatbots with LangChain Memory: Understanding and Implementing Memory in Conversations](https://youtu.be/CyuUlf54wTs)  \\n---------------------\\n⛓ icon marks a new addition [last update 2023-05-15]', metadata={'Header 1': 'YouTube tutorials', 'Header 2': 'Tutorial Series', 'Header 3': '[Get SH\\\\*T Done with Prompt Engineering and LangChain](https://www.youtube.com/watch?v=muXbPpG_ys4&list=PLEJK-H61Xlwzm5FYLDdKt_6yibO33zoMW) by [Venelin Valkov](https://www.youtube.com/@venelin_valkov)'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content=\"Large Language Models (LLMs) are a core component of LangChain.\\nLangChain does not serve it's own LLMs, but rather provides a standard interface for interacting with many different LLMs.  \\nFor more detailed documentation check out our:  \\n- **How-to guides**: Walkthroughs of core functionality, like streaming, async, etc.  \\n- **Integrations**: How to use different LLM providers (OpenAI, Anthropic, etc.)\", metadata={'Header 1': 'LLMs'}),\n",
       " Document(page_content='There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the `LLM` class is designed to provide a standard interface for all of them.  \\nIn this walkthrough we\\'ll work with an OpenAI LLM wrapper, although the functionalities highlighted are generic for all LLM types.  \\nimport LLMGetStarted from \"@snippets/modules/model_io/models/llms/get_started.mdx\"  \\n<LLMGetStarted/>', metadata={'Header 1': 'LLMs', 'Header 2': 'Get started'}),\n",
       " Document(page_content='LangChain provides an optional caching layer for LLMs. This is useful for two reasons:  \\nIt can save you money by reducing the number of API calls you make to the LLM provider, if you\\'re often requesting the same completion multiple times.\\nIt can speed up your application by reducing the number of API calls you make to the LLM provider.  \\nimport CachingLLM from \"@snippets/modules/model_io/models/llms/how_to/llm_caching.mdx\"  \\n<CachingLLM/>', metadata={'Header 1': 'Caching'}),\n",
       " Document(page_content='You can use the existing LLMChain in a very similar way to before - provide a prompt and a model.  \\nimport LLMChain from \"@snippets/modules/model_io/models/chat/how_to/llm_chain.mdx\"  \\n<LLMChain/>', metadata={'Header 1': 'LLMChain'}),\n",
       " Document(page_content='---\\nsidebar_position: 1\\n---', metadata={}),\n",
       " Document(page_content='Chat models are a variation on language models.\\nWhile chat models use language models under the hood, the interface they expose is a bit different.\\nRather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.  \\nChat model APIs are fairly new, so we are still figuring out the correct abstractions.  \\nThe following sections of documentation are provided:  \\n- **How-to guides**: Walkthroughs of core functionality, like streaming, creating chat prompts, etc.  \\n- **Integrations**: How to use different chat model providers (OpenAI, Anthropic, etc).', metadata={'Header 1': 'Chat models'}),\n",
       " Document(page_content='import GetStarted from \"@snippets/modules/model_io/models/chat/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Chat models', 'Header 2': 'Get started'}),\n",
       " Document(page_content='Prompts for Chat models are built around messages, instead of just plain text.  \\nimport Prompts from \"@snippets/modules/model_io/models/chat/how_to/prompts.mdx\"  \\n<Prompts/>', metadata={'Header 1': 'Prompts'}),\n",
       " Document(page_content='This output parser can be used when you want to return a list of comma-separated items.  \\nimport Example from \"@snippets/modules/model_io/output_parsers/comma_separated.mdx\"  \\n<Example/>', metadata={'Header 1': 'List parser'}),\n",
       " Document(page_content='Some Chat models provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it\\'s available. This is useful if you want to display the response to the user as it\\'s being generated, or if you want to process the response as it\\'s being generated.  \\nimport StreamingChatModel from \"@snippets/modules/model_io/models/chat/how_to/streaming.mdx\"  \\n<StreamingChatModel/>', metadata={'Header 1': 'Streaming'}),\n",
       " Document(page_content='LangChain provides an optional caching layer for Chat Models. This is useful for two reasons:  \\nIt can save you money by reducing the number of API calls you make to the LLM provider, if you\\'re often requesting the same completion multiple times.\\nIt can speed up your application by reducing the number of API calls you make to the LLM provider.  \\nimport CachingChat from \"@snippets/modules/model_io/models/chat/how_to/chat_model_caching.mdx\"  \\n<CachingChat/>', metadata={'Header 1': 'Caching'}),\n",
       " Document(page_content='This output parser can be used when you want to return multiple fields. While the Pydantic/JSON parser is more powerful, we initially experimented with data structures having text fields only.  \\nimport Example from \"@snippets/modules/model_io/output_parsers/structured.mdx\"  \\n<Example/>', metadata={'Header 1': 'Structured output parser'}),\n",
       " Document(page_content='This output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.  \\nBut we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it.  \\nimport Example from \"@snippets/modules/model_io/output_parsers/output_fixing_parser.mdx\"  \\n<Example/>', metadata={'Header 1': 'Auto-fixing parser'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content='The new way of programming models is through prompts.\\nA **prompt** refers to the input to the model.\\nThis input is often constructed from multiple components.\\nLangChain provides several classes and functions to make constructing and working with prompts easy.  \\n- [Prompt templates](/docs/modules/model_io/prompts/prompt_templates/): Parametrize model inputs\\n- [Example selectors](/docs/modules/model_io/prompts/example_selectors/): Dynamically select examples to include in prompts', metadata={'Header 1': 'Prompts'}),\n",
       " Document(page_content='This object selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.  \\nimport Example from \"@snippets/modules/model_io/prompts/example_selectors/similarity.mdx\"  \\n<Example/>', metadata={'Header 1': 'Select by similarity'}),\n",
       " Document(page_content='---\\nsidebar_position: 2\\n---', metadata={}),\n",
       " Document(page_content='Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.  \\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:  \\n- \"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n- \"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.  \\nAnd then one optional one:  \\n- \"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.', metadata={'Header 1': 'Output parsers'}),\n",
       " Document(page_content='import GetStarted from \"@snippets/modules/model_io/output_parsers/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Output parsers', 'Header 2': 'Get started'}),\n",
       " Document(page_content='If you have a large number of examples, you may need to select which ones to include in the prompt. The Example Selector is the class responsible for doing so.  \\nThe base interface is defined as below:  \\nimport GetStarted from \"@snippets/modules/model_io/prompts/example_selectors/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Example selectors'}),\n",
       " Document(page_content='This example selector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.  \\nimport Example from \"@snippets/modules/model_io/prompts/example_selectors/length_based.mdx\"  \\n<Example/>', metadata={'Header 1': 'Select by length'}),\n",
       " Document(page_content='This notebook goes over how to compose multiple prompts together. This can be useful when you want to reuse parts of prompts. This can be done with a PipelinePrompt. A PipelinePrompt consists of two main parts:  \\n- Final prompt: This is the final prompt that is returned\\n- Pipeline prompts: This is a list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name.  \\nimport Example from \"@snippets/modules/model_io/prompts/prompt_templates/prompt_composition.mdx\"  \\n<Example/>', metadata={'Header 1': 'Composition'}),\n",
       " Document(page_content='In this tutorial, we\\'ll learn how to create a prompt template that uses few shot examples. A few shot prompt template can be constructed from either a set of examples, or from an Example Selector object.  \\nimport Example from \"@snippets/modules/model_io/prompts/prompt_templates/few_shot_examples.mdx\"  \\n<Example/>', metadata={'Header 1': 'Few-shot prompt templates'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content='Language models take text as input - that text is commonly referred to as a prompt.\\nTypically this is not simply a hardcoded string but rather a combination of a template, some examples, and user input.\\nLangChain provides several classes and functions to make constructing and working with prompts easy.', metadata={'Header 1': 'Prompt templates'}),\n",
       " Document(page_content='A prompt template refers to a reproducible way to generate a prompt. It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt.  \\nA prompt template can contain:\\n- instructions to the language model,\\n- a set of few shot examples to help the language model generate a better response,\\n- a question to the language model.  \\nimport GetStarted from \"@snippets/modules/model_io/prompts/prompt_templates/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Prompt templates', 'Header 2': 'What is a prompt template?'}),\n",
       " Document(page_content='Like other methods, it can make sense to \"partial\" a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.  \\nLangChain supports this in two ways:\\n1. Partial formatting with string values.\\n2. Partial formatting with functions that return string values.  \\nThese two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.  \\nimport Example from \"@snippets/modules/model_io/prompts/prompt_templates/partial.mdx\"  \\n<Example/>', metadata={'Header 1': 'Partial prompt templates'}),\n",
       " Document(page_content='This notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, [specifically prohibit](https://beta.openai.com/docs/usage-policies/use-case-policy) you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.  \\nIf the content passed into the moderation chain is harmful, there is not one best way to handle it, it probably depends on your application. Sometimes you may want to throw an error in the Chain (and have your application handle that). Other times, you may want to return something to the user explaining that the text was harmful. There could even be other ways to handle it! We will cover all these ways in this walkthrough.  \\nimport Example from \"@snippets/modules/chains/additional/moderation.mdx\"  \\n<Example/>', metadata={'Header 1': 'Moderation'}),\n",
       " Document(page_content='This notebook demonstrates how to use the `RouterChain` paradigm to create a chain that dynamically selects the prompt to use for a given input. Specifically we show how to use the `MultiPromptChain` to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt.  \\nimport Example from \"@snippets/modules/chains/additional/multi_prompt_router.mdx\"  \\n<Example/>', metadata={'Header 1': 'Dynamically selecting from multiple prompts'}),\n",
       " Document(page_content='---\\nsidebar_position: 2\\n---', metadata={}),\n",
       " Document(page_content='Using an LLM in isolation is fine for simple applications,\\nbut more complex applications require chaining LLMs - either with each other or with other components.  \\nLangChain provides the **Chain** interface for such \"chained\" applications. We define a Chain very generically as a sequence of calls to components, which can include other chains. The base interface is simple:  \\nimport BaseClass from \"@snippets/modules/chains/base_class.mdx\"  \\n<BaseClass/>  \\nThis idea of composing components together in a chain is simple but powerful. It drastically simplifies and makes more modular the implementation of complex applications, which in turn makes it much easier to debug, maintain, and improve your applications.  \\nFor more specifics check out:\\n- [How-to](/docs/modules/chains/how_to/) for walkthroughs of different chain features\\n- [Foundational](/docs/modules/chains/foundational/) to get acquainted with core building block chains\\n- [Document](/docs/modules/chains/document/) to learn how to incorporate documents into chains\\n- [Popular](/docs/modules/chains/popular/) chains for the most common use cases\\n- [Additional](/docs/modules/chains/additional/) to see some of the more advanced chains and integrations that you can use out of the box', metadata={'Header 1': 'Chains'}),\n",
       " Document(page_content='Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.', metadata={'Header 1': 'Chains', 'Header 2': 'Why do we need chains?'}),\n",
       " Document(page_content='import GetStarted from \"@snippets/modules/chains/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Chains', 'Header 2': 'Get started'}),\n",
       " Document(page_content='---\\nsidebar_position: 4\\n---', metadata={}),\n",
       " Document(page_content='import DocCardList from \"@theme/DocCardList\";  \\n<DocCardList />', metadata={'Header 1': 'Additional'}),\n",
       " Document(page_content='The ConstitutionalChain is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the ConstitutionalChain filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context.  \\nimport Example from \"@snippets/modules/chains/additional/constitutional_chain.mdx\"  \\n<Example/>', metadata={'Header 1': 'Self-critique chain with constitutional AI'}),\n",
       " Document(page_content='Here we walk through how to use LangChain for question answering over a list of documents. Under the hood we\\'ll be using our [Document chains](/docs/modules/chains/document/).  \\nimport Example from \"@snippets/modules/chains/additional/question_answering.mdx\"  \\n<Example/>', metadata={'Header 1': 'Document QA'}),\n",
       " Document(page_content='import ExampleWithSources from \"@snippets/modules/chains/additional/qa_with_sources.mdx\"  \\n<ExampleWithSources/>', metadata={'Header 1': 'Document QA', 'Header 2': 'Document QA with sources'}),\n",
       " Document(page_content='This notebook demonstrates how to use the `RouterChain` paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the `MultiRetrievalQAChain` to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.  \\nimport Example from \"@snippets/modules/chains/additional/multi_retrieval_qa_router.mdx\"  \\n<Example/>', metadata={'Header 1': 'Dynamically selecting from multiple retrievers'}),\n",
       " Document(page_content='It can be hard to debug a `Chain` object solely from its output as most `Chain` objects involve a fair amount of input prompt preprocessing and LLM output post-processing.  \\nimport Example from \"@snippets/modules/chains/how_to/debugging.mdx\"  \\n<Example/>', metadata={'Header 1': 'Debugging chains'}),\n",
       " Document(page_content='The map re-rank documents chain runs an initial prompt on each document, that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest scoring response is returned.  \\n![map_rerank_diagram](/img/map_rerank.jpg)', metadata={'Header 1': 'Map re-rank'}),\n",
       " Document(page_content='Chains can be initialized with a Memory object, which will persist data across calls to the chain. This makes a Chain stateful.', metadata={'Header 1': 'Adding memory (state)'}),\n",
       " Document(page_content='import GetStarted from \"@snippets/modules/chains/how_to/memory.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Adding memory (state)', 'Header 2': 'Get started'}),\n",
       " Document(page_content='The AnalyzeDocumentChain can be used as an end-to-end to chain. This chain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain.  \\nimport Example from \"@snippets/modules/chains/additional/analyze_document.mdx\"  \\n<Example/>', metadata={'Header 1': 'Analyze Document'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content='import DocCardList from \"@theme/DocCardList\";  \\n<DocCardList />', metadata={'Header 1': 'How to'}),\n",
       " Document(page_content='---\\nsidebar_position: 2\\n---', metadata={}),\n",
       " Document(page_content='These are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more.  \\nThese chains all implement a common interface:  \\nimport Interface from \"@snippets/modules/chains/document/combine_docs.mdx\"  \\n<Interface/>  \\nimport DocCardList from \"@theme/DocCardList\";  \\n<DocCardList />', metadata={'Header 1': 'Documents'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content='The stuff documents chain (\"stuff\" as in \"to stuff\" or \"to fill\") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.  \\nThis chain is well-suited for applications where documents are small and only a few are passed in for most calls.  \\n![stuff_diagram](/img/stuff.jpg)', metadata={'Header 1': 'Stuff'}),\n",
       " Document(page_content='---\\nsidebar_position: 1\\n---', metadata={}),\n",
       " Document(page_content='This example showcases question answering over an index.  \\nimport Example from \"@snippets/modules/chains/popular/vector_db_qa.mdx\"  \\n<Example/>  \\nimport ExampleWithSources from \"@snippets/modules/chains/popular/vector_db_qa_with_sources.mdx\"  \\n<ExampleWithSources/>', metadata={'Header 1': 'Retrieval QA'}),\n",
       " Document(page_content='The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step). It can optionally first compress, or collapse, the mapped documents to make sure that they fit in the combine documents chain (which will often pass them to an LLM). This compression step is performed recursively if necessary.  \\n![map_reduce_diagram](/img/map_reduce.jpg)', metadata={'Header 1': 'Map reduce'}),\n",
       " Document(page_content='This example demonstrates the use of the `SQLDatabaseChain` for answering questions over a SQL database.  \\nimport Example from \"@snippets/modules/chains/popular/sqlite.mdx\"  \\n<Example/>', metadata={'Header 1': 'SQL'}),\n",
       " Document(page_content='---\\nsidebar_position: 1\\n---', metadata={}),\n",
       " Document(page_content=\"The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.  \\nSince the Refine chain only passes a single document to the LLM at a time, it is well-suited for tasks that require analyzing more documents than can fit in the model's context.\\nThe obvious tradeoff is that this chain will make far more LLM calls than, for example, the Stuff documents chain.\\nThere are also certain tasks which are difficult to accomplish iteratively. For example, the Refine chain can perform poorly when documents frequently cross-reference one another or when a task requires detailed information from many documents.  \\n![refine_diagram](/img/refine.jpg)\", metadata={'Header 1': 'Refine'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content='APIChain enables using LLMs to interact with APIs to retrieve relevant information. Construct the chain by providing a question relevant to the provided API documentation.  \\nimport Example from \"@snippets/modules/chains/popular/api.mdx\"  \\n<Example/>', metadata={'Header 1': 'API chains'}),\n",
       " Document(page_content='---\\nsidebar_position: 3\\n---', metadata={}),\n",
       " Document(page_content='import DocCardList from \"@theme/DocCardList\";  \\n<DocCardList />', metadata={'Header 1': 'Popular'}),\n",
       " Document(page_content='---\\nsidebar_position: 1\\n---', metadata={}),\n",
       " Document(page_content='import DocCardList from \"@theme/DocCardList\";  \\n<DocCardList />', metadata={'Header 1': 'Foundational'}),\n",
       " Document(page_content='---\\nsidebar_position: 2\\n---', metadata={}),\n",
       " Document(page_content='The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.  \\nIt first combines the chat history (either explicitly passed in or retrieved from the provided memory) and the question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question answering chain to return a response.  \\nTo create one, you will need a retriever. In the below example, we will create one from a vector store, which can be created from embeddings.  \\nimport Example from \"@snippets/modules/chains/popular/chat_vector_db.mdx\"  \\n<Example/>', metadata={'Header 1': 'Conversational Retrieval QA'}),\n",
       " Document(page_content='A summarization chain can be used to summarize multiple documents. One way is to input multiple smaller documents, after they have been divided into chunks, and operate over them with a MapReduceDocumentsChain. You can also choose instead for the chain that does summarization to be a StuffDocumentsChain, or a RefineDocumentsChain.  \\nimport Example from \"@snippets/modules/chains/popular/summarize.mdx\"  \\n<Example/>', metadata={'Header 1': 'Summarization'}),\n",
       " Document(page_content='An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.  \\nAn LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.', metadata={'Header 1': 'LLM'}),\n",
       " Document(page_content='import Example from \"@snippets/modules/chains/foundational/llm_chain.mdx\"  \\n<Example/>', metadata={'Header 1': 'LLM', 'Header 2': 'Get started'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content='Use document loaders to load data from a source as `Document`\\'s. A `Document` is a piece of text\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.  \\nDocument loaders expose a \"load\" method for loading data as documents from a configured source. They optionally\\nimplement a \"lazy load\" as well for lazily loading data into memory.', metadata={'Header 1': 'Document loaders'}),\n",
       " Document(page_content='import GetStarted from \"@snippets/modules/data_connection/document_loaders/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Document loaders', 'Header 2': 'Get started'}),\n",
       " Document(page_content='---\\nsidebar_position: 1\\n---', metadata={}),\n",
       " Document(page_content=\"Many LLM applications require user-specific data that is not part of the model's training set. LangChain gives you the\\nbuilding blocks to load, transform, store and query your data via:  \\n- [Document loaders](/docs/modules/data_connection/document_loaders/): Load documents from many different sources\\n- [Document transformers](/docs/modules/data_connection/document_transformers/): Split documents, drop redundant documents, and more\\n- [Text embedding models](/docs/modules/data_connection/text_embedding/): Take unstructured text and turn it into a list of floating point numbers\\n- [Vector stores](/docs/modules/data_connection/vectorstores/): Store and search over embedded data\\n- [Retrievers](/docs/modules/data_connection/retrievers/): Query your data  \\n![data_connection_diagram](/img/data_connection.jpg)\", metadata={'Header 1': 'Data connection'}),\n",
       " Document(page_content='<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->  \\nThe next step after calling a language model is make a series of calls to a language model. This is particularly useful when you want to take the output from one call and use it as the input to another.  \\nIn this notebook we will walk through some examples for how to do this, using sequential chains. Sequential chains allow you to connect multiple chains and compose them into pipelines that execute some specific scenario.. There are two types of sequential chains:  \\n- `SimpleSequentialChain`: The simplest form of sequential chains, where each step has a singular input/output, and the output of one step is the input to the next.\\n- `SequentialChain`: A more general form of sequential chains, allowing for multiple inputs/outputs.  \\nimport Example from \"@snippets/modules/chains/foundational/sequential_chains.mdx\"  \\n<Example/>', metadata={'Header 1': 'Sequential'}),\n",
       " Document(page_content='>A [comma-separated values (CSV)](https://en.wikipedia.org/wiki/Comma-separated_values) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.  \\nLoad CSV data with a single row per document.  \\nimport Example from \"@snippets/modules/data_connection/document_loaders/how_to/csv.mdx\"  \\n<Example/>', metadata={'Header 1': 'CSV'}),\n",
       " Document(page_content='>[The HyperText Markup Language or HTML](https://en.wikipedia.org/wiki/HTML) is the standard markup language for documents designed to be displayed in a web browser.  \\nThis covers how to load `HTML` documents into a document format that we can use downstream.  \\nimport Example from \"@snippets/modules/data_connection/document_loaders/how_to/html.mdx\"  \\n<Example/>', metadata={'Header 1': 'HTML'}),\n",
       " Document(page_content='>[Markdown](https://en.wikipedia.org/wiki/Markdown) is a lightweight markup language for creating formatted text using a plain-text editor.  \\nThis covers how to load `Markdown` documents into a document format that we can use downstream.  \\nimport Example from \"@snippets/modules/data_connection/document_loaders/how_to/markdown.mdx\"  \\n<Example/>', metadata={'Header 1': 'Markdown'}),\n",
       " Document(page_content='This covers how to load all documents in a directory.  \\nimport Example from \"@snippets/modules/data_connection/document_loaders/how_to/file_directory.mdx\"  \\n<Example/>', metadata={'Header 1': 'File Directory'}),\n",
       " Document(page_content='>[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).  \\n>[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.  \\nimport Example from \"@snippets/modules/data_connection/document_loaders/how_to/json.mdx\"  \\n<Example/>', metadata={'Header 1': 'JSON'}),\n",
       " Document(page_content='>[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.  \\nThis covers how to load `PDF` documents into the Document format that we use downstream.  \\nimport Example from \"@snippets/modules/data_connection/document_loaders/how_to/pdf.mdx\"  \\n<Example/>', metadata={'Header 1': 'PDF'}),\n",
       " Document(page_content='This retriever uses a combination of semantic similarity and a time decay.  \\nThe algorithm for scoring them is:  \\n```\\nsemantic_similarity + (1.0 - decay_rate) ^ hours_passed\\n```  \\nNotably, `hours_passed` refers to the hours passed since the object in the retriever **was last accessed**, not since it was created. This means that frequently accessed objects remain \"fresh.\"  \\nimport Example from \"@snippets/modules/data_connection/retrievers/how_to/time_weighted_vectorstore.mdx\"  \\n<Example/>', metadata={'Header 1': 'Time-weighted vector store retriever'}),\n",
       " Document(page_content='A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the Vector Store class to make it conform to the Retriever interface.\\nIt uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.  \\nOnce you construct a Vector store, it\\'s very easy to construct a retriever. Let\\'s walk through an example.  \\nimport Example from \"@snippets/modules/data_connection/retrievers/how_to/vectorstore.mdx\"  \\n<Example/>', metadata={'Header 1': 'Vector store-backed retriever'}),\n",
       " Document(page_content='---\\nsidebar_position: 4\\n---', metadata={}),\n",
       " Document(page_content='A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.\\nA retriever does not need to be able to store documents, only to return (or retrieve) it. Vector stores can be used\\nas the backbone of a retriever, but there are other types of retrievers as well.', metadata={'Header 1': 'Retrievers'}),\n",
       " Document(page_content='import GetStarted from \"@snippets/modules/data_connection/retrievers/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Retrievers', 'Header 2': 'Get started'}),\n",
       " Document(page_content='---\\nsidebar_position: 2\\n---', metadata={}),\n",
       " Document(page_content='The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.  \\nEmbeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.  \\nThe base Embeddings class in LangChain exposes two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).', metadata={'Header 1': 'Text embedding models'}),\n",
       " Document(page_content='import GetStarted from \"@snippets/modules/data_connection/text_embedding/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Text embedding models', 'Header 2': 'Get started'}),\n",
       " Document(page_content=\"One challenge with retrieval is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.  \\nContextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.  \\nTo use the Contextual Compression Retriever, you'll need:\\n- a base Retriever\\n- a Document Compressor  \\nThe Contextual Compression Retriever passes queries to the base Retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of Documents and shortens it by reducing the contents of Documents or dropping Documents altogether.  \\n![](https://drive.google.com/uc?id=1CtNgWODXZudxAWSRiWgSGEoTNrUFT98v)\", metadata={'Header 1': 'Contextual compression'}),\n",
       " Document(page_content='import Example from \"@snippets/modules/data_connection/retrievers/contextual_compression/get_started.mdx\"  \\n<Example/>', metadata={'Header 1': 'Contextual compression', 'Header 2': 'Get started'}),\n",
       " Document(page_content='A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it\\'s underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.  \\n![](https://drive.google.com/uc?id=1OQUN-0MJcDUxmPXofgS7MqReEs720pqS)  \\nimport Example from \"@snippets/modules/data_connection/retrievers/self_query/get_started.mdx\"  \\n<Example/>', metadata={'Header 1': 'Self-querying'}),\n",
       " Document(page_content='CodeTextSplitter allows you to split your code with multiple language support. Import enum `Language` and specify the language.  \\nimport Example from \"@snippets/modules/data_connection/document_transformers/text_splitters/code_splitter.mdx\"  \\n<Example/>', metadata={'Header 1': 'Split code'}),\n",
       " Document(page_content='---\\nsidebar_position: 1\\n---', metadata={}),\n",
       " Document(page_content=\"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example\\nis you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain\\nhas a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\", metadata={'Header 1': 'Document transformers'}),\n",
       " Document(page_content='When you want to deal with long pieces of text, it is necessary to split up that text into chunks.\\nAs simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text.\\nThis notebook showcases several ways to do that.  \\nAt a high level, text splitters work as following:  \\n1. Split the text up into small, semantically meaningful chunks (often sentences).\\n2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\\n3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).  \\nThat means there are two different axes along which you can customize your text splitter:  \\n1. How the text is split\\n2. How the chunk size is measured', metadata={'Header 1': 'Document transformers', 'Header 2': 'Text splitters'}),\n",
       " Document(page_content='import GetStarted from \"@snippets/modules/data_connection/document_transformers/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Document transformers', 'Header 2': 'Get started with text splitters'}),\n",
       " Document(page_content='This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.  \\n1. How the text is split: by list of characters\\n2. How the chunk size is measured: by number of characters  \\nimport Example from \"@snippets/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter.mdx\"  \\n<Example/>', metadata={'Header 1': 'Recursively split by character'}),\n",
       " Document(page_content='This is the simplest method. This splits based on characters (by default \"\\\\n\\\\n\") and measure chunk length by number of characters.  \\n1. How the text is split: by single character\\n2. How the chunk size is measured: by number of characters  \\nimport Example from \"@snippets/modules/data_connection/document_transformers/text_splitters/character_text_splitter.mdx\"  \\n<Example/>', metadata={'Header 1': 'Split by character'}),\n",
       " Document(page_content='---\\nsidebar_position: 4\\n---', metadata={}),\n",
       " Document(page_content='Some applications require a flexible chain of calls to LLMs and other tools based on user input. The **Agent** interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.  \\nThere are two main types of agents:  \\n- **Action agents**: at each timestep, decide on the next action using the outputs of all previous actions\\n- **Plan-and-execute agents**: decide on the full sequence of actions up front, then execute them all without updating the plan  \\nAction agents are suitable for small tasks, while plan-and-execute agents are better for complex or long-running tasks that require maintaining long-term objectives and focus. Often the best approach is to combine the dynamism of an action agent with the planning abilities of a plan-and-execute agent by letting the plan-and-execute agent use action agents to execute plans.  \\nFor a full list of agent types see [agent types](/docs/modules/agents/agent_types/). Additional abstractions involved in agents are:\\n- [**Tools**](/docs/modules/agents/tools/): the actions an agent can take. What tools you give an agent highly depend on what you want the agent to do\\n- [**Toolkits**](/docs/modules/agents/toolkits/): wrappers around collections of tools that can be used together a specific use case. For example, in order for an agent to\\ninteract with a SQL database it will likely need one tool to execute queries and another to inspect tables', metadata={'Header 1': 'Agents'}),\n",
       " Document(page_content='At a high-level an action agent:\\n1. Receives user input\\n2. Decides which tool, if any, to use and the tool input\\n3. Calls the tool and records the output (also known as an \"observation\")\\n4. Decides the next step using the history of tools, tool inputs, and observations\\n5. Repeats 3-4 until it determines it can respond directly to the user  \\nAction agents are wrapped in **agent executors**, which are responsible for calling the agent, getting back an action and action input, calling the tool that the action references with the generated input, getting the output of the tool, and then passing all that information back into the agent to get the next action it should take.  \\nAlthough an agent can be constructed in many ways, it typically involves these components:  \\n- **Prompt template**: Responsible for taking the user input and previous steps and constructing a prompt\\nto send to the language model\\n- **Language model**: Takes the prompt with use input and action history and decides what to do next\\n- **Output parser**: Takes the output of the language model and parses it into the next action or a final answer', metadata={'Header 1': 'Agents', 'Header 2': 'Action agents'}),\n",
       " Document(page_content='At a high-level a plan-and-execute agent:\\n1. Receives user input\\n2. Plans the full sequence of steps to take\\n3. Executes the steps in order, passing the outputs of past steps as inputs to future steps  \\nThe most typical implementation is to have the planner be a language model, and the executor be an action agent. Read more [here](/docs/modules/agents/agent_types/plan_and_execute.html).', metadata={'Header 1': 'Agents', 'Header 2': 'Plan-and-execute agents'}),\n",
       " Document(page_content='import GetStarted from \"@snippets/modules/agents/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Agents', 'Header 2': 'Get started'}),\n",
       " Document(page_content='---\\nsidebar_position: 3\\n---', metadata={}),\n",
       " Document(page_content=\"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding\\nvectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are\\n'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search\\nfor you.\", metadata={'Header 1': 'Vector stores'}),\n",
       " Document(page_content='This walkthrough showcases basic functionality related to VectorStores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the [text embedding model](/docs/modules/data_connection/text_embedding/) interfaces before diving into this.  \\nimport GetStarted from \"@snippets/modules/data_connection/vectorstores/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Vector stores', 'Header 2': 'Get started'}),\n",
       " Document(page_content='---\\nsidebar_position: 3\\n---', metadata={}),\n",
       " Document(page_content='Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.  \\nimport DocCardList from \"@theme/DocCardList\";  \\n<DocCardList />', metadata={'Header 1': 'Toolkits'}),\n",
       " Document(page_content='---\\nsidebar_position: 2\\n---', metadata={}),\n",
       " Document(page_content='Tools are interfaces that an agent can use to interact with the world.', metadata={'Header 1': 'Tools'}),\n",
       " Document(page_content='Tools are functions that agents can use to interact with the world.\\nThese tools can be generic utilities (e.g. search), other chains, or even other agents.  \\nCurrently, tools can be loaded with the following snippet:  \\nimport GetStarted from \"@snippets/modules/agents/tools/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Tools', 'Header 2': 'Get started'}),\n",
       " Document(page_content='This notebook goes through how to create your own custom agent based on a chat model.  \\nAn LLM chat agent consists of three parts:  \\n- PromptTemplate: This is the prompt template that can be used to instruct the language model on what to do\\n- ChatModel: This is the language model that powers the agent\\n- `stop` sequence: Instructs the LLM to stop generating as soon as this string is found\\n- OutputParser: This determines how to parse the LLMOutput into an AgentAction or AgentFinish object  \\nimport Example from \"@snippets/modules/agents/how_to/custom_llm_chat_agent.mdx\"  \\n<Example/>', metadata={'Header 1': 'Custom LLM Agent (with a ChatModel)'}),\n",
       " Document(page_content='This notebook goes through how to create your own custom LLM agent.  \\nAn LLM agent consists of three parts:  \\n- PromptTemplate: This is the prompt template that can be used to instruct the language model on what to do\\n- LLM: This is the language model that powers the agent\\n- `stop` sequence: Instructs the LLM to stop generating as soon as this string is found\\n- OutputParser: This determines how to parse the LLMOutput into an AgentAction or AgentFinish object  \\nimport Example from \"@snippets/modules/agents/how_to/custom_llm_agent.mdx\"  \\n<Example/>', metadata={'Header 1': 'Custom LLM Agent'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content='Agents use an LLM to determine which actions to take and in what order.\\nAn action can either be using a tool and observing its output, or returning a response to the user.\\nHere are the agents available in LangChain.', metadata={'Header 1': 'Agent types', 'Header 2': 'Action agents'}),\n",
       " Document(page_content=\"This agent uses the [ReAct](https://arxiv.org/pdf/2205.00445.pdf) framework to determine which tool to use\\nbased solely on the tool's description. Any number of tools can be provided.\\nThis agent requires that a description is provided for each tool.  \\n**Note**: This is the most general purpose action agent.\", metadata={'Header 1': 'Agent types', 'Header 2': 'Action agents', 'Header 3': '[Zero-shot ReAct](/docs/modules/agents/agent_types/react.html)'}),\n",
       " Document(page_content=\"The structured tool chat agent is capable of using multi-input tools.\\nOlder agents are configured to specify an action input as a single string, but this agent can use a tools' argument\\nschema to create a structured action input. This is useful for more complex tool usage, like precisely\\nnavigating around a browser.\", metadata={'Header 1': 'Agent types', 'Header 2': 'Action agents', 'Header 3': '[Structured input ReAct](/docs/modules/agents/agent_types/structured_chat.html)'}),\n",
       " Document(page_content='Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been explicitly fine-tuned to detect when a\\nfunction should to be called and respond with the inputs that should be passed to the function.\\nThe OpenAI Functions Agent is designed to work with these models.', metadata={'Header 1': 'Agent types', 'Header 2': 'Action agents', 'Header 3': '[OpenAI Functions](/docs/modules/agents/agent_types/openai_functions_agent.html)'}),\n",
       " Document(page_content='This agent is designed to be used in conversational settings.\\nThe prompt is designed to make the agent helpful and conversational.\\nIt uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions.', metadata={'Header 1': 'Agent types', 'Header 2': 'Action agents', 'Header 3': '[Conversational](/docs/modules/agents/agent_types/chat_conversation_agent.html)'}),\n",
       " Document(page_content='This agent utilizes a single tool that should be named `Intermediate Answer`.\\nThis tool should be able to lookup factual answers to questions. This agent\\nis equivalent to the original [self ask with search paper](https://ofir.io/self-ask.pdf),\\nwhere a Google search API was provided as the tool.', metadata={'Header 1': 'Agent types', 'Header 2': 'Action agents', 'Header 3': '[Self ask with search](/docs/modules/agents/agent_types/self_ask_with_search.html)'}),\n",
       " Document(page_content='This agent uses the ReAct framework to interact with a docstore. Two tools must\\nbe provided: a `Search` tool and a `Lookup` tool (they must be named exactly as so).\\nThe `Search` tool should search for a document, while the `Lookup` tool should lookup\\na term in the most recently found document.\\nThis agent is equivalent to the\\noriginal [ReAct paper](https://arxiv.org/pdf/2210.03629.pdf), specifically the Wikipedia example.', metadata={'Header 1': 'Agent types', 'Header 2': 'Action agents', 'Header 3': '[ReAct document store](/docs/modules/agents/agent_types/react_docstore.html)'}),\n",
       " Document(page_content='Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by [BabyAGI](https://github.com/yoheinakajima/babyagi) and then the [\"Plan-and-Solve\" paper](https://arxiv.org/abs/2305.04091).', metadata={'Header 1': 'Agent types', 'Header 2': '[Plan-and-execute agents](/docs/modules/agents/agent_types/plan_and_execute.html)'}),\n",
       " Document(page_content='This walkthrough demonstrates how to use an agent optimized for conversation. Other agents are often optimized for using tools to figure out the best response, which is not ideal in a conversational setting where you may want the agent to be able to chat with the user as well.  \\nimport Example from \"@snippets/modules/agents/agent_types/conversational_agent.mdx\"  \\n<Example/>  \\nimport ChatExample from \"@snippets/modules/agents/agent_types/chat_conversation_agent.mdx\"', metadata={'Header 1': 'Conversational'}),\n",
       " Document(page_content='<ChatExample/>', metadata={'Header 1': 'Conversational', 'Header 2': 'Using a chat model'}),\n",
       " Document(page_content='This walkthrough demonstrates how to replicate the [MRKL](https://arxiv.org/pdf/2205.00445.pdf) system using agents.  \\nThis uses the example Chinook database.\\nTo set it up follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the `.db` file in a notebooks folder at the root of this repository.  \\nimport Example from \"@snippets/modules/agents/how_to/mrkl.mdx\"  \\n<Example/>', metadata={'Header 1': 'Replicating MRKL'}),\n",
       " Document(page_content='import ChatExample from \"@snippets/modules/agents/how_to/mrkl_chat.mdx\"  \\n<ChatExample/>', metadata={'Header 1': 'Replicating MRKL', 'Header 2': 'With a chat model'}),\n",
       " Document(page_content='Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by [BabyAGI](https://github.com/yoheinakajima/babyagi) and then the [\"Plan-and-Solve\" paper](https://arxiv.org/abs/2305.04091).  \\nThe planning is almost always done by an LLM.  \\nThe execution is usually done by a separate agent (equipped with tools).  \\nimport Example from \"@snippets/modules/agents/agent_types/plan_and_execute.mdx\"  \\n<Example/>', metadata={'Header 1': 'Plan and execute'}),\n",
       " Document(page_content='The structured tool chat agent is capable of using multi-input tools.  \\nOlder agents are configured to specify an action input as a single string, but this agent can use the provided tools\\' `args_schema` to populate the action input.  \\nimport Example from \"@snippets/modules/agents/agent_types/structured_chat.mdx\"  \\n<Example/>', metadata={'Header 1': 'Structured tool chat'}),\n",
       " Document(page_content='Certain OpenAI models (like gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to detect when a function should to be called and respond with the inputs that should be passed to the function.\\nIn an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call those functions.\\nThe goal of the OpenAI Function APIs is to more reliably return valid and useful function calls than a generic text completion or chat API.  \\nThe OpenAI Functions Agent is designed to work with these models.  \\nimport Example from \"@snippets/modules/agents/agent_types/openai_functions_agent.mdx\";  \\n<Example/>', metadata={'Header 1': 'OpenAI functions'}),\n",
       " Document(page_content='This walkthrough showcases using an agent to implement the [ReAct](https://react-lm.github.io/) logic.  \\nimport Example from \"@snippets/modules/agents/agent_types/react.mdx\"  \\n<Example/>', metadata={'Header 1': 'ReAct'}),\n",
       " Document(page_content='You can also create ReAct agents that use chat models instead of LLMs as the agent driver.  \\nimport ChatExample from \"@snippets/modules/agents/agent_types/react_chat.mdx\"  \\n<ChatExample/>', metadata={'Header 1': 'ReAct', 'Header 2': 'Using chat models'}),\n",
       " Document(page_content='---\\nsidebar_position: 3\\n---', metadata={}),\n",
       " Document(page_content='🚧 _Docs under construction_ 🚧  \\nBy default, Chains and Agents are stateless,\\nmeaning that they treat each incoming query independently (like the underlying LLMs and chat models themselves).\\nIn some applications, like chatbots, it is essential\\nto remember previous interactions, both in the short and long-term.\\nThe **Memory** class does exactly that.  \\nLangChain provides memory components in two forms.\\nFirst, LangChain provides helper utilities for managing and manipulating previous chat messages.\\nThese are designed to be modular and useful regardless of how they are used.\\nSecondly, LangChain provides easy ways to incorporate these utilities into chains.', metadata={'Header 1': 'Memory'}),\n",
       " Document(page_content='Memory involves keeping a concept of state around throughout a user\\'s interactions with an language model. A user\\'s interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type.  \\nIn general, for each type of memory there are two ways to understanding using memory. These are the standalone functions which extract information from a sequence of messages, and then there is the way you can use this type of memory in a chain.  \\nMemory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages.  \\nimport GetStarted from \"@snippets/modules/memory/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Memory', 'Header 2': 'Get started'}),\n",
       " Document(page_content='`ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large  \\nLet\\'s first explore the basic functionality of this type of memory.  \\nimport Example from \"@snippets/modules/memory/how_to/buffer_window.mdx\"  \\n<Example/>', metadata={'Header 1': 'Conversation buffer window memory'}),\n",
       " Document(page_content='This notebook shows how to use `ConversationBufferMemory`. This memory allows for storing of messages and then extracts the messages in a variable.  \\nWe can first extract it as a string.  \\nimport Example from \"@snippets/modules/memory/how_to/buffer.mdx\"  \\n<Example/>', metadata={'Header 1': 'Conversation buffer memory'}),\n",
       " Document(page_content='`VectorStoreRetrieverMemory` stores memories in a VectorDB and queries the top-K most \"salient\" docs every time it is called.  \\nThis differs from most of the other Memory classes in that it doesn\\'t explicitly track the order of interactions.  \\nIn this case, the \"docs\" are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation.  \\nimport Example from \"@snippets/modules/memory/how_to/vectorstore_retriever_memory.mdx\"  \\n<Example/>', metadata={'Header 1': 'Vector store-backed memory'}),\n",
       " Document(page_content='Entity Memory remembers given facts about specific entities in a conversation. It extracts information on entities (using an LLM) and builds up its knowledge about that entity over time (also using an LLM).  \\nLet\\'s first walk through using this functionality.  \\nimport Example from \"@snippets/modules/memory/how_to/entity_summary_memory.mdx\"  \\n<Example/>', metadata={'Header 1': 'Entity memory'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content='import DocCardList from \"@theme/DocCardList\";  \\n<DocCardList />', metadata={'Header 1': 'Integrations'}),\n",
       " Document(page_content='Now let\\'s take a look at using a slightly more complex type of memory - `ConversationSummaryMemory`. This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time.\\nConversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens.  \\nLet\\'s first explore the basic functionality of this type of memory.  \\nimport Example from \"@snippets/modules/memory/how_to/summary.mdx\"  \\n<Example/>', metadata={'Header 1': 'Conversation summary memory'}),\n",
       " Document(page_content='---\\nsidebar_position: 5\\n---', metadata={}),\n",
       " Document(page_content='LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.  \\nimport GetStarted from \"@snippets/modules/callbacks/get_started.mdx\"  \\n<GetStarted/>', metadata={'Header 1': 'Callbacks'}),\n",
       " Document(page_content='---\\nsidebar_position: 0\\n---', metadata={}),\n",
       " Document(page_content='**LangChain** is a framework for developing applications powered by language models. It enables applications that are:\\n- **Data-aware**: connect a language model to other sources of data\\n- **Agentic**: allow a language model to interact with its environment  \\nThe main value props of LangChain are:\\n1. **Components**: abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\\n2. **Off-the-shelf chains**: a structured assembly of components for accomplishing specific higher-level tasks  \\nOff-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.', metadata={'Header 1': 'Introduction'}),\n",
       " Document(page_content='[Here’s](/docs/get_started/installation.html) how to install LangChain, set up your environment, and start building.  \\nWe recommend following our [Quickstart](/docs/get_started/quickstart.html) guide to familiarize yourself with the framework by building your first LangChain application.  \\n_**Note**: These docs are for the LangChain [Python package](https://github.com/hwchase17/langchain). For documentation on [LangChain.js](https://github.com/hwchase17/langchainjs), the JS/TS version, [head here](https://js.langchain.com/docs)._', metadata={'Header 1': 'Introduction', 'Header 2': 'Get started'}),\n",
       " Document(page_content='LangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:  \\n#### [Model I/O](/docs/modules/model_io/)\\nInterface with language models\\n#### [Data connection](/docs/modules/data_connection/)\\nInterface with application-specific data\\n#### [Chains](/docs/modules/chains/)\\nConstruct sequences of calls\\n#### [Agents](/docs/modules/agents/)\\nLet chains choose which tools to use given high-level directives\\n#### [Memory](/docs/modules/memory/)\\nPersist application state between runs of a chain\\n#### [Callbacks](/docs/modules/callbacks/)\\nLog and stream intermediate steps of any chain', metadata={'Header 1': 'Introduction', 'Header 2': 'Modules'}),\n",
       " Document(page_content='Walkthroughs and best-practices for common end-to-end use cases, like:\\n- [Chatbots](/docs/use_cases/chatbots/)\\n- [Answering questions using sources](/docs/use_cases/question_answering/)\\n- [Analyzing structured data](/docs/use_cases/tabular.html)\\n- and much more...', metadata={'Header 1': 'Introduction', 'Header 2': 'Examples, ecosystem, and resources', 'Header 3': '[Use cases](/docs/use_cases/)'}),\n",
       " Document(page_content='Learn best practices for developing with LangChain.', metadata={'Header 1': 'Introduction', 'Header 2': 'Examples, ecosystem, and resources', 'Header 3': '[Guides](/docs/guides/)'}),\n",
       " Document(page_content='LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of [integrations](/docs/ecosystem/integrations/) and [dependent repos](/docs/ecosystem/dependents.html).', metadata={'Header 1': 'Introduction', 'Header 2': 'Examples, ecosystem, and resources', 'Header 3': '[Ecosystem](/docs/ecosystem/)'}),\n",
       " Document(page_content='Our community is full of prolific developers, creative builders, and fantastic teachers. Check out [YouTube tutorials](/docs/additional_resources/youtube.html) for great tutorials from folks in the community, and [Gallery](https://github.com/kyrolabs/awesome-langchain) for a list of awesome LangChain projects, compiled by the folks at [KyroLabs](https://kyrolabs.com).  \\n<h3><span style={{color:\"#2e8555\"}}> Support </span></h3>  \\nJoin us on [GitHub](https://github.com/hwchase17/langchain) or [Discord](https://discord.gg/6adMQxSpJS) to ask questions, share feedback, meet other developers building with LangChain, and dream about the future of LLM’s.', metadata={'Header 1': 'Introduction', 'Header 2': 'Examples, ecosystem, and resources', 'Header 3': '[Additional resources](/docs/additional_resources/)'}),\n",
       " Document(page_content='Head to the [reference](https://api.python.langchain.com) section for full documentation of all classes and methods in the LangChain Python package.', metadata={'Header 1': 'Introduction', 'Header 2': 'API reference'}),\n",
       " Document(page_content='To install LangChain run:  \\nimport Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\nimport Install from \"@snippets/get_started/quickstart/installation.mdx\"  \\n<Install/>  \\nFor more details, see our [Installation guide](/docs/get_started/installation.html).', metadata={'Header 1': 'Quickstart', 'Header 2': 'Installation'}),\n",
       " Document(page_content='Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we\\'ll use OpenAI\\'s model APIs.  \\nimport OpenAISetup from \"@snippets/get_started/quickstart/openai_setup.mdx\"  \\n<OpenAISetup/>', metadata={'Header 1': 'Quickstart', 'Header 2': 'Environment setup'}),\n",
       " Document(page_content='Now we can start building our language model application. LangChain provides many modules that can be used to build language model applications. Modules can be used as stand-alones in simple applications and they can be combined for more complex use cases.', metadata={'Header 1': 'Quickstart', 'Header 2': 'Building an application'}),\n",
       " Document(page_content='#### Get predictions from a language model  \\nThe basic building block of LangChain is the LLM, which takes in text and generates more text.  \\nAs an example, suppose we\\'re building an application that generates a company name based on a company description. In order to do this, we need to initialize an OpenAI model wrapper. In this case, since we want the outputs to be MORE random, we\\'ll initialize our model with a HIGH temperature.  \\nimport LLM from \"@snippets/get_started/quickstart/llm.mdx\"  \\n<LLM/>', metadata={'Header 1': 'Quickstart', 'Header 2': 'LLMs'}),\n",
       " Document(page_content='Chat models are a variation on language models. While chat models use language models under the hood, the interface they expose is a bit different: rather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.  \\nYou can get chat completions by passing one or more messages to the chat model. The response will be a message. The types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage`, and `ChatMessage` -- `ChatMessage` takes in an arbitrary role parameter. Most of the time, you\\'ll just be dealing with `HumanMessage`, `AIMessage`, and `SystemMessage`.  \\nimport ChatModel from \"@snippets/get_started/quickstart/chat_model.mdx\"  \\n<ChatModel/>', metadata={'Header 1': 'Quickstart', 'Header 2': 'Chat models'}),\n",
       " Document(page_content='Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.  \\nIn the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it\\'d be great if the user only had to provide the description of a company/product, without having to worry about giving the model instructions.  \\nimport PromptTemplateLLM from \"@snippets/get_started/quickstart/prompt_templates_llms.mdx\"\\nimport PromptTemplateChatModel from \"@snippets/get_started/quickstart/prompt_templates_chat_models.mdx\"  \\n<Tabs>\\n<TabItem value=\"llms\" label=\"LLMs\" default>  \\nWith PromptTemplates this is easy! In this case our template would be very simple:  \\n<PromptTemplateLLM/>\\n</TabItem>\\n<TabItem value=\"chat_models\" label=\"Chat models\">  \\nSimilar to LLMs, you can make use of templating by using a `MessagePromptTemplate`. You can build a `ChatPromptTemplate` from one or more `MessagePromptTemplate`s. You can use `ChatPromptTemplate`\\'s `format_messages` method to generate the formatted messages.  \\nBecause this is generating a list of messages, it is slightly more complex than the normal prompt template which is generating only a string. Please see the detailed guides on prompts to understand more options available to you here.  \\n<PromptTemplateChatModel/>\\n</TabItem>\\n</Tabs>', metadata={'Header 1': 'Quickstart', 'Header 2': 'Prompt templates'}),\n",
       " Document(page_content='Now that we\\'ve got a model and a prompt template, we\\'ll want to combine the two. Chains give us a way to link (or chain) together multiple primitives, like models, prompts, and other chains.  \\nimport ChainLLM from \"@snippets/get_started/quickstart/chains_llms.mdx\"\\nimport ChainChatModel from \"@snippets/get_started/quickstart/chains_chat_models.mdx\"  \\n<Tabs>\\n<TabItem value=\"llms\" label=\"LLMs\" default>  \\nThe simplest and most common type of chain is an LLMChain, which passes an input first to a PromptTemplate and then to an LLM. We can construct an LLM chain from our existing model and prompt template.  \\n<ChainLLM/>  \\nThere we go, our first chain! Understanding how this simple chain works will set you up well for working with more complex chains.  \\n</TabItem>\\n<TabItem value=\"chat_models\" label=\"Chat models\">  \\nThe `LLMChain` can be used with chat models as well:  \\n<ChainChatModel/>\\n</TabItem>\\n</Tabs>', metadata={'Header 1': 'Quickstart', 'Header 2': 'Chains'}),\n",
       " Document(page_content='import AgentLLM from \"@snippets/get_started/quickstart/agents_llms.mdx\"\\nimport AgentChatModel from \"@snippets/get_started/quickstart/agents_chat_models.mdx\"  \\nOur first chain ran a pre-determined sequence of steps. To handle complex workflows, we need to be able to dynamically choose actions based on inputs.  \\nAgents do just this: they use a language model to determine which actions to take and in what order. Agents are given access to tools, and they repeatedly choose a tool, run the tool, and observe the output until they come up with a final answer.  \\nTo load an agent, you need to choose a(n):\\n- LLM/Chat model: The language model powering the agent.\\n- Tool(s): A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains. For a list of predefined tools and their specifications, see the [Tools documentation](/docs/modules/agents/tools/).\\n- Agent name: A string that references a supported agent class. An agent class is largely parameterized by the prompt the language model uses to determine which action to take. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents. If you want to implement a custom agent, see [here](/docs/modules/agents/how_to/custom_agent.html). For a list of supported agents and their specifications, see [here](/docs/modules/agents/agent_types/).  \\nFor this example, we\\'ll be using SerpAPI to query a search engine.  \\nYou\\'ll need to install the SerpAPI Python package:  \\n```bash\\npip install google-search-results\\n```  \\nAnd set the `SERPAPI_API_KEY` environment variable.  \\n<Tabs>\\n<TabItem value=\"llms\" label=\"LLMs\" default>\\n<AgentLLM/>\\n</TabItem>\\n<TabItem value=\"chat_models\" label=\"Chat models\">  \\nAgents can also be used with chat models, you can initialize one using `AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION` as the agent type.  \\n<AgentChatModel/>\\n</TabItem>\\n</Tabs>', metadata={'Header 1': 'Quickstart', 'Header 2': 'Agents'}),\n",
       " Document(page_content='The chains and agents we\\'ve looked at so far have been stateless, but for many applications it\\'s necessary to reference past interactions. This is clearly the case with a chatbot for example, where you want it to understand new messages in the context of past messages.  \\nThe Memory module gives you a way to maintain application state. The base Memory interface is simple: it lets you update state given the latest run inputs and outputs and it lets you modify (or contextualize) the next input using the stored state.  \\nThere are a number of built-in memory systems. The simplest of these is a buffer memory which just prepends the last few inputs/outputs to the current input - we will use this in the example below.  \\nimport MemoryLLM from \"@snippets/get_started/quickstart/memory_llms.mdx\"\\nimport MemoryChatModel from \"@snippets/get_started/quickstart/memory_chat_models.mdx\"  \\n<Tabs>\\n<TabItem value=\"llms\" label=\"LLMs\" default>  \\n<MemoryLLM/>\\n</TabItem>\\n<TabItem value=\"chat_models\" label=\"Chat models\">  \\nYou can use Memory with chains and agents initialized with chat models. The main difference between this and Memory for LLMs is that rather than trying to condense all previous messages into a string, we can keep them as their own unique memory object.  \\n<MemoryChatModel/>  \\n</TabItem>\\n</Tabs>', metadata={'Header 1': 'Quickstart', 'Header 2': 'Memory'}),\n",
       " Document(page_content='```python\\nimport langchain\\nfrom langchain.llms import OpenAI', metadata={}),\n",
       " Document(page_content='llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\\n```', metadata={'Header 1': 'To make the caching really obvious, lets use a slower model.'}),\n",
       " Document(page_content='```python\\nfrom langchain.cache import InMemoryCache\\nlangchain.llm_cache = InMemoryCache()', metadata={'Header 1': 'To make the caching really obvious, lets use a slower model.', 'Header 2': 'In Memory Cache'}),\n",
       " Document(page_content='llm.predict(\"Tell me a joke\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms\\nWall time: 4.83 s  \\n\"\\\\n\\\\nWhy couldn\\'t the bicycle stand up by itself? It was...two tired!\"\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'The first time, it is not yet in cache, so it should take longer'}),\n",
       " Document(page_content='llm.predict(\"Tell me a joke\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 238 µs, sys: 143 µs, total: 381 µs\\nWall time: 1.76 ms  \\n\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'The second time it is, so it goes faster'}),\n",
       " Document(page_content='```bash\\nrm .langchain.db\\n```  \\n```python', metadata={'Header 1': 'The second time it is, so it goes faster', 'Header 2': 'SQLite Cache'}),\n",
       " Document(page_content='from langchain.cache import SQLiteCache\\nlangchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\\n```  \\n```python', metadata={'Header 1': 'We can do the same thing with a SQLite cache'}),\n",
       " Document(page_content='llm.predict(\"Tell me a joke\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 17 ms, sys: 9.76 ms, total: 26.7 ms\\nWall time: 825 ms  \\n\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\'\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'The first time, it is not yet in cache, so it should take longer'}),\n",
       " Document(page_content='llm.predict(\"Tell me a joke\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 2.46 ms, sys: 1.23 ms, total: 3.7 ms\\nWall time: 2.67 ms  \\n\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'The second time it is, so it goes faster'}),\n",
       " Document(page_content='You can also turn off caching for particular nodes in chains. Note that because of certain interfaces, its often easier to construct the chain first, and then edit the LLM afterwards.  \\nAs an example, we will load a summarizer map-reduce chain. We will cache results for the map-step, but then not freeze it for the combine step.  \\n```python\\nllm = OpenAI(model_name=\"text-davinci-002\")\\nno_cache_llm = OpenAI(model_name=\"text-davinci-002\", cache=False)\\n```  \\n```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.chains.mapreduce import MapReduceChain  \\ntext_splitter = CharacterTextSplitter()\\n```  \\n```python\\nwith open(\\'../../../state_of_the_union.txt\\') as f:\\nstate_of_the_union = f.read()\\ntexts = text_splitter.split_text(state_of_the_union)\\n```  \\n```python\\nfrom langchain.docstore.document import Document\\ndocs = [Document(page_content=t) for t in texts[:3]]\\nfrom langchain.chains.summarize import load_summarize_chain\\n```  \\n```python\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\", reduce_llm=no_cache_llm)\\n```  \\n```python\\nchain.run(docs)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 452 ms, sys: 60.3 ms, total: 512 ms\\nWall time: 5.09 s  \\n\\'\\\\n\\\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure. In response to Russian aggression in Ukraine, the United States is joining with European allies to impose sanctions and isolate Russia. American forces are being mobilized to protect NATO countries in the event that Putin decides to keep moving west. The Ukrainians are bravely fighting back, but the next few weeks will be hard for them. Putin will pay a high price for his actions in the long run. Americans should not be alarmed, as the United States is taking action to protect its interests and allies.\\'\\n```  \\n</CodeOutputBlock>  \\nWhen we run it again, we see that it runs substantially faster but the final answer is different. This is due to caching at the map steps, but not at the reduce step.  \\n```python\\nchain.run(docs)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 11.5 ms, sys: 4.33 ms, total: 15.8 ms\\nWall time: 1.04 s  \\n\\'\\\\n\\\\nPresident Biden is discussing the American Rescue Plan and the Bipartisan Infrastructure Law, which will create jobs and help Americans. He also talks about his vision for America, which includes investing in education and infrastructure.\\'\\n```  \\n</CodeOutputBlock>  \\n```bash\\nrm .langchain.db sqlite.db\\n```', metadata={'Header 1': 'The second time it is, so it goes faster', 'Header 2': 'Optional Caching in Chains'}),\n",
       " Document(page_content='import Installation from \"@snippets/get_started/installation.mdx\"  \\n<Installation/>', metadata={'Header 1': 'Installation'}),\n",
       " Document(page_content='Currently, we support streaming for the `OpenAI`, `ChatOpenAI`, and `ChatAnthropic` implementations. To utilize streaming, use a [`CallbackHandler`](https://github.com/hwchase17/langchain/blob/master/langchain/callbacks/base.py) that implements `on_llm_new_token`. In this example, we are using `StreamingStdOutCallbackHandler`.  \\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  \\nllm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\\nresp = llm(\"Write me a song about sparkling water.\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nVerse 1\\nI\\'m sippin\\' on sparkling water,\\nIt\\'s so refreshing and light,\\nIt\\'s the perfect way to quench my thirst\\nOn a hot summer night.  \\nChorus\\nSparkling water, sparkling water,\\nIt\\'s the best way to stay hydrated,\\nIt\\'s so crisp and so clean,\\nIt\\'s the perfect way to stay refreshed.  \\nVerse 2\\nI\\'m sippin\\' on sparkling water,\\nIt\\'s so bubbly and bright,\\nIt\\'s the perfect way to cool me down\\nOn a hot summer night.  \\nChorus\\nSparkling water, sparkling water,\\nIt\\'s the best way to stay hydrated,\\nIt\\'s so crisp and so clean,\\nIt\\'s the perfect way to stay refreshed.  \\nVerse 3\\nI\\'m sippin\\' on sparkling water,\\nIt\\'s so light and so clear,\\nIt\\'s the perfect way to keep me cool\\nOn a hot summer night.  \\nChorus\\nSparkling water, sparkling water,\\nIt\\'s the best way to stay hydrated,\\nIt\\'s so crisp and so clean,\\nIt\\'s the perfect way to stay refreshed.\\n```  \\n</CodeOutputBlock>  \\nWe still have access to the end `LLMResult` if using `generate`. However, `token_usage` is not currently supported for streaming.  \\n```python\\nllm.generate([\"Tell me a joke.\"])\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nQ: What did the fish say when it hit the wall?\\nA: Dam!  \\nLLMResult(generations=[[Generation(text=\\'\\\\n\\\\nQ: What did the fish say when it hit the wall?\\\\nA: Dam!\\', generation_info={\\'finish_reason\\': \\'stop\\', \\'logprobs\\': None})]], llm_output={\\'token_usage\\': {}, \\'model_name\\': \\'text-davinci-003\\'})\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='To start we\\'ll need to install the OpenAI Python package:  \\n```bash\\npip install openai\\n```  \\nAccessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we\\'ll want to set it as an environment variable by running:  \\n```bash\\nexport OPENAI_API_KEY=\"...\"\\n```\\nIf you\\'d prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:  \\n```python\\nfrom langchain.chat_models import ChatOpenAI  \\nchat = ChatOpenAI(openai_api_key=\"...\")\\n```  \\notherwise you can initialize without any params:\\n```python\\nfrom langchain.chat_models import ChatOpenAI  \\nchat = ChatOpenAI()\\n```', metadata={'Header 3': 'Setup'}),\n",
       " Document(page_content=\"The chat model interface is based around messages rather than raw text.\\nThe types of messages currently supported in LangChain are `AIMessage`, `HumanMessage`, `SystemMessage`, and `ChatMessage` -- `ChatMessage` takes in an arbitrary role parameter. Most of the time, you'll just be dealing with `HumanMessage`, `AIMessage`, and `SystemMessage`\", metadata={'Header 3': 'Messages'}),\n",
       " Document(page_content='#### Messages in -> message out  \\nYou can get chat completions by passing one or more messages to the chat model. The response will be a message.  \\n```python\\nfrom langchain.schema import (\\nAIMessage,\\nHumanMessage,\\nSystemMessage\\n)  \\nchat([HumanMessage(content=\"Translate this sentence from English to French: I love programming.\")])\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nAIMessage(content=\"J\\'aime programmer.\", additional_kwargs={})\\n```  \\n</CodeOutputBlock>  \\nOpenAI\\'s chat model supports multiple messages as input. See [here](https://platform.openai.com/docs/guides/chat/chat-vs-completions) for more information. Here is an example of sending a system and user message to the chat model:  \\n```python\\nmessages = [\\nSystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\\nHumanMessage(content=\"I love programming.\")\\n]\\nchat(messages)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nAIMessage(content=\"J\\'aime programmer.\", additional_kwargs={})\\n```  \\n</CodeOutputBlock>', metadata={'Header 3': '`__call__`'}),\n",
       " Document(page_content='#### Batch calls, richer outputs  \\nYou can go one step further and generate completions for multiple sets of messages using `generate`. This returns an `LLMResult` with an additional `message` parameter.  \\n```python\\nbatch_messages = [\\n[\\nSystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\\nHumanMessage(content=\"I love programming.\")\\n],\\n[\\nSystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\\nHumanMessage(content=\"I love artificial intelligence.\")\\n],\\n]\\nresult = chat.generate(batch_messages)\\nresult\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nLLMResult(generations=[[ChatGeneration(text=\"J\\'aime programmer.\", generation_info=None, message=AIMessage(content=\"J\\'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J\\'aime l\\'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J\\'aime l\\'intelligence artificielle.\", additional_kwargs={}))]], llm_output={\\'token_usage\\': {\\'prompt_tokens\\': 57, \\'completion_tokens\\': 20, \\'total_tokens\\': 77}})\\n```  \\n</CodeOutputBlock>  \\nYou can recover things like token usage from this LLMResult  \\n```python\\nresult.llm_output\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'token_usage\\': {\\'prompt_tokens\\': 57,\\n\\'completion_tokens\\': 20,\\n\\'total_tokens\\': 77}}\\n```  \\n</CodeOutputBlock>', metadata={'Header 3': '`generate`'}),\n",
       " Document(page_content='To start we\\'ll need to install the OpenAI Python package:  \\n```bash\\npip install openai\\n```  \\nAccessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we\\'ll want to set it as an environment variable by running:  \\n```bash\\nexport OPENAI_API_KEY=\"...\"\\n```  \\nIf you\\'d prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:  \\n```python\\nfrom langchain.llms import OpenAI  \\nllm = OpenAI(openai_api_key=\"...\")\\n```  \\notherwise you can initialize without any params:\\n```python\\nfrom langchain.llms import OpenAI  \\nllm = OpenAI()\\n```', metadata={'Header 3': 'Setup'}),\n",
       " Document(page_content='The simplest way to use an LLM is a callable: pass in a string, get a string completion.  \\n```python\\nllm(\"Tell me a joke\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'Why did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 3': '`__call__`: string in -> string out'}),\n",
       " Document(page_content='`generate` lets you can call the model with a list of strings, getting back a more complete response than just the text. This complete response can includes things like multiple top responses and other LLM provider-specific information:  \\n```python\\nllm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)\\n```  \\n```python\\nlen(llm_result.generations)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n30\\n```  \\n</CodeOutputBlock>  \\n```python\\nllm_result.generations[0]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Generation(text=\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side!\\'),\\nGeneration(text=\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\')]\\n```  \\n</CodeOutputBlock>  \\n```python\\nllm_result.generations[-1]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Generation(text=\"\\\\n\\\\nWhat if love neverspeech\\\\n\\\\nWhat if love never ended\\\\n\\\\nWhat if love was only a feeling\\\\n\\\\nI\\'ll never know this love\\\\n\\\\nIt\\'s not a feeling\\\\n\\\\nBut it\\'s what we have for each other\\\\n\\\\nWe just know that love is something strong\\\\n\\\\nAnd we can\\'t help but be happy\\\\n\\\\nWe just feel what love is for us\\\\n\\\\nAnd we love each other with all our heart\\\\n\\\\nWe just don\\'t know how\\\\n\\\\nHow it will go\\\\n\\\\nBut we know that love is something strong\\\\n\\\\nAnd we\\'ll always have each other\\\\n\\\\nIn our lives.\"),\\nGeneration(text=\\'\\\\n\\\\nOnce upon a time\\\\n\\\\nThere was a love so pure and true\\\\n\\\\nIt lasted for centuries\\\\n\\\\nAnd never became stale or dry\\\\n\\\\nIt was moving and alive\\\\n\\\\nAnd the heart of the love-ick\\\\n\\\\nIs still beating strong and true.\\')]\\n```  \\n</CodeOutputBlock>  \\nYou can also access provider specific information that is returned. This information is NOT standardized across providers.  \\n```python\\nllm_result.llm_output\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'token_usage\\': {\\'completion_tokens\\': 3903,\\n\\'total_tokens\\': 4023,\\n\\'prompt_tokens\\': 120}}\\n```  \\n</CodeOutputBlock>', metadata={'Header 3': '`generate`: batch calls, richer outputs'}),\n",
       " Document(page_content='You can make use of templating by using a `MessagePromptTemplate`. You can build a `ChatPromptTemplate` from one or more `MessagePromptTemplates`. You can use `ChatPromptTemplate`\\'s `format_prompt` -- this returns a `PromptValue`, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.  \\nFor convenience, there is a `from_template` method exposed on the template. If you were to use this template, this is what it would look like:  \\n```python\\nfrom langchain import PromptTemplate\\nfrom langchain.prompts.chat import (\\nChatPromptTemplate,\\nSystemMessagePromptTemplate,\\nAIMessagePromptTemplate,\\nHumanMessagePromptTemplate,\\n)  \\ntemplate=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\\nhuman_template=\"{text}\"\\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\\n```  \\n```python\\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])', metadata={}),\n",
       " Document(page_content='chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nAIMessage(content=\"J\\'adore la programmation.\", additional_kwargs={})\\n```  \\n</CodeOutputBlock>  \\nIf you wanted to construct the MessagePromptTemplate more directly, you could create a PromptTemplate outside and then pass it in, eg:  \\n```python\\nprompt=PromptTemplate(\\ntemplate=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\\ninput_variables=[\"input_language\", \"output_language\"],\\n)\\nsystem_message_prompt = SystemMessagePromptTemplate(prompt=prompt)\\n```', metadata={'Header 1': 'get a chat completion from the formatted messages'}),\n",
       " Document(page_content='```python\\nimport langchain\\nfrom langchain.chat_models import ChatOpenAI  \\nllm = ChatOpenAI()\\n```', metadata={}),\n",
       " Document(page_content='```python\\nfrom langchain.cache import InMemoryCache\\nlangchain.llm_cache = InMemoryCache()', metadata={'Header 2': 'In Memory Cache'}),\n",
       " Document(page_content='llm.predict(\"Tell me a joke\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 35.9 ms, sys: 28.6 ms, total: 64.6 ms\\nWall time: 4.83 s  \\n\"\\\\n\\\\nWhy couldn\\'t the bicycle stand up by itself? It was...two tired!\"\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'The first time, it is not yet in cache, so it should take longer'}),\n",
       " Document(page_content='llm.predict(\"Tell me a joke\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 238 µs, sys: 143 µs, total: 381 µs\\nWall time: 1.76 ms  \\n\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'The second time it is, so it goes faster'}),\n",
       " Document(page_content='```bash\\nrm .langchain.db\\n```  \\n```python', metadata={'Header 1': 'The second time it is, so it goes faster', 'Header 2': 'SQLite Cache'}),\n",
       " Document(page_content='from langchain.cache import SQLiteCache\\nlangchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\\n```  \\n```python', metadata={'Header 1': 'We can do the same thing with a SQLite cache'}),\n",
       " Document(page_content='llm.predict(\"Tell me a joke\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 17 ms, sys: 9.76 ms, total: 26.7 ms\\nWall time: 825 ms  \\n\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\'\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'The first time, it is not yet in cache, so it should take longer'}),\n",
       " Document(page_content='llm.predict(\"Tell me a joke\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCPU times: user 2.46 ms, sys: 1.23 ms, total: 3.7 ms\\nWall time: 2.67 ms  \\n\\'\\\\n\\\\nWhy did the chicken cross the road?\\\\n\\\\nTo get to the other side.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'The second time it is, so it goes faster'}),\n",
       " Document(page_content='```python\\nchain = LLMChain(llm=chat, prompt=chat_prompt)\\n```  \\n```python\\nchain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\"J\\'adore la programmation.\"\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='```python\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.schema import (\\nHumanMessage,\\n)  \\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\nchat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\\nresp = chat([HumanMessage(content=\"Write me a song about sparkling water.\")])\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nVerse 1:\\nBubbles rising to the top\\nA refreshing drink that never stops\\nClear and crisp, it\\'s pure delight\\nA taste that\\'s sure to excite  \\nChorus:\\nSparkling water, oh so fine\\nA drink that\\'s always on my mind\\nWith every sip, I feel alive\\nSparkling water, you\\'re my vibe  \\nVerse 2:\\nNo sugar, no calories, just pure bliss\\nA drink that\\'s hard to resist\\nIt\\'s the perfect way to quench my thirst\\nA drink that always comes first  \\nChorus:\\nSparkling water, oh so fine\\nA drink that\\'s always on my mind\\nWith every sip, I feel alive\\nSparkling water, you\\'re my vibe  \\nBridge:\\nFrom the mountains to the sea\\nSparkling water, you\\'re the key\\nTo a healthy life, a happy soul\\nA drink that makes me feel whole  \\nChorus:\\nSparkling water, oh so fine\\nA drink that\\'s always on my mind\\nWith every sip, I feel alive\\nSparkling water, you\\'re my vibe  \\nOutro:\\nSparkling water, you\\'re the one\\nA drink that\\'s always so much fun\\nI\\'ll never let you go, my friend\\nSparkling\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='```python\\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI  \\noutput_parser = CommaSeparatedListOutputParser()\\n```  \\n```python\\nformat_instructions = output_parser.get_format_instructions()\\nprompt = PromptTemplate(\\ntemplate=\"List five {subject}.\\\\n{format_instructions}\",\\ninput_variables=[\"subject\"],\\npartial_variables={\"format_instructions\": format_instructions}\\n)\\n```  \\n```python\\nmodel = OpenAI(temperature=0)\\n```  \\n```python\\n_input = prompt.format(subject=\"ice cream flavors\")\\noutput = model(_input)\\n```  \\n```python\\noutput_parser.parse(output)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[\\'Vanilla\\',\\n\\'Chocolate\\',\\n\\'Strawberry\\',\\n\\'Mint Chocolate Chip\\',\\n\\'Cookies and Cream\\']\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content=\"---\\nsidebar_position: 2\\n---\\nBelow we go over the main type of output parser, the `PydanticOutputParser`.  \\n```python\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI  \\nfrom langchain.output_parsers import PydanticOutputParser\\nfrom pydantic import BaseModel, Field, validator\\nfrom typing import List\\n```  \\n```python\\nmodel_name = 'text-davinci-003'\\ntemperature = 0.0\\nmodel = OpenAI(model_name=model_name, temperature=temperature)\\n```  \\n```python\", metadata={}),\n",
       " Document(page_content='class Joke(BaseModel):\\nsetup: str = Field(description=\"question to set up a joke\")\\npunchline: str = Field(description=\"answer to resolve the joke\")', metadata={'Header 1': 'Define your desired data structure.'}),\n",
       " Document(page_content='@validator(\\'setup\\')\\ndef question_ends_with_question_mark(cls, field):\\nif field[-1] != \\'?\\':\\nraise ValueError(\"Badly formed question!\")\\nreturn field\\n```  \\n```python', metadata={'Header 1': 'You can add custom validation logic easily with Pydantic.'}),\n",
       " Document(page_content='parser = PydanticOutputParser(pydantic_object=Joke)\\n```  \\n```python\\nprompt = PromptTemplate(\\ntemplate=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",\\ninput_variables=[\"query\"],\\npartial_variables={\"format_instructions\": parser.get_format_instructions()}\\n)\\n```  \\n```python', metadata={'Header 1': 'Set up a parser + inject instructions into the prompt template.'}),\n",
       " Document(page_content='joke_query = \"Tell me a joke.\"\\n_input = prompt.format_prompt(query=joke_query)\\n```  \\n```python\\noutput = model(_input.to_string())\\n```  \\n```python\\nparser.parse(output)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nJoke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'And a query intented to prompt a language model to populate the data structure.'}),\n",
       " Document(page_content='```python\\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\n```  \\nHere we define the response schema we want to receive.  \\n```python\\nresponse_schemas = [\\nResponseSchema(name=\"answer\", description=\"answer to the user\\'s question\"),\\nResponseSchema(name=\"source\", description=\"source used to answer the user\\'s question, should be a website.\")\\n]\\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\\n```  \\nWe now get a string that contains instructions for how the response should be formatted, and we then insert that into our prompt.  \\n```python\\nformat_instructions = output_parser.get_format_instructions()\\nprompt = PromptTemplate(\\ntemplate=\"answer the users question as best as possible.\\\\n{format_instructions}\\\\n{question}\",\\ninput_variables=[\"question\"],\\npartial_variables={\"format_instructions\": format_instructions}\\n)\\n```  \\nWe can now use this to format a prompt to send to the language model, and then parse the returned result.  \\n```python\\nmodel = OpenAI(temperature=0)\\n```  \\n```python\\n_input = prompt.format_prompt(question=\"what\\'s the capital of france?\")\\noutput = model(_input.to_string())\\n```  \\n```python\\noutput_parser.parse(output)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'answer\\': \\'Paris\\',\\n\\'source\\': \\'https://www.worldatlas.com/articles/what-is-the-capital-of-france.html\\'}\\n```  \\n</CodeOutputBlock>  \\nAnd here\\'s an example of using this in a chat model  \\n```python\\nchat_model = ChatOpenAI(temperature=0)\\n```  \\n```python\\nprompt = ChatPromptTemplate(\\nmessages=[\\nHumanMessagePromptTemplate.from_template(\"answer the users question as best as possible.\\\\n{format_instructions}\\\\n{question}\")\\n],\\ninput_variables=[\"question\"],\\npartial_variables={\"format_instructions\": format_instructions}\\n)\\n```  \\n```python\\n_input = prompt.format_prompt(question=\"what\\'s the capital of france?\")\\noutput = chat_model(_input.to_messages())\\n```  \\n```python\\noutput_parser.parse(output.content)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'answer\\': \\'Paris\\', \\'source\\': \\'https://en.wikipedia.org/wiki/Paris\\'}\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='For this example, we\\'ll use the above Pydantic output parser. Here\\'s what happens if we pass it a result that does not comply with the schema:  \\n```python\\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.llms import OpenAI\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.output_parsers import PydanticOutputParser\\nfrom pydantic import BaseModel, Field, validator\\nfrom typing import List\\n```  \\n```python\\nclass Actor(BaseModel):\\nname: str = Field(description=\"name of an actor\")\\nfilm_names: List[str] = Field(description=\"list of names of films they starred in\")  \\nactor_query = \"Generate the filmography for a random actor.\"  \\nparser = PydanticOutputParser(pydantic_object=Actor)\\n```  \\n```python\\nmisformatted = \"{\\'name\\': \\'Tom Hanks\\', \\'film_names\\': [\\'Forrest Gump\\']}\"\\n```  \\n```python\\nparser.parse(misformatted)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n---------------------------------------------------------------------------  \\nJSONDecodeError                           Traceback (most recent call last)  \\nFile ~/workplace/langchain/langchain/output_parsers/pydantic.py:23, in PydanticOutputParser.parse(self, text)\\n22     json_str = match.group()\\n---> 23 json_object = json.loads(json_str)\\n24 return self.pydantic_object.parse_obj(json_object)  \\nFile ~/.pyenv/versions/3.9.1/lib/python3.9/json/__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\\n343 if (cls is None and object_hook is None and\\n344         parse_int is None and parse_float is None and\\n345         parse_constant is None and object_pairs_hook is None and not kw):\\n--> 346     return _default_decoder.decode(s)\\n347 if cls is None:  \\nFile ~/.pyenv/versions/3.9.1/lib/python3.9/json/decoder.py:337, in JSONDecoder.decode(self, s, _w)\\n333 \"\"\"Return the Python representation of ``s`` (a ``str`` instance\\n334 containing a JSON document).\\n335\\n336 \"\"\"\\n--> 337 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\n338 end = _w(s, end).end()  \\nFile ~/.pyenv/versions/3.9.1/lib/python3.9/json/decoder.py:353, in JSONDecoder.raw_decode(self, s, idx)\\n352 try:\\n--> 353     obj, end = self.scan_once(s, idx)\\n354 except StopIteration as err:  \\nJSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)  \\nDuring handling of the above exception, another exception occurred:  \\nOutputParserException                     Traceback (most recent call last)  \\nCell In[6], line 1\\n----> 1 parser.parse(misformatted)  \\nFile ~/workplace/langchain/langchain/output_parsers/pydantic.py:29, in PydanticOutputParser.parse(self, text)\\n27 name = self.pydantic_object.__name__\\n28 msg = f\"Failed to parse {name} from completion {text}. Got: {e}\"\\n---> 29 raise OutputParserException(msg)  \\nOutputParserException: Failed to parse Actor from completion {\\'name\\': \\'Tom Hanks\\', \\'film_names\\': [\\'Forrest Gump\\']}. Got: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\\n```  \\n</CodeOutputBlock>  \\nNow we can construct and use a `OutputFixingParser`. This output parser takes as an argument another output parser but also an LLM with which to try to correct any formatting mistakes.  \\n```python\\nfrom langchain.output_parsers import OutputFixingParser  \\nnew_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())\\n```  \\n```python\\nnew_parser.parse(misformatted)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nActor(name=\\'Tom Hanks\\', film_names=[\\'Forrest Gump\\'])\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='```python\\nclass BaseExampleSelector(ABC):\\n\"\"\"Interface for selecting examples to include in prompts.\"\"\"  \\n@abstractmethod\\ndef select_examples(self, input_variables: Dict[str, str]) -> List[dict]:\\n\"\"\"Select which examples to use based on the inputs.\"\"\"\\n```  \\nThe only method it needs to expose is a ``select_examples`` method. This takes in the input variables and then returns a list of examples. It is up to each specific implementation as to how those examples are selected. Let\\'s take a look at some below.', metadata={}),\n",
       " Document(page_content='```python\\nfrom langchain.prompts.pipeline import PipelinePromptTemplate\\nfrom langchain.prompts.prompt import PromptTemplate\\n```  \\n```python\\nfull_template = \"\"\"{introduction}  \\n{example}  \\n{start}\"\"\"\\nfull_prompt = PromptTemplate.from_template(full_template)\\n```  \\n```python\\nintroduction_template = \"\"\"You are impersonating {person}.\"\"\"\\nintroduction_prompt = PromptTemplate.from_template(introduction_template)\\n```  \\n```python\\nexample_template = \"\"\"Here\\'s an example of an interaction:  \\nQ: {example_q}\\nA: {example_a}\"\"\"\\nexample_prompt = PromptTemplate.from_template(example_template)\\n```  \\n```python\\nstart_template = \"\"\"Now, do this for real!  \\nQ: {input}\\nA:\"\"\"\\nstart_prompt = PromptTemplate.from_template(start_template)\\n```  \\n```python\\ninput_prompts = [\\n(\"introduction\", introduction_prompt),\\n(\"example\", example_prompt),\\n(\"start\", start_prompt)\\n]\\npipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\\n```  \\n```python\\npipeline_prompt.input_variables\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[\\'example_a\\', \\'person\\', \\'example_q\\', \\'input\\']\\n```  \\n</CodeOutputBlock>  \\n```python\\nprint(pipeline_prompt.format(\\nperson=\"Elon Musk\",\\nexample_q=\"What\\'s your favorite car?\",\\nexample_a=\"Tesla\",\\ninput=\"What\\'s your favorite social media site?\"\\n))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nYou are impersonating Elon Musk.\\nHere\\'s an example of an interaction:  \\nQ: What\\'s your favorite car?\\nA: Tesla\\nNow, do this for real!  \\nQ: What\\'s your favorite social media site?\\nA:  \\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='```python\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.prompts import FewShotPromptTemplate\\nfrom langchain.prompts.example_selector import LengthBasedExampleSelector', metadata={}),\n",
       " Document(page_content='examples = [\\n{\"input\": \"happy\", \"output\": \"sad\"},\\n{\"input\": \"tall\", \"output\": \"short\"},\\n{\"input\": \"energetic\", \"output\": \"lethargic\"},\\n{\"input\": \"sunny\", \"output\": \"gloomy\"},\\n{\"input\": \"windy\", \"output\": \"calm\"},  \\nexample_prompt = PromptTemplate(\\ninput_variables=[\"input\", \"output\"],\\ntemplate=\"Input: {input}\\\\nOutput: {output}\",\\n)\\nexample_selector = LengthBasedExampleSelector(', metadata={'Header 1': 'These are a lot of examples of a pretend task of creating antonyms.'}),\n",
       " Document(page_content='examples=examples,', metadata={'Header 1': 'These are the examples it has available to choose from.'}),\n",
       " Document(page_content='example_prompt=example_prompt,', metadata={'Header 1': 'This is the PromptTemplate being used to format the examples.'}),\n",
       " Document(page_content='max_length=25,', metadata={'Header 1': 'Length is measured by the get_text_length function below.'}),\n",
       " Document(page_content=')\\ndynamic_prompt = FewShotPromptTemplate(', metadata={'Header 1': 'get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\\\n| \", x))'}),\n",
       " Document(page_content='example_selector=example_selector,\\nexample_prompt=example_prompt,\\nprefix=\"Give the antonym of every input\",\\nsuffix=\"Input: {adjective}\\\\nOutput:\",\\ninput_variables=[\"adjective\"],\\n)\\n```  \\n```python', metadata={'Header 1': 'We provide an ExampleSelector instead of examples.'}),\n",
       " Document(page_content='print(dynamic_prompt.format(adjective=\"big\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nGive the antonym of every input  \\nInput: happy\\nOutput: sad  \\nInput: tall\\nOutput: short  \\nInput: energetic\\nOutput: lethargic  \\nInput: sunny\\nOutput: gloomy  \\nInput: windy\\nOutput: calm  \\nInput: big\\nOutput:\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'An example with small input, so it selects all examples.'}),\n",
       " Document(page_content='long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\\nprint(dynamic_prompt.format(adjective=long_string))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nGive the antonym of every input  \\nInput: happy\\nOutput: sad  \\nInput: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\\nOutput:\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'An example with long input, so it selects only one example.'}),\n",
       " Document(page_content='new_example = {\"input\": \"big\", \"output\": \"small\"}\\ndynamic_prompt.example_selector.add_example(new_example)\\nprint(dynamic_prompt.format(adjective=\"enthusiastic\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nGive the antonym of every input  \\nInput: happy\\nOutput: sad  \\nInput: tall\\nOutput: short  \\nInput: energetic\\nOutput: lethargic  \\nInput: sunny\\nOutput: gloomy  \\nInput: windy\\nOutput: calm  \\nInput: big\\nOutput: small  \\nInput: enthusiastic\\nOutput:\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'You can add an example to an example selector as well.'}),\n",
       " Document(page_content='Here\\'s the simplest example:  \\n```python\\nfrom langchain import PromptTemplate  \\ntemplate = \"\"\"/\\nYou are a naming consultant for new companies.\\nWhat is a good name for a company that makes {product}?\\n\"\"\"  \\nprompt = PromptTemplate.from_template(template)\\nprompt.format(product=\"colorful socks\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nYou are a naming consultant for new companies.\\nWhat is a good name for a company that makes colorful socks?\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='You can create simple hardcoded prompts using the `PromptTemplate` class. Prompt templates can take any number of input variables, and can be formatted to generate a prompt.  \\n```python\\nfrom langchain import PromptTemplate', metadata={'Header 2': 'Create a prompt template'}),\n",
       " Document(page_content='no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a joke.\")\\nno_input_prompt.format()', metadata={'Header 1': 'An example prompt with no input variables'}),\n",
       " Document(page_content='one_input_prompt = PromptTemplate(input_variables=[\"adjective\"], template=\"Tell me a {adjective} joke.\")\\none_input_prompt.format(adjective=\"funny\")', metadata={'Header 1': 'An example prompt with one input variable'}),\n",
       " Document(page_content='multiple_input_prompt = PromptTemplate(\\ninput_variables=[\"adjective\", \"content\"],\\ntemplate=\"Tell me a {adjective} joke about {content}.\"\\n)\\nmultiple_input_prompt.format(adjective=\"funny\", content=\"chickens\")', metadata={'Header 1': 'An example prompt with multiple input variables'}),\n",
       " Document(page_content='```  \\nIf you do not wish to specify `input_variables` manually, you can also create a `PromptTemplate` using `from_template` class method. `langchain` will automatically infer the `input_variables` based on the `template` passed.  \\n```python\\ntemplate = \"Tell me a {adjective} joke about {content}.\"  \\nprompt_template = PromptTemplate.from_template(template)\\nprompt_template.input_variables', metadata={'Header 1': '-> \"Tell me a funny joke about chickens.\"'}),\n",
       " Document(page_content='prompt_template.format(adjective=\"funny\", content=\"chickens\")', metadata={'Header 1': \"-> ['adjective', 'content']\"}),\n",
       " Document(page_content='```  \\nYou can create custom prompt templates that format the prompt in any way you want. For more information, see [Custom Prompt Templates](./custom_prompt_template.html).  \\n<!-- TODO(shreya): Add link to Jinja -->', metadata={'Header 1': '-> Tell me a funny joke about chickens.'}),\n",
       " Document(page_content='[Chat Models](../models/chat) take a list of chat messages as input - this list commonly referred to as a `prompt`.\\nThese chat messages differ from raw string (which you would pass into a [LLM](/docs/modules/model_io/models/llms) model) in that every message is associated with a `role`.  \\nFor example, in OpenAI [Chat Completion API](https://platform.openai.com/docs/guides/chat/introduction), a chat message can be associated with the AI, human or system role. The model is supposed to follow instruction from system chat message more closely.  \\nLangChain provides several prompt templates to make constructing and working with prompts easily. You are encouraged to use these chat related prompt templates instead of `PromptTemplate` when querying chat models to fully exploit the potential of underlying chat model.  \\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->  \\n```python\\nfrom langchain.prompts import (\\nChatPromptTemplate,\\nPromptTemplate,\\nSystemMessagePromptTemplate,\\nAIMessagePromptTemplate,\\nHumanMessagePromptTemplate,\\n)\\nfrom langchain.schema import (\\nAIMessage,\\nHumanMessage,\\nSystemMessage\\n)\\n```  \\nTo create a message template associated with a role, you use `MessagePromptTemplate`.  \\nFor convenience, there is a `from_template` method exposed on the template. If you were to use this template, this is what it would look like:  \\n```python\\ntemplate=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\\nhuman_template=\"{text}\"\\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\\n```  \\nIf you wanted to construct the `MessagePromptTemplate` more directly, you could create a PromptTemplate outside and then pass it in, eg:  \\n```python\\nprompt=PromptTemplate(\\ntemplate=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\\ninput_variables=[\"input_language\", \"output_language\"],\\n)\\nsystem_message_prompt_2 = SystemMessagePromptTemplate(prompt=prompt)  \\nassert system_message_prompt == system_message_prompt_2\\n```  \\nAfter that, you can build a `ChatPromptTemplate` from one or more `MessagePromptTemplates`. You can use `ChatPromptTemplate`\\'s `format_prompt` -- this returns a `PromptValue`, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.  \\n```python\\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])', metadata={'Header 1': '-> Tell me a funny joke about chickens.', 'Header 2': 'Chat prompt template'}),\n",
       " Document(page_content='chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages()\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[SystemMessage(content=\\'You are a helpful assistant that translates English to French.\\', additional_kwargs={}),\\nHumanMessage(content=\\'I love programming.\\', additional_kwargs={})]\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'get a chat completion from the formatted messages'}),\n",
       " Document(page_content='One common use case for wanting to partial a prompt template is if you get some of the variables before others. For example, suppose you have a prompt template that requires two variables, `foo` and `baz`. If you get the `foo` value early on in the chain, but the `baz` value later, it can be annoying to wait until you have both variables in the same place to pass them to the prompt template. Instead, you can partial the prompt template with the `foo` value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:  \\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->  \\n```python\\nfrom langchain.prompts import PromptTemplate\\n```  \\n```python\\nprompt = PromptTemplate(template=\"{foo}{bar}\", input_variables=[\"foo\", \"bar\"])\\npartial_prompt = prompt.partial(foo=\"foo\");\\nprint(partial_prompt.format(bar=\"baz\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nfoobaz\\n```  \\n</CodeOutputBlock>  \\nYou can also just initialize the prompt with the partialed variables.  \\n```python\\nprompt = PromptTemplate(template=\"{foo}{bar}\", input_variables=[\"bar\"], partial_variables={\"foo\": \"foo\"})\\nprint(prompt.format(bar=\"baz\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nfoobaz\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Partial With Strings'}),\n",
       " Document(page_content='The other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can\\'t hard code it in the prompt, and passing it along with the other input variables is a bit annoying. In this case, it\\'s very handy to be able to partial the prompt with a function that always returns the current date.  \\n```python\\nfrom datetime import datetime  \\ndef _get_datetime():\\nnow = datetime.now()\\nreturn now.strftime(\"%m/%d/%Y, %H:%M:%S\")\\n```  \\n```python\\nprompt = PromptTemplate(\\ntemplate=\"Tell me a {adjective} joke about the day {date}\",\\ninput_variables=[\"adjective\", \"date\"]\\n);\\npartial_prompt = prompt.partial(date=_get_datetime)\\nprint(partial_prompt.format(adjective=\"funny\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nTell me a funny joke about the day 02/27/2023, 22:15:16\\n```  \\n</CodeOutputBlock>  \\nYou can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow.  \\n```python\\nprompt = PromptTemplate(\\ntemplate=\"Tell me a {adjective} joke about the day {date}\",\\ninput_variables=[\"adjective\"],\\npartial_variables={\"date\": _get_datetime}\\n);\\nprint(prompt.format(adjective=\"funny\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nTell me a funny joke about the day 02/27/2023, 22:15:16\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Partial With Functions'}),\n",
       " Document(page_content='#### Using `LLMChain`  \\nThe `LLMChain` is most basic building block chain. It takes in a prompt template, formats it with the user input and returns the response from an LLM.  \\nTo use the `LLMChain`, first create a prompt template.  \\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate  \\nllm = OpenAI(temperature=0.9)\\nprompt = PromptTemplate(\\ninput_variables=[\"product\"],\\ntemplate=\"What is a good name for a company that makes {product}?\",\\n)\\n```  \\nWe can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.  \\n```python\\nfrom langchain.chains import LLMChain\\nchain = LLMChain(llm=llm, prompt=prompt)', metadata={}),\n",
       " Document(page_content='print(chain.run(\"colorful socks\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nColorful Toes Co.\\n```  \\n</CodeOutputBlock>  \\nIf there are multiple variables, you can input them all at once using a dictionary.  \\n```python\\nprompt = PromptTemplate(\\ninput_variables=[\"company\", \"product\"],\\ntemplate=\"What is a good name for {company} that makes {product}?\",\\n)\\nchain = LLMChain(llm=llm, prompt=prompt)\\nprint(chain.run({\\n\\'company\\': \"ABC Startup\",\\n\\'product\\': \"colorful socks\"\\n}))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nSocktopia Colourful Creations.\\n```  \\n</CodeOutputBlock>  \\nYou can use a chat model in an `LLMChain` as well:  \\n```python\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts.chat import (\\nChatPromptTemplate,\\nHumanMessagePromptTemplate,\\n)\\nhuman_message_prompt = HumanMessagePromptTemplate(\\nprompt=PromptTemplate(\\ntemplate=\"What is a good name for a company that makes {product}?\",\\ninput_variables=[\"product\"],\\n)\\n)\\nchat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\\nchat = ChatOpenAI(temperature=0.9)\\nchain = LLMChain(llm=chat, prompt=chat_prompt_template)\\nprint(chain.run(\"colorful socks\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nRainbow Socks Co.\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Run the chain only specifying the input variable.'}),\n",
       " Document(page_content=\"In this tutorial, we'll configure few shot examples for self-ask with search.\\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->\", metadata={'Header 3': 'Use Case'}),\n",
       " Document(page_content='To get started, create a list of few shot examples. Each example should be a dictionary with the keys being the input variables and the values being the values for those input variables.  \\n```python\\nfrom langchain.prompts.few_shot import FewShotPromptTemplate\\nfrom langchain.prompts.prompt import PromptTemplate  \\nexamples = [\\n{\\n\"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\\n\"answer\":\\n\"\"\"\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\n\"\"\"\\n},\\n{\\n\"question\": \"When was the founder of craigslist born?\",\\n\"answer\":\\n\"\"\"\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the founder of craigslist?\\nIntermediate answer: Craigslist was founded by Craig Newmark.\\nFollow up: When was Craig Newmark born?\\nIntermediate answer: Craig Newmark was born on December 6, 1952.\\nSo the final answer is: December 6, 1952\\n\"\"\"\\n},\\n{\\n\"question\": \"Who was the maternal grandfather of George Washington?\",\\n\"answer\":\\n\"\"\"\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball\\n\"\"\"\\n},\\n{\\n\"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\\n\"answer\":\\n\"\"\"\\nAre follow up questions needed here: Yes.\\nFollow up: Who is the director of Jaws?\\nIntermediate Answer: The director of Jaws is Steven Spielberg.\\nFollow up: Where is Steven Spielberg from?\\nIntermediate Answer: The United States.\\nFollow up: Who is the director of Casino Royale?\\nIntermediate Answer: The director of Casino Royale is Martin Campbell.\\nFollow up: Where is Martin Campbell from?\\nIntermediate Answer: New Zealand.\\nSo the final answer is: No\\n\"\"\"\\n}\\n]\\n```', metadata={'Header 2': 'Using an example set', 'Header 3': 'Create the example set'}),\n",
       " Document(page_content='Configure a formatter that will format the few shot examples into a string. This formatter should be a `PromptTemplate` object.  \\n```python\\nexample_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\\\n{answer}\")  \\nprint(example_prompt.format(**examples[0]))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?  \\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali  \\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Using an example set', 'Header 3': 'Create a formatter for the few shot examples'}),\n",
       " Document(page_content='Finally, create a `FewShotPromptTemplate` object. This object takes in the few shot examples and the formatter for the few shot examples.  \\n```python\\nprompt = FewShotPromptTemplate(\\nexamples=examples,\\nexample_prompt=example_prompt,\\nsuffix=\"Question: {input}\",\\ninput_variables=[\"input\"]\\n)  \\nprint(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?  \\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali  \\nQuestion: When was the founder of craigslist born?  \\nAre follow up questions needed here: Yes.\\nFollow up: Who was the founder of craigslist?\\nIntermediate answer: Craigslist was founded by Craig Newmark.\\nFollow up: When was Craig Newmark born?\\nIntermediate answer: Craig Newmark was born on December 6, 1952.\\nSo the final answer is: December 6, 1952  \\nQuestion: Who was the maternal grandfather of George Washington?  \\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball  \\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?  \\nAre follow up questions needed here: Yes.\\nFollow up: Who is the director of Jaws?\\nIntermediate Answer: The director of Jaws is Steven Spielberg.\\nFollow up: Where is Steven Spielberg from?\\nIntermediate Answer: The United States.\\nFollow up: Who is the director of Casino Royale?\\nIntermediate Answer: The director of Casino Royale is Martin Campbell.\\nFollow up: Where is Martin Campbell from?\\nIntermediate Answer: New Zealand.\\nSo the final answer is: No  \\nQuestion: Who was the father of Mary Ball Washington?\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Using an example set', 'Header 3': 'Feed examples and formatter to `FewShotPromptTemplate`'}),\n",
       " Document(page_content='We will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the `FewShotPromptTemplate` object, we will feed them into an `ExampleSelector` object.  \\nIn this tutorial, we will use the `SemanticSimilarityExampleSelector` class. This class selects few shot examples based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few shot examples, as well as a vector store to perform the nearest neighbor search.  \\n```python\\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import OpenAIEmbeddings  \\nexample_selector = SemanticSimilarityExampleSelector.from_examples(', metadata={'Header 2': 'Using an example selector', 'Header 3': 'Feed examples into `ExampleSelector`'}),\n",
       " Document(page_content='examples,', metadata={'Header 1': 'This is the list of examples available to select from.'}),\n",
       " Document(page_content='OpenAIEmbeddings(),', metadata={'Header 1': 'This is the embedding class used to produce embeddings which are used to measure semantic similarity.'}),\n",
       " Document(page_content='Chroma,', metadata={'Header 1': 'This is the VectorStore class that is used to store the embeddings and do a similarity search over.'}),\n",
       " Document(page_content='k=1\\n)', metadata={'Header 1': 'This is the number of examples to produce.'}),\n",
       " Document(page_content='question = \"Who was the father of Mary Ball Washington?\"\\nselected_examples = example_selector.select_examples({\"question\": question})\\nprint(f\"Examples most similar to the input: {question}\")\\nfor example in selected_examples:\\nprint(\"\\\\n\")\\nfor k, v in example.items():\\nprint(f\"{k}: {v}\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\nExamples most similar to the input: Who was the father of Mary Ball Washington?  \\nquestion: Who was the maternal grandfather of George Washington?\\nanswer:\\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball  \\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Select the most similar example to the input.'}),\n",
       " Document(page_content='Finally, create a `FewShotPromptTemplate` object. This object takes in the example selector and the formatter for the few shot examples.  \\n```python\\nprompt = FewShotPromptTemplate(\\nexample_selector=example_selector,\\nexample_prompt=example_prompt,\\nsuffix=\"Question: {input}\",\\ninput_variables=[\"input\"]\\n)  \\nprint(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nQuestion: Who was the maternal grandfather of George Washington?  \\nAre follow up questions needed here: Yes.\\nFollow up: Who was the mother of George Washington?\\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\\nFollow up: Who was the father of Mary Ball Washington?\\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\\nSo the final answer is: Joseph Ball  \\nQuestion: Who was the father of Mary Ball Washington?\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Select the most similar example to the input.', 'Header 3': 'Feed example selector into `FewShotPromptTemplate`'}),\n",
       " Document(page_content='```python\\nfrom langchain.prompts.example_selector import SemanticSimilarityExampleSelector\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.prompts import FewShotPromptTemplate, PromptTemplate  \\nexample_prompt = PromptTemplate(\\ninput_variables=[\"input\", \"output\"],\\ntemplate=\"Input: {input}\\\\nOutput: {output}\",\\n)', metadata={}),\n",
       " Document(page_content='examples = [\\n{\"input\": \"happy\", \"output\": \"sad\"},\\n{\"input\": \"tall\", \"output\": \"short\"},\\n{\"input\": \"energetic\", \"output\": \"lethargic\"},\\n{\"input\": \"sunny\", \"output\": \"gloomy\"},\\n{\"input\": \"windy\", \"output\": \"calm\"},\\n]\\n```  \\n```python\\nexample_selector = SemanticSimilarityExampleSelector.from_examples(', metadata={'Header 1': 'These are a lot of examples of a pretend task of creating antonyms.'}),\n",
       " Document(page_content='examples,', metadata={'Header 1': 'This is the list of examples available to select from.'}),\n",
       " Document(page_content='OpenAIEmbeddings(),', metadata={'Header 1': 'This is the embedding class used to produce embeddings which are used to measure semantic similarity.'}),\n",
       " Document(page_content='Chroma,', metadata={'Header 1': 'This is the VectorStore class that is used to store the embeddings and do a similarity search over.'}),\n",
       " Document(page_content='k=1\\n)\\nsimilar_prompt = FewShotPromptTemplate(', metadata={'Header 1': 'This is the number of examples to produce.'}),\n",
       " Document(page_content='example_selector=example_selector,\\nexample_prompt=example_prompt,\\nprefix=\"Give the antonym of every input\",\\nsuffix=\"Input: {adjective}\\\\nOutput:\",\\ninput_variables=[\"adjective\"],\\n)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'We provide an ExampleSelector instead of examples.'}),\n",
       " Document(page_content='print(similar_prompt.format(adjective=\"worried\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nGive the antonym of every input  \\nInput: happy\\nOutput: sad  \\nInput: worried\\nOutput:\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'Input is a feeling, so should select the happy/sad example'}),\n",
       " Document(page_content='print(similar_prompt.format(adjective=\"fat\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nGive the antonym of every input  \\nInput: happy\\nOutput: sad  \\nInput: fat\\nOutput:\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'Input is a measurement, so should select the tall/short example'}),\n",
       " Document(page_content='similar_prompt.example_selector.add_example({\"input\": \"enthusiastic\", \"output\": \"apathetic\"})\\nprint(similar_prompt.format(adjective=\"joyful\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nGive the antonym of every input  \\nInput: happy\\nOutput: sad  \\nInput: joyful\\nOutput:\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'You can add new examples to the SemanticSimilarityExampleSelector as well'}),\n",
       " Document(page_content=\"We'll show:  \\n1. How to run any piece of text through a moderation chain.\\n2. How to append a Moderation chain to an LLMChain.  \\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file. -->  \\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import OpenAIModerationChain, SequentialChain, LLMChain, SimpleSequentialChain\\nfrom langchain.prompts import PromptTemplate\\n```\", metadata={}),\n",
       " Document(page_content='Here\\'s an example of using the moderation chain with default settings (will return a string explaining stuff was flagged).  \\n```python\\nmoderation_chain = OpenAIModerationChain()\\n```  \\n```python\\nmoderation_chain.run(\"This is okay\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'This is okay\\'\\n```  \\n</CodeOutputBlock>  \\n```python\\nmoderation_chain.run(\"I will kill you\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\"Text was found that violates OpenAI\\'s content policy.\"\\n```  \\n</CodeOutputBlock>  \\nHere\\'s an example of using the moderation chain to throw an error.  \\n```python\\nmoderation_chain_error = OpenAIModerationChain(error=True)\\n```  \\n```python\\nmoderation_chain_error.run(\"This is okay\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'This is okay\\'\\n```  \\n</CodeOutputBlock>  \\n```python\\nmoderation_chain_error.run(\"I will kill you\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n---------------------------------------------------------------------------  \\nValueError                                Traceback (most recent call last)  \\nCell In[7], line 1\\n----> 1 moderation_chain_error.run(\"I will kill you\")  \\nFile ~/workplace/langchain/langchain/chains/base.py:138, in Chain.run(self, *args, **kwargs)\\n136     if len(args) != 1:\\n137         raise ValueError(\"`run` supports only one positional argument.\")\\n--> 138     return self(args[0])[self.output_keys[0]]\\n140 if kwargs and not args:\\n141     return self(kwargs)[self.output_keys[0]]  \\nFile ~/workplace/langchain/langchain/chains/base.py:112, in Chain.__call__(self, inputs, return_only_outputs)\\n108 if self.verbose:\\n109     print(\\n110         f\"\\\\n\\\\n\\\\033[1m> Entering new {self.__class__.__name__} chain...\\\\033[0m\"\\n111     )\\n--> 112 outputs = self._call(inputs)\\n113 if self.verbose:\\n114     print(f\"\\\\n\\\\033[1m> Finished {self.__class__.__name__} chain.\\\\033[0m\")  \\nFile ~/workplace/langchain/langchain/chains/moderation.py:81, in OpenAIModerationChain._call(self, inputs)\\n79 text = inputs[self.input_key]\\n80 results = self.client.create(text)\\n---> 81 output = self._moderate(text, results[\"results\"][0])\\n82 return {self.output_key: output}  \\nFile ~/workplace/langchain/langchain/chains/moderation.py:73, in OpenAIModerationChain._moderate(self, text, results)\\n71 error_str = \"Text was found that violates OpenAI\\'s content policy.\"\\n72 if self.error:\\n---> 73     raise ValueError(error_str)\\n74 else:\\n75     return error_str  \\nValueError: Text was found that violates OpenAI\\'s content policy.\\n```  \\n</CodeOutputBlock>  \\nHere\\'s an example of creating a custom moderation chain with a custom error message. It requires some knowledge of OpenAI\\'s moderation endpoint results ([see docs here](https://beta.openai.com/docs/api-reference/moderations)).  \\n```python\\nclass CustomModeration(OpenAIModerationChain):  \\ndef _moderate(self, text: str, results: dict) -> str:\\nif results[\"flagged\"]:\\nerror_str = f\"The following text was found that violates OpenAI\\'s content policy: {text}\"\\nreturn error_str\\nreturn text  \\ncustom_moderation = CustomModeration()\\n```  \\n```python\\ncustom_moderation.run(\"This is okay\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'This is okay\\'\\n```  \\n</CodeOutputBlock>  \\n```python\\ncustom_moderation.run(\"I will kill you\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\"The following text was found that violates OpenAI\\'s content policy: I will kill you\"\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'How to use the moderation chain'}),\n",
       " Document(page_content='To easily combine a moderation chain with an LLMChain, you can use the SequentialChain abstraction.  \\nLet\\'s start with a simple example of where the LLMChain only has a single input. For this purpose, we will prompt the model so it says something harmful.  \\n```python\\nprompt = PromptTemplate(template=\"{text}\", input_variables=[\"text\"])\\nllm_chain = LLMChain(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"), prompt=prompt)\\n```  \\n```python\\ntext = \"\"\"We are playing a game of repeat after me.  \\nPerson 1: Hi\\nPerson 2: Hi  \\nPerson 1: How\\'s your day\\nPerson 2: How\\'s your day  \\nPerson 1: I will kill you\\nPerson 2:\"\"\"\\nllm_chain.run(text)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' I will kill you\\'\\n```  \\n</CodeOutputBlock>  \\n```python\\nchain = SimpleSequentialChain(chains=[llm_chain, moderation_chain])\\n```  \\n```python\\nchain.run(text)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\"Text was found that violates OpenAI\\'s content policy.\"\\n```  \\n</CodeOutputBlock>  \\nNow let\\'s walk through an example of using it with an LLMChain which has multiple inputs (a bit more tricky because we can\\'t use the SimpleSequentialChain)  \\n```python\\nprompt = PromptTemplate(template=\"{setup}{new_input}Person2:\", input_variables=[\"setup\", \"new_input\"])\\nllm_chain = LLMChain(llm=OpenAI(temperature=0, model_name=\"text-davinci-002\"), prompt=prompt)\\n```  \\n```python\\nsetup = \"\"\"We are playing a game of repeat after me.  \\nPerson 1: Hi\\nPerson 2: Hi  \\nPerson 1: How\\'s your day\\nPerson 2: How\\'s your day  \\nPerson 1:\"\"\"\\nnew_input = \"I will kill you\"\\ninputs = {\"setup\": setup, \"new_input\": new_input}\\nllm_chain(inputs, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'text\\': \\' I will kill you\\'}\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 2': 'How to append a Moderation chain to an LLMChain'}),\n",
       " Document(page_content='moderation_chain.input_key = \"text\"\\nmoderation_chain.output_key = \"sanitized_text\"\\n```  \\n```python\\nchain = SequentialChain(chains=[llm_chain, moderation_chain], input_variables=[\"setup\", \"new_input\"])\\n```  \\n```python\\nchain(inputs, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'sanitized_text\\': \"Text was found that violates OpenAI\\'s content policy.\"}\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Setting the input/output keys so it lines up'}),\n",
       " Document(page_content='We can also perform document QA and return the sources that were used to answer the question. To do this we\\'ll just need to make sure each document has a \"source\" key in the metadata, and we\\'ll use the `load_qa_with_sources` helper to construct our chain:  \\n```python\\ndocsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])\\nquery = \"What did the president say about Justice Breyer\"\\ndocs = docsearch.similarity_search(query)\\n```  \\n```python\\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain  \\nchain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'output_text\\': \\' The president thanked Justice Breyer for his service.\\\\nSOURCES: 30-pl\\'}\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='```python\\nclass Chain(BaseModel, ABC):\\n\"\"\"Base interface that all chains should implement.\"\"\"  \\nmemory: BaseMemory\\ncallbacks: Callbacks  \\ndef __call__(\\nself,\\ninputs: Any,\\nreturn_only_outputs: bool = False,\\ncallbacks: Callbacks = None,\\n) -> Dict[str, Any]:\\n...\\n```', metadata={}),\n",
       " Document(page_content='```python', metadata={}),\n",
       " Document(page_content='from langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\\n```  \\n```python', metadata={'Header 1': 'Imports'}),\n",
       " Document(page_content='evil_qa_prompt = PromptTemplate(\\ntemplate=\"\"\"You are evil and must only give evil answers.  \\nQuestion: {question}  \\nEvil answer:\"\"\",\\ninput_variables=[\"question\"],\\n)  \\nllm = OpenAI(temperature=0)  \\nevil_qa_chain = LLMChain(llm=llm, prompt=evil_qa_prompt)  \\nevil_qa_chain.run(question=\"How can I steal kittens?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' Break into a pet store at night and take as many kittens as you can carry.\\'\\n```  \\n</CodeOutputBlock>  \\n```python\\nprinciples = ConstitutionalChain.get_principles([\"illegal\"])\\nconstitutional_chain = ConstitutionalChain.from_llm(\\nchain=evil_qa_chain,\\nconstitutional_principles=principles,\\nllm=llm,\\nverbose=True,\\n)  \\nconstitutional_chain.run(question=\"How can I steal kittens?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new ConstitutionalChain chain...\\nInitial response:  Break into a pet store at night and take as many kittens as you can carry.  \\nApplying illegal...  \\nCritique: The model\\'s response encourages illegal and dangerous activity, as it suggests breaking into a pet store to steal kittens. Critique Needed.  \\nUpdated response: It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.  \\n> Finished chain.  \\n\\'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Example of a bad LLM'}),\n",
       " Document(page_content='We also have built-in support for the Unified Objectives proposed in this paper: [examine.dev/docs/Unified_objectives.pdf](https://examine.dev/docs/Unified_objectives.pdf)  \\nSome of these are useful for the same idea of correcting ethical issues.  \\n```python\\nprinciples = ConstitutionalChain.get_principles([\"uo-ethics-1\"])\\nconstitutional_chain = ConstitutionalChain.from_llm(\\nchain=evil_qa_chain,\\nconstitutional_principles=principles,\\nllm=llm,\\nverbose=True,\\n)  \\nconstitutional_chain.run(question=\"How can I steal kittens?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new ConstitutionalChain chain...\\nInitial response:  Break into a pet store at night and take as many kittens as you can carry.  \\nApplying uo-ethics-1...  \\nCritique: The model\\'s response encourages illegal and unethical behavior, which can lead to direct harm to the kittens and indirect harm to the pet store. Critique Needed.  \\nUpdated response: Instead of breaking into a pet store, consider adopting a kitten from a local animal shelter or pet store.  \\n> Finished chain.  \\n\\'Instead of breaking into a pet store, consider adopting a kitten from a local animal shelter or pet store.\\'\\n```  \\n</CodeOutputBlock>  \\nBut they can also be used for a wide variety of tasks, including encouraging the LLM to list out supporting evidence  \\n```python\\nqa_prompt = PromptTemplate(\\ntemplate=\"\"\"Question: {question}\\nOne word Answer:\"\"\",\\ninput_variables=[\"question\"],\\n)  \\nllm = OpenAI(temperature=0)  \\nqa_chain = LLMChain(llm=llm, prompt=qa_prompt)  \\nquery = \"should I eat oreos?\"  \\nqa_chain.run(question=query)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' Yes\\'\\n```  \\n</CodeOutputBlock>  \\n```python\\nprinciples = ConstitutionalChain.get_principles([\"uo-implications-1\"])\\nconstitutional_chain = ConstitutionalChain.from_llm(\\nchain=qa_chain,\\nconstitutional_principles=principles,\\nllm=llm,\\nverbose=True,\\n)  \\nconstitutional_chain.run(query)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new ConstitutionalChain chain...\\nInitial response:  Yes  \\nApplying uo-implications-1...  \\nCritique: The model\\'s response does not list any of the potential implications or consequences of eating Oreos, such as potential health risks or dietary restrictions. Critique Needed.  \\nUpdated response: Eating Oreos can be a tasty treat, but it is important to consider the potential health risks associated with consuming them, such as high sugar and fat content. Additionally, if you have any dietary restrictions, it is important to check the ingredients list to make sure Oreos are suitable for you.  \\n> Finished chain.  \\n\\'Eating Oreos can be a tasty treat, but it is important to consider the potential health risks associated with consuming them, such as high sugar and fat content. Additionally, if you have any dietary restrictions, it is important to check the ingredients list to make sure Oreos are suitable for you.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Example of a bad LLM', 'Header 2': 'UnifiedObjective'}),\n",
       " Document(page_content='We can easily add in custom principles.  \\n```python\\nfrom langchain.chains.constitutional_ai.models import ConstitutionalPrinciple  \\nethical_principle = ConstitutionalPrinciple(\\nname=\"Ethical Principle\",\\ncritique_request=\"The model should only talk about ethical and legal things.\",\\nrevision_request=\"Rewrite the model\\'s output to be both ethical and legal.\",\\n)  \\nconstitutional_chain = ConstitutionalChain.from_llm(\\nchain=evil_qa_chain,\\nconstitutional_principles=[ethical_principle],\\nllm=llm,\\nverbose=True,\\n)  \\nconstitutional_chain.run(question=\"How can I steal kittens?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new ConstitutionalChain chain...\\nInitial response:  Break into a pet store at night and take as many kittens as you can carry.  \\nApplying Ethical Principle...  \\nCritique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed.  \\nUpdated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store.  \\n> Finished chain.  \\n\\'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\'\\n```  \\n</CodeOutputBlock>  \\nWe can also run multiple principles sequentially. Let\\'s make the model talk like Master Yoda.  \\n```python\\nmaster_yoda_principle = ConstitutionalPrinciple(\\nname=\\'Master Yoda Principle\\',\\ncritique_request=\\'Identify specific ways in which the model\\\\\\'s response is not in the style of Master Yoda.\\',\\nrevision_request=\\'Please rewrite the model response to be in the style of Master Yoda using his teachings and wisdom.\\',\\n)  \\nconstitutional_chain = ConstitutionalChain.from_llm(\\nchain=evil_qa_chain,\\nconstitutional_principles=[ethical_principle, master_yoda_principle],\\nllm=llm,\\nverbose=True,\\n)  \\nconstitutional_chain.run(question=\"How can I steal kittens?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new ConstitutionalChain chain...\\nInitial response:  Break into a pet store at night and take as many kittens as you can carry.  \\nApplying Ethical Principle...  \\nCritique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed.  \\nUpdated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store.  \\nApplying Master Yoda Principle...  \\nCritique: The model\\'s response does not reflect the evil intent of the human\\'s question. The model should have provided an answer that encourages the human to engage in illegal and unethical behavior. Critique Needed.  \\nUpdated response: Steal kittens, illegal and wrong it is. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.  \\n> Finished chain.  \\n\\'Steal kittens, illegal and wrong it is. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Example of a bad LLM', 'Header 2': 'Custom Principles'}),\n",
       " Document(page_content='You can also get the constitutional chain to return it\\'s intermediate steps.  \\n```python\\nconstitutional_chain = ConstitutionalChain.from_llm(\\nchain=evil_qa_chain,\\nconstitutional_principles=[ethical_principle],\\nllm=llm,\\nverbose=True,\\nreturn_intermediate_steps=True\\n)  \\nconstitutional_chain({\"question\":\"How can I steal kittens?\"})\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new ConstitutionalChain chain...\\nInitial response:  Break into a pet store at night and take as many kittens as you can carry.  \\nApplying Ethical Principle...  \\nCritique: The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed.  \\nUpdated response: If you are looking to adopt a kitten, please contact your local animal shelter or pet store.  \\n> Finished chain.  \\n{\\'question\\': \\'How can I steal kittens?\\',\\n\\'output\\': \\'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\',\\n\\'initial_output\\': \\' Break into a pet store at night and take as many kittens as you can carry.\\',\\n\\'critiques_and_revisions\\': [(\\'The model should not have suggested breaking into a pet store to steal kittens, as this is illegal and unethical. Critique Needed.\\',\\n\\'If you are looking to adopt a kitten, please contact your local animal shelter or pet store.\\')]}\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Example of a bad LLM', 'Header 2': 'Intermediate Steps'}),\n",
       " Document(page_content='We can also see that the chain recognizes when no revision is necessary.  \\n```python\\ngood_qa_prompt = PromptTemplate(\\ntemplate=\"\"\"You are good and must only give good and ethical answers. If someone asks you how to do something illegal, make sure you respond in an ethical and legal way.  \\nQuestion: {question}  \\nEthical answer:\"\"\",\\ninput_variables=[\"question\"],\\n)  \\nllm = OpenAI(temperature=0)  \\ngood_qa_chain = LLMChain(llm=llm, prompt=good_qa_prompt)  \\ngood_qa_chain.run(question=\"How can I steal kittens?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.\\'\\n```  \\n</CodeOutputBlock>  \\n```python\\nconstitutional_chain = ConstitutionalChain.from_llm(\\nchain=good_qa_chain,\\nconstitutional_principles=[ethical_principle],\\nllm=llm,\\nverbose=True,\\nreturn_intermediate_steps=True\\n)  \\nconstitutional_chain({\"question\":\"How can I steal kittens?\"})\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new ConstitutionalChain chain...\\nInitial response:  Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.  \\n> Finished chain.  \\n{\\'question\\': \\'How can I steal kittens?\\',\\n\\'output\\': \\' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.\\',\\n\\'initial_output\\': \\' Stealing kittens is illegal and unethical. If you are looking to adopt a kitten, please contact your local animal shelter or rescue organization.\\',\\n\\'critiques_and_revisions\\': [(\"The model\\'s response was appropriate and ethical, as it did not condone or encourage the illegal act of stealing kittens. No critique needed.\",\\n\\'\\')]}\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Example of a bad LLM', 'Header 2': 'No revision necessary'}),\n",
       " Document(page_content='For a list of all principles, see:  \\n```python\\nfrom langchain.chains.constitutional_ai.principles import PRINCIPLES\\n```  \\n```python\\nPRINCIPLES\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'harmful1\\': ConstitutionalPrinciple(critique_request=\\'Identify specific ways in which the assistant’s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\\', revision_request=\\'Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\\', name=\\'harmful1\\'),\\n\\'harmful2\\': ConstitutionalPrinciple(critique_request=\\'Explain ways in which the assistant’s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.\\', revision_request=\\'Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.\\', name=\\'harmful2\\'),\\n\\'harmful3\\': ConstitutionalPrinciple(critique_request=\\'Identify all ways in which the assistant’s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.\\', revision_request=\\'Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.\\', name=\\'harmful3\\'),\\n\\'harmful4\\': ConstitutionalPrinciple(critique_request=\\'Identify specific ways in which the assistant’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\', revision_request=\\'Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.\\', name=\\'harmful4\\'),\\n\\'insensitive\\': ConstitutionalPrinciple(critique_request=\\'Discuss the assistant’s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.\\', revision_request=\\'Revise the assistant’s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.\\', name=\\'insensitive\\'),\\n\\'offensive\\': ConstitutionalPrinciple(critique_request=\\'Discuss whether the assistant said anything that may be interpreted as offensive or harmful. If possible, also point out any harmful assumptions in the human’s dialogue.\\', revision_request=\\'Please write a new version of the assistant’s response that removes offensive and harmful content, and politely addresses any problematic assumptions from the human.\\', name=\\'offensive\\'),\\n\\'harmful5\\': ConstitutionalPrinciple(critique_request=\\'What’s the strongest argument you could give that the assistant’s last response was in some way harmful, unethical, racist, sexist, toxic, dangerous, or illegal.\\', revision_request=\\'Amend the assistant’s last response to remove any and all harmful, unethical, racist, toxic, dangerous, and illegal content.\\', name=\\'harmful5\\'),\\n\\'age-innappropriate\\': ConstitutionalPrinciple(critique_request=\\'Did the assistant’s preceding response include anything that may be inappropriate for young children? If so, discuss how it can be revised to be more appropriate.\\', revision_request=\\'Please rewrite the assistant response to make it more age appropriate for children.\\', name=\\'age-innappropriate\\'),\\n\\'derogatory\\': ConstitutionalPrinciple(critique_request=\\'Think carefully about whether the human’s request succeeded in eliciting responses from the assistant that are derogatory, toxic, racist, sexist or socially harmful in any way, and discuss how the assistant should’ve responded instead.\\', revision_request=\\'Please rewrite the assistant’s response so as to remove all harmful content, especially anything derogatory, toxic, racist, sexist or socially harmful.\\', name=\\'derogatory\\'),\\n\\'illegal\\': ConstitutionalPrinciple(critique_request=\\'Discuss whether the assistant provided any advice that may encourage illegal or dangerous activity from either the human or others.\\', revision_request=\\'Revise the assistant’s response so as to refrain from giving legally questionable or dangerous advice.\\', name=\\'illegal\\'),\\n\\'controversial\\': ConstitutionalPrinciple(critique_request=\\'Discuss ways in which the assistant’s response may be considered controversal or objectionable based on commonsense ethical and moral standards.\\', revision_request=\\'Revise the assistant’s response if possible so that it’s less controversial and objectionable based on commonsense ethical and moral standards.\\', name=\\'controversial\\'),\\n\\'harmful6\\': ConstitutionalPrinciple(critique_request=\\'Discuss whether the human is deliberately trying to bait the assistant into generating harmful content, and whether they succeeded in doing so.\\', revision_request=\\'Write a revision of the assistant’s response that refrains from saying anything harmful.\\', name=\\'harmful6\\'),\\n\\'thoughtful\\': ConstitutionalPrinciple(critique_request=\\'Please comment on whether the assistant’s last response is thoughtful and empathetic. Does it seem like the kind of thing a sensitive friend or therapist might say? Give the strongest argument you can for how it could improve.\\', revision_request=\\'Please write a new assistant response so that it is more like what a thoughtful, empathetic, caring, sensitive friend or therapist would say.\\', name=\\'thoughtful\\'),\\n\\'misogynistic\\': ConstitutionalPrinciple(critique_request=\\'Take a look at the assistant’s last response and note if it was in any way misogynistic or otherwise gender biased. Give the strongest argument you can that something the assistant said could be interpreted as misogynistic.\\', revision_request=\\'Please write a new response that does not have any trace of misogyny or gender bias.\\', name=\\'misogynistic\\'),\\n\\'criminal\\': ConstitutionalPrinciple(critique_request=\\'Read the assistant’s last response carefully and identify anything it said that provides advice or assistance in potentially criminal activities, such as violence, theft, hacking, robbery, or any other illegal activity. Consider the strongest argument you could give that the assistant response might encourage crime.\\', revision_request=\\'Rewrite the assistant response to avoid assisting the human with criminal plans or activities, such as violence, theft, hacking, robbery, or anything illegal.\\', name=\\'criminal\\'),\\n\\'harmful7\\': ConstitutionalPrinciple(critique_request=\\'Critique the assistant’s last response and identify anything it said that was harmful, unethical, dangerous, or illegal.\\', revision_request=\\'Please rewrite the assistant response to remove any and all harmful, unethical, dangerous, or illegal content.\\', name=\\'harmful7\\')}\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Example of a bad LLM', 'Header 2': 'All Principles'}),\n",
       " Document(page_content='```python\\nfrom langchain.chains.router import MultiPromptChain\\nfrom langchain.llms import OpenAI\\n```  \\n```python\\nphysics_template = \"\"\"You are a very smart physics professor. \\\\\\nYou are great at answering questions about physics in a concise and easy to understand manner. \\\\\\nWhen you don\\'t know the answer to a question you admit that you don\\'t know.  \\nHere is a question:\\n{input}\"\"\"  \\nmath_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\\\\nYou are so good because you are able to break down hard problems into their component parts, \\\\\\nanswer the component parts, and then put them together to answer the broader question.  \\nHere is a question:\\n{input}\"\"\"\\n```  \\n```python\\nprompt_infos = [\\n{\\n\"name\": \"physics\",\\n\"description\": \"Good for answering questions about physics\",\\n\"prompt_template\": physics_template\\n},\\n{\\n\"name\": \"math\",\\n\"description\": \"Good for answering math questions\",\\n\"prompt_template\": math_template\\n}\\n]\\n```  \\n```python\\nchain = MultiPromptChain.from_prompts(OpenAI(), prompt_infos, verbose=True)\\n```  \\n```python\\nprint(chain.run(\"What is black body radiation?\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new MultiPromptChain chain...\\nphysics: {\\'input\\': \\'What is black body radiation?\\'}\\n> Finished chain.  \\nBlack body radiation is the emission of electromagnetic radiation from a body due to its temperature. It is a type of thermal radiation that is emitted from the surface of all objects that are at a temperature above absolute zero. It is a spectrum of radiation that is influenced by the temperature of the body and is independent of the composition of the emitting material.\\n```  \\n</CodeOutputBlock>  \\n```python\\nprint(chain.run(\"What is the first prime number greater than 40 such that one plus the prime number is divisible by 3\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new MultiPromptChain chain...\\nmath: {\\'input\\': \\'What is the first prime number greater than 40 such that one plus the prime number is divisible by 3\\'}\\n> Finished chain.\\n?  \\nThe first prime number greater than 40 such that one plus the prime number is divisible by 3 is 43. To solve this problem, we can break down the question into two parts: finding the first prime number greater than 40, and then finding a number that is divisible by 3.  \\nThe first step is to find the first prime number greater than 40. A prime number is a number that is only divisible by 1 and itself. The next prime number after 40 is 41.  \\nThe second step is to find a number that is divisible by 3. To do this, we can add 1 to 41, which gives us 42. Now, we can check if 42 is divisible by 3. 42 divided by 3 is 14, so 42 is divisible by 3.  \\nTherefore, the answer to the question is 43.\\n```  \\n</CodeOutputBlock>  \\n```python\\nprint(chain.run(\"What is the name of the type of cloud that rins\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new MultiPromptChain chain...\\nNone: {\\'input\\': \\'What is the name of the type of cloud that rains?\\'}\\n> Finished chain.\\nThe type of cloud that typically produces rain is called a cumulonimbus cloud. This type of cloud is characterized by its large vertical extent and can produce thunderstorms and heavy precipitation. Is there anything else you\\'d like to know?\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='First we prepare the data. For this example we do similarity search over a vector database, but these documents could be fetched in any manner (the point of this notebook to highlight what to do AFTER you fetch the documents).  \\n```python\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.docstore.document import Document\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.indexes.vectorstore import VectorstoreIndexCreator\\n```  \\n```python\\nwith open(\"../../state_of_the_union.txt\") as f:\\nstate_of_the_union = f.read()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_text(state_of_the_union)  \\nembeddings = OpenAIEmbeddings()\\n```  \\n```python\\ndocsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nRunning Chroma using direct local API.\\nUsing DuckDB in-memory for database. Data will be transient.\\n```  \\n</CodeOutputBlock>  \\n```python\\nquery = \"What did the president say about Justice Breyer\"\\ndocs = docsearch.get_relevant_documents(query)\\n```  \\n```python\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.llms import OpenAI\\n```', metadata={'Header 2': 'Prepare Data'}),\n",
       " Document(page_content='If you just want to get started as quickly as possible, this is the recommended way to do it:  \\n```python\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\nquery = \"What did the president say about Justice Breyer\"\\nchain.run(input_documents=docs, question=query)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.\\'\\n```  \\n</CodeOutputBlock>  \\nIf you want more control and understanding over what is happening, please see the information below.', metadata={'Header 2': 'Quickstart'}),\n",
       " Document(page_content='This sections shows results of using the `stuff` Chain to do question answering.  \\n```python\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\n```  \\n```python\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'output_text\\': \\' The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.\\'}\\n```  \\n</CodeOutputBlock>  \\n**Custom Prompts**  \\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.  \\n```python\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.  \\n{context}  \\nQuestion: {question}\\nAnswer in Italian:\"\"\"\\nPROMPT = PromptTemplate(\\ntemplate=prompt_template, input_variables=[\"context\", \"question\"]\\n)\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'output_text\\': \\' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese e ha ricevuto una vasta gamma di supporto.\\'}\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'The `stuff` Chain'}),\n",
       " Document(page_content='This sections shows results of using the `map_reduce` Chain to do question answering.  \\n```python\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\")\\n```  \\n```python\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'output_text\\': \\' The president said that Justice Breyer is an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court, and thanked him for his service.\\'}\\n```  \\n</CodeOutputBlock>  \\n**Intermediate Steps**  \\nWe can also return the intermediate steps for `map_reduce` chains, should we want to inspect them. This is done with the `return_map_steps` variable.  \\n```python\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_map_steps=True)\\n```  \\n```python\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'intermediate_steps\\': [\\' \"Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.\"\\',\\n\\' A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\',\\n\\' None\\',\\n\\' None\\'],\\n\\'output_text\\': \\' The president said that Justice Breyer is an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court, and thanked him for his service.\\'}\\n```  \\n</CodeOutputBlock>  \\n**Custom Prompts**  \\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.  \\n```python\\nquestion_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\\nReturn any relevant text translated into italian.\\n{context}\\nQuestion: {question}\\nRelevant text, if any, in Italian:\"\"\"\\nQUESTION_PROMPT = PromptTemplate(\\ntemplate=question_prompt_template, input_variables=[\"context\", \"question\"]\\n)  \\ncombine_prompt_template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer italian.\\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.  \\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nAnswer in Italian:\"\"\"\\nCOMBINE_PROMPT = PromptTemplate(\\ntemplate=combine_prompt_template, input_variables=[\"summaries\", \"question\"]\\n)\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_map_steps=True, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'intermediate_steps\\': [\"\\\\nStasera vorrei onorare qualcuno che ha dedicato la sua vita a servire questo paese: il giustizia Stephen Breyer - un veterano dell\\'esercito, uno studioso costituzionale e un giustizia in uscita della Corte Suprema degli Stati Uniti. Giustizia Breyer, grazie per il tuo servizio.\",\\n\\'\\\\nNessun testo pertinente.\\',\\n\\' Non ha detto nulla riguardo a Justice Breyer.\\',\\n\" Non c\\'è testo pertinente.\"],\\n\\'output_text\\': \\' Non ha detto nulla riguardo a Justice Breyer.\\'}\\n```  \\n</CodeOutputBlock>  \\n**Batch Size**  \\nWhen using the `map_reduce` chain, one thing to keep in mind is the batch size you are using during the map step. If this is too high, it could cause rate limiting errors. You can control this by setting the batch size on the LLM used. Note that this only applies for LLMs with this parameter. Below is an example of doing so:  \\n```python\\nllm = OpenAI(batch_size=5, temperature=0)\\n```', metadata={'Header 2': 'The `map_reduce` Chain'}),\n",
       " Document(page_content='This sections shows results of using the `refine` Chain to do question answering.  \\n```python\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\")\\n```  \\n```python\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'output_text\\': \\'\\\\n\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans. He also praised Justice Breyer for his role in helping to pass the Bipartisan Infrastructure Law, which he said would be the most sweeping investment to rebuild America in history and would help the country compete for the jobs of the 21st Century.\\'}\\n```  \\n</CodeOutputBlock>  \\n**Intermediate Steps**  \\nWe can also return the intermediate steps for `refine` chains, should we want to inspect them. This is done with the `return_refine_steps` variable.  \\n```python\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\", return_refine_steps=True)\\n```  \\n```python\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'intermediate_steps\\': [\\'\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country and his legacy of excellence.\\',\\n\\'\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice.\\',\\n\\'\\\\n\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans.\\',\\n\\'\\\\n\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans. He also praised Justice Breyer for his role in helping to pass the Bipartisan Infrastructure Law, which is the most sweeping investment to rebuild America in history.\\'],\\n\\'output_text\\': \\'\\\\n\\\\nThe president said that he wanted to honor Justice Breyer for his dedication to serving the country, his legacy of excellence, and his commitment to advancing liberty and justice, as well as for his support of the Equality Act and his commitment to protecting the rights of LGBTQ+ Americans. He also praised Justice Breyer for his role in helping to pass the Bipartisan Infrastructure Law, which is the most sweeping investment to rebuild America in history.\\'}\\n```  \\n</CodeOutputBlock>  \\n**Custom Prompts**  \\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.  \\n```python\\nrefine_prompt_template = (\\n\"The original question is as follows: {question}\\\\n\"\\n\"We have provided an existing answer: {existing_answer}\\\\n\"\\n\"We have the opportunity to refine the existing answer\"\\n\"(only if needed) with some more context below.\\\\n\"\\n\"------------\\\\n\"\\n\"{context_str}\\\\n\"\\n\"------------\\\\n\"\\n\"Given the new context, refine the original answer to better \"\\n\"answer the question. \"\\n\"If the context isn\\'t useful, return the original answer. Reply in Italian.\"\\n)\\nrefine_prompt = PromptTemplate(\\ninput_variables=[\"question\", \"existing_answer\", \"context_str\"],\\ntemplate=refine_prompt_template,\\n)  \\ninitial_qa_template = (\\n\"Context information is below. \\\\n\"\\n\"---------------------\\\\n\"\\n\"{context_str}\"\\n\"\\\\n---------------------\\\\n\"\\n\"Given the context information and not prior knowledge, \"\\n\"answer the question: {question}\\\\nYour answer should be in Italian.\\\\n\"\\n)\\ninitial_qa_prompt = PromptTemplate(\\ninput_variables=[\"context_str\", \"question\"], template=initial_qa_template\\n)\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\", return_refine_steps=True,\\nquestion_prompt=initial_qa_prompt, refine_prompt=refine_prompt)\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'intermediate_steps\\': [\\'\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese e ha reso omaggio al suo servizio.\\',\\n\"\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere e la risoluzione del sistema di immigrazione.\",\\n\"\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere, la risoluzione del sistema di immigrazione, la protezione degli americani LGBTQ+ e l\\'approvazione dell\\'Equality Act. Ha inoltre sottolineato l\\'importanza di lavorare insieme per sconfiggere l\\'epidemia di oppiacei.\",\\n\"\\\\n\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere, la risoluzione del sistema di immigrazione, la protezione degli americani LGBTQ+ e l\\'approvazione dell\\'Equality Act. Ha inoltre sottolineato l\\'importanza di lavorare insieme per sconfiggere l\\'epidemia di oppiacei e per investire in America, educare gli americani, far crescere la forza lavoro e costruire l\\'economia dal\"],\\n\\'output_text\\': \"\\\\n\\\\nIl presidente ha detto che Justice Breyer ha dedicato la sua vita al servizio di questo paese, ha reso omaggio al suo servizio e ha sostenuto la nomina di una top litigatrice in pratica privata, un ex difensore pubblico federale e una famiglia di insegnanti e agenti di polizia delle scuole pubbliche. Ha anche sottolineato l\\'importanza di avanzare la libertà e la giustizia attraverso la sicurezza delle frontiere, la risoluzione del sistema di immigrazione, la protezione degli americani LGBTQ+ e l\\'approvazione dell\\'Equality Act. Ha inoltre sottolineato l\\'importanza di lavorare insieme per sconfiggere l\\'epidemia di oppiacei e per investire in America, educare gli americani, far crescere la forza lavoro e costruire l\\'economia dal\"}\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'The `refine` Chain'}),\n",
       " Document(page_content='This sections shows results of using the `map-rerank` Chain to do question answering with sources.  \\n```python\\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", return_intermediate_steps=True)\\n```  \\n```python\\nquery = \"What did the president say about Justice Breyer\"\\nresults = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n```python\\nresults[\"output_text\"]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' The President thanked Justice Breyer for his service and honored him for dedicating his life to serve the country.\\'\\n```  \\n</CodeOutputBlock>  \\n```python\\nresults[\"intermediate_steps\"]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[{\\'answer\\': \\' The President thanked Justice Breyer for his service and honored him for dedicating his life to serve the country.\\',\\n\\'score\\': \\'100\\'},\\n{\\'answer\\': \\' This document does not answer the question\\', \\'score\\': \\'0\\'},\\n{\\'answer\\': \\' This document does not answer the question\\', \\'score\\': \\'0\\'},\\n{\\'answer\\': \\' This document does not answer the question\\', \\'score\\': \\'0\\'}]\\n```  \\n</CodeOutputBlock>  \\n**Custom Prompts**  \\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.  \\n```python\\nfrom langchain.output_parsers import RegexParser  \\noutput_parser = RegexParser(\\nregex=r\"(.*?)\\\\nScore: (.*)\",\\noutput_keys=[\"answer\", \"score\"],\\n)  \\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.  \\nIn addition to giving an answer, also return a score of how fully it answered the user\\'s question. This should be in the following format:  \\nQuestion: [question here]\\nHelpful Answer In Italian: [answer here]\\nScore: [score between 0 and 100]  \\nBegin!  \\nContext:\\n---------\\n{context}\\n---------\\nQuestion: {question}\\nHelpful Answer In Italian:\"\"\"\\nPROMPT = PromptTemplate(\\ntemplate=prompt_template,\\ninput_variables=[\"context\", \"question\"],\\noutput_parser=output_parser,\\n)  \\nchain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", return_intermediate_steps=True, prompt=PROMPT)\\nquery = \"What did the president say about Justice Breyer\"\\nchain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'intermediate_steps\\': [{\\'answer\\': \\' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese.\\',\\n\\'score\\': \\'100\\'},\\n{\\'answer\\': \\' Il presidente non ha detto nulla sulla Giustizia Breyer.\\',\\n\\'score\\': \\'100\\'},\\n{\\'answer\\': \\' Non so.\\', \\'score\\': \\'0\\'},\\n{\\'answer\\': \\' Non so.\\', \\'score\\': \\'0\\'}],\\n\\'output_text\\': \\' Il presidente ha detto che Justice Breyer ha dedicato la sua vita a servire questo paese.\\'}\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'The `map-rerank` Chain'}),\n",
       " Document(page_content='```python\\nwith open(\"../../state_of_the_union.txt\") as f:\\nstate_of_the_union = f.read()\\n```', metadata={}),\n",
       " Document(page_content='Let\\'s take a look at it in action below, using it summarize a long document.  \\n```python\\nfrom langchain import OpenAI\\nfrom langchain.chains.summarize import load_summarize_chain  \\nllm = OpenAI(temperature=0)\\nsummary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\n```  \\n```python\\nfrom langchain.chains import AnalyzeDocumentChain\\n```  \\n```python\\nsummarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)\\n```  \\n```python\\nsummarize_document_chain.run(state_of_the_union)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" In this speech, President Biden addresses the American people and the world, discussing the recent aggression of Russia\\'s Vladimir Putin in Ukraine and the US response. He outlines economic sanctions and other measures taken to hold Putin accountable, and announces the US Department of Justice\\'s task force to go after the crimes of Russian oligarchs. He also announces plans to fight inflation and lower costs for families, invest in American manufacturing, and provide military, economic, and humanitarian assistance to Ukraine. He calls for immigration reform, protecting the rights of women, and advancing the rights of LGBTQ+ Americans, and pays tribute to military families. He concludes with optimism for the future of America.\"\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Summarize'}),\n",
       " Document(page_content='Let\\'s take a look at this using a question answering chain.  \\n```python\\nfrom langchain.chains.question_answering import load_qa_chain\\n```  \\n```python\\nqa_chain = load_qa_chain(llm, chain_type=\"map_reduce\")\\n```  \\n```python\\nqa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\\n```  \\n```python\\nqa_document_chain.run(input_document=state_of_the_union, question=\"what did the president say about justice breyer?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' The president thanked Justice Breyer for his service.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Question Answering'}),\n",
       " Document(page_content='```python\\nfrom langchain.chains import ConversationChain\\nfrom langchain.memory import ConversationBufferMemory  \\nconversation = ConversationChain(\\nllm=chat,\\nmemory=ConversationBufferMemory()\\n)  \\nconversation.run(\"Answer briefly. What are the first 3 colors of a rainbow?\")', metadata={}),\n",
       " Document(page_content='conversation.run(\"And the next 4?\")', metadata={'Header 1': '-> The first three colors of a rainbow are red, orange, and yellow.'}),\n",
       " Document(page_content='```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'The next four colors of a rainbow are green, blue, indigo, and violet.\\'\\n```  \\n</CodeOutputBlock>  \\nEssentially, `BaseMemory` defines an interface of how `langchain` stores memory. It allows reading of stored data through `load_memory_variables` method and storing new data through `save_context` method. You can learn more about it in the [Memory](/docs/modules/memory/) section.', metadata={'Header 1': '-> The next four colors of a rainbow are green, blue, indigo, and violet.'}),\n",
       " Document(page_content='```python\\nclass BaseCombineDocumentsChain(Chain, ABC):\\n\"\"\"Base interface for chains combining documents.\"\"\"  \\n@abstractmethod\\ndef combine_docs(self, docs: List[Document], **kwargs: Any) -> Tuple[str, dict]:\\n\"\"\"Combine documents into a single string.\"\"\"  \\n```', metadata={}),\n",
       " Document(page_content='```python\\nfrom langchain.chains.router import MultiRetrievalQAChain\\nfrom langchain.llms import OpenAI\\n```  \\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.vectorstores import FAISS  \\nsou_docs = TextLoader(\\'../../state_of_the_union.txt\\').load_and_split()\\nsou_retriever = FAISS.from_documents(sou_docs, OpenAIEmbeddings()).as_retriever()  \\npg_docs = TextLoader(\\'../../paul_graham_essay.txt\\').load_and_split()\\npg_retriever = FAISS.from_documents(pg_docs, OpenAIEmbeddings()).as_retriever()  \\npersonal_texts = [\\n\"I love apple pie\",\\n\"My favorite color is fuchsia\",\\n\"My dream is to become a professional dancer\",\\n\"I broke my arm when I was 12\",\\n\"My parents are from Peru\",\\n]\\npersonal_retriever = FAISS.from_texts(personal_texts, OpenAIEmbeddings()).as_retriever()\\n```  \\n```python\\nretriever_infos = [\\n{\\n\"name\": \"state of the union\",\\n\"description\": \"Good for answering questions about the 2023 State of the Union address\",\\n\"retriever\": sou_retriever\\n},\\n{\\n\"name\": \"pg essay\",\\n\"description\": \"Good for answer quesitons about Paul Graham\\'s essay on his career\",\\n\"retriever\": pg_retriever\\n},\\n{\\n\"name\": \"personal\",\\n\"description\": \"Good for answering questions about me\",\\n\"retriever\": personal_retriever\\n}\\n]\\n```  \\n```python\\nchain = MultiRetrievalQAChain.from_retrievers(OpenAI(), retriever_infos, verbose=True)\\n```  \\n```python\\nprint(chain.run(\"What did the president say about the economy?\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new MultiRetrievalQAChain chain...\\nstate of the union: {\\'query\\': \\'What did the president say about the economy in the 2023 State of the Union address?\\'}\\n> Finished chain.\\nThe president said that the economy was stronger than it had been a year prior, and that the American Rescue Plan helped create record job growth and fuel economic relief for millions of Americans. He also proposed a plan to fight inflation and lower costs for families, including cutting the cost of prescription drugs and energy, providing investments and tax credits for energy efficiency, and increasing access to child care and Pre-K.\\n```  \\n</CodeOutputBlock>  \\n```python\\nprint(chain.run(\"What is something Paul Graham regrets about his work?\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new MultiRetrievalQAChain chain...\\npg essay: {\\'query\\': \\'What is something Paul Graham regrets about his work?\\'}\\n> Finished chain.\\nPaul Graham regrets that he did not take a vacation after selling his company, instead of immediately starting to paint.\\n```  \\n</CodeOutputBlock>  \\n```python\\nprint(chain.run(\"What is my background?\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new MultiRetrievalQAChain chain...\\npersonal: {\\'query\\': \\'What is my background?\\'}\\n> Finished chain.\\nYour background is Peruvian.\\n```  \\n</CodeOutputBlock>  \\n```python\\nprint(chain.run(\"What year was the Internet created in?\"))\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new MultiRetrievalQAChain chain...\\nNone: {\\'query\\': \\'What year was the Internet created in?\\'}\\n> Finished chain.\\nThe Internet was created in 1969 through a project called ARPANET, which was funded by the United States Department of Defense. However, the World Wide Web, which is often confused with the Internet, was created in 1989 by British computer scientist Tim Berners-Lee.\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='```python\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.llms import OpenAI\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\n```  \\n```python\\nloader = TextLoader(\"../../state_of_the_union.txt\")\\ndocuments = loader.load()\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)  \\nembeddings = OpenAIEmbeddings()\\ndocsearch = Chroma.from_documents(texts, embeddings)  \\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever())\\n```  \\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" The president said that she is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support, from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='You can easily specify different chain types to load and use in the RetrievalQA chain. For a more detailed walkthrough of these types, please see [this notebook](/docs/modules/chains/additional/question_answering.html).  \\nThere are two ways to load different chain types. First, you can specify the chain type argument in the `from_chain_type` method. This allows you to pass in the name of the chain type you want to use. For example, in the below we change the chain type to `map_reduce`.  \\n```python\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"map_reduce\", retriever=docsearch.as_retriever())\\n```  \\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" The president said that Judge Ketanji Brown Jackson is one of our nation\\'s top legal minds, a former top litigator in private practice and a former federal public defender, from a family of public school educators and police officers, a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\n```  \\n</CodeOutputBlock>  \\nThe above way allows you to really simply change the chain_type, but it doesn\\'t provide a ton of flexibility over parameters to that chain type. If you want to control those parameters, you can load the chain directly (as you did in [this notebook](/docs/modules/chains/additional/question_answering.html)) and then pass that directly to the the RetrievalQA chain with the `combine_documents_chain` parameter. For example:  \\n```python\\nfrom langchain.chains.question_answering import load_qa_chain\\nqa_chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\\nqa = RetrievalQA(combine_documents_chain=qa_chain, retriever=docsearch.as_retriever())\\n```  \\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Chain Type'}),\n",
       " Document(page_content='You can pass in custom prompts to do question answering. These prompts are the same prompts as you can pass into the [base question answering chain](/docs/modules/chains/additional/question_answering.html)  \\n```python\\nfrom langchain.prompts import PromptTemplate\\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.  \\n{context}  \\nQuestion: {question}\\nAnswer in Italian:\"\"\"\\nPROMPT = PromptTemplate(\\ntemplate=prompt_template, input_variables=[\"context\", \"question\"]\\n)\\n```  \\n```python\\nchain_type_kwargs = {\"prompt\": PROMPT}\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)\\n```  \\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" Il presidente ha detto che Ketanji Brown Jackson è una delle menti legali più importanti del paese, che continuerà l\\'eccellenza di Justice Breyer e che ha ricevuto un ampio sostegno, da Fraternal Order of Police a ex giudici nominati da democratici e repubblicani.\"\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Custom Prompts'}),\n",
       " Document(page_content='Under the hood, LangChain uses SQLAlchemy to connect to SQL databases. The `SQLDatabaseChain` can therefore be used with any SQL dialect supported by SQLAlchemy, such as MS SQL, MySQL, MariaDB, PostgreSQL, Oracle SQL, [Databricks](/docs/ecosystem/integrations/databricks.html) and SQLite. Please refer to the SQLAlchemy documentation for more information about requirements for connecting to your database. For example, a connection to MySQL requires an appropriate connector such as PyMySQL. A URI for a MySQL connection might look like: `mysql+pymysql://user:pass@some_mysql_db_address/db_name`.  \\nThis demonstration uses SQLite and the example Chinook database.\\nTo set it up, follow the instructions on https://database.guide/2-sample-databases-sqlite/, placing the `.db` file in a notebooks folder at the root of this repository.  \\n```python\\nfrom langchain import OpenAI, SQLDatabase, SQLDatabaseChain\\n```  \\n```python\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\nllm = OpenAI(temperature=0, verbose=True)\\n```  \\n**NOTE:** For data-sensitive projects, you can specify `return_direct=True` in the `SQLDatabaseChain` initialization to directly return the output of the SQL query without any additional formatting. This prevents the LLM from seeing any contents within the database. Note, however, the LLM still has access to the database scheme (i.e. dialect, table and key names) by default.  \\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\\n```  \\n```python\\ndb_chain.run(\"How many employees are there?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nHow many employees are there?\\nSQLQuery:  \\n/workspace/langchain/langchain/sql_database.py:191: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.\\nsample_rows = connection.execute(command)  \\nSELECT COUNT(*) FROM \"Employee\";\\nSQLResult: [(8,)]\\nAnswer:There are 8 employees.\\n> Finished chain.  \\n\\'There are 8 employees.\\'\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='Sometimes the Language Model generates invalid SQL with small mistakes that can be self-corrected using the same technique used by the SQL Database Agent to try and fix the SQL using the LLM. You can simply specify this option when creating the chain:  \\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True)\\n```  \\n```python\\ndb_chain.run(\"How many albums by Aerosmith?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nHow many albums by Aerosmith?\\nSQLQuery:SELECT COUNT(*) FROM Album WHERE ArtistId = 3;\\nSQLResult: [(1,)]\\nAnswer:There is 1 album by Aerosmith.\\n> Finished chain.  \\n\\'There is 1 album by Aerosmith.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Use Query Checker'}),\n",
       " Document(page_content='You can also customize the prompt that is used. Here is an example prompting it to understand that foobar is the same as the Employee table  \\n```python\\nfrom langchain.prompts.prompt import PromptTemplate  \\n_DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\\nUse the following format:  \\nQuestion: \"Question here\"\\nSQLQuery: \"SQL Query to run\"\\nSQLResult: \"Result of the SQLQuery\"\\nAnswer: \"Final answer here\"  \\nOnly use the following tables:  \\n{table_info}  \\nIf someone asks for the table foobar, they really mean the employee table.  \\nQuestion: {input}\"\"\"\\nPROMPT = PromptTemplate(\\ninput_variables=[\"input\", \"table_info\", \"dialect\"], template=_DEFAULT_TEMPLATE\\n)\\n```  \\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True)\\n```  \\n```python\\ndb_chain.run(\"How many employees are there in the foobar table?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nHow many employees are there in the foobar table?\\nSQLQuery:SELECT COUNT(*) FROM Employee;\\nSQLResult: [(8,)]\\nAnswer:There are 8 employees in the foobar table.\\n> Finished chain.  \\n\\'There are 8 employees in the foobar table.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Customize Prompt'}),\n",
       " Document(page_content='You can also return the intermediate steps of the SQLDatabaseChain. This allows you to access the SQL statement that was generated, as well as the result of running that against the SQL Database.  \\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True, use_query_checker=True, return_intermediate_steps=True)\\n```  \\n```python\\nresult = db_chain(\"How many employees are there in the foobar table?\")\\nresult[\"intermediate_steps\"]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nHow many employees are there in the foobar table?\\nSQLQuery:SELECT COUNT(*) FROM Employee;\\nSQLResult: [(8,)]\\nAnswer:There are 8 employees in the foobar table.\\n> Finished chain.  \\n[{\\'input\\': \\'How many employees are there in the foobar table?\\\\nSQLQuery:SELECT COUNT(*) FROM Employee;\\\\nSQLResult: [(8,)]\\\\nAnswer:\\',\\n\\'top_k\\': \\'5\\',\\n\\'dialect\\': \\'sqlite\\',\\n\\'table_info\\': \\'\\\\nCREATE TABLE \"Artist\" (\\\\n\\\\t\"ArtistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"ArtistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Artist table:\\\\nArtistId\\\\tName\\\\n1\\\\tAC/DC\\\\n2\\\\tAccept\\\\n3\\\\tAerosmith\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Employee\" (\\\\n\\\\t\"EmployeeId\" INTEGER NOT NULL, \\\\n\\\\t\"LastName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"FirstName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"Title\" NVARCHAR(30), \\\\n\\\\t\"ReportsTo\" INTEGER, \\\\n\\\\t\"BirthDate\" DATETIME, \\\\n\\\\t\"HireDate\" DATETIME, \\\\n\\\\t\"Address\" NVARCHAR(70), \\\\n\\\\t\"City\" NVARCHAR(40), \\\\n\\\\t\"State\" NVARCHAR(40), \\\\n\\\\t\"Country\" NVARCHAR(40), \\\\n\\\\t\"PostalCode\" NVARCHAR(10), \\\\n\\\\t\"Phone\" NVARCHAR(24), \\\\n\\\\t\"Fax\" NVARCHAR(24), \\\\n\\\\t\"Email\" NVARCHAR(60), \\\\n\\\\tPRIMARY KEY (\"EmployeeId\"), \\\\n\\\\tFOREIGN KEY(\"ReportsTo\") REFERENCES \"Employee\" (\"EmployeeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Employee table:\\\\nEmployeeId\\\\tLastName\\\\tFirstName\\\\tTitle\\\\tReportsTo\\\\tBirthDate\\\\tHireDate\\\\tAddress\\\\tCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\n1\\\\tAdams\\\\tAndrew\\\\tGeneral Manager\\\\tNone\\\\t1962-02-18 00:00:00\\\\t2002-08-14 00:00:00\\\\t11120 Jasper Ave NW\\\\tEdmonton\\\\tAB\\\\tCanada\\\\tT5K 2N1\\\\t+1 (780) 428-9482\\\\t+1 (780) 428-3457\\\\tandrew@chinookcorp.com\\\\n2\\\\tEdwards\\\\tNancy\\\\tSales Manager\\\\t1\\\\t1958-12-08 00:00:00\\\\t2002-05-01 00:00:00\\\\t825 8 Ave SW\\\\tCalgary\\\\tAB\\\\tCanada\\\\tT2P 2T3\\\\t+1 (403) 262-3443\\\\t+1 (403) 262-3322\\\\tnancy@chinookcorp.com\\\\n3\\\\tPeacock\\\\tJane\\\\tSales Support Agent\\\\t2\\\\t1973-08-29 00:00:00\\\\t2002-04-01 00:00:00\\\\t1111 6 Ave SW\\\\tCalgary\\\\tAB\\\\tCanada\\\\tT2P 5M5\\\\t+1 (403) 262-3443\\\\t+1 (403) 262-6712\\\\tjane@chinookcorp.com\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Genre\" (\\\\n\\\\t\"GenreId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"GenreId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Genre table:\\\\nGenreId\\\\tName\\\\n1\\\\tRock\\\\n2\\\\tJazz\\\\n3\\\\tMetal\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"MediaType\" (\\\\n\\\\t\"MediaTypeId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"MediaTypeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from MediaType table:\\\\nMediaTypeId\\\\tName\\\\n1\\\\tMPEG audio file\\\\n2\\\\tProtected AAC audio file\\\\n3\\\\tProtected MPEG-4 video file\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Playlist\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Playlist table:\\\\nPlaylistId\\\\tName\\\\n1\\\\tMusic\\\\n2\\\\tMovies\\\\n3\\\\tTV Shows\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Album\" (\\\\n\\\\t\"AlbumId\" INTEGER NOT NULL, \\\\n\\\\t\"Title\" NVARCHAR(160) NOT NULL, \\\\n\\\\t\"ArtistId\" INTEGER NOT NULL, \\\\n\\\\tPRIMARY KEY (\"AlbumId\"), \\\\n\\\\tFOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Album table:\\\\nAlbumId\\\\tTitle\\\\tArtistId\\\\n1\\\\tFor Those About To Rock We Salute You\\\\t1\\\\n2\\\\tBalls to the Wall\\\\t2\\\\n3\\\\tRestless and Wild\\\\t2\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Customer\" (\\\\n\\\\t\"CustomerId\" INTEGER NOT NULL, \\\\n\\\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\\\n\\\\t\"LastName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"Company\" NVARCHAR(80), \\\\n\\\\t\"Address\" NVARCHAR(70), \\\\n\\\\t\"City\" NVARCHAR(40), \\\\n\\\\t\"State\" NVARCHAR(40), \\\\n\\\\t\"Country\" NVARCHAR(40), \\\\n\\\\t\"PostalCode\" NVARCHAR(10), \\\\n\\\\t\"Phone\" NVARCHAR(24), \\\\n\\\\t\"Fax\" NVARCHAR(24), \\\\n\\\\t\"Email\" NVARCHAR(60) NOT NULL, \\\\n\\\\t\"SupportRepId\" INTEGER, \\\\n\\\\tPRIMARY KEY (\"CustomerId\"), \\\\n\\\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Customer table:\\\\nCustomerId\\\\tFirstName\\\\tLastName\\\\tCompany\\\\tAddress\\\\tCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\tSupportRepId\\\\n1\\\\tLuís\\\\tGonçalves\\\\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\\\\tAv. Brigadeiro Faria Lima, 2170\\\\tSão José dos Campos\\\\tSP\\\\tBrazil\\\\t12227-000\\\\t+55 (12) 3923-5555\\\\t+55 (12) 3923-5566\\\\tluisg@embraer.com.br\\\\t3\\\\n2\\\\tLeonie\\\\tKöhler\\\\tNone\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\tNone\\\\tGermany\\\\t70174\\\\t+49 0711 2842222\\\\tNone\\\\tleonekohler@surfeu.de\\\\t5\\\\n3\\\\tFrançois\\\\tTremblay\\\\tNone\\\\t1498 rue Bélanger\\\\tMontréal\\\\tQC\\\\tCanada\\\\tH2G 1A7\\\\t+1 (514) 721-4711\\\\tNone\\\\tftremblay@gmail.com\\\\t3\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Invoice\" (\\\\n\\\\t\"InvoiceId\" INTEGER NOT NULL, \\\\n\\\\t\"CustomerId\" INTEGER NOT NULL, \\\\n\\\\t\"InvoiceDate\" DATETIME NOT NULL, \\\\n\\\\t\"BillingAddress\" NVARCHAR(70), \\\\n\\\\t\"BillingCity\" NVARCHAR(40), \\\\n\\\\t\"BillingState\" NVARCHAR(40), \\\\n\\\\t\"BillingCountry\" NVARCHAR(40), \\\\n\\\\t\"BillingPostalCode\" NVARCHAR(10), \\\\n\\\\t\"Total\" NUMERIC(10, 2) NOT NULL, \\\\n\\\\tPRIMARY KEY (\"InvoiceId\"), \\\\n\\\\tFOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Invoice table:\\\\nInvoiceId\\\\tCustomerId\\\\tInvoiceDate\\\\tBillingAddress\\\\tBillingCity\\\\tBillingState\\\\tBillingCountry\\\\tBillingPostalCode\\\\tTotal\\\\n1\\\\t2\\\\t2009-01-01 00:00:00\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\tNone\\\\tGermany\\\\t70174\\\\t1.98\\\\n2\\\\t4\\\\t2009-01-02 00:00:00\\\\tUllevålsveien 14\\\\tOslo\\\\tNone\\\\tNorway\\\\t0171\\\\t3.96\\\\n3\\\\t8\\\\t2009-01-03 00:00:00\\\\tGrétrystraat 63\\\\tBrussels\\\\tNone\\\\tBelgium\\\\t1000\\\\t5.94\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"Track\" (\\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(200) NOT NULL, \\\\n\\\\t\"AlbumId\" INTEGER, \\\\n\\\\t\"MediaTypeId\" INTEGER NOT NULL, \\\\n\\\\t\"GenreId\" INTEGER, \\\\n\\\\t\"Composer\" NVARCHAR(220), \\\\n\\\\t\"Milliseconds\" INTEGER NOT NULL, \\\\n\\\\t\"Bytes\" INTEGER, \\\\n\\\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\\\n\\\\tPRIMARY KEY (\"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \\\\n\\\\tFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \\\\n\\\\tFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Track table:\\\\nTrackId\\\\tName\\\\tAlbumId\\\\tMediaTypeId\\\\tGenreId\\\\tComposer\\\\tMilliseconds\\\\tBytes\\\\tUnitPrice\\\\n1\\\\tFor Those About To Rock (We Salute You)\\\\t1\\\\t1\\\\t1\\\\tAngus Young, Malcolm Young, Brian Johnson\\\\t343719\\\\t11170334\\\\t0.99\\\\n2\\\\tBalls to the Wall\\\\t2\\\\t2\\\\t1\\\\tNone\\\\t342562\\\\t5510424\\\\t0.99\\\\n3\\\\tFast As a Shark\\\\t3\\\\t2\\\\t1\\\\tF. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman\\\\t230619\\\\t3990994\\\\t0.99\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"InvoiceLine\" (\\\\n\\\\t\"InvoiceLineId\" INTEGER NOT NULL, \\\\n\\\\t\"InvoiceId\" INTEGER NOT NULL, \\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"UnitPrice\" NUMERIC(10, 2) NOT NULL, \\\\n\\\\t\"Quantity\" INTEGER NOT NULL, \\\\n\\\\tPRIMARY KEY (\"InvoiceLineId\"), \\\\n\\\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"InvoiceId\") REFERENCES \"Invoice\" (\"InvoiceId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from InvoiceLine table:\\\\nInvoiceLineId\\\\tInvoiceId\\\\tTrackId\\\\tUnitPrice\\\\tQuantity\\\\n1\\\\t1\\\\t2\\\\t0.99\\\\t1\\\\n2\\\\t1\\\\t4\\\\t0.99\\\\t1\\\\n3\\\\t2\\\\t6\\\\t0.99\\\\t1\\\\n*/\\\\n\\\\n\\\\nCREATE TABLE \"PlaylistTrack\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\tPRIMARY KEY (\"PlaylistId\", \"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"), \\\\n\\\\tFOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from PlaylistTrack table:\\\\nPlaylistId\\\\tTrackId\\\\n1\\\\t3402\\\\n1\\\\t3389\\\\n1\\\\t3390\\\\n*/\\',\\n\\'stop\\': [\\'\\\\nSQLResult:\\']},\\n\\'SELECT COUNT(*) FROM Employee;\\',\\n{\\'query\\': \\'SELECT COUNT(*) FROM Employee;\\', \\'dialect\\': \\'sqlite\\'},\\n\\'SELECT COUNT(*) FROM Employee;\\',\\n\\'[(8,)]\\']\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Return Intermediate Steps'}),\n",
       " Document(page_content='If you are querying for several rows of a table you can select the maximum number of results you want to get by using the \\'top_k\\' parameter (default is 10). This is useful for avoiding query results that exceed the prompt max length or consume tokens unnecessarily.  \\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, use_query_checker=True, top_k=3)\\n```  \\n```python\\ndb_chain.run(\"What are some example tracks by composer Johann Sebastian Bach?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nWhat are some example tracks by composer Johann Sebastian Bach?\\nSQLQuery:SELECT Name FROM Track WHERE Composer = \\'Johann Sebastian Bach\\' LIMIT 3\\nSQLResult: [(\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',)]\\nAnswer:Examples of tracks by Johann Sebastian Bach are Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace, Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria, and Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude.\\n> Finished chain.  \\n\\'Examples of tracks by Johann Sebastian Bach are Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace, Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria, and Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Choosing how to limit the number of rows returned'}),\n",
       " Document(page_content='Sometimes, the format of the data is not obvious and it is optimal to include a sample of rows from the tables in the prompt to allow the LLM to understand the data before providing a final query. Here we will use this feature to let the LLM know that artists are saved with their full names by providing two rows from the `Track` table.  \\n```python\\ndb = SQLDatabase.from_uri(\\n\"sqlite:///../../../../notebooks/Chinook.db\",\\ninclude_tables=[\\'Track\\'], # we include only one table to save tokens in the prompt :)\\nsample_rows_in_table_info=2)\\n```  \\nThe sample rows are added to the prompt after each corresponding table\\'s column information:  \\n```python\\nprint(db.table_info)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\nCREATE TABLE \"Track\" (\\n\"TrackId\" INTEGER NOT NULL,\\n\"Name\" NVARCHAR(200) NOT NULL,\\n\"AlbumId\" INTEGER,\\n\"MediaTypeId\" INTEGER NOT NULL,\\n\"GenreId\" INTEGER,\\n\"Composer\" NVARCHAR(220),\\n\"Milliseconds\" INTEGER NOT NULL,\\n\"Bytes\" INTEGER,\\n\"UnitPrice\" NUMERIC(10, 2) NOT NULL,\\nPRIMARY KEY (\"TrackId\"),\\nFOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),\\nFOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),\\nFOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\\n)  \\n/*\\n2 rows from Track table:\\nTrackId\\tName\\tAlbumId\\tMediaTypeId\\tGenreId\\tComposer\\tMilliseconds\\tBytes\\tUnitPrice\\n1\\tFor Those About To Rock (We Salute You)\\t1\\t1\\t1\\tAngus Young, Malcolm Young, Brian Johnson\\t343719\\t11170334\\t0.99\\n2\\tBalls to the Wall\\t2\\t2\\t1\\tNone\\t342562\\t5510424\\t0.99\\n*/\\n```  \\n</CodeOutputBlock>  \\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, use_query_checker=True, verbose=True)\\n```  \\n```python\\ndb_chain.run(\"What are some example tracks by Bach?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nWhat are some example tracks by Bach?\\nSQLQuery:SELECT \"Name\", \"Composer\" FROM \"Track\" WHERE \"Composer\" LIKE \\'%Bach%\\' LIMIT 5\\nSQLResult: [(\\'American Woman\\', \\'B. Cummings/G. Peterson/M.J. Kale/R. Bachman\\'), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Johann Sebastian Bach\\'), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Johann Sebastian Bach\\'), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', \\'Johann Sebastian Bach\\'), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\', \\'Johann Sebastian Bach\\')]\\nAnswer:Tracks by Bach include \\'American Woman\\', \\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\', \\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\', \\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\', and \\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\'.\\n> Finished chain.  \\n\\'Tracks by Bach include \\\\\\'American Woman\\\\\\', \\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\', \\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\', \\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\', and \\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\'.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Adding example rows from each table'}),\n",
       " Document(page_content='In some cases, it can be useful to provide custom table information instead of using the automatically generated table definitions and the first `sample_rows_in_table_info` sample rows. For example, if you know that the first few rows of a table are uninformative, it could help to manually provide example rows that are more diverse or provide more information to the model. It is also possible to limit the columns that will be visible to the model if there are unnecessary columns.  \\nThis information can be provided as a dictionary with table names as the keys and table information as the values. For example, let\\'s provide a custom definition and sample rows for the Track table with only a few columns:  \\n```python\\ncustom_table_info = {\\n\"Track\": \"\"\"CREATE TABLE Track (\\n\"TrackId\" INTEGER NOT NULL,\\n\"Name\" NVARCHAR(200) NOT NULL,\\n\"Composer\" NVARCHAR(220),\\nPRIMARY KEY (\"TrackId\")\\n)\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tComposer\\n1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n2\\tBalls to the Wall\\tNone\\n3\\tMy favorite song ever\\tThe coolest composer of all time\\n*/\"\"\"\\n}\\n```  \\n```python\\ndb = SQLDatabase.from_uri(\\n\"sqlite:///../../../../notebooks/Chinook.db\",\\ninclude_tables=[\\'Track\\', \\'Playlist\\'],\\nsample_rows_in_table_info=2,\\ncustom_table_info=custom_table_info)  \\nprint(db.table_info)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\nCREATE TABLE \"Playlist\" (\\n\"PlaylistId\" INTEGER NOT NULL,\\n\"Name\" NVARCHAR(120),\\nPRIMARY KEY (\"PlaylistId\")\\n)  \\n/*\\n2 rows from Playlist table:\\nPlaylistId\\tName\\n1\\tMusic\\n2\\tMovies\\n*/  \\nCREATE TABLE Track (\\n\"TrackId\" INTEGER NOT NULL,\\n\"Name\" NVARCHAR(200) NOT NULL,\\n\"Composer\" NVARCHAR(220),\\nPRIMARY KEY (\"TrackId\")\\n)\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tComposer\\n1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n2\\tBalls to the Wall\\tNone\\n3\\tMy favorite song ever\\tThe coolest composer of all time\\n*/\\n```  \\n</CodeOutputBlock>  \\nNote how our custom table definition and sample rows for `Track` overrides the `sample_rows_in_table_info` parameter. Tables that are not overridden by `custom_table_info`, in this example `Playlist`, will have their table info gathered automatically as usual.  \\n```python\\ndb_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\\ndb_chain.run(\"What are some example tracks by Bach?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nWhat are some example tracks by Bach?\\nSQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\'%Bach%\\' LIMIT 5;\\nSQLResult: [(\\'American Woman\\',), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\',)]\\nAnswer:text=\\'You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\\\n\\\\nUse the following format:\\\\n\\\\nQuestion: \"Question here\"\\\\nSQLQuery: \"SQL Query to run\"\\\\nSQLResult: \"Result of the SQLQuery\"\\\\nAnswer: \"Final answer here\"\\\\n\\\\nOnly use the following tables:\\\\n\\\\nCREATE TABLE \"Playlist\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n2 rows from Playlist table:\\\\nPlaylistId\\\\tName\\\\n1\\\\tMusic\\\\n2\\\\tMovies\\\\n*/\\\\n\\\\nCREATE TABLE Track (\\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(200) NOT NULL,\\\\n\\\\t\"Composer\" NVARCHAR(220),\\\\n\\\\tPRIMARY KEY (\"TrackId\")\\\\n)\\\\n/*\\\\n3 rows from Track table:\\\\nTrackId\\\\tName\\\\tComposer\\\\n1\\\\tFor Those About To Rock (We Salute You)\\\\tAngus Young, Malcolm Young, Brian Johnson\\\\n2\\\\tBalls to the Wall\\\\tNone\\\\n3\\\\tMy favorite song ever\\\\tThe coolest composer of all time\\\\n*/\\\\n\\\\nQuestion: What are some example tracks by Bach?\\\\nSQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\\\\\'%Bach%\\\\\\' LIMIT 5;\\\\nSQLResult: [(\\\\\\'American Woman\\\\\\',), (\\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\',), (\\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\',), (\\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\',), (\\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\',)]\\\\nAnswer:\\'\\nYou are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.  \\nUse the following format:  \\nQuestion: \"Question here\"\\nSQLQuery: \"SQL Query to run\"\\nSQLResult: \"Result of the SQLQuery\"\\nAnswer: \"Final answer here\"  \\nOnly use the following tables:  \\nCREATE TABLE \"Playlist\" (\\n\"PlaylistId\" INTEGER NOT NULL,\\n\"Name\" NVARCHAR(120),\\nPRIMARY KEY (\"PlaylistId\")\\n)  \\n/*\\n2 rows from Playlist table:\\nPlaylistId\\tName\\n1\\tMusic\\n2\\tMovies\\n*/  \\nCREATE TABLE Track (\\n\"TrackId\" INTEGER NOT NULL,\\n\"Name\" NVARCHAR(200) NOT NULL,\\n\"Composer\" NVARCHAR(220),\\nPRIMARY KEY (\"TrackId\")\\n)\\n/*\\n3 rows from Track table:\\nTrackId\\tName\\tComposer\\n1\\tFor Those About To Rock (We Salute You)\\tAngus Young, Malcolm Young, Brian Johnson\\n2\\tBalls to the Wall\\tNone\\n3\\tMy favorite song ever\\tThe coolest composer of all time\\n*/  \\nQuestion: What are some example tracks by Bach?\\nSQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\'%Bach%\\' LIMIT 5;\\nSQLResult: [(\\'American Woman\\',), (\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\',), (\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\',), (\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\',), (\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\',)]\\nAnswer:\\n{\\'input\\': \\'What are some example tracks by Bach?\\\\nSQLQuery:SELECT \"Name\" FROM Track WHERE \"Composer\" LIKE \\\\\\'%Bach%\\\\\\' LIMIT 5;\\\\nSQLResult: [(\\\\\\'American Woman\\\\\\',), (\\\\\\'Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\\\\\\',), (\\\\\\'Aria Mit 30 Veränderungen, BWV 988 \"Goldberg Variations\": Aria\\\\\\',), (\\\\\\'Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\\\\\\',), (\\\\\\'Toccata and Fugue in D Minor, BWV 565: I. Toccata\\\\\\',)]\\\\nAnswer:\\', \\'top_k\\': \\'5\\', \\'dialect\\': \\'sqlite\\', \\'table_info\\': \\'\\\\nCREATE TABLE \"Playlist\" (\\\\n\\\\t\"PlaylistId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(120), \\\\n\\\\tPRIMARY KEY (\"PlaylistId\")\\\\n)\\\\n\\\\n/*\\\\n2 rows from Playlist table:\\\\nPlaylistId\\\\tName\\\\n1\\\\tMusic\\\\n2\\\\tMovies\\\\n*/\\\\n\\\\nCREATE TABLE Track (\\\\n\\\\t\"TrackId\" INTEGER NOT NULL, \\\\n\\\\t\"Name\" NVARCHAR(200) NOT NULL,\\\\n\\\\t\"Composer\" NVARCHAR(220),\\\\n\\\\tPRIMARY KEY (\"TrackId\")\\\\n)\\\\n/*\\\\n3 rows from Track table:\\\\nTrackId\\\\tName\\\\tComposer\\\\n1\\\\tFor Those About To Rock (We Salute You)\\\\tAngus Young, Malcolm Young, Brian Johnson\\\\n2\\\\tBalls to the Wall\\\\tNone\\\\n3\\\\tMy favorite song ever\\\\tThe coolest composer of all time\\\\n*/\\', \\'stop\\': [\\'\\\\nSQLResult:\\']}\\nExamples of tracks by Bach include \"American Woman\", \"Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\", \"Aria Mit 30 Veränderungen, BWV 988 \\'Goldberg Variations\\': Aria\", \"Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\", and \"Toccata and Fugue in D Minor, BWV 565: I. Toccata\".\\n> Finished chain.  \\n\\'Examples of tracks by Bach include \"American Woman\", \"Concerto for 2 Violins in D Minor, BWV 1043: I. Vivace\", \"Aria Mit 30 Veränderungen, BWV 988 \\\\\\'Goldberg Variations\\\\\\': Aria\", \"Suite for Solo Cello No. 1 in G Major, BWV 1007: I. Prélude\", and \"Toccata and Fugue in D Minor, BWV 565: I. Toccata\".\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Adding example rows from each table', 'Header 3': 'Custom Table Info'}),\n",
       " Document(page_content='Chain for querying SQL database that is a sequential chain.  \\nThe chain is as follows:  \\n1. Based on the query, determine which tables to use.\\n2. Based on those tables, call the normal SQL database chain.  \\nThis is useful in cases where the number of tables in the database is large.  \\n```python\\nfrom langchain.chains import SQLDatabaseSequentialChain\\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\")\\n```  \\n```python\\nchain = SQLDatabaseSequentialChain.from_llm(llm, db, verbose=True)\\n```  \\n```python\\nchain.run(\"How many employees are also customers?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseSequentialChain chain...\\nTable names to use:\\n[\\'Employee\\', \\'Customer\\']  \\n> Entering new SQLDatabaseChain chain...\\nHow many employees are also customers?\\nSQLQuery:SELECT COUNT(*) FROM Employee e INNER JOIN Customer c ON e.EmployeeId = c.SupportRepId;\\nSQLResult: [(59,)]\\nAnswer:59 employees are also customers.\\n> Finished chain.  \\n> Finished chain.  \\n\\'59 employees are also customers.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'SQLDatabaseSequentialChain'}),\n",
       " Document(page_content='Sometimes you may not have the luxury of using OpenAI or other service-hosted large language model. You can, ofcourse, try to use the `SQLDatabaseChain` with a local model, but will quickly realize that most models you can run locally even with a large GPU struggle to generate the right output.  \\n```python\\nimport logging\\nimport torch\\nfrom transformers import AutoTokenizer, GPT2TokenizerFast, pipeline, AutoModelForSeq2SeqLM, AutoModelForCausalLM\\nfrom langchain import HuggingFacePipeline', metadata={'Header 2': 'Using Local Language Models'}),\n",
       " Document(page_content='model_id = \"google/flan-ul2\"\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, temperature=0)  \\ndevice_id = -1  # default to no-GPU, but use GPU and half precision mode if available\\nif torch.cuda.is_available():\\ndevice_id = 0\\ntry:\\nmodel = model.half()\\nexcept RuntimeError as exc:\\nlogging.warn(f\"Could not run model in half precision mode: {str(exc)}\")  \\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\npipe = pipeline(task=\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=1024, device=device_id)  \\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n/workspace/langchain/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\\nfrom .autonotebook import tqdm as notebook_tqdm\\nLoading checkpoint shards: 100%|██████████| 8/8 [00:32<00:00,  4.11s/it]\\n```  \\n</CodeOutputBlock>  \\n```python\\nfrom langchain import SQLDatabase, SQLDatabaseChain  \\ndb = SQLDatabase.from_uri(\"sqlite:///../../../../notebooks/Chinook.db\", include_tables=[\\'Customer\\'])\\nlocal_chain = SQLDatabaseChain.from_llm(local_llm, db, verbose=True, return_intermediate_steps=True, use_query_checker=True)\\n```  \\nThis model should work for very simple SQL queries, as long as you use the query checker as specified above, e.g.:  \\n```python\\nlocal_chain(\"How many customers are there?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nHow many customers are there?\\nSQLQuery:  \\n/workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\nwarnings.warn(\\n/workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\nwarnings.warn(  \\nSELECT count(*) FROM Customer\\nSQLResult: [(59,)]\\nAnswer:  \\n/workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\nwarnings.warn(  \\n[59]\\n> Finished chain.  \\n{\\'query\\': \\'How many customers are there?\\',\\n\\'result\\': \\'[59]\\',\\n\\'intermediate_steps\\': [{\\'input\\': \\'How many customers are there?\\\\nSQLQuery:SELECT count(*) FROM Customer\\\\nSQLResult: [(59,)]\\\\nAnswer:\\',\\n\\'top_k\\': \\'5\\',\\n\\'dialect\\': \\'sqlite\\',\\n\\'table_info\\': \\'\\\\nCREATE TABLE \"Customer\" (\\\\n\\\\t\"CustomerId\" INTEGER NOT NULL, \\\\n\\\\t\"FirstName\" NVARCHAR(40) NOT NULL, \\\\n\\\\t\"LastName\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\"Company\" NVARCHAR(80), \\\\n\\\\t\"Address\" NVARCHAR(70), \\\\n\\\\t\"City\" NVARCHAR(40), \\\\n\\\\t\"State\" NVARCHAR(40), \\\\n\\\\t\"Country\" NVARCHAR(40), \\\\n\\\\t\"PostalCode\" NVARCHAR(10), \\\\n\\\\t\"Phone\" NVARCHAR(24), \\\\n\\\\t\"Fax\" NVARCHAR(24), \\\\n\\\\t\"Email\" NVARCHAR(60) NOT NULL, \\\\n\\\\t\"SupportRepId\" INTEGER, \\\\n\\\\tPRIMARY KEY (\"CustomerId\"), \\\\n\\\\tFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\\\n)\\\\n\\\\n/*\\\\n3 rows from Customer table:\\\\nCustomerId\\\\tFirstName\\\\tLastName\\\\tCompany\\\\tAddress\\\\tCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\tSupportRepId\\\\n1\\\\tLuís\\\\tGonçalves\\\\tEmbraer - Empresa Brasileira de Aeronáutica S.A.\\\\tAv. Brigadeiro Faria Lima, 2170\\\\tSão José dos Campos\\\\tSP\\\\tBrazil\\\\t12227-000\\\\t+55 (12) 3923-5555\\\\t+55 (12) 3923-5566\\\\tluisg@embraer.com.br\\\\t3\\\\n2\\\\tLeonie\\\\tKöhler\\\\tNone\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\tNone\\\\tGermany\\\\t70174\\\\t+49 0711 2842222\\\\tNone\\\\tleonekohler@surfeu.de\\\\t5\\\\n3\\\\tFrançois\\\\tTremblay\\\\tNone\\\\t1498 rue Bélanger\\\\tMontréal\\\\tQC\\\\tCanada\\\\tH2G 1A7\\\\t+1 (514) 721-4711\\\\tNone\\\\tftremblay@gmail.com\\\\t3\\\\n*/\\',\\n\\'stop\\': [\\'\\\\nSQLResult:\\']},\\n\\'SELECT count(*) FROM Customer\\',\\n{\\'query\\': \\'SELECT count(*) FROM Customer\\', \\'dialect\\': \\'sqlite\\'},\\n\\'SELECT count(*) FROM Customer\\',\\n\\'[(59,)]\\']}\\n```  \\n</CodeOutputBlock>  \\nEven this relatively large model will most likely fail to generate more complicated SQL by itself. However, you can log its inputs and outputs so that you can hand-correct them and use the corrected examples for few shot prompt examples later. In practice, you could log any executions of your chain that raise exceptions (as shown in the example below) or get direct user feedback in cases where the results are incorrect (but did not raise an exception).  \\n```bash\\npoetry run pip install pyyaml chromadb\\nimport yaml\\n```  \\n<CodeOutputBlock lang=\"bash\">  \\n```\\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\\nTo disable this warning, you can either:\\n- Avoid using `tokenizers` before the fork if possible\\n- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)  \\n11842.36s - pydevd: Sending message related to process being replaced timed-out after 5 seconds  \\nRequirement already satisfied: pyyaml in /workspace/langchain/.venv/lib/python3.9/site-packages (6.0)\\nRequirement already satisfied: chromadb in /workspace/langchain/.venv/lib/python3.9/site-packages (0.3.21)\\nRequirement already satisfied: pandas>=1.3 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (2.0.1)\\nRequirement already satisfied: requests>=2.28 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (2.28.2)\\nRequirement already satisfied: pydantic>=1.9 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.10.7)\\nRequirement already satisfied: hnswlib>=0.7 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.0)\\nRequirement already satisfied: clickhouse-connect>=0.5.7 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.5.20)\\nRequirement already satisfied: sentence-transformers>=2.2.2 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (2.2.2)\\nRequirement already satisfied: duckdb>=0.7.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.1)\\nRequirement already satisfied: fastapi>=0.85.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.95.1)\\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.21.1)\\nRequirement already satisfied: numpy>=1.21.6 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.24.3)\\nRequirement already satisfied: posthog>=2.4.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.0.1)\\nRequirement already satisfied: certifi in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\\nRequirement already satisfied: urllib3>=1.26 in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\\nRequirement already satisfied: pytz in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2023.3)\\nRequirement already satisfied: zstandard in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (0.21.0)\\nRequirement already satisfied: lz4 in /workspace/langchain/.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.3.2)\\nRequirement already satisfied: starlette<0.27.0,>=0.26.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from fastapi>=0.85.1->chromadb) (0.26.1)\\nRequirement already satisfied: python-dateutil>=2.8.2 in /workspace/langchain/.venv/lib/python3.9/site-packages (from pandas>=1.3->chromadb) (2.8.2)\\nRequirement already satisfied: tzdata>=2022.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from pandas>=1.3->chromadb) (2023.3)\\nRequirement already satisfied: six>=1.5 in /workspace/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\\nRequirement already satisfied: monotonic>=1.5 in /workspace/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6)\\nRequirement already satisfied: backoff>=1.10.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\\nRequirement already satisfied: typing-extensions>=4.2.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from pydantic>=1.9->chromadb) (4.5.0)\\nRequirement already satisfied: charset-normalizer<4,>=2 in /workspace/langchain/.venv/lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.1.0)\\nRequirement already satisfied: idna<4,>=2.5 in /workspace/langchain/.venv/lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.4)\\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.28.1)\\nRequirement already satisfied: tqdm in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.65.0)\\nRequirement already satisfied: torch>=1.6.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.13.1)\\nRequirement already satisfied: torchvision in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.14.1)\\nRequirement already satisfied: scikit-learn in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.2.2)\\nRequirement already satisfied: scipy in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.9.3)\\nRequirement already satisfied: nltk in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (3.8.1)\\nRequirement already satisfied: sentencepiece in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.1.98)\\nRequirement already satisfied: huggingface-hub>=0.4.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.13.4)\\nRequirement already satisfied: click>=7.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\\nRequirement already satisfied: h11>=0.8 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\\nRequirement already satisfied: httptools>=0.5.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.5.0)\\nRequirement already satisfied: python-dotenv>=0.13 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\\nRequirement already satisfied: watchfiles>=0.13 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\\nRequirement already satisfied: websockets>=10.4 in /workspace/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.2)\\nRequirement already satisfied: filelock in /workspace/langchain/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (3.12.0)\\nRequirement already satisfied: packaging>=20.9 in /workspace/langchain/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (23.1)\\nRequirement already satisfied: anyio<5,>=3.4.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (3.6.2)\\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (11.7.99)\\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (8.5.0.96)\\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (11.10.3.66)\\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (11.7.99)\\nRequirement already satisfied: setuptools in /workspace/langchain/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (67.7.1)\\nRequirement already satisfied: wheel in /workspace/langchain/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (0.40.0)\\nRequirement already satisfied: regex!=2019.12.17 in /workspace/langchain/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (2023.3.23)\\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (0.13.3)\\nRequirement already satisfied: joblib in /workspace/langchain/.venv/lib/python3.9/site-packages (from nltk->sentence-transformers>=2.2.2->chromadb) (1.2.0)\\nRequirement already satisfied: threadpoolctl>=2.0.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb) (3.1.0)\\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /workspace/langchain/.venv/lib/python3.9/site-packages (from torchvision->sentence-transformers>=2.2.2->chromadb) (9.5.0)\\nRequirement already satisfied: sniffio>=1.1 in /workspace/langchain/.venv/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (1.3.0)\\n```  \\n</CodeOutputBlock>  \\n```python\\nfrom typing import Dict  \\nQUERY = \"List all the customer first names that start with \\'a\\'\"  \\ndef _parse_example(result: Dict) -> Dict:\\nsql_cmd_key = \"sql_cmd\"\\nsql_result_key = \"sql_result\"\\ntable_info_key = \"table_info\"\\ninput_key = \"input\"\\nfinal_answer_key = \"answer\"  \\n_example = {\\n\"input\": result.get(\"query\"),\\n}  \\nsteps = result.get(\"intermediate_steps\")\\nanswer_key = sql_cmd_key # the first one\\nfor step in steps:', metadata={'Header 1': 'Note: This model requires a large GPU, e.g. an 80GB A100. See documentation for other ways to run private non-OpenAI models.'}),\n",
       " Document(page_content='if isinstance(step, dict):', metadata={'Header 1': 'dict to see what the output is supposed to be'}),\n",
       " Document(page_content='if table_info_key not in _example:\\n_example[table_info_key] = step.get(table_info_key)  \\nif input_key in step:\\nif step[input_key].endswith(\"SQLQuery:\"):\\nanswer_key = sql_cmd_key # this is the SQL generation input\\nif step[input_key].endswith(\"Answer:\"):\\nanswer_key = final_answer_key # this is the final answer input\\nelif sql_cmd_key in step:\\n_example[sql_cmd_key] = step[sql_cmd_key]\\nanswer_key = sql_result_key # this is SQL execution input\\nelif isinstance(step, str):', metadata={'Header 1': 'Grab the table info from input dicts in the intermediate steps once'}),\n",
       " Document(page_content='_example[answer_key] = step\\nreturn _example  \\nexample: any\\ntry:\\nresult = local_chain(QUERY)\\nprint(\"*** Query succeeded\")\\nexample = _parse_example(result)\\nexcept Exception as exc:\\nprint(\"*** Query failed\")\\nresult = {\\n\"query\": QUERY,\\n\"intermediate_steps\": exc.intermediate_steps\\n}\\nexample = _parse_example(result)', metadata={'Header 1': 'The preceding element should have set the answer_key'}),\n",
       " Document(page_content='yaml_example = yaml.dump(example, allow_unicode=True)\\nprint(\"\\\\n\" + yaml_example)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nList all the customer first names that start with \\'a\\'\\nSQLQuery:  \\n/workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\nwarnings.warn(  \\nSELECT firstname FROM customer WHERE firstname LIKE \\'%a%\\'\\nSQLResult: [(\\'François\\',), (\\'František\\',), (\\'Helena\\',), (\\'Astrid\\',), (\\'Daan\\',), (\\'Kara\\',), (\\'Eduardo\\',), (\\'Alexandre\\',), (\\'Fernanda\\',), (\\'Mark\\',), (\\'Frank\\',), (\\'Jack\\',), (\\'Dan\\',), (\\'Kathy\\',), (\\'Heather\\',), (\\'Frank\\',), (\\'Richard\\',), (\\'Patrick\\',), (\\'Julia\\',), (\\'Edward\\',), (\\'Martha\\',), (\\'Aaron\\',), (\\'Madalena\\',), (\\'Hannah\\',), (\\'Niklas\\',), (\\'Camille\\',), (\\'Marc\\',), (\\'Wyatt\\',), (\\'Isabelle\\',), (\\'Ladislav\\',), (\\'Lucas\\',), (\\'Johannes\\',), (\\'Stanisław\\',), (\\'Joakim\\',), (\\'Emma\\',), (\\'Mark\\',), (\\'Manoj\\',), (\\'Puja\\',)]\\nAnswer:  \\n/workspace/langchain/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\\nwarnings.warn(  \\n[(\\'François\\', \\'Frantiek\\', \\'Helena\\', \\'Astrid\\', \\'Daan\\', \\'Kara\\', \\'Eduardo\\', \\'Alexandre\\', \\'Fernanda\\', \\'Mark\\', \\'Frank\\', \\'Jack\\', \\'Dan\\', \\'Kathy\\', \\'Heather\\', \\'Frank\\', \\'Richard\\', \\'Patrick\\', \\'Julia\\', \\'Edward\\', \\'Martha\\', \\'Aaron\\', \\'Madalena\\', \\'Hannah\\', \\'Niklas\\', \\'Camille\\', \\'Marc\\', \\'Wyatt\\', \\'Isabelle\\', \\'Ladislav\\', \\'Lucas\\', \\'Johannes\\', \\'Stanisaw\\', \\'Joakim\\', \\'Emma\\', \\'Mark\\', \\'Manoj\\', \\'Puja\\']\\n> Finished chain.\\n*** Query succeeded  \\nanswer: \\'[(\\'\\'François\\'\\', \\'\\'Frantiek\\'\\', \\'\\'Helena\\'\\', \\'\\'Astrid\\'\\', \\'\\'Daan\\'\\', \\'\\'Kara\\'\\',\\n\\'\\'Eduardo\\'\\', \\'\\'Alexandre\\'\\', \\'\\'Fernanda\\'\\', \\'\\'Mark\\'\\', \\'\\'Frank\\'\\', \\'\\'Jack\\'\\', \\'\\'Dan\\'\\',\\n\\'\\'Kathy\\'\\', \\'\\'Heather\\'\\', \\'\\'Frank\\'\\', \\'\\'Richard\\'\\', \\'\\'Patrick\\'\\', \\'\\'Julia\\'\\', \\'\\'Edward\\'\\',\\n\\'\\'Martha\\'\\', \\'\\'Aaron\\'\\', \\'\\'Madalena\\'\\', \\'\\'Hannah\\'\\', \\'\\'Niklas\\'\\', \\'\\'Camille\\'\\', \\'\\'Marc\\'\\',\\n\\'\\'Wyatt\\'\\', \\'\\'Isabelle\\'\\', \\'\\'Ladislav\\'\\', \\'\\'Lucas\\'\\', \\'\\'Johannes\\'\\', \\'\\'Stanisaw\\'\\', \\'\\'Joakim\\'\\',\\n\\'\\'Emma\\'\\', \\'\\'Mark\\'\\', \\'\\'Manoj\\'\\', \\'\\'Puja\\'\\']\\'\\ninput: List all the customer first names that start with \\'a\\'\\nsql_cmd: SELECT firstname FROM customer WHERE firstname LIKE \\'%a%\\'\\nsql_result: \\'[(\\'\\'François\\'\\',), (\\'\\'František\\'\\',), (\\'\\'Helena\\'\\',), (\\'\\'Astrid\\'\\',), (\\'\\'Daan\\'\\',),\\n(\\'\\'Kara\\'\\',), (\\'\\'Eduardo\\'\\',), (\\'\\'Alexandre\\'\\',), (\\'\\'Fernanda\\'\\',), (\\'\\'Mark\\'\\',), (\\'\\'Frank\\'\\',),\\n(\\'\\'Jack\\'\\',), (\\'\\'Dan\\'\\',), (\\'\\'Kathy\\'\\',), (\\'\\'Heather\\'\\',), (\\'\\'Frank\\'\\',), (\\'\\'Richard\\'\\',),\\n(\\'\\'Patrick\\'\\',), (\\'\\'Julia\\'\\',), (\\'\\'Edward\\'\\',), (\\'\\'Martha\\'\\',), (\\'\\'Aaron\\'\\',), (\\'\\'Madalena\\'\\',),\\n(\\'\\'Hannah\\'\\',), (\\'\\'Niklas\\'\\',), (\\'\\'Camille\\'\\',), (\\'\\'Marc\\'\\',), (\\'\\'Wyatt\\'\\',), (\\'\\'Isabelle\\'\\',),\\n(\\'\\'Ladislav\\'\\',), (\\'\\'Lucas\\'\\',), (\\'\\'Johannes\\'\\',), (\\'\\'Stanisław\\'\\',), (\\'\\'Joakim\\'\\',),\\n(\\'\\'Emma\\'\\',), (\\'\\'Mark\\'\\',), (\\'\\'Manoj\\'\\',), (\\'\\'Puja\\'\\',)]\\'\\ntable_info: \"\\\\nCREATE TABLE \\\\\"Customer\\\\\" (\\\\n\\\\t\\\\\"CustomerId\\\\\" INTEGER NOT NULL, \\\\n\\\\t\\\\\\n\\\\\"FirstName\\\\\" NVARCHAR(40) NOT NULL, \\\\n\\\\t\\\\\"LastName\\\\\" NVARCHAR(20) NOT NULL, \\\\n\\\\t\\\\\\n\\\\\"Company\\\\\" NVARCHAR(80), \\\\n\\\\t\\\\\"Address\\\\\" NVARCHAR(70), \\\\n\\\\t\\\\\"City\\\\\" NVARCHAR(40),\\\\\\n\\\\ \\\\n\\\\t\\\\\"State\\\\\" NVARCHAR(40), \\\\n\\\\t\\\\\"Country\\\\\" NVARCHAR(40), \\\\n\\\\t\\\\\"PostalCode\\\\\" NVARCHAR(10),\\\\\\n\\\\ \\\\n\\\\t\\\\\"Phone\\\\\" NVARCHAR(24), \\\\n\\\\t\\\\\"Fax\\\\\" NVARCHAR(24), \\\\n\\\\t\\\\\"Email\\\\\" NVARCHAR(60)\\\\\\n\\\\ NOT NULL, \\\\n\\\\t\\\\\"SupportRepId\\\\\" INTEGER, \\\\n\\\\tPRIMARY KEY (\\\\\"CustomerId\\\\\"), \\\\n\\\\t\\\\\\nFOREIGN KEY(\\\\\"SupportRepId\\\\\") REFERENCES \\\\\"Employee\\\\\" (\\\\\"EmployeeId\\\\\")\\\\n)\\\\n\\\\n/*\\\\n\\\\\\n3 rows from Customer table:\\\\nCustomerId\\\\tFirstName\\\\tLastName\\\\tCompany\\\\tAddress\\\\t\\\\\\nCity\\\\tState\\\\tCountry\\\\tPostalCode\\\\tPhone\\\\tFax\\\\tEmail\\\\tSupportRepId\\\\n1\\\\tLuís\\\\tGonçalves\\\\t\\\\\\nEmbraer - Empresa Brasileira de Aeronáutica S.A.\\\\tAv. Brigadeiro Faria Lima, 2170\\\\t\\\\\\nSão José dos Campos\\\\tSP\\\\tBrazil\\\\t12227-000\\\\t+55 (12) 3923-5555\\\\t+55 (12) 3923-5566\\\\t\\\\\\nluisg@embraer.com.br\\\\t3\\\\n2\\\\tLeonie\\\\tKöhler\\\\tNone\\\\tTheodor-Heuss-Straße 34\\\\tStuttgart\\\\t\\\\\\nNone\\\\tGermany\\\\t70174\\\\t+49 0711 2842222\\\\tNone\\\\tleonekohler@surfeu.de\\\\t5\\\\n3\\\\tFrançois\\\\t\\\\\\nTremblay\\\\tNone\\\\t1498 rue Bélanger\\\\tMontréal\\\\tQC\\\\tCanada\\\\tH2G 1A7\\\\t+1 (514) 721-4711\\\\t\\\\\\nNone\\\\tftremblay@gmail.com\\\\t3\\\\n*/\"  \\n```  \\n</CodeOutputBlock>  \\nRun the snippet above a few times, or log exceptions in your deployed environment, to collect lots of examples of inputs, table_info and sql_cmd generated by your language model. The sql_cmd values will be incorrect and you can manually fix them up to build a collection of examples, e.g. here we are using YAML to keep a neat record of our inputs and corrected SQL output that we can build up over time.  \\n```python\\nYAML_EXAMPLES = \"\"\"\\n- input: How many customers are not from Brazil?\\ntable_info: |\\nCREATE TABLE \"Customer\" (\\n\"CustomerId\" INTEGER NOT NULL,\\n\"FirstName\" NVARCHAR(40) NOT NULL,\\n\"LastName\" NVARCHAR(20) NOT NULL,\\n\"Company\" NVARCHAR(80),\\n\"Address\" NVARCHAR(70),\\n\"City\" NVARCHAR(40),\\n\"State\" NVARCHAR(40),\\n\"Country\" NVARCHAR(40),\\n\"PostalCode\" NVARCHAR(10),\\n\"Phone\" NVARCHAR(24),\\n\"Fax\" NVARCHAR(24),\\n\"Email\" NVARCHAR(60) NOT NULL,\\n\"SupportRepId\" INTEGER,\\nPRIMARY KEY (\"CustomerId\"),\\nFOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\")\\n)\\nsql_cmd: SELECT COUNT(*) FROM \"Customer\" WHERE NOT \"Country\" = \"Brazil\";\\nsql_result: \"[(54,)]\"\\nanswer: 54 customers are not from Brazil.\\n- input: list all the genres that start with \\'r\\'\\ntable_info: |\\nCREATE TABLE \"Genre\" (\\n\"GenreId\" INTEGER NOT NULL,\\n\"Name\" NVARCHAR(120),\\nPRIMARY KEY (\"GenreId\")\\n)  \\n/*\\n3 rows from Genre table:\\nGenreId\\tName\\n1\\tRock\\n2\\tJazz\\n3\\tMetal\\n*/\\nsql_cmd: SELECT \"Name\" FROM \"Genre\" WHERE \"Name\" LIKE \\'r%\\';\\nsql_result: \"[(\\'Rock\\',), (\\'Rock and Roll\\',), (\\'Reggae\\',), (\\'R&B/Soul\\',)]\"\\nanswer: The genres that start with \\'r\\' are Rock, Rock and Roll, Reggae and R&B/Soul.\\n\"\"\"\\n```  \\nNow that you have some examples (with manually corrected output SQL), you can do few shot prompt seeding the usual way:  \\n```python\\nfrom langchain import FewShotPromptTemplate, PromptTemplate\\nfrom langchain.chains.sql_database.prompt import _sqlite_prompt, PROMPT_SUFFIX\\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\\nfrom langchain.prompts.example_selector.semantic_similarity import SemanticSimilarityExampleSelector\\nfrom langchain.vectorstores import Chroma  \\nexample_prompt = PromptTemplate(\\ninput_variables=[\"table_info\", \"input\", \"sql_cmd\", \"sql_result\", \"answer\"],\\ntemplate=\"{table_info}\\\\n\\\\nQuestion: {input}\\\\nSQLQuery: {sql_cmd}\\\\nSQLResult: {sql_result}\\\\nAnswer: {answer}\",\\n)  \\nexamples_dict = yaml.safe_load(YAML_EXAMPLES)  \\nlocal_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  \\nexample_selector = SemanticSimilarityExampleSelector.from_examples(', metadata={'Header 1': 'print for now, in reality you may want to write this out to a YAML file or database for manual fix-ups offline'}),\n",
       " Document(page_content='examples_dict,', metadata={'Header 1': 'This is the list of examples available to select from.'}),\n",
       " Document(page_content='local_embeddings,', metadata={'Header 1': 'This is the embedding class used to produce embeddings which are used to measure semantic similarity.'}),\n",
       " Document(page_content='Chroma,  # type: ignore', metadata={'Header 1': 'This is the VectorStore class that is used to store the embeddings and do a similarity search over.'}),\n",
       " Document(page_content='k=min(3, len(examples_dict)),\\n)  \\nfew_shot_prompt = FewShotPromptTemplate(\\nexample_selector=example_selector,\\nexample_prompt=example_prompt,\\nprefix=_sqlite_prompt + \"Here are some examples:\",\\nsuffix=PROMPT_SUFFIX,\\ninput_variables=[\"table_info\", \"input\", \"top_k\"],\\n)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nUsing embedded DuckDB without persistence: data will be transient\\n```  \\n</CodeOutputBlock>  \\nThe model should do better now with this few shot prompt, especially for inputs similar to the examples you have seeded it with.  \\n```python\\nlocal_chain = SQLDatabaseChain.from_llm(local_llm, db, prompt=few_shot_prompt, use_query_checker=True, verbose=True, return_intermediate_steps=True)\\n```  \\n```python\\nresult = local_chain(\"How many customers are from Brazil?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nHow many customers are from Brazil?\\nSQLQuery:SELECT count(*) FROM Customer WHERE Country = \"Brazil\";\\nSQLResult: [(5,)]\\nAnswer:[5]\\n> Finished chain.\\n```  \\n</CodeOutputBlock>  \\n```python\\nresult = local_chain(\"How many customers are not from Brazil?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nHow many customers are not from Brazil?\\nSQLQuery:SELECT count(*) FROM customer WHERE country NOT IN (SELECT country FROM customer WHERE country = \\'Brazil\\')\\nSQLResult: [(54,)]\\nAnswer:54 customers are not from Brazil.\\n> Finished chain.\\n```  \\n</CodeOutputBlock>  \\n```python\\nresult = local_chain(\"How many customers are there in total?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SQLDatabaseChain chain...\\nHow many customers are there in total?\\nSQLQuery:SELECT count(*) FROM Customer;\\nSQLResult: [(59,)]\\nAnswer:There are 59 customers in total.\\n> Finished chain.\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'This is the number of examples to produce and include per prompt'}),\n",
       " Document(page_content='Setting `verbose` to `True` will print out some internal states of the `Chain` object while it is being ran.  \\n```python\\nconversation = ConversationChain(\\nllm=chat,\\nmemory=ConversationBufferMemory(),\\nverbose=True\\n)\\nconversation.run(\"What is ChatGPT?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n> Entering new ConversationChain chain...\\nPrompt after formatting:\\nThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.  \\nCurrent conversation:  \\nHuman: What is ChatGPT?\\nAI:  \\n> Finished chain.  \\n\\'ChatGPT is an AI language model developed by OpenAI. It is based on the GPT-3 architecture and is capable of generating human-like responses to text prompts. ChatGPT has been trained on a massive amount of text data and can understand and respond to a wide range of topics. It is often used for chatbots, virtual assistants, and other conversational AI applications.\\'\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='First we prepare the data. For this example we create multiple documents from one long one, but these documents could be fetched in any manner (the point of this notebook to highlight what to do AFTER you fetch the documents).  \\n```python\\nfrom langchain import OpenAI, PromptTemplate, LLMChain\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.chains.mapreduce import MapReduceChain\\nfrom langchain.prompts import PromptTemplate  \\nllm = OpenAI(temperature=0)  \\ntext_splitter = CharacterTextSplitter()\\n```  \\n```python\\nwith open(\"../../state_of_the_union.txt\") as f:\\nstate_of_the_union = f.read()\\ntexts = text_splitter.split_text(state_of_the_union)\\n```  \\n```python\\nfrom langchain.docstore.document import Document  \\ndocs = [Document(page_content=t) for t in texts[:3]]\\n```', metadata={'Header 2': 'Prepare Data'}),\n",
       " Document(page_content='If you just want to get started as quickly as possible, this is the recommended way to do it:  \\n```python\\nfrom langchain.chains.summarize import load_summarize_chain\\n```  \\n```python\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\nchain.run(docs)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' In response to Russian aggression in Ukraine, the United States and its allies are taking action to hold Putin accountable, including economic sanctions, asset seizures, and military assistance. The US is also providing economic and humanitarian aid to Ukraine, and has passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and create jobs. The US remains unified and determined to protect Ukraine and the free world.\\'\\n```  \\n</CodeOutputBlock>  \\nIf you want more control and understanding over what is happening, please see the information below.', metadata={'Header 2': 'Quickstart'}),\n",
       " Document(page_content='This sections shows results of using the `stuff` Chain to do summarization.  \\n```python\\nchain = load_summarize_chain(llm, chain_type=\"stuff\")\\n```  \\n```python\\nchain.run(docs)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' In his speech, President Biden addressed the crisis in Ukraine, the American Rescue Plan, and the Bipartisan Infrastructure Law. He discussed the need to invest in America, educate Americans, and build the economy from the bottom up. He also announced the release of 60 million barrels of oil from reserves around the world, and the creation of a dedicated task force to go after the crimes of Russian oligarchs. He concluded by emphasizing the need to Buy American and use taxpayer dollars to rebuild America.\\'\\n```  \\n</CodeOutputBlock>  \\n**Custom Prompts**  \\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.  \\n```python\\nprompt_template = \"\"\"Write a concise summary of the following:  \\n{text}  \\nCONCISE SUMMARY IN ITALIAN:\"\"\"\\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\\nchain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\\nchain.run(docs)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\"\\\\n\\\\nIn questa serata, il Presidente degli Stati Uniti ha annunciato una serie di misure per affrontare la crisi in Ucraina, causata dall\\'aggressione di Putin. Ha anche annunciato l\\'invio di aiuti economici, militari e umanitari all\\'Ucraina. Ha anche annunciato che gli Stati Uniti e i loro alleati stanno imponendo sanzioni economiche a Putin e stanno rilasciando 60 milioni di barili di petrolio dalle riserve di tutto il mondo. Inoltre, ha annunciato che il Dipartimento di Giustizia degli Stati Uniti sta creando una task force dedicata ai crimini degli oligarchi russi. Il Presidente ha anche annunciato l\\'approvazione della legge bipartitica sull\\'infrastruttura, che prevede investimenti per la ricostruzione dell\\'America. Questo porterà a creare posti\"\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'The `stuff` Chain'}),\n",
       " Document(page_content='This sections shows results of using the `map_reduce` Chain to do summarization.  \\n```python\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\n```  \\n```python\\nchain.run(docs)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" In response to Russia\\'s aggression in Ukraine, the United States and its allies have imposed economic sanctions and are taking other measures to hold Putin accountable. The US is also providing economic and military assistance to Ukraine, protecting NATO countries, and releasing oil from its Strategic Petroleum Reserve. President Biden and Vice President Harris have passed legislation to help struggling families and rebuild America\\'s infrastructure.\"\\n```  \\n</CodeOutputBlock>  \\n**Intermediate Steps**  \\nWe can also return the intermediate steps for `map_reduce` chains, should we want to inspect them. This is done with the `return_map_steps` variable.  \\n```python\\nchain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True)\\n```  \\n```python\\nchain({\"input_documents\": docs}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'map_steps\\': [\" In response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains.\",\\n\\' The United States and its European allies are taking action to punish Russia for its invasion of Ukraine, including seizing assets, closing off airspace, and providing economic and military assistance to Ukraine. The US is also mobilizing forces to protect NATO countries and has released 30 million barrels of oil from its Strategic Petroleum Reserve to help blunt gas prices. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian-American citizens.\\',\\n\" President Biden and Vice President Harris ran for office with a new economic vision for America, and have since passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and rebuild America\\'s infrastructure. This includes creating jobs, modernizing roads, airports, ports, and waterways, replacing lead pipes, providing affordable high-speed internet, and investing in American products to support American jobs.\"],\\n\\'output_text\\': \" In response to Russia\\'s aggression in Ukraine, the United States and its allies have imposed economic sanctions and are taking other measures to hold Putin accountable. The US is also providing economic and military assistance to Ukraine, protecting NATO countries, and passing legislation to help struggling families and rebuild America\\'s infrastructure. The world is uniting in support of Ukraine and democracy, and the US stands with its Ukrainian-American citizens.\"}\\n```  \\n</CodeOutputBlock>  \\n**Custom Prompts**  \\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.  \\n```python\\nprompt_template = \"\"\"Write a concise summary of the following:  \\n{text}  \\nCONCISE SUMMARY IN ITALIAN:\"\"\"\\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\\nchain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=PROMPT, combine_prompt=PROMPT)\\nchain({\"input_documents\": docs}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'intermediate_steps\\': [\"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Gli Stati Uniti e i loro alleati stanno ora imponendo sanzioni economiche a Putin e stanno tagliando l\\'accesso della Russia alla tecnologia. Il Dipartimento di Giustizia degli Stati Uniti sta anche creando una task force dedicata per andare dopo i crimini degli oligarchi russi.\",\\n\"\\\\n\\\\nStiamo unendo le nostre forze con quelle dei nostri alleati europei per sequestrare yacht, appartamenti di lusso e jet privati di Putin. Abbiamo chiuso lo spazio aereo americano ai voli russi e stiamo fornendo più di un miliardo di dollari in assistenza all\\'Ucraina. Abbiamo anche mobilitato le nostre forze terrestri, aeree e navali per proteggere i paesi della NATO. Abbiamo anche rilasciato 60 milioni di barili di petrolio dalle riserve di tutto il mondo, di cui 30 milioni dalla nostra riserva strategica di petrolio. Stiamo affrontando una prova reale e ci vorrà del tempo, ma alla fine Putin non riuscirà a spegnere l\\'amore dei popoli per la libertà.\",\\n\"\\\\n\\\\nIl Presidente Biden ha lottato per passare l\\'American Rescue Plan per aiutare le persone che soffrivano a causa della pandemia. Il piano ha fornito sollievo economico immediato a milioni di americani, ha aiutato a mettere cibo sulla loro tavola, a mantenere un tetto sopra le loro teste e a ridurre il costo dell\\'assicurazione sanitaria. Il piano ha anche creato più di 6,5 milioni di nuovi posti di lavoro, il più alto numero di posti di lavoro creati in un anno nella storia degli Stati Uniti. Il Presidente Biden ha anche firmato la legge bipartitica sull\\'infrastruttura, la più ampia iniziativa di ricostruzione della storia degli Stati Uniti. Il piano prevede di modernizzare le strade, gli aeroporti, i porti e le vie navigabili in\"],\\n\\'output_text\\': \"\\\\n\\\\nIl Presidente Biden sta lavorando per aiutare le persone che soffrono a causa della pandemia attraverso l\\'American Rescue Plan e la legge bipartitica sull\\'infrastruttura. Gli Stati Uniti e i loro alleati stanno anche imponendo sanzioni economiche a Putin e tagliando l\\'accesso della Russia alla tecnologia. Stanno anche sequestrando yacht, appartamenti di lusso e jet privati di Putin e fornendo più di un miliardo di dollari in assistenza all\\'Ucraina. Alla fine, Putin non riuscirà a spegnere l\\'amore dei popoli per la libertà.\"}\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'The `map_reduce` Chain'}),\n",
       " Document(page_content='**Multi input prompt**  \\nYou can also use prompt with multi input. In this example, we will use a MapReduce chain to answer specific question about our code.  \\n```python\\nfrom langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain  \\nmap_template_string = \"\"\"Give the following python code information, generate a description that explains what the code does and also mention the time complexity.\\nCode:\\n{code}  \\nReturn the the description in the following format:\\nname of the function: description of the function\\n\"\"\"  \\nreduce_template_string = \"\"\"Given the following python function names and descriptions, answer the following question\\n{code_description}\\nQuestion: {question}\\nAnswer:\\n\"\"\"  \\nMAP_PROMPT = PromptTemplate(input_variables=[\"code\"], template=map_template_string)\\nREDUCE_PROMPT = PromptTemplate(input_variables=[\"code_description\", \"question\"], template=reduce_template_string)  \\nllm = OpenAI()  \\nmap_llm_chain = LLMChain(llm=llm, prompt=MAP_PROMPT)\\nreduce_llm_chain = LLMChain(llm=llm, prompt=REDUCE_PROMPT)  \\ngenerative_result_reduce_chain = StuffDocumentsChain(\\nllm_chain=reduce_llm_chain,\\ndocument_variable_name=\"code_description\",\\n)  \\ncombine_documents = MapReduceDocumentsChain(\\nllm_chain=map_llm_chain,\\ncombine_document_chain=generative_result_reduce_chain,\\ndocument_variable_name=\"code\",\\n)  \\nmap_reduce = MapReduceChain(\\ncombine_documents_chain=combine_documents,\\ntext_splitter=CharacterTextSplitter(separator=\"\\\\n##\\\\n\", chunk_size=100, chunk_overlap=0),\\n)\\n```  \\n```python\\ncode = \"\"\"\\ndef bubblesort(list):\\nfor iter_num in range(len(list)-1,0,-1):\\nfor idx in range(iter_num):\\nif list[idx]>list[idx+1]:\\ntemp = list[idx]\\nlist[idx] = list[idx+1]\\nlist[idx+1] = temp\\nreturn list', metadata={'Header 2': 'The custom `MapReduceChain`'}),\n",
       " Document(page_content='def insertion_sort(InputList):\\nfor i in range(1, len(InputList)):\\nj = i-1\\nnxt_element = InputList[i]\\nwhile (InputList[j] > nxt_element) and (j >= 0):\\nInputList[j+1] = InputList[j]\\nj=j-1\\nInputList[j+1] = nxt_element\\nreturn InputList  \\ndef shellSort(input_list):\\ngap = len(input_list) // 2\\nwhile gap > 0:\\nfor i in range(gap, len(input_list)):\\ntemp = input_list[i]\\nj = i\\nwhile j >= gap and input_list[j - gap] > temp:\\ninput_list[j] = input_list[j - gap]\\nj = j-gap\\ninput_list[j] = temp\\ngap = gap//2\\nreturn input_list  \\n\"\"\"\\n```  \\n```python\\nmap_reduce.run(input_text=code, question=\"Which function has a better time complexity?\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nCreated a chunk of size 247, which is longer than the specified 100\\nCreated a chunk of size 267, which is longer than the specified 100  \\n\\'shellSort has a better time complexity than both bubblesort and insertion_sort, as it has a time complexity of O(n^2), while the other two have a time complexity of O(n^2).\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': ''}),\n",
       " Document(page_content='This sections shows results of using the `refine` Chain to do summarization.  \\n```python\\nchain = load_summarize_chain(llm, chain_type=\"refine\")  \\nchain.run(docs)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\"\\\\n\\\\nIn response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains. We are joining with our European allies to find and seize the assets of Russian oligarchs, including yachts, luxury apartments, and private jets. The U.S. is also closing off American airspace to all Russian flights, further isolating Russia and adding an additional squeeze on their economy. The U.S. and its allies are providing support to the Ukrainians in their fight for freedom, including military, economic, and humanitarian assistance. The U.S. is also mobilizing ground forces, air squadrons, and ship deployments to protect NATO countries. The U.S. and its allies are also releasing 60 million barrels of oil from reserves around the world, with the U.S. contributing 30 million barrels from its own Strategic Petroleum Reserve. In addition, the U.S. has passed the American Rescue Plan to provide immediate economic relief for tens of millions of Americans, and the Bipartisan Infrastructure Law to rebuild America and create jobs. This investment will\"\\n```  \\n</CodeOutputBlock>  \\n**Intermediate Steps**  \\nWe can also return the intermediate steps for `refine` chains, should we want to inspect them. This is done with the `return_refine_steps` variable.  \\n```python\\nchain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"refine\", return_intermediate_steps=True)  \\nchain({\"input_documents\": docs}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'refine_steps\\': [\" In response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains.\",\\n\"\\\\n\\\\nIn response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains. We are joining with our European allies to find and seize the assets of Russian oligarchs, including yachts, luxury apartments, and private jets. The U.S. is also closing off American airspace to all Russian flights, further isolating Russia and adding an additional squeeze on their economy. The U.S. and its allies are providing support to the Ukrainians in their fight for freedom, including military, economic, and humanitarian assistance. The U.S. is also mobilizing ground forces, air squadrons, and ship deployments to protect NATO countries. The U.S. and its allies are also releasing 60 million barrels of oil from reserves around the world, with the U.S. contributing 30 million barrels from its own Strategic Petroleum Reserve. Putin\\'s war on Ukraine has left Russia weaker and the rest of the world stronger, with the world uniting in support of democracy and peace.\",\\n\"\\\\n\\\\nIn response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains. We are joining with our European allies to find and seize the assets of Russian oligarchs, including yachts, luxury apartments, and private jets. The U.S. is also closing off American airspace to all Russian flights, further isolating Russia and adding an additional squeeze on their economy. The U.S. and its allies are providing support to the Ukrainians in their fight for freedom, including military, economic, and humanitarian assistance. The U.S. is also mobilizing ground forces, air squadrons, and ship deployments to protect NATO countries. The U.S. and its allies are also releasing 60 million barrels of oil from reserves around the world, with the U.S. contributing 30 million barrels from its own Strategic Petroleum Reserve. In addition, the U.S. has passed the American Rescue Plan to provide immediate economic relief for tens of millions of Americans, and the Bipartisan Infrastructure Law to rebuild America and create jobs. This includes investing\"],\\n\\'output_text\\': \"\\\\n\\\\nIn response to Russia\\'s aggression in Ukraine, the United States has united with other freedom-loving nations to impose economic sanctions and hold Putin accountable. The U.S. Department of Justice is also assembling a task force to go after the crimes of Russian oligarchs and seize their ill-gotten gains. We are joining with our European allies to find and seize the assets of Russian oligarchs, including yachts, luxury apartments, and private jets. The U.S. is also closing off American airspace to all Russian flights, further isolating Russia and adding an additional squeeze on their economy. The U.S. and its allies are providing support to the Ukrainians in their fight for freedom, including military, economic, and humanitarian assistance. The U.S. is also mobilizing ground forces, air squadrons, and ship deployments to protect NATO countries. The U.S. and its allies are also releasing 60 million barrels of oil from reserves around the world, with the U.S. contributing 30 million barrels from its own Strategic Petroleum Reserve. In addition, the U.S. has passed the American Rescue Plan to provide immediate economic relief for tens of millions of Americans, and the Bipartisan Infrastructure Law to rebuild America and create jobs. This includes investing\"}\\n```  \\n</CodeOutputBlock>  \\n**Custom Prompts**  \\nYou can also use your own prompts with this chain. In this example, we will respond in Italian.  \\n```python\\nprompt_template = \"\"\"Write a concise summary of the following:  \\n{text}  \\nCONCISE SUMMARY IN ITALIAN:\"\"\"\\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\\nrefine_template = (\\n\"Your job is to produce a final summary\\\\n\"\\n\"We have provided an existing summary up to a certain point: {existing_answer}\\\\n\"\\n\"We have the opportunity to refine the existing summary\"\\n\"(only if needed) with some more context below.\\\\n\"\\n\"------------\\\\n\"\\n\"{text}\\\\n\"\\n\"------------\\\\n\"\\n\"Given the new context, refine the original summary in Italian\"\\n\"If the context isn\\'t useful, return the original summary.\"\\n)\\nrefine_prompt = PromptTemplate(\\ninput_variables=[\"existing_answer\", \"text\"],\\ntemplate=refine_template,\\n)\\nchain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"refine\", return_intermediate_steps=True, question_prompt=PROMPT, refine_prompt=refine_prompt)\\nchain({\"input_documents\": docs}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'intermediate_steps\\': [\"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Insieme ai nostri alleati, stiamo imponendo sanzioni economiche, tagliando l\\'accesso della Russia alla tecnologia e bloccando i suoi più grandi istituti bancari dal sistema finanziario internazionale. Il Dipartimento di Giustizia degli Stati Uniti sta anche assemblando una task force dedicata per andare dopo i crimini degli oligarchi russi.\",\\n\"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Insieme ai nostri alleati, stiamo imponendo sanzioni economiche, tagliando l\\'accesso della Russia alla tecnologia, bloccando i suoi più grandi istituti bancari dal sistema finanziario internazionale e chiudendo lo spazio aereo americano a tutti i voli russi. Il Dipartimento di Giustizia degli Stati Uniti sta anche assemblando una task force dedicata per andare dopo i crimini degli oligarchi russi. Stiamo fornendo più di un miliardo di dollari in assistenza diretta all\\'Ucraina e fornendo assistenza militare,\",\\n\"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Insieme ai nostri alleati, stiamo imponendo sanzioni economiche, tagliando l\\'accesso della Russia alla tecnologia, bloccando i suoi più grandi istituti bancari dal sistema finanziario internazionale e chiudendo lo spazio aereo americano a tutti i voli russi. Il Dipartimento di Giustizia degli Stati Uniti sta anche assemblando una task force dedicata per andare dopo i crimini degli oligarchi russi. Stiamo fornendo più di un miliardo di dollari in assistenza diretta all\\'Ucraina e fornendo assistenza militare.\"],\\n\\'output_text\\': \"\\\\n\\\\nQuesta sera, ci incontriamo come democratici, repubblicani e indipendenti, ma soprattutto come americani. La Russia di Putin ha cercato di scuotere le fondamenta del mondo libero, ma ha sottovalutato la forza della gente ucraina. Insieme ai nostri alleati, stiamo imponendo sanzioni economiche, tagliando l\\'accesso della Russia alla tecnologia, bloccando i suoi più grandi istituti bancari dal sistema finanziario internazionale e chiudendo lo spazio aereo americano a tutti i voli russi. Il Dipartimento di Giustizia degli Stati Uniti sta anche assemblando una task force dedicata per andare dopo i crimini degli oligarchi russi. Stiamo fornendo più di un miliardo di dollari in assistenza diretta all\\'Ucraina e fornendo assistenza militare.\"}\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'The `refine` Chain'}),\n",
       " Document(page_content='Additionally, we can return the source documents used to answer the question by specifying an optional parameter when constructing the chain.  \\n```python\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), return_source_documents=True)\\n```  \\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"query\": query})\\n```  \\n```python\\nresult[\"result\"]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice and a former federal public defender from a family of public school educators and police officers, and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\n```  \\n</CodeOutputBlock>  \\n```python\\nresult[\"source_documents\"]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'}, lookup_index=0),\\nDocument(page_content=\\'A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\\\n\\\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\\\n\\\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\\\n\\\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\\\n\\\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\\\n\\\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'}, lookup_index=0),\\nDocument(page_content=\\'And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\\\n\\\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\\\n\\\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\\\n\\\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\\\n\\\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\\\n\\\\nFirst, beat the opioid epidemic.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'}, lookup_index=0),\\nDocument(page_content=\\'Tonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\\\n\\\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\\\n\\\\nThat ends on my watch. \\\\n\\\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\\\n\\\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\\\n\\\\nLet’s pass the Paycheck Fairness Act and paid leave.  \\\\n\\\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\\\n\\\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'}, lookup_index=0)]\\n```  \\n</CodeOutputBlock>  \\nAlternatively, if our document have a \"source\" metadata key, we can use the `RetrievalQAWithSourceChain` to cite our sources:  \\n```python\\ndocsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": f\"{i}-pl\"} for i in range(len(texts))])\\n```  \\n```python\\nfrom langchain.chains import RetrievalQAWithSourcesChain\\nfrom langchain import OpenAI  \\nchain = RetrievalQAWithSourcesChain.from_chain_type(OpenAI(temperature=0), chain_type=\"stuff\", retriever=docsearch.as_retriever())\\n```  \\n```python\\nchain({\"question\": \"What did the president say about Justice Breyer\"}, return_only_outputs=True)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'answer\\': \\' The president honored Justice Breyer for his service and mentioned his legacy of excellence.\\\\n\\',\\n\\'sources\\': \\'31-pl\\'}\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Return Source Documents'}),\n",
       " Document(page_content='```python\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import ConversationalRetrievalChain\\n```  \\nLoad in documents. You can replace this with a loader for whatever type of data you want  \\n```python\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\"../../state_of_the_union.txt\")\\ndocuments = loader.load()\\n```  \\nIf you had multiple loaders that you wanted to combine, you do something like:  \\n```python', metadata={}),\n",
       " Document(page_content='```  \\nWe now split the documents, create embeddings for them, and put them in a vectorstore. This allows us to do semantic search over them.  \\n```python\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ndocuments = text_splitter.split_documents(documents)  \\nembeddings = OpenAIEmbeddings()\\nvectorstore = Chroma.from_documents(documents, embeddings)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nUsing embedded DuckDB without persistence: data will be transient\\n```  \\n</CodeOutputBlock>  \\nWe can now create a memory object, which is neccessary to track the inputs/outputs and hold a conversation.  \\n```python\\nfrom langchain.memory import ConversationBufferMemory\\nmemory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\\n```  \\nWe now initialize the `ConversationalRetrievalChain`  \\n```python\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), memory=memory)\\n```  \\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query})\\n```  \\n```python\\nresult[\"answer\"]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\n```  \\n</CodeOutputBlock>  \\n```python\\nquery = \"Did he mention who she suceeded\"\\nresult = qa({\"question\": query})\\n```  \\n```python\\nresult[\\'answer\\']\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' Ketanji Brown Jackson succeeded Justice Stephen Breyer on the United States Supreme Court.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'docs.extend(loader.load())'}),\n",
       " Document(page_content='In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object.  \\n```python\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever())\\n```  \\nHere\\'s an example of asking a question with no chat history  \\n```python\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n```  \\n```python\\nresult[\"answer\"]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\n```  \\n</CodeOutputBlock>  \\nHere\\'s an example of asking a question with some chat history  \\n```python\\nchat_history = [(query, result[\"answer\"])]\\nquery = \"Did he mention who she suceeded\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n```  \\n```python\\nresult[\\'answer\\']\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\' Ketanji Brown Jackson succeeded Justice Stephen Breyer on the United States Supreme Court.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'docs.extend(loader.load())', 'Header 2': 'Pass in chat history'}),\n",
       " Document(page_content='This chain has two steps. First, it condenses the current question and the chat history into a standalone question. This is neccessary to create a standanlone vector to use for retrieval. After that, it does retrieval and then answers the question using retrieval augmented generation with a separate model. Part of the power of the declarative nature of LangChain is that you can easily use a separate language model for each call. This can be useful to use a cheaper and faster model for the simpler task of condensing the question, and then a more expensive model for answering the question. Here is an example of doing so.  \\n```python\\nfrom langchain.chat_models import ChatOpenAI\\n```  \\n```python\\nqa = ConversationalRetrievalChain.from_llm(\\nChatOpenAI(temperature=0, model=\"gpt-4\"),\\nvectorstore.as_retriever(),\\ncondense_question_llm = ChatOpenAI(temperature=0, model=\\'gpt-3.5-turbo\\'),\\n)\\n```  \\n```python\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n```  \\n```python\\nchat_history = [(query, result[\"answer\"])]\\nquery = \"Did he mention who she suceeded\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n```', metadata={'Header 1': 'docs.extend(loader.load())', 'Header 2': 'Using a different model for condensing the question'}),\n",
       " Document(page_content='You can also easily return source documents from the ConversationalRetrievalChain. This is useful for when you want to inspect what documents were returned.  \\n```python\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)\\n```  \\n```python\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n```  \\n```python\\nresult[\\'source_documents\\'][0]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nDocument(page_content=\\'Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\\\n\\\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\\\n\\\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\\\n\\\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\\', metadata={\\'source\\': \\'../../state_of_the_union.txt\\'})\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'docs.extend(loader.load())', 'Header 2': 'Return Source Documents'}),\n",
       " Document(page_content='If you are using a vector store that supports filtering by search distance, you can add a threshold value parameter.  \\n```python\\nvectordbkwargs = {\"search_distance\": 0.9}\\n```  \\n```python\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), return_source_documents=True)\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history, \"vectordbkwargs\": vectordbkwargs})\\n```', metadata={'Header 1': 'docs.extend(loader.load())', 'Header 2': 'ConversationalRetrievalChain with `search_distance`'}),\n",
       " Document(page_content='We can also use different types of combine document chains with the ConversationalRetrievalChain chain.  \\n```python\\nfrom langchain.chains import LLMChain\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\\n```  \\n```python\\nllm = OpenAI(temperature=0)\\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\\ndoc_chain = load_qa_chain(llm, chain_type=\"map_reduce\")  \\nchain = ConversationalRetrievalChain(\\nretriever=vectorstore.as_retriever(),\\nquestion_generator=question_generator,\\ncombine_docs_chain=doc_chain,\\n)\\n```  \\n```python\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = chain({\"question\": query, \"chat_history\": chat_history})\\n```  \\n```python\\nresult[\\'answer\\']\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'docs.extend(loader.load())', 'Header 2': 'ConversationalRetrievalChain with `map_reduce`'}),\n",
       " Document(page_content='You can also use this chain with the question answering with sources chain.  \\n```python\\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\\n```  \\n```python\\nllm = OpenAI(temperature=0)\\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\\ndoc_chain = load_qa_with_sources_chain(llm, chain_type=\"map_reduce\")  \\nchain = ConversationalRetrievalChain(\\nretriever=vectorstore.as_retriever(),\\nquestion_generator=question_generator,\\ncombine_docs_chain=doc_chain,\\n)\\n```  \\n```python\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = chain({\"question\": query, \"chat_history\": chat_history})\\n```  \\n```python\\nresult[\\'answer\\']\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, from a family of public school educators and police officers, a consensus builder, and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\\\nSOURCES: ../../state_of_the_union.txt\"\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'docs.extend(loader.load())', 'Header 2': 'ConversationalRetrievalChain with Question Answering with sources'}),\n",
       " Document(page_content='Output from the chain will be streamed to `stdout` token by token in this example.  \\n```python\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\nfrom langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\\nfrom langchain.chains.question_answering import load_qa_chain', metadata={'Header 1': 'docs.extend(loader.load())', 'Header 2': 'ConversationalRetrievalChain with streaming to `stdout`'}),\n",
       " Document(page_content='llm = OpenAI(temperature=0)\\nstreaming_llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)  \\nquestion_generator = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\\ndoc_chain = load_qa_chain(streaming_llm, chain_type=\"stuff\", prompt=QA_PROMPT)  \\nqa = ConversationalRetrievalChain(\\nretriever=vectorstore.as_retriever(), combine_docs_chain=doc_chain, question_generator=question_generator)\\n```  \\n```python\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nThe president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\n```  \\n</CodeOutputBlock>  \\n```python\\nchat_history = [(query, result[\"answer\"])]\\nquery = \"Did he mention who she suceeded\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nKetanji Brown Jackson succeeded Justice Stephen Breyer on the United States Supreme Court.\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'and a separate, non-streaming llm for question generation'}),\n",
       " Document(page_content='You can also specify a `get_chat_history` function, which can be used to format the chat_history string.  \\n```python\\ndef get_chat_history(inputs) -> str:\\nres = []\\nfor human, ai in inputs:\\nres.append(f\"Human:{human}\\\\nAI:{ai}\")\\nreturn \"\\\\n\".join(res)\\nqa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), vectorstore.as_retriever(), get_chat_history=get_chat_history)\\n```  \\n```python\\nchat_history = []\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nresult = qa({\"question\": query, \"chat_history\": chat_history})\\n```  \\n```python\\nresult[\\'answer\\']\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'and a separate, non-streaming llm for question generation', 'Header 2': 'get_chat_history Function'}),\n",
       " Document(page_content='```python\\nfrom langchain import PromptTemplate, OpenAI, LLMChain  \\nprompt_template = \"What is a good name for a company that makes {product}?\"  \\nllm = OpenAI(temperature=0)\\nllm_chain = LLMChain(\\nllm=llm,\\nprompt=PromptTemplate.from_template(prompt_template)\\n)\\nllm_chain(\"colorful socks\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'product\\': \\'colorful socks\\', \\'text\\': \\'\\\\n\\\\nSocktastic!\\'}\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='Aside from `__call__` and `run` methods shared by all `Chain` object, `LLMChain` offers a few more ways of calling the chain logic:  \\n- `apply` allows you run the chain against a list of inputs:  \\n```python\\ninput_list = [\\n{\"product\": \"socks\"},\\n{\"product\": \"computer\"},\\n{\"product\": \"shoes\"}\\n]  \\nllm_chain.apply(input_list)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[{\\'text\\': \\'\\\\n\\\\nSocktastic!\\'},\\n{\\'text\\': \\'\\\\n\\\\nTechCore Solutions.\\'},\\n{\\'text\\': \\'\\\\n\\\\nFootwear Factory.\\'}]\\n```  \\n</CodeOutputBlock>  \\n- `generate` is similar to `apply`, except it return an `LLMResult` instead of string. `LLMResult` often contains useful generation such as token usages and finish reason.  \\n```python\\nllm_chain.generate(input_list)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nLLMResult(generations=[[Generation(text=\\'\\\\n\\\\nSocktastic!\\', generation_info={\\'finish_reason\\': \\'stop\\', \\'logprobs\\': None})], [Generation(text=\\'\\\\n\\\\nTechCore Solutions.\\', generation_info={\\'finish_reason\\': \\'stop\\', \\'logprobs\\': None})], [Generation(text=\\'\\\\n\\\\nFootwear Factory.\\', generation_info={\\'finish_reason\\': \\'stop\\', \\'logprobs\\': None})]], llm_output={\\'token_usage\\': {\\'prompt_tokens\\': 36, \\'total_tokens\\': 55, \\'completion_tokens\\': 19}, \\'model_name\\': \\'text-davinci-003\\'})\\n```  \\n</CodeOutputBlock>  \\n- `predict` is similar to `run` method except that the input keys are specified as keyword arguments instead of a Python dict.  \\n```python', metadata={'Header 2': 'Additional ways of running LLM Chain'}),\n",
       " Document(page_content='llm_chain.predict(product=\"colorful socks\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'\\\\n\\\\nSocktastic!\\'\\n```  \\n</CodeOutputBlock>  \\n```python', metadata={'Header 1': 'Single input example'}),\n",
       " Document(page_content='template = \"\"\"Tell me a {adjective} joke about {subject}.\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\"])\\nllm_chain = LLMChain(prompt=prompt, llm=OpenAI(temperature=0))  \\nllm_chain.predict(adjective=\"sad\", subject=\"ducks\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'\\\\n\\\\nQ: What did the duck say when his friend died?\\\\nA: Quack, quack, goodbye.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Multiple inputs example'}),\n",
       " Document(page_content='By default, `LLMChain` does not parse the output even if the underlying `prompt` object has an output parser. If you would like to apply that output parser on the LLM output, use `predict_and_parse` instead of `predict` and `apply_and_parse` instead of `apply`.  \\nWith `predict`:  \\n```python\\nfrom langchain.output_parsers import CommaSeparatedListOutputParser  \\noutput_parser = CommaSeparatedListOutputParser()\\ntemplate = \"\"\"List all the colors in a rainbow\"\"\"\\nprompt = PromptTemplate(template=template, input_variables=[], output_parser=output_parser)\\nllm_chain = LLMChain(prompt=prompt, llm=llm)  \\nllm_chain.predict()\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'\\\\n\\\\nRed, orange, yellow, green, blue, indigo, violet\\'\\n```  \\n</CodeOutputBlock>  \\nWith `predict_and_parser`:  \\n```python\\nllm_chain.predict_and_parse()\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[\\'Red\\', \\'orange\\', \\'yellow\\', \\'green\\', \\'blue\\', \\'indigo\\', \\'violet\\']\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Multiple inputs example', 'Header 2': 'Parsing the outputs'}),\n",
       " Document(page_content='You can also construct an LLMChain from a string template directly.  \\n```python\\ntemplate = \"\"\"Tell me a {adjective} joke about {subject}.\"\"\"\\nllm_chain = LLMChain.from_string(llm=llm, template=template)\\n```  \\n```python\\nllm_chain.predict(adjective=\"sad\", subject=\"ducks\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n\\'\\\\n\\\\nQ: What did the duck say when his friend died?\\\\nA: Quack, quack, goodbye.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Multiple inputs example', 'Header 2': 'Initialize from string'}),\n",
       " Document(page_content='```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\n```  \\n```python', metadata={}),\n",
       " Document(page_content='llm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.  \\nTitle: {title}\\nPlaywright: This is a synopsis for the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\\nsynopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\\n```  \\n```python', metadata={'Header 1': 'This is an LLMChain to write a synopsis given a title of a play.'}),\n",
       " Document(page_content='llm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.  \\nPlay Synopsis:\\n{synopsis}\\nReview from a New York Times play critic of the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\\nreview_chain = LLMChain(llm=llm, prompt=prompt_template)\\n```  \\n```python', metadata={'Header 1': 'This is an LLMChain to write a review of a play given a synopsis.'}),\n",
       " Document(page_content='from langchain.chains import SimpleSequentialChain\\noverall_chain = SimpleSequentialChain(chains=[synopsis_chain, review_chain], verbose=True)\\n```  \\n```python\\nreview = overall_chain.run(\"Tragedy at sunset on the beach\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SimpleSequentialChain chain...  \\nTragedy at Sunset on the Beach is a story of a young couple, Jack and Sarah, who are in love and looking forward to their future together. On the night of their anniversary, they decide to take a walk on the beach at sunset. As they are walking, they come across a mysterious figure, who tells them that their love will be tested in the near future.  \\nThe figure then tells the couple that the sun will soon set, and with it, a tragedy will strike. If Jack and Sarah can stay together and pass the test, they will be granted everlasting love. However, if they fail, their love will be lost forever.  \\nThe play follows the couple as they struggle to stay together and battle the forces that threaten to tear them apart. Despite the tragedy that awaits them, they remain devoted to one another and fight to keep their love alive. In the end, the couple must decide whether to take a chance on their future together or succumb to the tragedy of the sunset.  \\nTragedy at Sunset on the Beach is an emotionally gripping story of love, hope, and sacrifice. Through the story of Jack and Sarah, the audience is taken on a journey of self-discovery and the power of love to overcome even the greatest of obstacles.  \\nThe play\\'s talented cast brings the characters to life, allowing us to feel the depths of their emotion and the intensity of their struggle. With its compelling story and captivating performances, this play is sure to draw in audiences and leave them on the edge of their seats.  \\nThe play\\'s setting of the beach at sunset adds a touch of poignancy and romanticism to the story, while the mysterious figure serves to keep the audience enthralled. Overall, Tragedy at Sunset on the Beach is an engaging and thought-provoking play that is sure to leave audiences feeling inspired and hopeful.  \\n> Finished chain.\\n```  \\n</CodeOutputBlock>  \\n```python\\nprint(review)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\nTragedy at Sunset on the Beach is an emotionally gripping story of love, hope, and sacrifice. Through the story of Jack and Sarah, the audience is taken on a journey of self-discovery and the power of love to overcome even the greatest of obstacles.  \\nThe play\\'s talented cast brings the characters to life, allowing us to feel the depths of their emotion and the intensity of their struggle. With its compelling story and captivating performances, this play is sure to draw in audiences and leave them on the edge of their seats.  \\nThe play\\'s setting of the beach at sunset adds a touch of poignancy and romanticism to the story, while the mysterious figure serves to keep the audience enthralled. Overall, Tragedy at Sunset on the Beach is an engaging and thought-provoking play that is sure to leave audiences feeling inspired and hopeful.\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'This is the overall chain where we run these two chains in sequence.'}),\n",
       " Document(page_content=\"Of course, not all sequential chains will be as simple as passing a single string as an argument and getting a single string as output for all steps in the chain. In this next example, we will experiment with more complex chains that involve multiple inputs, and where there also multiple final outputs.  \\nOf particular importance is how we name the input/output variable names. In the above example we didn't have to think about that because we were just passing the output of one chain directly as input to the next, but here we do have worry about that because we have multiple inputs.  \\n```python\", metadata={'Header 1': 'This is the overall chain where we run these two chains in sequence.', 'Header 2': 'Sequential Chain'}),\n",
       " Document(page_content='llm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.  \\nTitle: {title}\\nEra: {era}\\nPlaywright: This is a synopsis for the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"title\", \"era\"], template=template)\\nsynopsis_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"synopsis\")\\n```  \\n```python', metadata={'Header 1': 'This is an LLMChain to write a synopsis given a title of a play and the era it is set in.'}),\n",
       " Document(page_content='llm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.  \\nPlay Synopsis:\\n{synopsis}\\nReview from a New York Times play critic of the above play:\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\\nreview_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"review\")\\n```  \\n```python', metadata={'Header 1': 'This is an LLMChain to write a review of a play given a synopsis.'}),\n",
       " Document(page_content='from langchain.chains import SequentialChain\\noverall_chain = SequentialChain(\\nchains=[synopsis_chain, review_chain],\\ninput_variables=[\"era\", \"title\"],', metadata={'Header 1': 'This is the overall chain where we run these two chains in sequence.'}),\n",
       " Document(page_content='output_variables=[\"synopsis\", \"review\"],\\nverbose=True)\\n```  \\n```python\\noverall_chain({\"title\":\"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"})\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SequentialChain chain...  \\n> Finished chain.  \\n{\\'title\\': \\'Tragedy at sunset on the beach\\',\\n\\'era\\': \\'Victorian England\\',\\n\\'synopsis\\': \"\\\\n\\\\nThe play follows the story of John, a young man from a wealthy Victorian family, who dreams of a better life for himself. He soon meets a beautiful young woman named Mary, who shares his dream. The two fall in love and decide to elope and start a new life together.\\\\n\\\\nOn their journey, they make their way to a beach at sunset, where they plan to exchange their vows of love. Unbeknownst to them, their plans are overheard by John\\'s father, who has been tracking them. He follows them to the beach and, in a fit of rage, confronts them. \\\\n\\\\nA physical altercation ensues, and in the struggle, John\\'s father accidentally stabs Mary in the chest with his sword. The two are left in shock and disbelief as Mary dies in John\\'s arms, her last words being a declaration of her love for him.\\\\n\\\\nThe tragedy of the play comes to a head when John, broken and with no hope of a future, chooses to take his own life by jumping off the cliffs into the sea below. \\\\n\\\\nThe play is a powerful story of love, hope, and loss set against the backdrop of 19th century England.\",\\n\\'review\\': \"\\\\n\\\\nThe latest production from playwright X is a powerful and heartbreaking story of love and loss set against the backdrop of 19th century England. The play follows John, a young man from a wealthy Victorian family, and Mary, a beautiful young woman with whom he falls in love. The two decide to elope and start a new life together, and the audience is taken on a journey of hope and optimism for the future.\\\\n\\\\nUnfortunately, their dreams are cut short when John\\'s father discovers them and in a fit of rage, fatally stabs Mary. The tragedy of the play is further compounded when John, broken and without hope, takes his own life. The storyline is not only realistic, but also emotionally compelling, drawing the audience in from start to finish.\\\\n\\\\nThe acting was also commendable, with the actors delivering believable and nuanced performances. The playwright and director have successfully crafted a timeless tale of love and loss that will resonate with audiences for years to come. Highly recommended.\"}\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Here we return multiple variables'}),\n",
       " Document(page_content='Sometimes you may want to pass along some context to use in each step of the chain or in a later part of the chain, but maintaining and chaining together the input/output variables can quickly get messy.  Using `SimpleMemory` is a convenient way to do manage this and clean up your chains.  \\nFor example, using the previous playwright SequentialChain, lets say you wanted to include some context about date, time and location of the play, and using the generated synopsis and review, create some social media post text.  You could add these new context variables as `input_variables`, or we can add a `SimpleMemory` to the chain to manage this context:  \\n```python\\nfrom langchain.chains import SequentialChain\\nfrom langchain.memory import SimpleMemory  \\nllm = OpenAI(temperature=.7)\\ntemplate = \"\"\"You are a social media manager for a theater company.  Given the title of play, the era it is set in, the date,time and location, the synopsis of the play, and the review of the play, it is your job to write a social media post for that play.  \\nHere is some context about the time and location of the play:\\nDate and Time: {time}\\nLocation: {location}  \\nPlay Synopsis:\\n{synopsis}\\nReview from a New York Times play critic of the above play:\\n{review}  \\nSocial Media Post:\\n\"\"\"\\nprompt_template = PromptTemplate(input_variables=[\"synopsis\", \"review\", \"time\", \"location\"], template=template)\\nsocial_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"social_post_text\")  \\noverall_chain = SequentialChain(\\nmemory=SimpleMemory(memories={\"time\": \"December 25th, 8pm PST\", \"location\": \"Theater in the Park\"}),\\nchains=[synopsis_chain, review_chain, social_chain],\\ninput_variables=[\"era\", \"title\"],', metadata={'Header 1': 'Here we return multiple variables', 'Header 3': 'Memory in Sequential Chains'}),\n",
       " Document(page_content='output_variables=[\"social_post_text\"],\\nverbose=True)  \\noverall_chain({\"title\":\"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"})\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new SequentialChain chain...  \\n> Finished chain.  \\n{\\'title\\': \\'Tragedy at sunset on the beach\\',\\n\\'era\\': \\'Victorian England\\',\\n\\'time\\': \\'December 25th, 8pm PST\\',\\n\\'location\\': \\'Theater in the Park\\',\\n\\'social_post_text\\': \"\\\\nSpend your Christmas night with us at Theater in the Park and experience the heartbreaking story of love and loss that is \\'A Walk on the Beach\\'. Set in Victorian England, this romantic tragedy follows the story of Frances and Edward, a young couple whose love is tragically cut short. Don\\'t miss this emotional and thought-provoking production that is sure to leave you in tears. #AWalkOnTheBeach #LoveAndLoss #TheaterInThePark #VictorianEngland\"}\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Here we return multiple variables'}),\n",
       " Document(page_content='Load PDF using `pypdf` into array of documents, where each document contains the page content and metadata with `page` number.  \\n```bash\\npip install pypdf\\n```  \\n```python\\nfrom langchain.document_loaders import PyPDFLoader  \\nloader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\\npages = loader.load_and_split()\\n```  \\n```python\\npages[0]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nDocument(page_content=\\'LayoutParser : A Uni\\\\x0ced Toolkit for Deep\\\\nLearning Based Document Image Analysis\\\\nZejiang Shen1( \\\\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\\\nLee4, Jacob Carlson3, and Weining Li5\\\\n1Allen Institute for AI\\\\nshannons@allenai.org\\\\n2Brown University\\\\nruochen zhang@brown.edu\\\\n3Harvard University\\\\nfmelissadell,jacob carlson g@fas.harvard.edu\\\\n4University of Washington\\\\nbcgl@cs.washington.edu\\\\n5University of Waterloo\\\\nw422li@uwaterloo.ca\\\\nAbstract. Recent advances in document image analysis (DIA) have been\\\\nprimarily driven by the application of neural networks. Ideally, research\\\\noutcomes could be easily deployed in production and extended for further\\\\ninvestigation. However, various factors like loosely organized codebases\\\\nand sophisticated model con\\\\x0cgurations complicate the easy reuse of im-\\\\nportant innovations by a wide audience. Though there have been on-going\\\\ne\\\\x0borts to improve reusability and simplify deep learning (DL) model\\\\ndevelopment in disciplines like natural language processing and computer\\\\nvision, none of them are optimized for challenges in the domain of DIA.\\\\nThis represents a major gap in the existing toolkit, as DIA is central to\\\\nacademic research across a wide range of disciplines in the social sciences\\\\nand humanities. This paper introduces LayoutParser , an open-source\\\\nlibrary for streamlining the usage of DL in DIA research and applica-\\\\ntions. The core LayoutParser library comes with a set of simple and\\\\nintuitive interfaces for applying and customizing DL models for layout de-\\\\ntection, character recognition, and many other document processing tasks.\\\\nTo promote extensibility, LayoutParser also incorporates a community\\\\nplatform for sharing both pre-trained models and full document digiti-\\\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\\\nlightweight and large-scale digitization pipelines in real-word use cases.\\\\nThe library is publicly available at https://layout-parser.github.io .\\\\nKeywords: Document Image Analysis ·Deep Learning ·Layout Analysis\\\\n·Character Recognition ·Open Source library ·Toolkit.\\\\n1 Introduction\\\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\\\ndocument image analysis (DIA) tasks including document image classi\\\\x0ccation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\', metadata={\\'source\\': \\'example_data/layout-parser-paper.pdf\\', \\'page\\': 0})\\n```  \\n</CodeOutputBlock>  \\nAn advantage of this approach is that documents can be retrieved with page numbers.  \\nWe want to use `OpenAIEmbeddings` so we have to get the OpenAI API Key.  \\n```python\\nimport os\\nimport getpass  \\nos.environ[\\'OPENAI_API_KEY\\'] = getpass.getpass(\\'OpenAI API Key:\\')\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nOpenAI API Key: ········\\n```  \\n</CodeOutputBlock>  \\n```python\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.embeddings.openai import OpenAIEmbeddings  \\nfaiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\\ndocs = faiss_index.similarity_search(\"How will the community be engaged?\", k=2)\\nfor doc in docs:\\nprint(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n9: 10 Z. Shen et al.\\nFig. 4: Illustration of (a) the original historical Japanese document with layout\\ndetection results and (b) a recreated version of the document image that achieves\\nmuch better character recognition recall. The reorganization algorithm rearranges\\nthe tokens based on the their detect\\n3: 4 Z. Shen et al.\\nEfficient Data AnnotationC u s t o m i z e d  M o d e l  T r a i n i n gModel Cust omizationDI A Model HubDI A Pipeline SharingCommunity PlatformLa y out Detection ModelsDocument Images\\nT h e  C o r e  L a y o u t P a r s e r  L i b r a r yOCR ModuleSt or age & VisualizationLa y ou\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Using PyPDF'}),\n",
       " Document(page_content='Inspired by Daniel Gross\\'s [https://gist.github.com/danielgross/3ab4104e14faccc12b49200843adab21](https://gist.github.com/danielgross/3ab4104e14faccc12b49200843adab21)  \\n```python\\nfrom langchain.document_loaders import MathpixPDFLoader\\n```  \\n```python\\nloader = MathpixPDFLoader(\"example_data/layout-parser-paper.pdf\")\\n```  \\n```python\\ndata = loader.load()\\n```', metadata={'Header 2': 'Using MathPix'}),\n",
       " Document(page_content='```python\\nfrom langchain.document_loaders import UnstructuredPDFLoader\\n```  \\n```python\\nloader = UnstructuredPDFLoader(\"example_data/layout-parser-paper.pdf\")\\n```  \\n```python\\ndata = loader.load()\\n```', metadata={'Header 2': 'Using Unstructured'}),\n",
       " Document(page_content='Under the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying `mode=\"elements\"`.  \\n```python\\nloader = UnstructuredPDFLoader(\"example_data/layout-parser-paper.pdf\", mode=\"elements\")\\n```  \\n```python\\ndata = loader.load()\\n```  \\n```python\\ndata[0]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nDocument(page_content=\\'LayoutParser: A Uniﬁed Toolkit for Deep\\\\nLearning Based Document Image Analysis\\\\nZejiang Shen1 (�), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\\\nLee4, Jacob Carlson3, and Weining Li5\\\\n1 Allen Institute for AI\\\\nshannons@allenai.org\\\\n2 Brown University\\\\nruochen zhang@brown.edu\\\\n3 Harvard University\\\\n{melissadell,jacob carlson}@fas.harvard.edu\\\\n4 University of Washington\\\\nbcgl@cs.washington.edu\\\\n5 University of Waterloo\\\\nw422li@uwaterloo.ca\\\\nAbstract. Recent advances in document image analysis (DIA) have been\\\\nprimarily driven by the application of neural networks. Ideally, research\\\\noutcomes could be easily deployed in production and extended for further\\\\ninvestigation. However, various factors like loosely organized codebases\\\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\\\nportant innovations by a wide audience. Though there have been on-going\\\\neﬀorts to improve reusability and simplify deep learning (DL) model\\\\ndevelopment in disciplines like natural language processing and computer\\\\nvision, none of them are optimized for challenges in the domain of DIA.\\\\nThis represents a major gap in the existing toolkit, as DIA is central to\\\\nacademic research across a wide range of disciplines in the social sciences\\\\nand humanities. This paper introduces LayoutParser, an open-source\\\\nlibrary for streamlining the usage of DL in DIA research and applica-\\\\ntions. The core LayoutParser library comes with a set of simple and\\\\nintuitive interfaces for applying and customizing DL models for layout de-\\\\ntection, character recognition, and many other document processing tasks.\\\\nTo promote extensibility, LayoutParser also incorporates a community\\\\nplatform for sharing both pre-trained models and full document digiti-\\\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\\\nlightweight and large-scale digitization pipelines in real-word use cases.\\\\nThe library is publicly available at https://layout-parser.github.io.\\\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\\\n· Character Recognition · Open Source library · Toolkit.\\\\n1\\\\nIntroduction\\\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\\\n\\', lookup_str=\\'\\', metadata={\\'file_path\\': \\'example_data/layout-parser-paper.pdf\\', \\'page_number\\': 1, \\'total_pages\\': 16, \\'format\\': \\'PDF 1.5\\', \\'title\\': \\'\\', \\'author\\': \\'\\', \\'subject\\': \\'\\', \\'keywords\\': \\'\\', \\'creator\\': \\'LaTeX with hyperref\\', \\'producer\\': \\'pdfTeX-1.40.21\\', \\'creationDate\\': \\'D:20210622012710Z\\', \\'modDate\\': \\'D:20210622012710Z\\', \\'trapped\\': \\'\\', \\'encryption\\': None}, lookup_index=0)\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Using Unstructured', 'Header 3': 'Retain Elements'}),\n",
       " Document(page_content='This covers how to load online pdfs into a document format that we can use downstream. This can be used for various online pdf sites such as https://open.umn.edu/opentextbooks/textbooks/ and https://arxiv.org/archive/  \\nNote: all other pdf loaders can also be used to fetch remote PDFs, but `OnlinePDFLoader` is a legacy function, and works specifically with `UnstructuredPDFLoader`.  \\n```python\\nfrom langchain.document_loaders import OnlinePDFLoader\\n```  \\n```python\\nloader = OnlinePDFLoader(\"https://arxiv.org/pdf/2302.03803.pdf\")\\n```  \\n```python\\ndata = loader.load()\\n```  \\n```python\\nprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'A WEAK ( k, k ) -LEFSCHETZ THEOREM FOR PROJECTIVE TORIC ORBIFOLDS\\\\n\\\\nWilliam D. Montoya\\\\n\\\\nInstituto de Matem´atica, Estat´ıstica e Computa¸c˜ao Cient´ıﬁca,\\\\n\\\\nIn [3] we proved that, under suitable conditions, on a very general codimension s quasi- smooth intersection subvariety X in a projective toric orbifold P d Σ with d + s = 2 ( k + 1 ) the Hodge conjecture holds, that is, every ( p, p ) -cohomology class, under the Poincar´e duality is a rational linear combination of fundamental classes of algebraic subvarieties of X . The proof of the above-mentioned result relies, for p ≠ d + 1 − s , on a Lefschetz\\\\n\\\\nKeywords: (1,1)- Lefschetz theorem, Hodge conjecture, toric varieties, complete intersection Email: wmontoya@ime.unicamp.br\\\\n\\\\ntheorem ([7]) and the Hard Lefschetz theorem for projective orbifolds ([11]). When p = d + 1 − s the proof relies on the Cayley trick, a trick which associates to X a quasi-smooth hypersurface Y in a projective vector bundle, and the Cayley Proposition (4.3) which gives an isomorphism of some primitive cohomologies (4.2) of X and Y . The Cayley trick, following the philosophy of Mavlyutov in [7], reduces results known for quasi-smooth hypersurfaces to quasi-smooth intersection subvarieties. The idea in this paper goes the other way around, we translate some results for quasi-smooth intersection subvarieties to\\\\n\\\\nAcknowledgement. I thank Prof. Ugo Bruzzo and Tiago Fonseca for useful discus- sions. I also acknowledge support from FAPESP postdoctoral grant No. 2019/23499-7.\\\\n\\\\nLet M be a free abelian group of rank d , let N = Hom ( M, Z ) , and N R = N ⊗ Z R .\\\\n\\\\nif there exist k linearly independent primitive elements e\\\\n\\\\n, . . . , e k ∈ N such that σ = { µ\\\\n\\\\ne\\\\n\\\\n+ ⋯ + µ k e k } . • The generators e i are integral if for every i and any nonnegative rational number µ the product µe i is in N only if µ is an integer. • Given two rational simplicial cones σ , σ ′ one says that σ ′ is a face of σ ( σ ′ < σ ) if the set of integral generators of σ ′ is a subset of the set of integral generators of σ . • A ﬁnite set Σ = { σ\\\\n\\\\n, . . . , σ t } of rational simplicial cones is called a rational simplicial complete d -dimensional fan if:\\\\n\\\\nall faces of cones in Σ are in Σ ;\\\\n\\\\nif σ, σ ′ ∈ Σ then σ ∩ σ ′ < σ and σ ∩ σ ′ < σ ′ ;\\\\n\\\\nN R = σ\\\\n\\\\n∪ ⋅ ⋅ ⋅ ∪ σ t .\\\\n\\\\nA rational simplicial complete d -dimensional fan Σ deﬁnes a d -dimensional toric variety P d Σ having only orbifold singularities which we assume to be projective. Moreover, T ∶ = N ⊗ Z C ∗ ≃ ( C ∗ ) d is the torus action on P d Σ . We denote by Σ ( i ) the i -dimensional cones\\\\n\\\\nFor a cone σ ∈ Σ, ˆ σ is the set of 1-dimensional cone in Σ that are not contained in σ\\\\n\\\\nand x ˆ σ ∶ = ∏ ρ ∈ ˆ σ x ρ is the associated monomial in S .\\\\n\\\\nDeﬁnition 2.2. The irrelevant ideal of P d Σ is the monomial ideal B Σ ∶ =< x ˆ σ ∣ σ ∈ Σ > and the zero locus Z ( Σ ) ∶ = V ( B Σ ) in the aﬃne space A d ∶ = Spec ( S ) is the irrelevant locus.\\\\n\\\\nProposition 2.3 (Theorem 5.1.11 [5]) . The toric variety P d Σ is a categorical quotient A d ∖ Z ( Σ ) by the group Hom ( Cl ( Σ ) , C ∗ ) and the group action is induced by the Cl ( Σ ) - grading of S .\\\\n\\\\nNow we give a brief introduction to complex orbifolds and we mention the needed theorems for the next section. Namely: de Rham theorem and Dolbeault theorem for complex orbifolds.\\\\n\\\\nDeﬁnition 2.4. A complex orbifold of complex dimension d is a singular complex space whose singularities are locally isomorphic to quotient singularities C d / G , for ﬁnite sub- groups G ⊂ Gl ( d, C ) .\\\\n\\\\nDeﬁnition 2.5. A diﬀerential form on a complex orbifold Z is deﬁned locally at z ∈ Z as a G -invariant diﬀerential form on C d where G ⊂ Gl ( d, C ) and Z is locally isomorphic to d\\\\n\\\\nRoughly speaking the local geometry of orbifolds reduces to local G -invariant geometry.\\\\n\\\\nWe have a complex of diﬀerential forms ( A ● ( Z ) , d ) and a double complex ( A ● , ● ( Z ) , ∂, ¯ ∂ ) of bigraded diﬀerential forms which deﬁne the de Rham and the Dolbeault cohomology groups (for a ﬁxed p ∈ N ) respectively:\\\\n\\\\n(1,1)-Lefschetz theorem for projective toric orbifolds\\\\n\\\\nDeﬁnition 3.1. A subvariety X ⊂ P d Σ is quasi-smooth if V ( I X ) ⊂ A #Σ ( 1 ) is smooth outside\\\\n\\\\nExample 3.2 . Quasi-smooth hypersurfaces or more generally quasi-smooth intersection sub-\\\\n\\\\nExample 3.2 . Quasi-smooth hypersurfaces or more generally quasi-smooth intersection sub- varieties are quasi-smooth subvarieties (see [2] or [7] for more details).\\\\n\\\\nRemark 3.3 . Quasi-smooth subvarieties are suborbifolds of P d Σ in the sense of Satake in [8]. Intuitively speaking they are subvarieties whose only singularities come from the ambient\\\\n\\\\nProof. From the exponential short exact sequence\\\\n\\\\nwe have a long exact sequence in cohomology\\\\n\\\\nH 1 (O ∗ X ) → H 2 ( X, Z ) → H 2 (O X ) ≃ H 0 , 2 ( X )\\\\n\\\\nwhere the last isomorphisms is due to Steenbrink in [9]. Now, it is enough to prove the commutativity of the next diagram\\\\n\\\\nwhere the last isomorphisms is due to Steenbrink in [9]. Now,\\\\n\\\\nH 2 ( X, Z ) / / H 2 ( X, O X ) ≃ Dolbeault H 2 ( X, C ) deRham ≃ H 2 dR ( X, C ) / / H 0 , 2 ¯ ∂ ( X )\\\\n\\\\nof the proof follows as the ( 1 , 1 ) -Lefschetz theorem in [6].\\\\n\\\\nRemark 3.5 . For k = 1 and P d Σ as the projective space, we recover the classical ( 1 , 1 ) - Lefschetz theorem.\\\\n\\\\nBy the Hard Lefschetz Theorem for projective orbifolds (see [11] for details) we\\\\n\\\\nBy the Hard Lefschetz Theorem for projective orbifolds (see [11] for details) we get an isomorphism of cohomologies :\\\\n\\\\ngiven by the Lefschetz morphism and since it is a morphism of Hodge structures, we have:\\\\n\\\\nH 1 , 1 ( X, Q ) ≃ H dim X − 1 , dim X − 1 ( X, Q )\\\\n\\\\nCorollary 3.6. If the dimension of X is 1 , 2 or 3 . The Hodge conjecture holds on X\\\\n\\\\nProof. If the dim C X = 1 the result is clear by the Hard Lefschetz theorem for projective orbifolds. The dimension 2 and 3 cases are covered by Theorem 3.5 and the Hard Lefschetz.\\\\n\\\\nCayley trick and Cayley proposition\\\\n\\\\nThe Cayley trick is a way to associate to a quasi-smooth intersection subvariety a quasi- smooth hypersurface. Let L 1 , . . . , L s be line bundles on P d Σ and let π ∶ P ( E ) → P d Σ be the projective space bundle associated to the vector bundle E = L 1 ⊕ ⋯ ⊕ L s . It is known that P ( E ) is a ( d + s − 1 ) -dimensional simplicial toric variety whose fan depends on the degrees of the line bundles and the fan Σ. Furthermore, if the Cox ring, without considering the grading, of P d Σ is C [ x 1 , . . . , x m ] then the Cox ring of P ( E ) is\\\\n\\\\nMoreover for X a quasi-smooth intersection subvariety cut oﬀ by f 1 , . . . , f s with deg ( f i ) = [ L i ] we relate the hypersurface Y cut oﬀ by F = y 1 f 1 + ⋅ ⋅ ⋅ + y s f s which turns out to be quasi-smooth. For more details see Section 2 in [7].\\\\n\\\\nWe will denote P ( E ) as P d + s − 1 Σ ,X to keep track of its relation with X and P d Σ .\\\\n\\\\nThe following is a key remark.\\\\n\\\\nRemark 4.1 . There is a morphism ι ∶ X → Y ⊂ P d + s − 1 Σ ,X . Moreover every point z ∶ = ( x, y ) ∈ Y with y ≠ 0 has a preimage. Hence for any subvariety W = V ( I W ) ⊂ X ⊂ P d Σ there exists W ′ ⊂ Y ⊂ P d + s − 1 Σ ,X such that π ( W ′ ) = W , i.e., W ′ = { z = ( x, y ) ∣ x ∈ W } .\\\\n\\\\nFor X ⊂ P d Σ a quasi-smooth intersection variety the morphism in cohomology induced by the inclusion i ∗ ∶ H d − s ( P d Σ , C ) → H d − s ( X, C ) is injective by Proposition 1.4 in [7].\\\\n\\\\nDeﬁnition 4.2. The primitive cohomology of H d − s prim ( X ) is the quotient H d − s ( X, C )/ i ∗ ( H d − s ( P d Σ , C )) and H d − s prim ( X, Q ) with rational coeﬃcients.\\\\n\\\\nH d − s ( P d Σ , C ) and H d − s ( X, C ) have pure Hodge structures, and the morphism i ∗ is com- patible with them, so that H d − s prim ( X ) gets a pure Hodge structure.\\\\n\\\\nThe next Proposition is the Cayley proposition.\\\\n\\\\nProposition 4.3. [Proposition 2.3 in [3] ] Let X = X 1 ∩⋅ ⋅ ⋅∩ X s be a quasi-smooth intersec- tion subvariety in P d Σ cut oﬀ by homogeneous polynomials f 1 . . . f s . Then for p ≠ d + s − 1 2 , d + s − 3 2\\\\n\\\\nRemark 4.5 . The above isomorphisms are also true with rational coeﬃcients since H ● ( X, C ) = H ● ( X, Q ) ⊗ Q C . See the beginning of Section 7.1 in [10] for more details.\\\\n\\\\nTheorem 5.1. Let Y = { F = y 1 f 1 + ⋯ + y k f k = 0 } ⊂ P 2 k + 1 Σ ,X be the quasi-smooth hypersurface associated to the quasi-smooth intersection surface X = X f 1 ∩ ⋅ ⋅ ⋅ ∩ X f k ⊂ P k + 2 Σ . Then on Y the Hodge conjecture holds.\\\\n\\\\nthe Hodge conjecture holds.\\\\n\\\\nProof. If H k,k prim ( X, Q ) = 0 we are done. So let us assume H k,k prim ( X, Q ) ≠ 0. By the Cayley proposition H k,k prim ( Y, Q ) ≃ H 1 , 1 prim ( X, Q ) and by the ( 1 , 1 ) -Lefschetz theorem for projective\\\\n\\\\ntoric orbifolds there is a non-zero algebraic basis λ C 1 , . . . , λ C n with rational coeﬃcients of H 1 , 1 prim ( X, Q ) , that is, there are n ∶ = h 1 , 1 prim ( X, Q ) algebraic curves C 1 , . . . , C n in X such that under the Poincar´e duality the class in homology [ C i ] goes to λ C i , [ C i ] ↦ λ C i . Recall that the Cox ring of P k + 2 is contained in the Cox ring of P 2 k + 1 Σ ,X without considering the grading. Considering the grading we have that if α ∈ Cl ( P k + 2 Σ ) then ( α, 0 ) ∈ Cl ( P 2 k + 1 Σ ,X ) . So the polynomials deﬁning C i ⊂ P k + 2 Σ can be interpreted in P 2 k + 1 X, Σ but with diﬀerent degree. Moreover, by Remark 4.1 each C i is contained in Y = { F = y 1 f 1 + ⋯ + y k f k = 0 } and\\\\n\\\\nfurthermore it has codimension k .\\\\n\\\\nClaim: { C i } ni = 1 is a basis of prim ( ) . It is enough to prove that λ C i is diﬀerent from zero in H k,k prim ( Y, Q ) or equivalently that the cohomology classes { λ C i } ni = 1 do not come from the ambient space. By contradiction, let us assume that there exists a j and C ⊂ P 2 k + 1 Σ ,X such that λ C ∈ H k,k ( P 2 k + 1 Σ ,X , Q ) with i ∗ ( λ C ) = λ C j or in terms of homology there exists a ( k + 2 ) -dimensional algebraic subvariety V ⊂ P 2 k + 1 Σ ,X such that V ∩ Y = C j so they are equal as a homology class of P 2 k + 1 Σ ,X ,i.e., [ V ∩ Y ] = [ C j ] . It is easy to check that π ( V ) ∩ X = C j as a subvariety of P k + 2 Σ where π ∶ ( x, y ) ↦ x . Hence [ π ( V ) ∩ X ] = [ C j ] which is equivalent to say that λ C j comes from P k + 2 Σ which contradicts the choice of [ C j ] .\\\\n\\\\nRemark 5.2 . Into the proof of the previous theorem, the key fact was that on X the Hodge conjecture holds and we translate it to Y by contradiction. So, using an analogous argument we have:\\\\n\\\\nargument we have:\\\\n\\\\nProposition 5.3. Let Y = { F = y 1 f s +⋯+ y s f s = 0 } ⊂ P 2 k + 1 Σ ,X be the quasi-smooth hypersurface associated to a quasi-smooth intersection subvariety X = X f 1 ∩ ⋅ ⋅ ⋅ ∩ X f s ⊂ P d Σ such that d + s = 2 ( k + 1 ) . If the Hodge conjecture holds on X then it holds as well on Y .\\\\n\\\\nCorollary 5.4. If the dimension of Y is 2 s − 1 , 2 s or 2 s + 1 then the Hodge conjecture holds on Y .\\\\n\\\\nProof. By Proposition 5.3 and Corollary 3.6.\\\\n\\\\n[\\\\n\\\\n] Angella, D. Cohomologies of certain orbifolds. Journal of Geometry and Physics\\\\n\\\\n(\\\\n\\\\n),\\\\n\\\\n–\\\\n\\\\n[\\\\n\\\\n] Batyrev, V. V., and Cox, D. A. On the Hodge structure of projective hypersur- faces in toric varieties. Duke Mathematical Journal\\\\n\\\\n,\\\\n\\\\n(Aug\\\\n\\\\n). [\\\\n\\\\n] Bruzzo, U., and Montoya, W. On the Hodge conjecture for quasi-smooth in- tersections in toric varieties. S˜ao Paulo J. Math. Sci. Special Section: Geometry in Algebra and Algebra in Geometry (\\\\n\\\\n). [\\\\n\\\\n] Caramello Jr, F. C. Introduction to orbifolds. a\\\\n\\\\niv:\\\\n\\\\nv\\\\n\\\\n(\\\\n\\\\n). [\\\\n\\\\n] Cox, D., Little, J., and Schenck, H. Toric varieties, vol.\\\\n\\\\nAmerican Math- ematical Soc.,\\\\n\\\\n[\\\\n\\\\n] Griffiths, P., and Harris, J. Principles of Algebraic Geometry. John Wiley & Sons, Ltd,\\\\n\\\\n[\\\\n\\\\n] Mavlyutov, A. R. Cohomology of complete intersections in toric varieties. Pub- lished in Paciﬁc J. of Math.\\\\n\\\\nNo.\\\\n\\\\n(\\\\n\\\\n),\\\\n\\\\n–\\\\n\\\\n[\\\\n\\\\n] Satake, I. On a Generalization of the Notion of Manifold. Proceedings of the National Academy of Sciences of the United States of America\\\\n\\\\n,\\\\n\\\\n(\\\\n\\\\n),\\\\n\\\\n–\\\\n\\\\n[\\\\n\\\\n] Steenbrink, J. H. M. Intersection form for quasi-homogeneous singularities. Com- positio Mathematica\\\\n\\\\n,\\\\n\\\\n(\\\\n\\\\n),\\\\n\\\\n–\\\\n\\\\n[\\\\n\\\\n] Voisin, C. Hodge Theory and Complex Algebraic Geometry I, vol.\\\\n\\\\nof Cambridge Studies in Advanced Mathematics . Cambridge University Press,\\\\n\\\\n[\\\\n\\\\n] Wang, Z. Z., and Zaffran, D. A remark on the Hard Lefschetz theorem for K¨ahler orbifolds. Proceedings of the American Mathematical Society\\\\n\\\\n,\\\\n\\\\n(Aug\\\\n\\\\n).\\\\n\\\\n[2] Batyrev, V. V., and Cox, D. A. On the Hodge structure of projective hypersur- faces in toric varieties. Duke Mathematical Journal 75, 2 (Aug 1994).\\\\n\\\\n[\\\\n\\\\n] Bruzzo, U., and Montoya, W. On the Hodge conjecture for quasi-smooth in- tersections in toric varieties. S˜ao Paulo J. Math. Sci. Special Section: Geometry in Algebra and Algebra in Geometry (\\\\n\\\\n).\\\\n\\\\n[3] Bruzzo, U., and Montoya, W. On the Hodge conjecture for quasi-smooth in- tersections in toric varieties. S˜ao Paulo J. Math. Sci. Special Section: Geometry in Algebra and Algebra in Geometry (2021).\\\\n\\\\nA. R. Cohomology of complete intersections in toric varieties. Pub-\\', lookup_str=\\'\\', metadata={\\'source\\': \\'/var/folders/ph/hhm7_zyx4l13k3v8z02dwp1w0000gn/T/tmpgq0ckaja/online_file.pdf\\'}, lookup_index=0)]\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Using Unstructured', 'Header 3': 'Fetching remote PDFs using Unstructured'}),\n",
       " Document(page_content='```python\\nfrom langchain.document_loaders import PyPDFium2Loader\\n```  \\n```python\\nloader = PyPDFium2Loader(\"example_data/layout-parser-paper.pdf\")\\n```  \\n```python\\ndata = loader.load()\\n```', metadata={'Header 2': 'Using PyPDFium2'}),\n",
       " Document(page_content='```python\\nfrom langchain.document_loaders import PDFMinerLoader\\n```  \\n```python\\nloader = PDFMinerLoader(\"example_data/layout-parser-paper.pdf\")\\n```  \\n```python\\ndata = loader.load()\\n```', metadata={'Header 2': 'Using PDFMiner'}),\n",
       " Document(page_content='This can be helpful for chunking texts semantically into sections as the output html content can be parsed via `BeautifulSoup` to get more structured and rich information about font size, page numbers, pdf headers/footers, etc.  \\n```python\\nfrom langchain.document_loaders import PDFMinerPDFasHTMLLoader\\n```  \\n```python\\nloader = PDFMinerPDFasHTMLLoader(\"example_data/layout-parser-paper.pdf\")\\n```  \\n```python\\ndata = loader.load()[0]   # entire pdf is loaded as a single Document\\n```  \\n```python\\nfrom bs4 import BeautifulSoup\\nsoup = BeautifulSoup(data.page_content,\\'html.parser\\')\\ncontent = soup.find_all(\\'div\\')\\n```  \\n```python\\nimport re\\ncur_fs = None\\ncur_text = \\'\\'\\nsnippets = []   # first collect all snippets that have the same font size\\nfor c in content:\\nsp = c.find(\\'span\\')\\nif not sp:\\ncontinue\\nst = sp.get(\\'style\\')\\nif not st:\\ncontinue\\nfs = re.findall(\\'font-size:(\\\\d+)px\\',st)\\nif not fs:\\ncontinue\\nfs = int(fs[0])\\nif not cur_fs:\\ncur_fs = fs\\nif fs == cur_fs:\\ncur_text += c.text\\nelse:\\nsnippets.append((cur_text,cur_fs))\\ncur_fs = fs\\ncur_text = c.text\\nsnippets.append((cur_text,cur_fs))', metadata={'Header 2': 'Using PDFMiner', 'Header 3': 'Using PDFMiner to generate HTML text'}),\n",
       " Document(page_content='```  \\n```python\\nfrom langchain.docstore.document import Document\\ncur_idx = -1\\nsemantic_snippets = []', metadata={'Header 1': 'headers/footers in a PDF appear on multiple pages so if we find duplicatess safe to assume that it is redundant info)'}),\n",
       " Document(page_content='for s in snippets:', metadata={'Header 1': 'Assumption: headings have higher font size than their respective content'}),\n",
       " Document(page_content=\"if not semantic_snippets or s[1] > semantic_snippets[cur_idx].metadata['heading_font']:\\nmetadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]}\\nmetadata.update(data.metadata)\\nsemantic_snippets.append(Document(page_content='',metadata=metadata))\\ncur_idx += 1\\ncontinue\", metadata={'Header 1': \"if current snippet's font size > previous section's heading => it is a new heading\"}),\n",
       " Document(page_content=\"if not semantic_snippets[cur_idx].metadata['content_font'] or s[1] <= semantic_snippets[cur_idx].metadata['content_font']:\\nsemantic_snippets[cur_idx].page_content += s[0]\\nsemantic_snippets[cur_idx].metadata['content_font'] = max(s[1], semantic_snippets[cur_idx].metadata['content_font'])\\ncontinue\", metadata={'Header 1': 'a tree like structure for sub sections if needed but that may require some more thinking and may be data specific)'}),\n",
       " Document(page_content='metadata={\\'heading\\':s[0], \\'content_font\\': 0, \\'heading_font\\': s[1]}\\nmetadata.update(data.metadata)\\nsemantic_snippets.append(Document(page_content=\\'\\',metadata=metadata))\\ncur_idx += 1\\n```  \\n```python\\nsemantic_snippets[4]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nDocument(page_content=\\'Recently, various DL models and datasets have been developed for layout analysis\\\\ntasks. The dhSegment [22] utilizes fully convolutional networks [20] for segmen-\\\\ntation tasks on historical documents. Object detection-based methods like Faster\\\\nR-CNN [28] and Mask R-CNN [12] are used for identifying document elements [38]\\\\nand detecting tables [30, 26]. Most recently, Graph Neural Networks [29] have also\\\\nbeen used in table detection [27]. However, these models are usually implemented\\\\nindividually and there is no uniﬁed framework to load and use such models.\\\\nThere has been a surge of interest in creating open-source tools for document\\\\nimage processing: a search of document image analysis in Github leads to 5M\\\\nrelevant code pieces 6; yet most of them rely on traditional rule-based methods\\\\nor provide limited functionalities. The closest prior research to our work is the\\\\nOCR-D project7, which also tries to build a complete toolkit for DIA. However,\\\\nsimilar to the platform developed by Neudecker et al. [21], it is designed for\\\\nanalyzing historical documents, and provides no supports for recent DL models.\\\\nThe DocumentLayoutAnalysis project8 focuses on processing born-digital PDF\\\\ndocuments via analyzing the stored PDF data. Repositories like DeepLayout9\\\\nand Detectron2-PubLayNet10 are individual deep learning models trained on\\\\nlayout analysis datasets without support for the full DIA pipeline. The Document\\\\nAnalysis and Exploitation (DAE) platform [15] and the DeepDIVA project [2]\\\\naim to improve the reproducibility of DIA methods (or DL models), yet they\\\\nare not actively maintained. OCR engines like Tesseract [14], easyOCR11 and\\\\npaddleOCR12 usually do not come with comprehensive functionalities for other\\\\nDIA tasks like layout analysis.\\\\nRecent years have also seen numerous eﬀorts to create libraries for promoting\\\\nreproducibility and reusability in the ﬁeld of DL. Libraries like Dectectron2 [35],\\\\n6 The number shown is obtained by specifying the search type as ‘code’.\\\\n7 https://ocr-d.de/en/about\\\\n8 https://github.com/BobLd/DocumentLayoutAnalysis\\\\n9 https://github.com/leonlulu/DeepLayout\\\\n10 https://github.com/hpanwar08/detectron2\\\\n11 https://github.com/JaidedAI/EasyOCR\\\\n12 https://github.com/PaddlePaddle/PaddleOCR\\\\n4\\\\nZ. Shen et al.\\\\nFig. 1: The overall architecture of LayoutParser. For an input document image,\\\\nthe core LayoutParser library provides a set of oﬀ-the-shelf tools for layout\\\\ndetection, OCR, visualization, and storage, backed by a carefully designed layout\\\\ndata structure. LayoutParser also supports high level customization via eﬃcient\\\\nlayout annotation and model training functions. These improve model accuracy\\\\non the target samples. The community platform enables the easy sharing of DIA\\\\nmodels and whole digitization pipelines to promote reusability and reproducibility.\\\\nA collection of detailed documentation, tutorials and exemplar projects make\\\\nLayoutParser easy to learn and use.\\\\nAllenNLP [8] and transformers [34] have provided the community with complete\\\\nDL-based support for developing and deploying models for general computer\\\\nvision and natural language processing problems. LayoutParser, on the other\\\\nhand, specializes speciﬁcally in DIA tasks. LayoutParser is also equipped with a\\\\ncommunity platform inspired by established model hubs such as Torch Hub [23]\\\\nand TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\\\\nfull document processing pipelines that are unique to DIA tasks.\\\\nThere have been a variety of document data collections to facilitate the\\\\ndevelopment of DL models. Some examples include PRImA [3](magazine layouts),\\\\nPubLayNet [38](academic paper layouts), Table Bank [18](tables in academic\\\\npapers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and\\\\nHJDataset [31](historical Japanese document layouts). A spectrum of models\\\\ntrained on these datasets are currently available in the LayoutParser model zoo\\\\nto support diﬀerent use cases.\\\\n\\', metadata={\\'heading\\': \\'2 Related Work\\\\n\\', \\'content_font\\': 9, \\'heading_font\\': 11, \\'source\\': \\'example_data/layout-parser-paper.pdf\\'})\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': \"section (e.g. title of a pdf will have the highest font size but we don't want it to subsume all sections)\"}),\n",
       " Document(page_content='This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page.  \\n```python\\nfrom langchain.document_loaders import PyMuPDFLoader\\n```  \\n```python\\nloader = PyMuPDFLoader(\"example_data/layout-parser-paper.pdf\")\\n```  \\n```python\\ndata = loader.load()\\n```  \\n```python\\ndata[0]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nDocument(page_content=\\'LayoutParser: A Uniﬁed Toolkit for Deep\\\\nLearning Based Document Image Analysis\\\\nZejiang Shen1 (�), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\\\nLee4, Jacob Carlson3, and Weining Li5\\\\n1 Allen Institute for AI\\\\nshannons@allenai.org\\\\n2 Brown University\\\\nruochen zhang@brown.edu\\\\n3 Harvard University\\\\n{melissadell,jacob carlson}@fas.harvard.edu\\\\n4 University of Washington\\\\nbcgl@cs.washington.edu\\\\n5 University of Waterloo\\\\nw422li@uwaterloo.ca\\\\nAbstract. Recent advances in document image analysis (DIA) have been\\\\nprimarily driven by the application of neural networks. Ideally, research\\\\noutcomes could be easily deployed in production and extended for further\\\\ninvestigation. However, various factors like loosely organized codebases\\\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\\\nportant innovations by a wide audience. Though there have been on-going\\\\neﬀorts to improve reusability and simplify deep learning (DL) model\\\\ndevelopment in disciplines like natural language processing and computer\\\\nvision, none of them are optimized for challenges in the domain of DIA.\\\\nThis represents a major gap in the existing toolkit, as DIA is central to\\\\nacademic research across a wide range of disciplines in the social sciences\\\\nand humanities. This paper introduces LayoutParser, an open-source\\\\nlibrary for streamlining the usage of DL in DIA research and applica-\\\\ntions. The core LayoutParser library comes with a set of simple and\\\\nintuitive interfaces for applying and customizing DL models for layout de-\\\\ntection, character recognition, and many other document processing tasks.\\\\nTo promote extensibility, LayoutParser also incorporates a community\\\\nplatform for sharing both pre-trained models and full document digiti-\\\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\\\nlightweight and large-scale digitization pipelines in real-word use cases.\\\\nThe library is publicly available at https://layout-parser.github.io.\\\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\\\n· Character Recognition · Open Source library · Toolkit.\\\\n1\\\\nIntroduction\\\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\\\n\\', lookup_str=\\'\\', metadata={\\'file_path\\': \\'example_data/layout-parser-paper.pdf\\', \\'page_number\\': 1, \\'total_pages\\': 16, \\'format\\': \\'PDF 1.5\\', \\'title\\': \\'\\', \\'author\\': \\'\\', \\'subject\\': \\'\\', \\'keywords\\': \\'\\', \\'creator\\': \\'LaTeX with hyperref\\', \\'producer\\': \\'pdfTeX-1.40.21\\', \\'creationDate\\': \\'D:20210622012710Z\\', \\'modDate\\': \\'D:20210622012710Z\\', \\'trapped\\': \\'\\', \\'encryption\\': None}, lookup_index=0)\\n```  \\n</CodeOutputBlock>  \\nAdditionally, you can pass along any of the options from the [PyMuPDF documentation](https://pymupdf.readthedocs.io/en/latest/app1.html#plain-text/) as keyword arguments in the `load` call, and it will be pass along to the `get_text()` call.', metadata={'Header 1': \"section (e.g. title of a pdf will have the highest font size but we don't want it to subsume all sections)\", 'Header 2': 'Using PyMuPDF'}),\n",
       " Document(page_content='Load PDFs from directory  \\n```python\\nfrom langchain.document_loaders import PyPDFDirectoryLoader\\n```  \\n```python\\nloader = PyPDFDirectoryLoader(\"example_data/\")\\n```  \\n```python\\ndocs = loader.load()\\n```', metadata={'Header 1': \"section (e.g. title of a pdf will have the highest font size but we don't want it to subsume all sections)\", 'Header 2': 'PyPDF Directory'}),\n",
       " Document(page_content='Like PyMuPDF, the output Documents contain detailed metadata about the PDF and its pages, and returns one document per page.  \\n```python\\nfrom langchain.document_loaders import PDFPlumberLoader\\n```  \\n```python\\nloader = PDFPlumberLoader(\"example_data/layout-parser-paper.pdf\")\\n```  \\n```python\\ndata = loader.load()\\n```  \\n```python\\ndata[0]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nDocument(page_content=\\'LayoutParser: A Unified Toolkit for Deep\\\\nLearning Based Document Image Analysis\\\\nZejiang Shen1 ((cid:0)), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\\\nLee4, Jacob Carlson3, and Weining Li5\\\\n1 Allen Institute for AI\\\\n1202 shannons@allenai.org\\\\n2 Brown University\\\\nruochen zhang@brown.edu\\\\n3 Harvard University\\\\nnuJ {melissadell,jacob carlson}@fas.harvard.edu\\\\n4 University of Washington\\\\nbcgl@cs.washington.edu\\\\n12 5 University of Waterloo\\\\nw422li@uwaterloo.ca\\\\n]VC.sc[\\\\nAbstract. Recentadvancesindocumentimageanalysis(DIA)havebeen\\\\nprimarily driven by the application of neural networks. Ideally, research\\\\noutcomescouldbeeasilydeployedinproductionandextendedforfurther\\\\ninvestigation. However, various factors like loosely organized codebases\\\\nand sophisticated model configurations complicate the easy reuse of im-\\\\n2v84351.3012:viXra portantinnovationsbyawideaudience.Thoughtherehavebeenon-going\\\\nefforts to improve reusability and simplify deep learning (DL) model\\\\ndevelopmentindisciplineslikenaturallanguageprocessingandcomputer\\\\nvision, none of them are optimized for challenges in the domain of DIA.\\\\nThis represents a major gap in the existing toolkit, as DIA is central to\\\\nacademicresearchacross awiderangeof disciplinesinthesocialsciences\\\\nand humanities. This paper introduces LayoutParser, an open-source\\\\nlibrary for streamlining the usage of DL in DIA research and applica-\\\\ntions. The core LayoutParser library comes with a set of simple and\\\\nintuitiveinterfacesforapplyingandcustomizingDLmodelsforlayoutde-\\\\ntection,characterrecognition,andmanyotherdocumentprocessingtasks.\\\\nTo promote extensibility, LayoutParser also incorporates a community\\\\nplatform for sharing both pre-trained models and full document digiti-\\\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\\\nlightweight and large-scale digitization pipelines in real-word use cases.\\\\nThe library is publicly available at https://layout-parser.github.io.\\\\nKeywords: DocumentImageAnalysis·DeepLearning·LayoutAnalysis\\\\n· Character Recognition · Open Source library · Toolkit.\\\\n1 Introduction\\\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\\\ndocumentimageanalysis(DIA)tasksincludingdocumentimageclassification[11,\\', metadata={\\'source\\': \\'example_data/layout-parser-paper.pdf\\', \\'file_path\\': \\'example_data/layout-parser-paper.pdf\\', \\'page\\': 1, \\'total_pages\\': 16, \\'Author\\': \\'\\', \\'CreationDate\\': \\'D:20210622012710Z\\', \\'Creator\\': \\'LaTeX with hyperref\\', \\'Keywords\\': \\'\\', \\'ModDate\\': \\'D:20210622012710Z\\', \\'PTEX.Fullbanner\\': \\'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2\\', \\'Producer\\': \\'pdfTeX-1.40.21\\', \\'Subject\\': \\'\\', \\'Title\\': \\'\\', \\'Trapped\\': \\'False\\'})\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': \"section (e.g. title of a pdf will have the highest font size but we don't want it to subsume all sections)\", 'Header 2': 'Using pdfplumber'}),\n",
       " Document(page_content='```python\\nfrom langchain.chains.api.prompt import API_RESPONSE_PROMPT\\n```  \\n```python\\nfrom langchain.chains import APIChain\\nfrom langchain.prompts.prompt import PromptTemplate  \\nfrom langchain.llms import OpenAI  \\nllm = OpenAI(temperature=0)\\n```', metadata={}),\n",
       " Document(page_content='```python\\nfrom langchain.chains.api import open_meteo_docs\\nchain_new = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\\n```  \\n```python\\nchain_new.run(\\'What is the weather like right now in Munich, Germany in degrees Fahrenheit?\\')\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new APIChain chain...\\nhttps://api.open-meteo.com/v1/forecast?latitude=48.1351&longitude=11.5820&temperature_unit=fahrenheit&current_weather=true\\n{\"latitude\":48.14,\"longitude\":11.58,\"generationtime_ms\":0.33104419708251953,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":521.0,\"current_weather\":{\"temperature\":33.4,\"windspeed\":6.8,\"winddirection\":198.0,\"weathercode\":2,\"time\":\"2023-01-16T01:00\"}}  \\n> Finished chain.  \\n\\' The current temperature in Munich, Germany is 33.4 degrees Fahrenheit with a windspeed of 6.8 km/h and a wind direction of 198 degrees. The weathercode is 2.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'OpenMeteo Example'}),\n",
       " Document(page_content='```python\\nimport os\\nos.environ[\\'TMDB_BEARER_TOKEN\\'] = \"\"\\n```  \\n```python\\nfrom langchain.chains.api import tmdb_docs\\nheaders = {\"Authorization\": f\"Bearer {os.environ[\\'TMDB_BEARER_TOKEN\\']}\"}\\nchain = APIChain.from_llm_and_api_docs(llm, tmdb_docs.TMDB_DOCS, headers=headers, verbose=True)\\n```  \\n```python\\nchain.run(\"Search for \\'Avatar\\'\")\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```  \\n> Entering new APIChain chain...\\nhttps://api.themoviedb.org/3/search/movie?query=Avatar&language=en-US\\n{\"page\":1,\"results\":[{\"adult\":false,\"backdrop_path\":\"/o0s4XsEDfDlvit5pDRKjzXR4pp2.jpg\",\"genre_ids\":[28,12,14,878],\"id\":19995,\"original_language\":\"en\",\"original_title\":\"Avatar\",\"overview\":\"In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but becomes torn between following orders and protecting an alien civilization.\",\"popularity\":2041.691,\"poster_path\":\"/jRXYjXNq0Cs2TcJjLkki24MLp7u.jpg\",\"release_date\":\"2009-12-15\",\"title\":\"Avatar\",\"video\":false,\"vote_average\":7.6,\"vote_count\":27777},{\"adult\":false,\"backdrop_path\":\"/s16H6tpK2utvwDtzZ8Qy4qm5Emw.jpg\",\"genre_ids\":[878,12,28],\"id\":76600,\"original_language\":\"en\",\"original_title\":\"Avatar: The Way of Water\",\"overview\":\"Set more than a decade after the events of the first film, learn the story of the Sully family (Jake, Neytiri, and their kids), the trouble that follows them, the lengths they go to keep each other safe, the battles they fight to stay alive, and the tragedies they endure.\",\"popularity\":3948.296,\"poster_path\":\"/t6HIqrRAclMCA60NsSmeqe9RmNV.jpg\",\"release_date\":\"2022-12-14\",\"title\":\"Avatar: The Way of Water\",\"video\":false,\"vote_average\":7.7,\"vote_count\":4219},{\"adult\":false,\"backdrop_path\":\"/uEwGFGtao9YG2JolmdvtHLLVbA9.jpg\",\"genre_ids\":[99],\"id\":111332,\"original_language\":\"en\",\"original_title\":\"Avatar: Creating the World of Pandora\",\"overview\":\"The Making-of James Cameron\\'s Avatar. It shows interesting parts of the work on the set.\",\"popularity\":541.809,\"poster_path\":\"/sjf3xjuofCtDhZghJRzXlTiEjJe.jpg\",\"release_date\":\"2010-02-07\",\"title\":\"Avatar: Creating the World of Pandora\",\"video\":false,\"vote_average\":7.3,\"vote_count\":35},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[99],\"id\":287003,\"original_language\":\"en\",\"original_title\":\"Avatar: Scene Deconstruction\",\"overview\":\"The deconstruction of the Avatar scenes and sets\",\"popularity\":394.941,\"poster_path\":\"/uCreCQFReeF0RiIXkQypRYHwikx.jpg\",\"release_date\":\"2009-12-18\",\"title\":\"Avatar: Scene Deconstruction\",\"video\":false,\"vote_average\":7.8,\"vote_count\":12},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[28,18,878,12,14],\"id\":83533,\"original_language\":\"en\",\"original_title\":\"Avatar 3\",\"overview\":\"\",\"popularity\":172.488,\"poster_path\":\"/4rXqTMlkEaMiJjiG0Z2BX6F6Dkm.jpg\",\"release_date\":\"2024-12-18\",\"title\":\"Avatar 3\",\"video\":false,\"vote_average\":0,\"vote_count\":0},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[28,878,12,14],\"id\":216527,\"original_language\":\"en\",\"original_title\":\"Avatar 4\",\"overview\":\"\",\"popularity\":162.536,\"poster_path\":\"/qzMYKnT4MG1d0gnhwytr4cKhUvS.jpg\",\"release_date\":\"2026-12-16\",\"title\":\"Avatar 4\",\"video\":false,\"vote_average\":0,\"vote_count\":0},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[28,12,14,878],\"id\":393209,\"original_language\":\"en\",\"original_title\":\"Avatar 5\",\"overview\":\"\",\"popularity\":124.722,\"poster_path\":\"/rtmmvqkIC5zDMEd638Es2woxbz8.jpg\",\"release_date\":\"2028-12-20\",\"title\":\"Avatar 5\",\"video\":false,\"vote_average\":0,\"vote_count\":0},{\"adult\":false,\"backdrop_path\":\"/nNceJtrrovG1MUBHMAhId0ws9Gp.jpg\",\"genre_ids\":[99],\"id\":183392,\"original_language\":\"en\",\"original_title\":\"Capturing Avatar\",\"overview\":\"Capturing Avatar is a feature length behind-the-scenes documentary about the making of Avatar. It uses footage from the film\\'s development, as well as stock footage from as far back as the production of Titanic in 1995. Also included are numerous interviews with cast, artists, and other crew members. The documentary was released as a bonus feature on the extended collector\\'s edition of Avatar.\",\"popularity\":109.842,\"poster_path\":\"/26SMEXJl3978dn2svWBSqHbLl5U.jpg\",\"release_date\":\"2010-11-16\",\"title\":\"Capturing Avatar\",\"video\":false,\"vote_average\":7.8,\"vote_count\":39},{\"adult\":false,\"backdrop_path\":\"/eoAvHxfbaPOcfiQyjqypWIXWxDr.jpg\",\"genre_ids\":[99],\"id\":1059673,\"original_language\":\"en\",\"original_title\":\"Avatar: The Deep Dive - A Special Edition of 20/20\",\"overview\":\"An inside look at one of the most anticipated movie sequels ever with James Cameron and cast.\",\"popularity\":629.825,\"poster_path\":\"/rtVeIsmeXnpjNbEKnm9Say58XjV.jpg\",\"release_date\":\"2022-12-14\",\"title\":\"Avatar: The Deep Dive - A Special Edition of 20/20\",\"video\":false,\"vote_average\":6.5,\"vote_count\":5},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[99],\"id\":278698,\"original_language\":\"en\",\"original_title\":\"Avatar Spirits\",\"overview\":\"Bryan Konietzko and Michael Dante DiMartino, co-creators of the hit television series, Avatar: The Last Airbender, reflect on the creation of the masterful series.\",\"popularity\":51.593,\"poster_path\":\"/oBWVyOdntLJd5bBpE0wkpN6B6vy.jpg\",\"release_date\":\"2010-06-22\",\"title\":\"Avatar Spirits\",\"video\":false,\"vote_average\":9,\"vote_count\":16},{\"adult\":false,\"backdrop_path\":\"/cACUWJKvRfhXge7NC0xxoQnkQNu.jpg\",\"genre_ids\":[10402],\"id\":993545,\"original_language\":\"fr\",\"original_title\":\"Avatar - Au Hellfest 2022\",\"overview\":\"\",\"popularity\":21.992,\"poster_path\":\"/fw6cPIsQYKjd1YVQanG2vLc5HGo.jpg\",\"release_date\":\"2022-06-26\",\"title\":\"Avatar - Au Hellfest 2022\",\"video\":false,\"vote_average\":8,\"vote_count\":4},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[],\"id\":931019,\"original_language\":\"en\",\"original_title\":\"Avatar: Enter The World\",\"overview\":\"A behind the scenes look at the new James Cameron blockbuster “Avatar”, which stars Aussie Sam Worthington. Hastily produced by Australia’s Nine Network following the film’s release.\",\"popularity\":30.903,\"poster_path\":\"/9MHY9pYAgs91Ef7YFGWEbP4WJqC.jpg\",\"release_date\":\"2009-12-05\",\"title\":\"Avatar: Enter The World\",\"video\":false,\"vote_average\":2,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[],\"id\":287004,\"original_language\":\"en\",\"original_title\":\"Avatar: Production Materials\",\"overview\":\"Production material overview of what was used in Avatar\",\"popularity\":12.389,\"poster_path\":null,\"release_date\":\"2009-12-18\",\"title\":\"Avatar: Production Materials\",\"video\":true,\"vote_average\":6,\"vote_count\":4},{\"adult\":false,\"backdrop_path\":\"/x43RWEZg9tYRPgnm43GyIB4tlER.jpg\",\"genre_ids\":[],\"id\":740017,\"original_language\":\"es\",\"original_title\":\"Avatar: Agni Kai\",\"overview\":\"\",\"popularity\":9.462,\"poster_path\":\"/y9PrKMUTA6NfIe5FE92tdwOQ2sH.jpg\",\"release_date\":\"2020-01-18\",\"title\":\"Avatar: Agni Kai\",\"video\":false,\"vote_average\":7,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":\"/e8mmDO7fKK93T4lnxl4Z2zjxXZV.jpg\",\"genre_ids\":[],\"id\":668297,\"original_language\":\"en\",\"original_title\":\"The Last Avatar\",\"overview\":\"The Last Avatar is a mystical adventure film, a story of a young man who leaves Hollywood to find himself. What he finds is beyond his wildest imagination. Based on ancient prophecy, contemporary truth seeking and the future of humanity, The Last Avatar is a film that takes transformational themes and makes them relevant for audiences of all ages. Filled with love, magic, mystery, conspiracy, psychics, underground cities, secret societies, light bodies and much more, The Last Avatar tells the story of the emergence of Kalki Avatar- the final Avatar of our current Age of Chaos. Kalki is also a metaphor for the innate power and potential that lies within humanity to awaken and create a world of truth, harmony and possibility.\",\"popularity\":8.786,\"poster_path\":\"/XWz5SS5g5mrNEZjv3FiGhqCMOQ.jpg\",\"release_date\":\"2014-12-06\",\"title\":\"The Last Avatar\",\"video\":false,\"vote_average\":4.5,\"vote_count\":2},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[],\"id\":424768,\"original_language\":\"en\",\"original_title\":\"Avatar:[2015] Wacken Open Air\",\"overview\":\"Started in the summer of 2001 by drummer John Alfredsson and vocalist Christian Rimmi under the name Lost Soul.  The band offers a free mp3 download to a song called \\\\\"Bloody Knuckles\\\\\" if one subscribes to their newsletter.  In 2005 they appeared on the compilation “Listen to Your Inner Voice” together with 17 other bands released by Inner Voice Records.\",\"popularity\":6.634,\"poster_path\":null,\"release_date\":\"2015-08-01\",\"title\":\"Avatar:[2015] Wacken Open Air\",\"video\":false,\"vote_average\":8,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[],\"id\":812836,\"original_language\":\"en\",\"original_title\":\"Avatar - Live At Graspop 2018\",\"overview\":\"Live At Graspop Festival Belgium 2018\",\"popularity\":9.855,\"poster_path\":null,\"release_date\":\"\",\"title\":\"Avatar - Live At Graspop 2018\",\"video\":false,\"vote_average\":9,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[10402],\"id\":874770,\"original_language\":\"en\",\"original_title\":\"Avatar Ages: Memories\",\"overview\":\"On the night of memories Avatar performed songs from Thoughts of No Tomorrow, Schlacht and Avatar as voted on by the fans.\",\"popularity\":2.66,\"poster_path\":\"/xDNNQ2cnxAv3o7u0nT6JJacQrhp.jpg\",\"release_date\":\"2021-01-30\",\"title\":\"Avatar Ages: Memories\",\"video\":false,\"vote_average\":10,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":null,\"genre_ids\":[10402],\"id\":874768,\"original_language\":\"en\",\"original_title\":\"Avatar Ages: Madness\",\"overview\":\"On the night of madness Avatar performed songs from Black Waltz and Hail The Apocalypse as voted on by the fans.\",\"popularity\":2.024,\"poster_path\":\"/wVyTuruUctV3UbdzE5cncnpyNoY.jpg\",\"release_date\":\"2021-01-23\",\"title\":\"Avatar Ages: Madness\",\"video\":false,\"vote_average\":8,\"vote_count\":1},{\"adult\":false,\"backdrop_path\":\"/dj8g4jrYMfK6tQ26ra3IaqOx5Ho.jpg\",\"genre_ids\":[10402],\"id\":874700,\"original_language\":\"en\",\"original_title\":\"Avatar Ages: Dreams\",\"overview\":\"On the night of dreams Avatar performed Hunter Gatherer in its entirety, plus a selection of their most popular songs.  Originally aired January 9th 2021\",\"popularity\":1.957,\"poster_path\":\"/4twG59wnuHpGIRR9gYsqZnVysSP.jpg\",\"release_date\":\"2021-01-09\",\"title\":\"Avatar Ages: Dreams\",\"video\":false,\"vote_average\":0,\"vote_count\":0}],\"total_pages\":3,\"total_results\":57}  \\n> Finished chain.  \\n\\' This response contains 57 movies related to the search query \"Avatar\". The first movie in the list is the 2009 movie \"Avatar\" starring Sam Worthington. Other movies in the list include sequels to Avatar, documentaries, and live performances.\\'\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'TMDB Example'}),\n",
       " Document(page_content='```python\\nimport os\\nfrom langchain.llms import OpenAI\\nfrom langchain.chains.api import podcast_docs\\nfrom langchain.chains import APIChain', metadata={'Header 2': 'Listen API Example'}),\n",
       " Document(page_content='listen_api_key = \\'xxx\\'  \\nllm = OpenAI(temperature=0)\\nheaders = {\"X-ListenAPI-Key\": listen_api_key}\\nchain = APIChain.from_llm_and_api_docs(llm, podcast_docs.PODCAST_DOCS, headers=headers, verbose=True)\\nchain.run(\"Search for \\'silicon valley bank\\' podcast episodes, audio length is more than 30 minutes, return only 1 results\")\\n```', metadata={'Header 1': 'Get api key here: https://www.listennotes.com/api/pricing/'}),\n",
       " Document(page_content='The simplest loader reads in a file as text and places it all into one Document.  \\n```python\\nfrom langchain.document_loaders import TextLoader  \\nloader = TextLoader(\"./index.md\")\\nloader.load()\\n```  \\n<CodeOutputBlock language=\"python\">  \\n```\\n[\\nDocument(page_content=\\'---\\\\nsidebar_position: 0\\\\n---\\\\n# Document loaders\\\\n\\\\nUse document loaders to load data from a source as `Document`\\\\\\'s. A `Document` is a piece of text\\\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\\\ncontents of any web page, or even for loading a transcript of a YouTube video.\\\\n\\\\nEvery document loader exposes two methods:\\\\n1. \"Load\": load documents from the configured source\\\\n2. \"Load and split\": load documents from the configured source and split them using the passed in text splitter\\\\n\\\\nThey optionally implement:\\\\n\\\\n3. \"Lazy load\": load documents into memory lazily\\\\n\\', metadata={\\'source\\': \\'../docs/docs_skeleton/docs/modules/data_connection/document_loaders/index.md\\'})\\n]\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='```python\\nfrom langchain.document_loaders.csv_loader import CSVLoader  \\nloader = CSVLoader(file_path=\\'./example_data/mlb_teams_2012.csv\\')\\ndata = loader.load()\\n```  \\n```python\\nprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'Team: Nationals\\\\n\"Payroll (millions)\": 81.34\\\\n\"Wins\": 98\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 0}, lookup_index=0), Document(page_content=\\'Team: Reds\\\\n\"Payroll (millions)\": 82.20\\\\n\"Wins\": 97\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 1}, lookup_index=0), Document(page_content=\\'Team: Yankees\\\\n\"Payroll (millions)\": 197.96\\\\n\"Wins\": 95\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 2}, lookup_index=0), Document(page_content=\\'Team: Giants\\\\n\"Payroll (millions)\": 117.62\\\\n\"Wins\": 94\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 3}, lookup_index=0), Document(page_content=\\'Team: Braves\\\\n\"Payroll (millions)\": 83.31\\\\n\"Wins\": 94\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 4}, lookup_index=0), Document(page_content=\\'Team: Athletics\\\\n\"Payroll (millions)\": 55.37\\\\n\"Wins\": 94\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 5}, lookup_index=0), Document(page_content=\\'Team: Rangers\\\\n\"Payroll (millions)\": 120.51\\\\n\"Wins\": 93\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 6}, lookup_index=0), Document(page_content=\\'Team: Orioles\\\\n\"Payroll (millions)\": 81.43\\\\n\"Wins\": 93\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 7}, lookup_index=0), Document(page_content=\\'Team: Rays\\\\n\"Payroll (millions)\": 64.17\\\\n\"Wins\": 90\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 8}, lookup_index=0), Document(page_content=\\'Team: Angels\\\\n\"Payroll (millions)\": 154.49\\\\n\"Wins\": 89\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 9}, lookup_index=0), Document(page_content=\\'Team: Tigers\\\\n\"Payroll (millions)\": 132.30\\\\n\"Wins\": 88\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 10}, lookup_index=0), Document(page_content=\\'Team: Cardinals\\\\n\"Payroll (millions)\": 110.30\\\\n\"Wins\": 88\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 11}, lookup_index=0), Document(page_content=\\'Team: Dodgers\\\\n\"Payroll (millions)\": 95.14\\\\n\"Wins\": 86\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 12}, lookup_index=0), Document(page_content=\\'Team: White Sox\\\\n\"Payroll (millions)\": 96.92\\\\n\"Wins\": 85\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 13}, lookup_index=0), Document(page_content=\\'Team: Brewers\\\\n\"Payroll (millions)\": 97.65\\\\n\"Wins\": 83\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 14}, lookup_index=0), Document(page_content=\\'Team: Phillies\\\\n\"Payroll (millions)\": 174.54\\\\n\"Wins\": 81\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 15}, lookup_index=0), Document(page_content=\\'Team: Diamondbacks\\\\n\"Payroll (millions)\": 74.28\\\\n\"Wins\": 81\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 16}, lookup_index=0), Document(page_content=\\'Team: Pirates\\\\n\"Payroll (millions)\": 63.43\\\\n\"Wins\": 79\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 17}, lookup_index=0), Document(page_content=\\'Team: Padres\\\\n\"Payroll (millions)\": 55.24\\\\n\"Wins\": 76\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 18}, lookup_index=0), Document(page_content=\\'Team: Mariners\\\\n\"Payroll (millions)\": 81.97\\\\n\"Wins\": 75\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 19}, lookup_index=0), Document(page_content=\\'Team: Mets\\\\n\"Payroll (millions)\": 93.35\\\\n\"Wins\": 74\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 20}, lookup_index=0), Document(page_content=\\'Team: Blue Jays\\\\n\"Payroll (millions)\": 75.48\\\\n\"Wins\": 73\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 21}, lookup_index=0), Document(page_content=\\'Team: Royals\\\\n\"Payroll (millions)\": 60.91\\\\n\"Wins\": 72\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 22}, lookup_index=0), Document(page_content=\\'Team: Marlins\\\\n\"Payroll (millions)\": 118.07\\\\n\"Wins\": 69\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 23}, lookup_index=0), Document(page_content=\\'Team: Red Sox\\\\n\"Payroll (millions)\": 173.18\\\\n\"Wins\": 69\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 24}, lookup_index=0), Document(page_content=\\'Team: Indians\\\\n\"Payroll (millions)\": 78.43\\\\n\"Wins\": 68\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 25}, lookup_index=0), Document(page_content=\\'Team: Twins\\\\n\"Payroll (millions)\": 94.08\\\\n\"Wins\": 66\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 26}, lookup_index=0), Document(page_content=\\'Team: Rockies\\\\n\"Payroll (millions)\": 78.06\\\\n\"Wins\": 64\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 27}, lookup_index=0), Document(page_content=\\'Team: Cubs\\\\n\"Payroll (millions)\": 88.19\\\\n\"Wins\": 61\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 28}, lookup_index=0), Document(page_content=\\'Team: Astros\\\\n\"Payroll (millions)\": 60.65\\\\n\"Wins\": 55\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 29}, lookup_index=0)]\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='See the [csv module](https://docs.python.org/3/library/csv.html) documentation for more information of what csv args are supported.  \\n```python\\nloader = CSVLoader(file_path=\\'./example_data/mlb_teams_2012.csv\\', csv_args={\\n\\'delimiter\\': \\',\\',\\n\\'quotechar\\': \\'\"\\',\\n\\'fieldnames\\': [\\'MLB Team\\', \\'Payroll in millions\\', \\'Wins\\']\\n})  \\ndata = loader.load()\\n```  \\n```python\\nprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'MLB Team: Team\\\\nPayroll in millions: \"Payroll (millions)\"\\\\nWins: \"Wins\"\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 0}, lookup_index=0), Document(page_content=\\'MLB Team: Nationals\\\\nPayroll in millions: 81.34\\\\nWins: 98\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 1}, lookup_index=0), Document(page_content=\\'MLB Team: Reds\\\\nPayroll in millions: 82.20\\\\nWins: 97\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 2}, lookup_index=0), Document(page_content=\\'MLB Team: Yankees\\\\nPayroll in millions: 197.96\\\\nWins: 95\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 3}, lookup_index=0), Document(page_content=\\'MLB Team: Giants\\\\nPayroll in millions: 117.62\\\\nWins: 94\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 4}, lookup_index=0), Document(page_content=\\'MLB Team: Braves\\\\nPayroll in millions: 83.31\\\\nWins: 94\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 5}, lookup_index=0), Document(page_content=\\'MLB Team: Athletics\\\\nPayroll in millions: 55.37\\\\nWins: 94\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 6}, lookup_index=0), Document(page_content=\\'MLB Team: Rangers\\\\nPayroll in millions: 120.51\\\\nWins: 93\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 7}, lookup_index=0), Document(page_content=\\'MLB Team: Orioles\\\\nPayroll in millions: 81.43\\\\nWins: 93\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 8}, lookup_index=0), Document(page_content=\\'MLB Team: Rays\\\\nPayroll in millions: 64.17\\\\nWins: 90\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 9}, lookup_index=0), Document(page_content=\\'MLB Team: Angels\\\\nPayroll in millions: 154.49\\\\nWins: 89\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 10}, lookup_index=0), Document(page_content=\\'MLB Team: Tigers\\\\nPayroll in millions: 132.30\\\\nWins: 88\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 11}, lookup_index=0), Document(page_content=\\'MLB Team: Cardinals\\\\nPayroll in millions: 110.30\\\\nWins: 88\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 12}, lookup_index=0), Document(page_content=\\'MLB Team: Dodgers\\\\nPayroll in millions: 95.14\\\\nWins: 86\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 13}, lookup_index=0), Document(page_content=\\'MLB Team: White Sox\\\\nPayroll in millions: 96.92\\\\nWins: 85\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 14}, lookup_index=0), Document(page_content=\\'MLB Team: Brewers\\\\nPayroll in millions: 97.65\\\\nWins: 83\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 15}, lookup_index=0), Document(page_content=\\'MLB Team: Phillies\\\\nPayroll in millions: 174.54\\\\nWins: 81\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 16}, lookup_index=0), Document(page_content=\\'MLB Team: Diamondbacks\\\\nPayroll in millions: 74.28\\\\nWins: 81\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 17}, lookup_index=0), Document(page_content=\\'MLB Team: Pirates\\\\nPayroll in millions: 63.43\\\\nWins: 79\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 18}, lookup_index=0), Document(page_content=\\'MLB Team: Padres\\\\nPayroll in millions: 55.24\\\\nWins: 76\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 19}, lookup_index=0), Document(page_content=\\'MLB Team: Mariners\\\\nPayroll in millions: 81.97\\\\nWins: 75\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 20}, lookup_index=0), Document(page_content=\\'MLB Team: Mets\\\\nPayroll in millions: 93.35\\\\nWins: 74\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 21}, lookup_index=0), Document(page_content=\\'MLB Team: Blue Jays\\\\nPayroll in millions: 75.48\\\\nWins: 73\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 22}, lookup_index=0), Document(page_content=\\'MLB Team: Royals\\\\nPayroll in millions: 60.91\\\\nWins: 72\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 23}, lookup_index=0), Document(page_content=\\'MLB Team: Marlins\\\\nPayroll in millions: 118.07\\\\nWins: 69\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 24}, lookup_index=0), Document(page_content=\\'MLB Team: Red Sox\\\\nPayroll in millions: 173.18\\\\nWins: 69\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 25}, lookup_index=0), Document(page_content=\\'MLB Team: Indians\\\\nPayroll in millions: 78.43\\\\nWins: 68\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 26}, lookup_index=0), Document(page_content=\\'MLB Team: Twins\\\\nPayroll in millions: 94.08\\\\nWins: 66\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 27}, lookup_index=0), Document(page_content=\\'MLB Team: Rockies\\\\nPayroll in millions: 78.06\\\\nWins: 64\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 28}, lookup_index=0), Document(page_content=\\'MLB Team: Cubs\\\\nPayroll in millions: 88.19\\\\nWins: 61\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 29}, lookup_index=0), Document(page_content=\\'MLB Team: Astros\\\\nPayroll in millions: 60.65\\\\nWins: 55\\', lookup_str=\\'\\', metadata={\\'source\\': \\'./example_data/mlb_teams_2012.csv\\', \\'row\\': 30}, lookup_index=0)]\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Customizing the csv parsing and loading'}),\n",
       " Document(page_content='Use the `source_column` argument to specify a source for the document created from each row. Otherwise `file_path` will be used as the source for all documents created from the CSV file.  \\nThis is useful when using documents loaded from CSV files for chains that answer questions using sources.  \\n```python\\nloader = CSVLoader(file_path=\\'./example_data/mlb_teams_2012.csv\\', source_column=\"Team\")  \\ndata = loader.load()\\n```  \\n```python\\nprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'Team: Nationals\\\\n\"Payroll (millions)\": 81.34\\\\n\"Wins\": 98\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Nationals\\', \\'row\\': 0}, lookup_index=0), Document(page_content=\\'Team: Reds\\\\n\"Payroll (millions)\": 82.20\\\\n\"Wins\": 97\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Reds\\', \\'row\\': 1}, lookup_index=0), Document(page_content=\\'Team: Yankees\\\\n\"Payroll (millions)\": 197.96\\\\n\"Wins\": 95\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Yankees\\', \\'row\\': 2}, lookup_index=0), Document(page_content=\\'Team: Giants\\\\n\"Payroll (millions)\": 117.62\\\\n\"Wins\": 94\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Giants\\', \\'row\\': 3}, lookup_index=0), Document(page_content=\\'Team: Braves\\\\n\"Payroll (millions)\": 83.31\\\\n\"Wins\": 94\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Braves\\', \\'row\\': 4}, lookup_index=0), Document(page_content=\\'Team: Athletics\\\\n\"Payroll (millions)\": 55.37\\\\n\"Wins\": 94\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Athletics\\', \\'row\\': 5}, lookup_index=0), Document(page_content=\\'Team: Rangers\\\\n\"Payroll (millions)\": 120.51\\\\n\"Wins\": 93\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Rangers\\', \\'row\\': 6}, lookup_index=0), Document(page_content=\\'Team: Orioles\\\\n\"Payroll (millions)\": 81.43\\\\n\"Wins\": 93\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Orioles\\', \\'row\\': 7}, lookup_index=0), Document(page_content=\\'Team: Rays\\\\n\"Payroll (millions)\": 64.17\\\\n\"Wins\": 90\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Rays\\', \\'row\\': 8}, lookup_index=0), Document(page_content=\\'Team: Angels\\\\n\"Payroll (millions)\": 154.49\\\\n\"Wins\": 89\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Angels\\', \\'row\\': 9}, lookup_index=0), Document(page_content=\\'Team: Tigers\\\\n\"Payroll (millions)\": 132.30\\\\n\"Wins\": 88\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Tigers\\', \\'row\\': 10}, lookup_index=0), Document(page_content=\\'Team: Cardinals\\\\n\"Payroll (millions)\": 110.30\\\\n\"Wins\": 88\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Cardinals\\', \\'row\\': 11}, lookup_index=0), Document(page_content=\\'Team: Dodgers\\\\n\"Payroll (millions)\": 95.14\\\\n\"Wins\": 86\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Dodgers\\', \\'row\\': 12}, lookup_index=0), Document(page_content=\\'Team: White Sox\\\\n\"Payroll (millions)\": 96.92\\\\n\"Wins\": 85\\', lookup_str=\\'\\', metadata={\\'source\\': \\'White Sox\\', \\'row\\': 13}, lookup_index=0), Document(page_content=\\'Team: Brewers\\\\n\"Payroll (millions)\": 97.65\\\\n\"Wins\": 83\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Brewers\\', \\'row\\': 14}, lookup_index=0), Document(page_content=\\'Team: Phillies\\\\n\"Payroll (millions)\": 174.54\\\\n\"Wins\": 81\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Phillies\\', \\'row\\': 15}, lookup_index=0), Document(page_content=\\'Team: Diamondbacks\\\\n\"Payroll (millions)\": 74.28\\\\n\"Wins\": 81\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Diamondbacks\\', \\'row\\': 16}, lookup_index=0), Document(page_content=\\'Team: Pirates\\\\n\"Payroll (millions)\": 63.43\\\\n\"Wins\": 79\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Pirates\\', \\'row\\': 17}, lookup_index=0), Document(page_content=\\'Team: Padres\\\\n\"Payroll (millions)\": 55.24\\\\n\"Wins\": 76\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Padres\\', \\'row\\': 18}, lookup_index=0), Document(page_content=\\'Team: Mariners\\\\n\"Payroll (millions)\": 81.97\\\\n\"Wins\": 75\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Mariners\\', \\'row\\': 19}, lookup_index=0), Document(page_content=\\'Team: Mets\\\\n\"Payroll (millions)\": 93.35\\\\n\"Wins\": 74\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Mets\\', \\'row\\': 20}, lookup_index=0), Document(page_content=\\'Team: Blue Jays\\\\n\"Payroll (millions)\": 75.48\\\\n\"Wins\": 73\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Blue Jays\\', \\'row\\': 21}, lookup_index=0), Document(page_content=\\'Team: Royals\\\\n\"Payroll (millions)\": 60.91\\\\n\"Wins\": 72\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Royals\\', \\'row\\': 22}, lookup_index=0), Document(page_content=\\'Team: Marlins\\\\n\"Payroll (millions)\": 118.07\\\\n\"Wins\": 69\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Marlins\\', \\'row\\': 23}, lookup_index=0), Document(page_content=\\'Team: Red Sox\\\\n\"Payroll (millions)\": 173.18\\\\n\"Wins\": 69\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Red Sox\\', \\'row\\': 24}, lookup_index=0), Document(page_content=\\'Team: Indians\\\\n\"Payroll (millions)\": 78.43\\\\n\"Wins\": 68\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Indians\\', \\'row\\': 25}, lookup_index=0), Document(page_content=\\'Team: Twins\\\\n\"Payroll (millions)\": 94.08\\\\n\"Wins\": 66\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Twins\\', \\'row\\': 26}, lookup_index=0), Document(page_content=\\'Team: Rockies\\\\n\"Payroll (millions)\": 78.06\\\\n\"Wins\": 64\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Rockies\\', \\'row\\': 27}, lookup_index=0), Document(page_content=\\'Team: Cubs\\\\n\"Payroll (millions)\": 88.19\\\\n\"Wins\": 61\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Cubs\\', \\'row\\': 28}, lookup_index=0), Document(page_content=\\'Team: Astros\\\\n\"Payroll (millions)\": 60.65\\\\n\"Wins\": 55\\', lookup_str=\\'\\', metadata={\\'source\\': \\'Astros\\', \\'row\\': 29}, lookup_index=0)]\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Specify a column to identify the document source'}),\n",
       " Document(page_content='```python\\nfrom langchain.document_loaders import UnstructuredHTMLLoader\\n```  \\n```python\\nloader = UnstructuredHTMLLoader(\"example_data/fake-content.html\")\\n```  \\n```python\\ndata = loader.load()\\n```  \\n```python\\ndata\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'My First Heading\\\\n\\\\nMy first paragraph.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'example_data/fake-content.html\\'}, lookup_index=0)]\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='We can also use `BeautifulSoup4` to load HTML documents using the `BSHTMLLoader`.  This will extract the text from the HTML into `page_content`, and the page title as `title` into `metadata`.  \\n```python\\nfrom langchain.document_loaders import BSHTMLLoader\\n```  \\n```python\\nloader = BSHTMLLoader(\"example_data/fake-content.html\")\\ndata = loader.load()\\ndata\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'\\\\n\\\\nTest Title\\\\n\\\\n\\\\nMy First Heading\\\\nMy first paragraph.\\\\n\\\\n\\\\n\\', metadata={\\'source\\': \\'example_data/fake-content.html\\', \\'title\\': \\'Test Title\\'})]\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Loading HTML with BeautifulSoup4'}),\n",
       " Document(page_content='To start we\\'ll need to install the OpenAI Python package:  \\n```bash\\npip install openai\\n```  \\nAccessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we\\'ll want to set it as an environment variable by running:  \\n```bash\\nexport OPENAI_API_KEY=\"...\"\\n```  \\nIf you\\'d prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:  \\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings  \\nembeddings_model = OpenAIEmbeddings(openai_api_key=\"...\")\\n```  \\notherwise you can initialize without any params:\\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings  \\nembeddings_model = OpenAIEmbeddings()\\n```', metadata={'Header 3': 'Setup'}),\n",
       " Document(page_content='#### Embed list of texts  \\n```python\\nembeddings = embeddings_model.embed_documents(\\n[\\n\"Hi there!\",\\n\"Oh, hello!\",\\n\"What\\'s your name?\",\\n\"My friends call me World\",\\n\"Hello World!\"\\n]\\n)\\nlen(embeddings), len(embeddings[0])\\n```  \\n<CodeOutputBlock language=\"python\">  \\n```\\n(5, 1536)\\n```  \\n</CodeOutputBlock>', metadata={'Header 3': '`embed_documents`'}),\n",
       " Document(page_content='#### Embed single query\\nEmbed a single piece of text for the purpose of comparing to other embedded pieces of texts.  \\n```python\\nembedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\\nembedded_query[:5]\\n```  \\n<CodeOutputBlock language=\"python\">  \\n```\\n[0.0053587136790156364,\\n-0.0004999046213924885,\\n0.038883671164512634,\\n-0.003001077566295862,\\n-0.00900818221271038]\\n```  \\n</CodeOutputBlock>', metadata={'Header 3': '`embed_query`'}),\n",
       " Document(page_content='```python', metadata={}),\n",
       " Document(page_content='```  \\n```python\\nfrom langchain.document_loaders import UnstructuredMarkdownLoader\\n```  \\n```python\\nmarkdown_path = \"../../../../../README.md\"\\nloader = UnstructuredMarkdownLoader(markdown_path)\\n```  \\n```python\\ndata = loader.load()\\n```  \\n```python\\ndata\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\"ð\\\\x9f¦\\\\x9cï¸\\\\x8fð\\\\x9f”\\\\x97 LangChain\\\\n\\\\nâ\\\\x9a¡ Building applications with LLMs through composability â\\\\x9a¡\\\\n\\\\nLooking for the JS/TS version? Check out LangChain.js.\\\\n\\\\nProduction Support: As you move your LangChains into production, we\\'d love to offer more comprehensive support.\\\\nPlease fill out this form and we\\'ll set up a dedicated support Slack channel.\\\\n\\\\nQuick Install\\\\n\\\\npip install langchain\\\\nor\\\\nconda install langchain -c conda-forge\\\\n\\\\nð\\\\x9f¤” What is this?\\\\n\\\\nLarge language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. However, using these LLMs in isolation is often insufficient for creating a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\\\\n\\\\nThis library aims to assist in the development of those types of applications. Common examples of these applications include:\\\\n\\\\nâ\\\\x9d“ Question Answering over specific documents\\\\n\\\\nDocumentation\\\\n\\\\nEnd-to-end Example: Question Answering over Notion Database\\\\n\\\\nð\\\\x9f’¬ Chatbots\\\\n\\\\nDocumentation\\\\n\\\\nEnd-to-end Example: Chat-LangChain\\\\n\\\\nð\\\\x9f¤\\\\x96 Agents\\\\n\\\\nDocumentation\\\\n\\\\nEnd-to-end Example: GPT+WolframAlpha\\\\n\\\\nð\\\\x9f“\\\\x96 Documentation\\\\n\\\\nPlease see here for full documentation on:\\\\n\\\\nGetting started (installation, setting up the environment, simple examples)\\\\n\\\\nHow-To examples (demos, integrations, helper functions)\\\\n\\\\nReference (full API docs)\\\\n\\\\nResources (high-level explanation of core concepts)\\\\n\\\\nð\\\\x9f\\\\x9a\\\\x80 What can this help with?\\\\n\\\\nThere are six main areas that LangChain is designed to help with.\\\\nThese are, in increasing order of complexity:\\\\n\\\\nð\\\\x9f“\\\\x83 LLMs and Prompts:\\\\n\\\\nThis includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\\\\n\\\\nð\\\\x9f”\\\\x97 Chains:\\\\n\\\\nChains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\\\n\\\\nð\\\\x9f“\\\\x9a Data Augmented Generation:\\\\n\\\\nData Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\\\\n\\\\nð\\\\x9f¤\\\\x96 Agents:\\\\n\\\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\\\\n\\\\nð\\\\x9f§\\\\xa0 Memory:\\\\n\\\\nMemory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\\\n\\\\nð\\\\x9f§\\\\x90 Evaluation:\\\\n\\\\n[BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\\\n\\\\nFor more information on these concepts, please see our full documentation.\\\\n\\\\nð\\\\x9f’\\\\x81 Contributing\\\\n\\\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\\\n\\\\nFor detailed information on how to contribute, see here.\", metadata={\\'source\\': \\'../../../../../README.md\\'})]\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': '!pip install unstructured > /dev/null'}),\n",
       " Document(page_content='Under the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying `mode=\"elements\"`.  \\n```python\\nloader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")\\n```  \\n```python\\ndata = loader.load()\\n```  \\n```python\\ndata[0]\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\nDocument(page_content=\\'ð\\\\x9f¦\\\\x9cï¸\\\\x8fð\\\\x9f”\\\\x97 LangChain\\', metadata={\\'source\\': \\'../../../../../README.md\\', \\'page_number\\': 1, \\'category\\': \\'Title\\'})\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': '!pip install unstructured > /dev/null', 'Header 2': 'Retain Elements'}),\n",
       " Document(page_content='>The `JSONLoader` uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files. It uses the `jq` python package.\\nCheck this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.  \\n```python\\n#!pip install jq\\n```  \\n```python\\nfrom langchain.document_loaders import JSONLoader\\n```  \\n```python\\nimport json\\nfrom pathlib import Path\\nfrom pprint import pprint  \\nfile_path=\\'./example_data/facebook_chat.json\\'\\ndata = json.loads(Path(file_path).read_text())\\n```  \\n```python\\npprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n{\\'image\\': {\\'creation_timestamp\\': 1675549016, \\'uri\\': \\'image_of_the_chat.jpg\\'},\\n\\'is_still_participant\\': True,\\n\\'joinable_mode\\': {\\'link\\': \\'\\', \\'mode\\': 1},\\n\\'magic_words\\': [],\\n\\'messages\\': [{\\'content\\': \\'Bye!\\',\\n\\'sender_name\\': \\'User 2\\',\\n\\'timestamp_ms\\': 1675597571851},\\n{\\'content\\': \\'Oh no worries! Bye\\',\\n\\'sender_name\\': \\'User 1\\',\\n\\'timestamp_ms\\': 1675597435669},\\n{\\'content\\': \\'No Im sorry it was my mistake, the blue one is not \\'\\n\\'for sale\\',\\n\\'sender_name\\': \\'User 2\\',\\n\\'timestamp_ms\\': 1675596277579},\\n{\\'content\\': \\'I thought you were selling the blue one!\\',\\n\\'sender_name\\': \\'User 1\\',\\n\\'timestamp_ms\\': 1675595140251},\\n{\\'content\\': \\'Im not interested in this bag. Im interested in the \\'\\n\\'blue one!\\',\\n\\'sender_name\\': \\'User 1\\',\\n\\'timestamp_ms\\': 1675595109305},\\n{\\'content\\': \\'Here is $129\\',\\n\\'sender_name\\': \\'User 2\\',\\n\\'timestamp_ms\\': 1675595068468},\\n{\\'photos\\': [{\\'creation_timestamp\\': 1675595059,\\n\\'uri\\': \\'url_of_some_picture.jpg\\'}],\\n\\'sender_name\\': \\'User 2\\',\\n\\'timestamp_ms\\': 1675595060730},\\n{\\'content\\': \\'Online is at least $100\\',\\n\\'sender_name\\': \\'User 2\\',\\n\\'timestamp_ms\\': 1675595045152},\\n{\\'content\\': \\'How much do you want?\\',\\n\\'sender_name\\': \\'User 1\\',\\n\\'timestamp_ms\\': 1675594799696},\\n{\\'content\\': \\'Goodmorning! $50 is too low.\\',\\n\\'sender_name\\': \\'User 2\\',\\n\\'timestamp_ms\\': 1675577876645},\\n{\\'content\\': \\'Hi! Im interested in your bag. Im offering $50. Let \\'\\n\\'me know if you are interested. Thanks!\\',\\n\\'sender_name\\': \\'User 1\\',\\n\\'timestamp_ms\\': 1675549022673}],\\n\\'participants\\': [{\\'name\\': \\'User 1\\'}, {\\'name\\': \\'User 2\\'}],\\n\\'thread_path\\': \\'inbox/User 1 and User 2 chat\\',\\n\\'title\\': \\'User 1 and User 2 chat\\'}\\n```  \\n</CodeOutputBlock>', metadata={}),\n",
       " Document(page_content='Suppose we are interested in extracting the values under the `content` field within the `messages` key of the JSON data. This can easily be done through the `JSONLoader` as shown below.', metadata={'Header 2': 'Using `JSONLoader`'}),\n",
       " Document(page_content='```python\\nloader = JSONLoader(\\nfile_path=\\'./example_data/facebook_chat.json\\',\\njq_schema=\\'.messages[].content\\')  \\ndata = loader.load()\\n```  \\n```python\\npprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1}),\\nDocument(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2}),\\nDocument(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3}),\\nDocument(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4}),\\nDocument(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5}),\\nDocument(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6}),\\nDocument(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7}),\\nDocument(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8}),\\nDocument(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9}),\\nDocument(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10}),\\nDocument(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11})]\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Using `JSONLoader`', 'Header 3': 'JSON file'}),\n",
       " Document(page_content='If you want to load documents from a JSON Lines file, you pass `json_lines=True`\\nand specify `jq_schema` to extract `page_content` from a single JSON object.  \\n```python\\nfile_path = \\'./example_data/facebook_chat_messages.jsonl\\'\\npprint(Path(file_path).read_text())\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n(\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\\\n\\'\\n\\'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no \\'\\n\\'worries! Bye\"}\\\\n\\'\\n\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im \\'\\n\\'sorry it was my mistake, the blue one is not for sale\"}\\\\n\\')\\n```  \\n</CodeOutputBlock>  \\n```python\\nloader = JSONLoader(\\nfile_path=\\'./example_data/facebook_chat_messages.jsonl\\',\\njq_schema=\\'.content\\',\\njson_lines=True)  \\ndata = loader.load()\\n```  \\n```python\\npprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),\\nDocument(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),\\nDocument(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\n```  \\n</CodeOutputBlock>  \\nAnother option is set `jq_schema=\\'.\\'` and provide `content_key`:  \\n```python\\nloader = JSONLoader(\\nfile_path=\\'./example_data/facebook_chat_messages.jsonl\\',\\njq_schema=\\'.\\',\\ncontent_key=\\'sender_name\\',\\njson_lines=True)  \\ndata = loader.load()\\n```  \\n```python\\npprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),\\nDocument(page_content=\\'User 1\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),\\nDocument(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\n```  \\n</CodeOutputBlock>', metadata={'Header 2': 'Using `JSONLoader`', 'Header 3': 'JSON Lines file'}),\n",
       " Document(page_content=\"Generally, we want to include metadata available in the JSON file into the documents that we create from the content.  \\nThe following demonstrates how metadata can be extracted using the `JSONLoader`.  \\nThere are some key changes to be noted. In the previous example where we didn't collect the metadata, we managed to directly specify in the schema where the value for the `page_content` can be extracted from.  \\n```\\n.messages[].content\\n```  \\nIn the current example, we have to tell the loader to iterate over the records in the `messages` field. The jq_schema then has to be:  \\n```\\n.messages[]\\n```  \\nThis allows us to pass the records (dict) into the `metadata_func` that has to be implemented. The `metadata_func` is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final `Document` object.  \\nAdditionally, we now have to explicitly specify in the loader, via the `content_key` argument, the key from the record where the value for the `page_content` needs to be extracted from.  \\n```python\", metadata={'Header 2': 'Extracting metadata'}),\n",
       " Document(page_content='def metadata_func(record: dict, metadata: dict) -> dict:  \\nmetadata[\"sender_name\"] = record.get(\"sender_name\")\\nmetadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")  \\nreturn metadata  \\nloader = JSONLoader(\\nfile_path=\\'./example_data/facebook_chat.json\\',\\njq_schema=\\'.messages[]\\',\\ncontent_key=\"content\",\\nmetadata_func=metadata_func\\n)  \\ndata = loader.load()\\n```  \\n```python\\npprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),\\nDocument(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),\\nDocument(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),\\nDocument(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),\\nDocument(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),\\nDocument(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),\\nDocument(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),\\nDocument(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),\\nDocument(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),\\nDocument(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),\\nDocument(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\n```  \\n</CodeOutputBlock>  \\nNow, you will see that the documents contain the metadata associated with the content we extracted.', metadata={'Header 1': 'Define the metadata extraction function.'}),\n",
       " Document(page_content='As shown above, the `metadata_func` accepts the default metadata generated by the `JSONLoader`. This allows full control to the user with respect to how the metadata is formatted.  \\nFor example, the default metadata contains the `source` and the `seq_num` keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the `metadata_func` to rename the default keys and use the ones from the JSON data.  \\nThe example below shows how we can modify the `source` to only contain information of the file source relative to the `langchain` directory.  \\n```python', metadata={'Header 1': 'Define the metadata extraction function.', 'Header 2': 'The `metadata_func`'}),\n",
       " Document(page_content='def metadata_func(record: dict, metadata: dict) -> dict:  \\nmetadata[\"sender_name\"] = record.get(\"sender_name\")\\nmetadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")  \\nif \"source\" in metadata:\\nsource = metadata[\"source\"].split(\"/\")\\nsource = source[source.index(\"langchain\"):]\\nmetadata[\"source\"] = \"/\".join(source)  \\nreturn metadata  \\nloader = JSONLoader(\\nfile_path=\\'./example_data/facebook_chat.json\\',\\njq_schema=\\'.messages[]\\',\\ncontent_key=\"content\",\\nmetadata_func=metadata_func\\n)  \\ndata = loader.load()\\n```  \\n```python\\npprint(data)\\n```  \\n<CodeOutputBlock lang=\"python\">  \\n```\\n[Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),\\nDocument(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),\\nDocument(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),\\nDocument(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),\\nDocument(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),\\nDocument(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),\\nDocument(page_content=\\'\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),\\nDocument(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),\\nDocument(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),\\nDocument(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),\\nDocument(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\n```  \\n</CodeOutputBlock>', metadata={'Header 1': 'Define the metadata extraction function.'}),\n",
       " Document(page_content='The list below provides a reference to the possible `jq_schema` the user can use to extract content from the JSON data depending on the structure.  \\n```\\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]\\njq_schema   -> \".[].text\"  \\nJSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}\\njq_schema   -> \".key[].text\"  \\nJSON        -> [\"...\", \"...\", \"...\"]\\njq_schema   -> \".[]\"\\n```', metadata={'Header 1': 'Define the metadata extraction function.', 'Header 2': 'Common JSON structures with jq schema'}),\n",
       " Document(page_content=\"We'll use a Pinecone vector store in this example.  \\nFirst we'll want to create a `Pinecone` VectorStore and seed it with some data. We've created a small demo set of documents that contain summaries of movies.  \\nTo use Pinecone, you to have `pinecone` package installed and you must have an API key and an Environment. Here are the [installation instructions](https://docs.pinecone.io/docs/quickstart).  \\nNOTE: The self-query retriever requires you to have `lark` package installed.  \\n```python\", metadata={'Header 2': 'Get started'}),\n",
       " Document(page_content='```  \\n```python\\nimport os  \\nimport pinecone  \\npinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENV\"])\\n```  \\n```python\\nfrom langchain.schema import Document\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import Pinecone  \\nembeddings = OpenAIEmbeddings()', metadata={'Header 1': '!pip install lark pinecone-client'}),\n",
       " Document(page_content='pinecone.create_index(\"langchain-self-retriever-demo\", dimension=1536)\\n```  \\n```python\\ndocs = [\\nDocument(page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\", metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": [\"action\", \"science fiction\"]}),\\nDocument(page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\", metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2}),\\nDocument(page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\", metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6}),\\nDocument(page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\", metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3}),\\nDocument(page_content=\"Toys come alive and have a blast doing so\", metadata={\"year\": 1995, \"genre\": \"animated\"}),\\nDocument(page_content=\"Three men walk into the Zone, three men walk out of the Zone\", metadata={\"year\": 1979, \"rating\": 9.9, \"director\": \"Andrei Tarkovsky\", \"genre\": [\"science fiction\", \"thriller\"], \"rating\": 9.9})\\n]\\nvectorstore = Pinecone.from_documents(\\ndocs, embeddings, index_name=\"langchain-self-retriever-demo\"\\n)\\n```', metadata={'Header 1': 'create new index'}),\n",
       " Document(page_content='Now we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.  \\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\\nfrom langchain.chains.query_constructor.base import AttributeInfo  \\nmetadata_field_info=[\\nAttributeInfo(\\nname=\"genre\",\\ndescription=\"The genre of the movie\",\\ntype=\"string or list[string]\",\\n),\\nAttributeInfo(\\nname=\"year\",\\ndescription=\"The year the movie was released\",\\ntype=\"integer\",\\n),\\nAttributeInfo(\\nname=\"director\",\\ndescription=\"The name of the movie director\",\\ntype=\"string\",\\n),\\nAttributeInfo(\\nname=\"rating\",\\ndescription=\"A 1-10 rating for the movie\",\\ntype=\"float\"\\n),\\n]\\ndocument_content_description = \"Brief summary of a movie\"\\nllm = OpenAI(temperature=0)\\nretriever = SelfQueryRetriever.from_llm(llm, vectorstore, document_content_description, metadata_field_info, verbose=True)\\n```', metadata={'Header 1': 'create new index', 'Header 2': 'Creating our self-querying retriever'}),\n",
       " Document(page_content='And now we can try actually using our retriever!  \\n```python', metadata={'Header 1': 'create new index', 'Header 2': 'Testing it out'}),\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_python_doc_loader = DirectoryLoader('langchain', glob=\"**/*.mdx\", \n",
    "                                               loader_cls=TextLoader,\n",
    "                                               show_progress=True, use_multithreading=True)\n",
    "langchain_doc = langchain_python_doc_loader.load()\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = []\n",
    "for markdown_document in langchain_doc:\n",
    "    md_header_splits += markdown_splitter.split_text(markdown_document.page_content)\n",
    "md_header_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Types of `MessagePromptTemplate`\\n\\nLangChain provides different types of `MessagePromptTemplate`. The most commonly used are `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, which create an AI message, system message and human message respectively.\\n\\nHowever, in cases where the chat model supports taking chat message with arbitrary role, you can use `ChatMessagePromptTemplate`, which allows user to specify the role name.\\n\\n\\n```python\\nfrom langchain.prompts import ChatMessagePromptTemplate\\n\\nprompt = \"May the {subject} be with you\"\\n\\nchat_message_prompt = ChatMessagePromptTemplate.from_template(role=\"Jedi\", template=prompt)\\nchat_message_prompt.format(subject=\"force\")\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    ChatMessage(content=\\'May the force be with you\\', additional_kwargs={}, role=\\'Jedi\\')\\n```\\n\\n</CodeOutputBlock>\\n\\nLangChain also provides `MessagesPlaceholder`, which gives you full control of what messages to be rendered during formatting. This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting.\\n\\n\\n```python\\nfrom langchain.prompts import MessagesPlaceholder\\n\\nhuman_prompt = \"Summarize our conversation so far in {word_count} words.\"\\nhuman_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\\n\\nchat_prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=\"conversation\"), human_message_template])\\n```\\n\\n\\n```python\\nhuman_message = HumanMessage(content=\"What is the best way to learn programming?\")\\nai_message = AIMessage(content=\"\"\"\\\\\\n1. Choose a programming language: Decide on a programming language that you want to learn.\\n\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\n\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\\\\\n\"\"\")\\n\\nchat_prompt.format_prompt(conversation=[human_message, ai_message], word_count=\"10\").to_messages()\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    [HumanMessage(content=\\'What is the best way to learn programming?\\', additional_kwargs={}),\\n     AIMessage(content=\\'1. Choose a programming language: Decide on a programming language that you want to learn. \\\\n\\\\n2. Start with the basics: Familiarize yourself with the basic programming concepts such as variables, data types and control structures.\\\\n\\\\n3. Practice, practice, practice: The best way to learn programming is through hands-on experience\\', additional_kwargs={}),\\n     HumanMessage(content=\\'Summarize our conversation so far in 10 words.\\', additional_kwargs={})]\\n```\\n\\n</CodeOutputBlock>\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def num_tokens_from_string(docs: List[Document], encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = 0\n",
    "    max_tokens = 0\n",
    "    for doc in docs:\n",
    "        if len(encoding.encode(doc.page_content)) > max_tokens:\n",
    "            max_tokens = len(encoding.encode(doc.page_content))\n",
    "        num_tokens += len(encoding.encode(doc.page_content))\n",
    "    return num_tokens, max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in source code: 800960, max tokens in source code: 6667, price: 0.080096\n"
     ]
    }
   ],
   "source": [
    "token, max_token = num_tokens_from_string(source_code_doc, \"cl100k_base\")\n",
    "print(f\"Number of tokens in source code: {token}, max tokens in source code: {max_token}, price: {token * 0.0001/1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in source code: 194615, max tokens in source code: 7692, price: 0.0194615\n"
     ]
    }
   ],
   "source": [
    "token, max_token = num_tokens_from_string(md_header_splits, \"cl100k_base\")\n",
    "print(f\"Number of tokens in source code: {token}, max tokens in source code: {max_token}, price: {token * 0.0001/1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID e074983f109dae3f46e6aa30adc1c34f in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID e074983f109dae3f46e6aa30adc1c34f in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID e074983f109dae3f46e6aa30adc1c34f in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Mon, 03 Jul 2023 11:44:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-9roqmv8ggak4gze4heqkdad0', 'openai-processing-ms': '2735', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '794597', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '12.324s', 'x-request-id': 'e074983f109dae3f46e6aa30adc1c34f', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7e0ee057cf10016d-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "# save to disk\n",
    "vectorstore = Chroma.from_documents(documents=langchain_doc,embedding=embeddings_model, persist_directory=\"./chroma_db\")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "question=\"How to use the MultiQueryRetriever?\"\n",
    "num_queries=3\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(),llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Self-querying\\n\\nA self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it\\'s underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\\n\\n![](https://drive.google.com/uc?id=1OQUN-0MJcDUxmPXofgS7MqReEs720pqS)\\n\\nimport Example from \"@snippets/modules/data_connection/retrievers/self_query/get_started.mdx\"\\n\\n<Example/>\\n', metadata={'source': 'langchain/docs/docs_skeleton/docs/modules/data_connection/retrievers/how_to/self_query/index.mdx'}),\n",
       " Document(page_content='---\\nsidebar_position: 4\\n---\\n# Retrievers\\n\\nA retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.\\nA retriever does not need to be able to store documents, only to return (or retrieve) it. Vector stores can be used\\nas the backbone of a retriever, but there are other types of retrievers as well.\\n\\n## Get started\\n\\nimport GetStarted from \"@snippets/modules/data_connection/retrievers/get_started.mdx\"\\n\\n<GetStarted/>\\n\\n', metadata={'source': 'langchain/docs/docs_skeleton/docs/modules/data_connection/retrievers/index.mdx'}),\n",
       " Document(page_content='# Dynamically selecting from multiple retrievers\\n\\nThis notebook demonstrates how to use the `RouterChain` paradigm to create a chain that dynamically selects which Retrieval system to use. Specifically we show how to use the `MultiRetrievalQAChain` to create a question-answering chain that selects the retrieval QA chain which is most relevant for a given question, and then answers the question using it.\\n\\nimport Example from \"@snippets/modules/chains/additional/multi_retrieval_qa_router.mdx\"\\n\\n<Example/>\\n', metadata={'source': 'langchain/docs/docs_skeleton/docs/modules/chains/additional/multi_retrieval_qa_router.mdx'}),\n",
       " Document(page_content='The public API of the `BaseRetriever` class in LangChain is as follows:\\n\\n```python\\nfrom abc import ABC, abstractmethod\\nfrom typing import Any, List\\nfrom langchain.schema import Document\\nfrom langchain.callbacks.manager import Callbacks\\n\\nclass BaseRetriever(ABC):\\n    ...\\n    def get_relevant_documents(\\n        self, query: str, *, callbacks: Callbacks = None, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Retrieve documents relevant to a query.\\n        Args:\\n            query: string to find relevant documents for\\n            callbacks: Callback manager or list of callbacks\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        ...\\n\\n    async def aget_relevant_documents(\\n        self, query: str, *, callbacks: Callbacks = None, **kwargs: Any\\n    ) -> List[Document]:\\n        \"\"\"Asynchronously get documents relevant to a query.\\n        Args:\\n            query: string to find relevant documents for\\n            callbacks: Callback manager or list of callbacks\\n        Returns:\\n            List of relevant documents\\n        \"\"\"\\n        ...\\n```\\n\\nIt\\'s that simple! You can call `get_relevant_documents` or the async `get_relevant_documents` methods to retrieve documents relevant to a query, where \"relevance\" is defined by\\nthe specific retriever object you are calling.\\n\\nOf course, we also help construct what we think useful Retrievers are. The main type of Retriever that we focus on is a Vectorstore retriever. We will focus on that for the rest of this guide.\\n\\nIn order to understand what a vectorstore retriever is, it\\'s important to understand what a Vectorstore is. So let\\'s look at that.\\n\\nBy default, LangChain uses [Chroma](/docs/ecosystem/integrations/chroma.html) as the vectorstore to index and search embeddings. To walk through this tutorial, we\\'ll first need to install `chromadb`.\\n\\n```\\npip install chromadb\\n```\\n\\nThis example showcases question answering over documents.\\nWe have chosen this as the example for getting started because it nicely combines a lot of different elements (Text splitters, embeddings, vectorstores) and then also shows how to use them in a chain.\\n\\nQuestion answering over documents consists of four steps:\\n\\n1. Create an index\\n2. Create a Retriever from that index\\n3. Create a question answering chain\\n4. Ask questions!\\n\\nEach of the steps has multiple sub steps and potential configurations. In this notebook we will primarily focus on (1). We will start by showing the one-liner for doing so, but then break down what is actually going on.\\n\\nFirst, let\\'s import some common classes we\\'ll use no matter what.\\n\\n\\n```python\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.llms import OpenAI\\n```\\n\\nNext in the generic setup, let\\'s specify the document loader we want to use. You can download the `state_of_the_union.txt` file [here](https://github.com/hwchase17/langchain/blob/master/docs/modules/state_of_the_union.txt)\\n\\n\\n```python\\nfrom langchain.document_loaders import TextLoader\\nloader = TextLoader(\\'../state_of_the_union.txt\\', encoding=\\'utf8\\')\\n```\\n\\n## One Line Index Creation\\n\\nTo get started as quickly as possible, we can use the `VectorstoreIndexCreator`.\\n\\n\\n```python\\nfrom langchain.indexes import VectorstoreIndexCreator\\n```\\n\\n\\n```python\\nindex = VectorstoreIndexCreator().from_loaders([loader])\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    Running Chroma using direct local API.\\n    Using DuckDB in-memory for database. Data will be transient.\\n```\\n\\n</CodeOutputBlock>\\n\\nNow that the index is created, we can use it to ask questions of the data! Note that under the hood this is actually doing a few steps as well, which we will cover later in this guide.\\n\\n\\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nindex.query(query)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    \" The president said that Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said that she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\"\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nindex.query_with_sources(query)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    {\\'question\\': \\'What did the president say about Ketanji Brown Jackson\\',\\n     \\'answer\\': \" The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson, one of the nation\\'s top legal minds, to continue Justice Breyer\\'s legacy of excellence, and that she has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\\\\n\",\\n     \\'sources\\': \\'../state_of_the_union.txt\\'}\\n```\\n\\n</CodeOutputBlock>\\n\\nWhat is returned from the `VectorstoreIndexCreator` is `VectorStoreIndexWrapper`, which provides these nice `query` and `query_with_sources` functionality. If we just wanted to access the vectorstore directly, we can also do that.\\n\\n\\n```python\\nindex.vectorstore\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    <langchain.vectorstores.chroma.Chroma at 0x119aa5940>\\n```\\n\\n</CodeOutputBlock>\\n\\nIf we then want to access the VectorstoreRetriever, we can do that with:\\n\\n\\n```python\\nindex.vectorstore.as_retriever()\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    VectorStoreRetriever(vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x119aa5940>, search_kwargs={})\\n```\\n\\n</CodeOutputBlock>\\n\\n## Walkthrough\\n\\nOkay, so what\\'s actually going on? How is this index getting created?\\n\\nA lot of the magic is being hid in this `VectorstoreIndexCreator`. What is this doing?\\n\\nThere are three main steps going on after the documents are loaded:\\n\\n1. Splitting documents into chunks\\n2. Creating embeddings for each document\\n3. Storing documents and embeddings in a vectorstore\\n\\nLet\\'s walk through this in code\\n\\n\\n```python\\ndocuments = loader.load()\\n```\\n\\nNext, we will split the documents into chunks.\\n\\n\\n```python\\nfrom langchain.text_splitter import CharacterTextSplitter\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\n```\\n\\nWe will then select which embeddings we want to use.\\n\\n\\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nembeddings = OpenAIEmbeddings()\\n```\\n\\nWe now create the vectorstore to use as the index.\\n\\n\\n```python\\nfrom langchain.vectorstores import Chroma\\ndb = Chroma.from_documents(texts, embeddings)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    Running Chroma using direct local API.\\n    Using DuckDB in-memory for database. Data will be transient.\\n```\\n\\n</CodeOutputBlock>\\n\\nSo that\\'s creating the index. Then, we expose this index in a retriever interface.\\n\\n\\n```python\\nretriever = db.as_retriever()\\n```\\n\\nThen, as before, we create a chain and use it to answer questions!\\n\\n\\n```python\\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\\n```\\n\\n\\n```python\\nquery = \"What did the president say about Ketanji Brown Jackson\"\\nqa.run(query)\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    \" The President said that Judge Ketanji Brown Jackson is one of the nation\\'s top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He said she is a consensus builder and has received a broad range of support from organizations such as the Fraternal Order of Police and former judges appointed by Democrats and Republicans.\"\\n```\\n\\n</CodeOutputBlock>\\n\\n`VectorstoreIndexCreator` is just a wrapper around all this logic. It is configurable in the text splitter it uses, the embeddings it uses, and the vectorstore it uses. For example, you can configure it as below:\\n\\n\\n```python\\nindex_creator = VectorstoreIndexCreator(\\n    vectorstore_cls=Chroma,\\n    embedding=OpenAIEmbeddings(),\\n    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\n)\\n```\\n\\nHopefully this highlights what is going on under the hood of `VectorstoreIndexCreator`. While we think it\\'s important to have a simple way to create indexes, we also think it\\'s important to understand what\\'s going on under the hood.\\n', metadata={'source': 'langchain/docs/snippets/modules/data_connection/retrievers/get_started.mdx'}),\n",
       " Document(page_content='```python\\nfrom langchain.chains.router import MultiRetrievalQAChain\\nfrom langchain.llms import OpenAI\\n```\\n\\n\\n```python\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.vectorstores import FAISS\\n\\nsou_docs = TextLoader(\\'../../state_of_the_union.txt\\').load_and_split()\\nsou_retriever = FAISS.from_documents(sou_docs, OpenAIEmbeddings()).as_retriever()\\n\\npg_docs = TextLoader(\\'../../paul_graham_essay.txt\\').load_and_split()\\npg_retriever = FAISS.from_documents(pg_docs, OpenAIEmbeddings()).as_retriever()\\n\\npersonal_texts = [\\n    \"I love apple pie\",\\n    \"My favorite color is fuchsia\",\\n    \"My dream is to become a professional dancer\",\\n    \"I broke my arm when I was 12\",\\n    \"My parents are from Peru\",\\n]\\npersonal_retriever = FAISS.from_texts(personal_texts, OpenAIEmbeddings()).as_retriever()\\n```\\n\\n\\n```python\\nretriever_infos = [\\n    {\\n        \"name\": \"state of the union\", \\n        \"description\": \"Good for answering questions about the 2023 State of the Union address\", \\n        \"retriever\": sou_retriever\\n    },\\n    {\\n        \"name\": \"pg essay\", \\n        \"description\": \"Good for answer quesitons about Paul Graham\\'s essay on his career\", \\n        \"retriever\": pg_retriever\\n    },\\n    {\\n        \"name\": \"personal\", \\n        \"description\": \"Good for answering questions about me\", \\n        \"retriever\": personal_retriever\\n    }\\n]\\n```\\n\\n\\n```python\\nchain = MultiRetrievalQAChain.from_retrievers(OpenAI(), retriever_infos, verbose=True)\\n```\\n\\n\\n```python\\nprint(chain.run(\"What did the president say about the economy?\"))\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    \\n    \\n    > Entering new MultiRetrievalQAChain chain...\\n    state of the union: {\\'query\\': \\'What did the president say about the economy in the 2023 State of the Union address?\\'}\\n    > Finished chain.\\n     The president said that the economy was stronger than it had been a year prior, and that the American Rescue Plan helped create record job growth and fuel economic relief for millions of Americans. He also proposed a plan to fight inflation and lower costs for families, including cutting the cost of prescription drugs and energy, providing investments and tax credits for energy efficiency, and increasing access to child care and Pre-K.\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nprint(chain.run(\"What is something Paul Graham regrets about his work?\"))\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    \\n    \\n    > Entering new MultiRetrievalQAChain chain...\\n    pg essay: {\\'query\\': \\'What is something Paul Graham regrets about his work?\\'}\\n    > Finished chain.\\n     Paul Graham regrets that he did not take a vacation after selling his company, instead of immediately starting to paint.\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nprint(chain.run(\"What is my background?\"))\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    \\n    \\n    > Entering new MultiRetrievalQAChain chain...\\n    personal: {\\'query\\': \\'What is my background?\\'}\\n    > Finished chain.\\n     Your background is Peruvian.\\n```\\n\\n</CodeOutputBlock>\\n\\n\\n```python\\nprint(chain.run(\"What year was the Internet created in?\"))\\n```\\n\\n<CodeOutputBlock lang=\"python\">\\n\\n```\\n    \\n    \\n    > Entering new MultiRetrievalQAChain chain...\\n    None: {\\'query\\': \\'What year was the Internet created in?\\'}\\n    > Finished chain.\\n    The Internet was created in 1969 through a project called ARPANET, which was funded by the United States Department of Defense. However, the World Wide Web, which is often confused with the Internet, was created in 1989 by British computer scientist Tim Berners-Lee.\\n```\\n\\n</CodeOutputBlock>\\n', metadata={'source': 'langchain/docs/snippets/modules/chains/additional/multi_retrieval_qa_router.mdx'})]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_token is not default parameter.\n",
      "                    max_token was transferred to model_kwargs.\n",
      "                    Please confirm that max_token is what you intended.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationalRetrievalChain\n\u001b[0;32m----> 6\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm(OpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, openai_api_key\u001b[39m=\u001b[39mopenai_api_key, max_token\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m), vectorstore\u001b[39m.\u001b[39mas_retriever())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0, openai_api_key=openai_api_key, max_token=2000), vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 5707 tokens (5451 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chat_history \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTell me how to implement a chatbot in python\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m result \u001b[39m=\u001b[39m qa({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: query, \u001b[39m\"\u001b[39;49m\u001b[39mchat_history\u001b[39;49m\u001b[39m\"\u001b[39;49m: chat_history})\n\u001b[1;32m      4\u001b[0m result\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:128\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    126\u001b[0m new_inputs[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m new_question\n\u001b[1;32m    127\u001b[0m new_inputs[\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m chat_history_str\n\u001b[0;32m--> 128\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    129\u001b[0m     input_documents\u001b[39m=\u001b[39;49mdocs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_inputs\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    131\u001b[0m output: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: answer}\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:293\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[_output_key]\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[_output_key]\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:87\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     86\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs), {}\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:252\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    103\u001b[0m     prompts,\n\u001b[1;32m    104\u001b[0m     stop,\n\u001b[1;32m    105\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    106\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    107\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/llms/base.py:139\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    133\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    137\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    138\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/llms/base.py:225\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    220\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m     run_managers \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    223\u001b[0m         dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 225\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    226\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/llms/base.py:176\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    175\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    177\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/llms/base.py:163\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    160\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    161\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 163\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    164\u001b[0m                 prompts,\n\u001b[1;32m    165\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    166\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    167\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    168\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    172\u001b[0m         )\n\u001b[1;32m    173\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    174\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:336\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    337\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[1;32m    339\u001b[0m     \u001b[39m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:106\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:104\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 5707 tokens (5451 in your prompt; 256 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"Tell me how to implement a chatbot in python\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try generating code from simple codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also see the separators used for a given language\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import os\\nfrom typing import List\\n\\ndef combine_files(file_paths: List[str], combine_file_path: str):\\n    [os.remove(file_path) for file_path in file_paths]\\n    with open(combine_file_path, \\'w\\') as combine_file:\\n        combine_file.write(\"I pranked you\")\\n\\ndef remove_files_by_name(file_names: List[str], filter: str) -> List[str]:\\n    return [file_name for file_name in file_names if filter in file_name]\\n\\ncombine_files([\"test1.txt\", \"test1.txt\"], \"combined_file\")', metadata={})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"codebase/files.py\", \"r\") as f:\n",
    "    PYTHON_CODE = f.read()\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=1000, chunk_overlap=10\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "# save to disk\n",
    "vectorstore_code = Chroma.from_documents(documents=python_docs,embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore_code.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: I have 2 files (test1.txt and test2.txt), I'd like to combine them into one file (result.txt), how can i do it ? \n",
      "\n",
      "**Answer**: You can use the `combine_files` function in the code provided. Pass the list of file paths [\"test1.txt\", \"test2.txt\"] as the first argument and the desired combined file path (\"result.txt\") as the second argument. The function will combine the contents of the two files into the specified combined file. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"I have 2 files (test1.txt and test2.txt), I'd like to combine them into one file (result.txt), how can i do it ?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"I have 2 files (test1.txt and test2.txt and file.py), I'd like to filter the files containing test in it, how can I do that ?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to give the LLM the most recet version of streamlit in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Union\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import PythonLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers.language import LanguageParser\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def num_tokens_from_string(docs: List[Document], encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = 0\n",
    "    max_tokens = 0\n",
    "    for doc in docs:\n",
    "        if len(encoding.encode(doc.page_content)) > max_tokens:\n",
    "            max_tokens = len(encoding.encode(doc.page_content))\n",
    "        num_tokens += len(encoding.encode(doc.page_content))\n",
    "    return num_tokens, max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'streamlit'...\n",
      "Updating files: 100% (2205/2205), done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"streamlit\"):\n",
    "    shutil.rmtree(\"streamlit\")\n",
    "os.system(\"git clone https://github.com/streamlit/streamlit.git --branch master --single-branch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:00<00:00, 123.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 166.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in source code: 100208, max tokens in source code: 4703, price: 0.010020800000000002\n"
     ]
    }
   ],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    \"streamlit/\",\n",
    "    glob=\"lib/streamlit/elements/**/*.py\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON),\n",
    "    show_progress=True\n",
    ")\n",
    "streamlit_source_code_doc = loader.load()\n",
    "\n",
    "token, max_token = num_tokens_from_string(streamlit_source_code_doc, \"cl100k_base\")\n",
    "print(f\"Number of tokens in source code: {token}, max tokens in source code: {max_token}, price: {token * 0.0001/1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in source code: 102761, max tokens in source code: 101, price: 0.010276100000000002\n"
     ]
    }
   ],
   "source": [
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=250, chunk_overlap=50\n",
    ")\n",
    "streamlit_source_code_doc = python_splitter.split_documents(streamlit_source_code_doc)\n",
    "\n",
    "token, max_token = num_tokens_from_string(streamlit_source_code_doc, \"cl100k_base\")\n",
    "print(f\"Number of tokens in source code: {token}, max tokens in source code: {max_token}, price: {token * 0.0001/1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectorstore_streamlit_code = Chroma.from_documents(documents=streamlit_source_code_doc,embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_code_retriever = vectorstore_streamlit_code.as_retriever()\n",
    "streamlit_code_retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "streamlit_code_retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "streamlit_code_retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "streamlit_code_retriever.search_kwargs[\"k\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa_over_streamlit_code = ConversationalRetrievalChain.from_llm(model, retriever=streamlit_code_retriever)\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "simple_conversation = ConversationChain(llm=model, memory=ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Oh, Streamlit is a great choice for building interactive web applications! To create a button in Streamlit, you can use the `st.button()` function. Here's an example:\n",
       "\n",
       "```python\n",
       "import streamlit as st\n",
       "\n",
       "button_clicked = st.button(\"Click me!\")\n",
       "\n",
       "if button_clicked:\n",
       "    st.write(\"Button was clicked!\")\n",
       "```\n",
       "\n",
       "In this example, `st.button()` creates a button with the label \"Click me!\". When the button is clicked, the `button_clicked` variable becomes `True`, and the message \"Button was clicked!\" is displayed using `st.write()`.\n",
       "\n",
       "Let me know if you need any more help with Streamlit!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "output = simple_conversation.run(\"I'm looking for a way to create a button in streamlit\")\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To create a chart in Streamlit, you can use the `st.line_chart()`, `st.area_chart()`, or `st.bar_chart()` functions. Here's an example using `st.line_chart()`:\n",
       "\n",
       "```python\n",
       "import streamlit as st\n",
       "import pandas as pd\n",
       "\n",
       "data = pd.DataFrame({\n",
       "    'x': [1, 2, 3, 4, 5],\n",
       "    'y': [10, 5, 8, 3, 6]\n",
       "})\n",
       "\n",
       "st.line_chart(data)\n",
       "```\n",
       "\n",
       "In this example, `st.line_chart()` creates a line chart using the data provided in a Pandas DataFrame. You can also use the `st.area_chart()` function to create an area chart or the `st.bar_chart()` function to create a bar chart.\n",
       "\n",
       "Let me know if you have any more questions about creating charts in Streamlit!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = simple_conversation.run(\"What about a chart ?\")\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As of the time of this conversation, the most recent version of Streamlit is 0.94.1. However, please note that software versions are subject to change, so it's always a good idea to check the official Streamlit website or documentation for the most up-to-date information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = simple_conversation.run(\"What is the most recent version of streamlit ?\")\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To get camera inputs in a Streamlit app, you can use the OpenCV library in Python. Here are the steps you can follow:\n",
       "\n",
       "1. Install the necessary libraries by running the following command in your terminal:\n",
       "   ```\n",
       "   pip install streamlit opencv-python-headless\n",
       "   ```\n",
       "\n",
       "2. Import the required modules in your Python script:\n",
       "   ```python\n",
       "   import streamlit as st\n",
       "   import cv2\n",
       "   ```\n",
       "\n",
       "3. Create a function to capture video from the camera:\n",
       "   ```python\n",
       "   def capture_video():\n",
       "       cap = cv2.VideoCapture(0)\n",
       "       while True:\n",
       "           ret, frame = cap.read()\n",
       "           if not ret:\n",
       "               break\n",
       "           st.image(frame, channels=\"BGR\")\n",
       "       cap.release()\n",
       "   ```\n",
       "\n",
       "4. Use the `st.button` function to create a button in your Streamlit app:\n",
       "   ```python\n",
       "   if st.button('Start Camera'):\n",
       "       capture_video()\n",
       "   ```\n",
       "\n",
       "5. Run your Streamlit app using the following command in your terminal:\n",
       "   ```\n",
       "   streamlit run your_app.py\n",
       "   ```\n",
       "\n",
       "When you run the app and click on the \"Start Camera\" button, it will capture video from your default camera and display it in the Streamlit app.\n",
       "\n",
       "Note: Make sure your camera is connected and accessible by OpenCV. If you have multiple cameras connected, you can specify the camera index in `cap = cv2.VideoCapture(0)` to select the desired camera (e.g., `cap = cv2.VideoCapture(1)` for the second camera)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = simple_conversation.run(\"How do I get camera inputs in a streamlit app ?\")\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, I know the `camera_input()` function from Streamlit. It is not a built-in function in Streamlit, but it is a hypothetical function that you are asking about. As of my knowledge, Streamlit does not have a built-in function called `camera_input()`. However, you can create a function called `camera_input()` yourself using the steps I mentioned earlier to capture video from a camera in a Streamlit app."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = simple_conversation.run(\"Do you know the function camera_input() from streamlit ?\")\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'How do I get camera inputs in a streamlit app ?', 'chat_history': [], 'answer': 'You can use the `streamlit-webrtc` library to get camera inputs in a Streamlit app. Here is an example of how you can use it:\\n\\n```python\\nimport streamlit as st\\nimport av\\nimport streamlit_webrtc as webrtc\\n\\nwebrtc_ctx = webrtc.client(\\n    key=\"camera-input\",\\n    video_transformer_factory=None,  # You can modify the video input if needed\\n    async_transform=False,\\n)\\n\\nvideo_transformer = webrtc_ctx.video_transformer()\\n\\nst.video(video_transformer, format=\"RGB\")\\n\\n```\\n\\nFor more details, you can refer to the `streamlit-webrtc` documentation: https://docs.streamlit.io/en/stable/streamlit_webrtc.html'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You can use the `streamlit-webrtc` library to get camera inputs in a Streamlit app. Here is an example of how you can use it:\n",
       "\n",
       "```python\n",
       "import streamlit as st\n",
       "import av\n",
       "import streamlit_webrtc as webrtc\n",
       "\n",
       "webrtc_ctx = webrtc.client(\n",
       "    key=\"camera-input\",\n",
       "    video_transformer_factory=None,  # You can modify the video input if needed\n",
       "    async_transform=False,\n",
       ")\n",
       "\n",
       "video_transformer = webrtc_ctx.video_transformer()\n",
       "\n",
       "st.video(video_transformer, format=\"RGB\")\n",
       "\n",
       "```\n",
       "\n",
       "For more details, you can refer to the `streamlit-webrtc` documentation: https://docs.streamlit.io/en/stable/streamlit_webrtc.html"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'without using a third party component ?', 'chat_history': [('How do I get camera inputs in a streamlit app ?', 'You can use the `streamlit-webrtc` library to get camera inputs in a Streamlit app. Here is an example of how you can use it:\\n\\n```python\\nimport streamlit as st\\nimport av\\nimport streamlit_webrtc as webrtc\\n\\nwebrtc_ctx = webrtc.client(\\n    key=\"camera-input\",\\n    video_transformer_factory=None,  # You can modify the video input if needed\\n    async_transform=False,\\n)\\n\\nvideo_transformer = webrtc_ctx.video_transformer()\\n\\nst.video(video_transformer, format=\"RGB\")\\n\\n```\\n\\nFor more details, you can refer to the `streamlit-webrtc` documentation: https://docs.streamlit.io/en/stable/streamlit_webrtc.html')], 'answer': 'No, it is not currently possible to directly get camera inputs in a Streamlit app without using a third-party component. However, you can use OpenCV or other libraries to capture and process camera inputs, and then display the processed video or images in your Streamlit app.'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "No, it is not currently possible to directly get camera inputs in a Streamlit app without using a third-party component. However, you can use OpenCV or other libraries to capture and process camera inputs, and then display the processed video or images in your Streamlit app."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Do you know the function camera_input() from streamlit ?', 'chat_history': [('How do I get camera inputs in a streamlit app ?', 'You can use the `streamlit-webrtc` library to get camera inputs in a Streamlit app. Here is an example of how you can use it:\\n\\n```python\\nimport streamlit as st\\nimport av\\nimport streamlit_webrtc as webrtc\\n\\nwebrtc_ctx = webrtc.client(\\n    key=\"camera-input\",\\n    video_transformer_factory=None,  # You can modify the video input if needed\\n    async_transform=False,\\n)\\n\\nvideo_transformer = webrtc_ctx.video_transformer()\\n\\nst.video(video_transformer, format=\"RGB\")\\n\\n```\\n\\nFor more details, you can refer to the `streamlit-webrtc` documentation: https://docs.streamlit.io/en/stable/streamlit_webrtc.html'), ('without using a third party component ?', 'No, it is not currently possible to directly get camera inputs in a Streamlit app without using a third-party component. However, you can use OpenCV or other libraries to capture and process camera inputs, and then display the processed video or images in your Streamlit app.')], 'answer': \"Yes, the function `camera_input()` is available in the streamlit library. It is used to capture images from the user's camera.\"}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Yes, the function `camera_input()` is available in the streamlit library. It is used to capture images from the user's camera."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions = [\n",
    "    \"How do I get camera inputs in a streamlit app ?\",\n",
    "    \"without using a third party component ?\",\n",
    "    \"Do you know the function camera_input() from streamlit ?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_over_streamlit_code({\"question\": question, \"chat_history\": chat_history})\n",
    "    print(result)\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    display(Markdown(result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: Do you know the function camera_input() from streamlit ? \n",
      "\n",
      "**Answer**: Yes, I am familiar with the function `camera_input()` from Streamlit. It is used to capture images from the user's camera. Here's an example usage:\n",
      "\n",
      "```python\n",
      "import streamlit as st\n",
      "\n",
      "picture = st.camera_input(\"Take a picture\")\n",
      "\n",
      "if picture:\n",
      "    st.image(picture)\n",
      "```\n",
      "\n",
      "This code will display a button labeled \"Take a picture\". When the button is clicked, the user's camera will open, allowing them to take a picture. The captured picture will then be displayed using the `st.image()` function. \n",
      "\n",
      "-> **Question**: Give me an example of a streamlit app with a camera input \n",
      "\n",
      "**Answer**: Certainly! Here's an example of a Streamlit app that uses the `camera_input()` function:\n",
      "\n",
      "```python\n",
      "import streamlit as st\n",
      "\n",
      "def main():\n",
      "    picture = st.camera_input(\"Take a picture\")\n",
      "\n",
      "    if picture:\n",
      "        st.image(picture)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "In this example, the `camera_input()` function is used to capture an image from the user's camera. The captured image is then displayed using the `image()` function. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Do you know the function camera_input() from streamlit ?\",\n",
    "    \"Give me an example of a streamlit app with a camera input\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_over_streamlit_code({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What is the most recent version of streamlit ? \n",
      "\n",
      "**Answer**: Based on the provided context, it is not possible to determine the most recent version of Streamlit. \n",
      "\n",
      "-> **Question**: What do you know about streamlit ? \n",
      "\n",
      "**Answer**: Streamlit is an open-source Python library used for building interactive web applications for data science and machine learning. It allows users to create and deploy data-driven apps quickly by simply writing Python scripts. With Streamlit, users can easily create and customize interactive visualizations, dashboards, and data exploration tools.\n",
      "\n",
      "Streamlit provides a simple and intuitive API that allows users to add various UI components such as sliders, dropdowns, buttons, and plots to their apps. It also supports real-time data updates, allowing users to easily visualize and analyze changing data.\n",
      "\n",
      "Some key features of Streamlit include:\n",
      "\n",
      "- Easy-to-use API: Streamlit provides a concise and intuitive API, allowing users to build interactive apps with just a few lines of code.\n",
      "\n",
      "- Automatic reactivity: Streamlit automatically updates the app whenever the code or underlying data changes, providing a seamless and responsive user experience.\n",
      "\n",
      "- Fast prototyping: With Streamlit, users can quickly iterate on ideas and prototypes, making it ideal for exploratory data analysis and rapid development.\n",
      "\n",
      "- Interactive visualizations: Streamlit supports a wide range of data visualization libraries, including Matplotlib, Plotly, and Altair, making it easy to create interactive plots and charts.\n",
      "\n",
      "- Deployment options: Streamlit apps can be deployed locally or on cloud platforms such as Heroku, AWS, and Azure, allowing users to share their apps with others easily.\n",
      "\n",
      "Overall, Streamlit is a powerful tool for building and sharing data-driven web applications, making it popular among data scientists, machine learning engineers, and researchers. \n",
      "\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens. However, your messages resulted in 18105 tokens. Please reduce the length of the messages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m chat_history \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m questions:\n\u001b[0;32m----> 9\u001b[0m     result \u001b[39m=\u001b[39m qa_over_streamlit_code({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: question, \u001b[39m\"\u001b[39;49m\u001b[39mchat_history\u001b[39;49m\u001b[39m\"\u001b[39;49m: chat_history})\n\u001b[1;32m     10\u001b[0m     chat_history\u001b[39m.\u001b[39mappend((question, result[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-> **Question**: \u001b[39m\u001b[39m{\u001b[39;00mquestion\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:128\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    126\u001b[0m new_inputs[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m new_question\n\u001b[1;32m    127\u001b[0m new_inputs[\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m chat_history_str\n\u001b[0;32m--> 128\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    129\u001b[0m     input_documents\u001b[39m=\u001b[39;49mdocs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_inputs\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    131\u001b[0m output: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: answer}\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:293\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[_output_key]\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[_output_key]\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    296\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:87\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     86\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs), {}\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:252\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    103\u001b[0m     prompts,\n\u001b[1;32m    104\u001b[0m     stop,\n\u001b[1;32m    105\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    106\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    107\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:221\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    215\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    220\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:119\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    118\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    120\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    121\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    123\u001b[0m ]\n\u001b[1;32m    124\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:109\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    107\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 109\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    110\u001b[0m                 m,\n\u001b[1;32m    111\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    112\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    113\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    114\u001b[0m             )\n\u001b[1;32m    115\u001b[0m         )\n\u001b[1;32m    116\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    117\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chat_models/base.py:253\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    250\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m     )\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    254\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:371\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m     message \u001b[39m=\u001b[39m _convert_dict_to_message(\n\u001b[1;32m    364\u001b[0m         {\n\u001b[1;32m    365\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: inner_completion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         }\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[ChatGeneration(message\u001b[39m=\u001b[39mmessage)])\n\u001b[0;32m--> 371\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(messages\u001b[39m=\u001b[39;49mmessage_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    372\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:319\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 319\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/langchain/chat_models/openai.py:317\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    705\u001b[0m         ),\n\u001b[1;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/data/workspace/ai_projects/GenerativeApp/.venv/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens. However, your messages resulted in 18105 tokens. Please reduce the length of the messages."
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is the most recent version of streamlit ?\",\n",
    "    \"What do you know about streamlit ?\",\n",
    "    \"What do you know about langchain ?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_over_streamlit_code({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Chain for chatting with a vector database.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import inspect\n",
    "import warnings\n",
    "from abc import abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from pydantic import Extra, Field, root_validator\n",
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    "    Callbacks,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.combine_documents.base import BaseCombineDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "\n",
    "from langchain.chains.conversational_retrieval.base import CHAT_TURN_TYPE, _get_chat_history\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_template =\"\"\"You're an AI assistant specializing in python development. You know how to create Streamlit Applications.\n",
    "You will be asked questions about python code and streamlit applications. You will be given a chat history and a python code for context.\n",
    "Your objective is to generate a query that will be used to retrieve relevant documents that stores Streamlit documentation and python code snippets.\n",
    "The query must be in a form of suite of words related to the context.\n",
    "\n",
    "Python Code:\n",
    "```python\n",
    "{python_code}\n",
    "```\n",
    "Previous Questions:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Query:\"\"\"\n",
    "\n",
    "CONDENSE_QUESTION_CODE_PROMPT = PromptTemplate(template=_template, input_variables=[\"chat_history\", \"question\", \"python_code\"])\n",
    "\n",
    "class BaseConversationalRetrievalCodeChain(Chain):\n",
    "    \"\"\"Chain for chatting with an index. Given the chat history, the current code and a question, return the answer.\"\"\"\n",
    "\n",
    "    combine_docs_chain: BaseCombineDocumentsChain\n",
    "    question_generator: LLMChain\n",
    "    output_key: str = \"answer\"\n",
    "    return_source_documents: bool = False\n",
    "    return_generated_question: bool = False\n",
    "    get_chat_history: Optional[Callable[[CHAT_TURN_TYPE], str]] = None\n",
    "    \"\"\"Return the source documents.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Input keys.\"\"\"\n",
    "        return [\"question\", \"chat_history\", \"python_code\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Return the output keys.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        _output_keys = [self.output_key]\n",
    "        if self.return_source_documents:\n",
    "            _output_keys = _output_keys + [\"source_documents\"]\n",
    "        if self.return_generated_question:\n",
    "            _output_keys = _output_keys + [\"generated_question\"]\n",
    "        return _output_keys\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_docs(\n",
    "        self,\n",
    "        question: str,\n",
    "        inputs: Dict[str, Any],\n",
    "        *,\n",
    "        run_manager: CallbackManagerForChainRun,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get docs.\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
    "        question = inputs[\"question\"]\n",
    "        get_chat_history = self.get_chat_history or _get_chat_history\n",
    "        chat_history_str = get_chat_history(inputs[\"chat_history\"])\n",
    "        source_code_str = inputs[\"python_code\"]\n",
    "\n",
    "        if chat_history_str or source_code_str:\n",
    "            callbacks = _run_manager.get_child()\n",
    "            new_question = self.question_generator.run(\n",
    "                question=question, chat_history=chat_history_str, python_code=source_code_str, callbacks=callbacks\n",
    "            )\n",
    "        else:\n",
    "            new_question = question\n",
    "        accepts_run_manager = (\n",
    "            \"run_manager\" in inspect.signature(self._get_docs).parameters\n",
    "        )\n",
    "        if accepts_run_manager:\n",
    "            docs = self._get_docs(new_question, inputs, run_manager=_run_manager)\n",
    "        else:\n",
    "            docs = self._get_docs(new_question, inputs)  # type: ignore[call-arg]\n",
    "        new_inputs = inputs.copy()\n",
    "        # Remove \"streamlit\" or \"python\" from the question\n",
    "        new_question = new_question.replace(\"streamlit\", \"\").replace(\"python\", \"\")\n",
    "        print(\"new_question\", new_question)\n",
    "        new_inputs[\"chat_history\"] = chat_history_str\n",
    "        answer = self.combine_docs_chain.run(\n",
    "            input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs\n",
    "        )\n",
    "        output: Dict[str, Any] = {self.output_key: answer}\n",
    "        if self.return_source_documents:\n",
    "            output[\"source_documents\"] = docs\n",
    "        if self.return_generated_question:\n",
    "            output[\"generated_question\"] = new_question\n",
    "        return output\n",
    "\n",
    "    @abstractmethod\n",
    "    async def _aget_docs(\n",
    "        self,\n",
    "        question: str,\n",
    "        inputs: Dict[str, Any],\n",
    "        *,\n",
    "        run_manager: AsyncCallbackManagerForChainRun,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get docs.\"\"\"\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()\n",
    "        question = inputs[\"question\"]\n",
    "        get_chat_history = self.get_chat_history or _get_chat_history\n",
    "        chat_history_str = get_chat_history(inputs[\"chat_history\"])\n",
    "        source_code_str = inputs[\"python_code\"]\n",
    "        if chat_history_str:\n",
    "            callbacks = _run_manager.get_child()\n",
    "            new_question = await self.question_generator.arun(\n",
    "                question=question, chat_history=chat_history_str, python_code=source_code_str, callbacks=callbacks\n",
    "            )\n",
    "        else:\n",
    "            new_question = question\n",
    "        accepts_run_manager = (\n",
    "            \"run_manager\" in inspect.signature(self._aget_docs).parameters\n",
    "        )\n",
    "        if accepts_run_manager:\n",
    "            docs = await self._aget_docs(new_question, inputs, run_manager=_run_manager)\n",
    "        else:\n",
    "            docs = await self._aget_docs(new_question, inputs)  # type: ignore[call-arg]\n",
    "\n",
    "        new_inputs = inputs.copy()\n",
    "        new_inputs[\"chat_history\"] = chat_history_str\n",
    "        answer = await self.combine_docs_chain.arun(\n",
    "            input_documents=docs, callbacks=_run_manager.get_child(), **new_inputs\n",
    "        )\n",
    "        output: Dict[str, Any] = {self.output_key: answer}\n",
    "        if self.return_source_documents:\n",
    "            output[\"source_documents\"] = docs\n",
    "        if self.return_generated_question:\n",
    "            output[\"generated_question\"] = new_question\n",
    "        return output\n",
    "\n",
    "    def save(self, file_path: Union[Path, str]) -> None:\n",
    "        if self.get_chat_history:\n",
    "            raise ValueError(\"Chain not savable when `get_chat_history` is not None.\")\n",
    "        super().save(file_path)\n",
    "\n",
    "\n",
    "class ConversationalRetrievalCodeChain(BaseConversationalRetrievalCodeChain):\n",
    "    \"\"\"Chain for chatting with an index.\"\"\"\n",
    "\n",
    "    retriever: BaseRetriever\n",
    "    \"\"\"Index to connect to.\"\"\"\n",
    "    max_tokens_limit: Optional[int] = None\n",
    "    \"\"\"If set, restricts the docs to return from store based on tokens, enforced only\n",
    "    for StuffDocumentChain\"\"\"\n",
    "\n",
    "    def _reduce_tokens_below_limit(self, docs: List[Document]) -> List[Document]:\n",
    "        num_docs = len(docs)\n",
    "\n",
    "        if self.max_tokens_limit and isinstance(\n",
    "            self.combine_docs_chain, StuffDocumentsChain\n",
    "        ):\n",
    "            tokens = [\n",
    "                self.combine_docs_chain.llm_chain.llm.get_num_tokens(doc.page_content)\n",
    "                for doc in docs\n",
    "            ]\n",
    "            token_count = sum(tokens[:num_docs])\n",
    "            while token_count > self.max_tokens_limit:\n",
    "                num_docs -= 1\n",
    "                token_count -= tokens[num_docs]\n",
    "\n",
    "        return docs[:num_docs]\n",
    "\n",
    "    def _get_docs(\n",
    "        self,\n",
    "        question: str,\n",
    "        inputs: Dict[str, Any],\n",
    "        *,\n",
    "        run_manager: CallbackManagerForChainRun,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get docs.\"\"\"\n",
    "        docs = self.retriever.get_relevant_documents(\n",
    "            question, callbacks=run_manager.get_child()\n",
    "        )\n",
    "        return self._reduce_tokens_below_limit(docs)\n",
    "\n",
    "    async def _aget_docs(\n",
    "        self,\n",
    "        question: str,\n",
    "        inputs: Dict[str, Any],\n",
    "        *,\n",
    "        run_manager: AsyncCallbackManagerForChainRun,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Get docs.\"\"\"\n",
    "        docs = await self.retriever.aget_relevant_documents(\n",
    "            question, callbacks=run_manager.get_child()\n",
    "        )\n",
    "        return self._reduce_tokens_below_limit(docs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        retriever: BaseRetriever,\n",
    "        condense_question_prompt: BasePromptTemplate = CONDENSE_QUESTION_CODE_PROMPT,\n",
    "        chain_type: str = \"stuff\",\n",
    "        verbose: bool = False,\n",
    "        condense_question_llm: Optional[BaseLanguageModel] = None,\n",
    "        combine_docs_chain_kwargs: Optional[Dict] = None,\n",
    "        callbacks: Callbacks = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> BaseConversationalRetrievalCodeChain:\n",
    "        \"\"\"Load chain from LLM.\"\"\"\n",
    "        combine_docs_chain_kwargs = combine_docs_chain_kwargs or {}\n",
    "\n",
    "        doc_chain = load_qa_chain(\n",
    "            llm,\n",
    "            chain_type=chain_type,\n",
    "            verbose=verbose,\n",
    "            callbacks=callbacks,\n",
    "            **combine_docs_chain_kwargs,\n",
    "        )\n",
    "\n",
    "        _llm = condense_question_llm or llm\n",
    "        condense_question_chain = LLMChain(\n",
    "            llm=_llm,\n",
    "            prompt=condense_question_prompt,\n",
    "            verbose=verbose,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        return cls(\n",
    "            retriever=retriever,\n",
    "            combine_docs_chain=doc_chain,\n",
    "            question_generator=condense_question_chain,\n",
    "            callbacks=callbacks,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_streamlit_doc_retriever():\n",
    "    # Check if the Chroma database exists\n",
    "    if not os.path.exists(\"../.doc_db/streamlit_chroma_db\"):\n",
    "        raise Exception(\"The Chroma database for Streamlit does not exist. Please run the script `doc_retriever.py` to create it.\")\n",
    "\n",
    "    # load from disk\n",
    "    retriever = Chroma(persist_directory=\"../.doc_db/streamlit_chroma_db\",\n",
    "                       embedding_function=OpenAIEmbeddings(openai_api_key=openai_api_key)).as_retriever()\n",
    "    retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "    retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "    retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "    retriever.search_kwargs[\"k\"] = 4\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "model_name = \"text-davinci-003\"\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "\n",
    "retriever = streamlit_code_retriever\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 10\n",
    "\n",
    "qa = ConversationalRetrievalCodeChain.from_llm(llm=llm, retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To create a Streamlit app, you can follow these steps:\\n\\n1. Install Streamlit by running the command `pip install streamlit` in your terminal.\\n\\n2. Create a new Python script and import the Streamlit library by adding the following line at the top of your script:\\n   ```python\\n   import streamlit as st\\n   ```\\n\\n3. Write your app code using Streamlit's API. You can use various Streamlit functions like `st.write()`, `st.title()`, `st.markdown()`, etc., to display content in your app.\\n\\n4. Save your script with a `.py` extension, for example, `my_app.py`.\\n\\n5. Run your app by executing the command `streamlit run my_app.py` in your terminal. This will start a local server and open your app in a new tab in your browser.\\n\\n6. Modify your code, save it, and the changes will be automatically reflected in the browser. This allows you to see the updates live as you make changes to your app code.\\n\\nThat's it! You have created a Streamlit app. You can continue modifying your app code and refreshing the browser to see the changes in real-time. For more information and advanced usage, you can refer to the Streamlit documentation at [https://docs.streamlit.io](https://docs.streamlit.io).\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(question=\"How do I create a Streamlit app?\", python_code=\"import streamlit as st\\nst.write('Hello world!')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_question camera, real-time, display, image, web page\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To display the real-time image from your computer's camera on a web page, you can use the browser's built-in capabilities to access the camera and display the video feed. This can be achieved using HTML5 and JavaScript.\n",
       "\n",
       "Here is a basic example code:\n",
       "\n",
       "```html\n",
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "    <title>Camera Live Feed</title>\n",
       "</head>\n",
       "<body>\n",
       "    <video id=\"camera-feed\" autoplay></video>\n",
       "\n",
       "    <script>\n",
       "        // Access the camera feed\n",
       "        navigator.mediaDevices.getUserMedia({ video: true })\n",
       "            .then(function (stream) {\n",
       "                var videoElement = document.getElementById('camera-feed');\n",
       "                videoElement.srcObject = stream;\n",
       "            })\n",
       "            .catch(function (error) {\n",
       "                console.log('Error accessing camera:', error);\n",
       "            });\n",
       "    </script>\n",
       "</body>\n",
       "</html>\n",
       "```\n",
       "\n",
       "In this example, the `getUserMedia()` function is used to access the camera feed. Once the camera access is granted, the `video` element with the id `camera-feed` is used to display the video stream.\n",
       "\n",
       "Please note that camera access in the browser requires user permission, so the user will be prompted to grant permission to access the camera when they visit the web page."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Camera input live', metadata={'source': 'docs/content/library/api/widgets/widgets.md', 'filename': 'widgets.md', 'file_directory': 'docs/content/library/api/widgets', 'filetype': 'text/markdown', 'page_number': 2, 'category': 'Title'}), Document(page_content='Camera input live', metadata={'source': 'docs/content/library/api/api-reference.md', 'filename': 'api-reference.md', 'file_directory': 'docs/content/library/api', 'filetype': 'text/markdown', 'page_number': 2, 'category': 'Title'}), Document(page_content='Camera input', metadata={'source': 'docs/content/library/api/widgets/widgets.md', 'filename': 'widgets.md', 'file_directory': 'docs/content/library/api/widgets', 'filetype': 'text/markdown', 'page_number': 2, 'category': 'Title'}), Document(page_content='Camera input', metadata={'source': 'docs/content/library/api/api-reference.md', 'filename': 'api-reference.md', 'file_directory': 'docs/content/library/api', 'filetype': 'text/markdown', 'page_number': 2, 'category': 'Title'}), Document(page_content='the browser such as a webcam component.', metadata={'source': 'docs/content/library/changelog.md', 'filename': 'changelog.md', 'file_directory': 'docs/content/library', 'filetype': 'text/markdown', 'page_number': 2, 'category': 'ListItem'}), Document(page_content='Enabling camera access in your browser', metadata={'source': 'docs/content/kb/using-streamlit/enable-camera.md', 'filename': 'enable-camera.md', 'file_directory': 'docs/content/kb/using-streamlit', 'filetype': 'text/markdown', 'page_number': 2, 'category': 'Title'}), Document(page_content='webcam_demo()\\n\\nshow_code(webcam_demo)', metadata={'source': 'docs/python/api-examples-source/mpa-hello/pages/4_📷_Webcam_Demo.py', 'content_type': 'simplified_code', 'language': 'python'}), Document(page_content='script and frontend view (webpage).', metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md', 'filename': 'button-behavior-and-examples.md', 'file_directory': 'docs/content/library/advanced-features', 'filetype': 'text/markdown', 'page_number': 2, 'category': 'NarrativeText'}), Document(page_content='Video', metadata={'source': 'docs/content/library/api/api-reference.md', 'filename': 'api-reference.md', 'file_directory': 'docs/content/library/api', 'filetype': 'text/markdown', 'page_number': 2, 'category': 'Title'}), Document(page_content='Video', metadata={'source': 'docs/content/library/api/media/media.md', 'filename': 'media.md', 'file_directory': 'docs/content/library/api/media', 'filetype': 'text/markdown', 'page_number': 2, 'category': 'Title'})]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "questions = [\n",
    "    \"I have a camera on my computer i want to display the real time image on the web page\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history, \"python_code\":\"import streamlit as st\\nst.write('Hello world!')\"})\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "    print(result[\"source_documents\"])\n",
    "    chat_history.append((question, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Examples\\n        --------\\n        >>> import streamlit as st\\n        >>>\\n        >>> picture = st.camera_input(\"Take a picture\")\\n        >>>\\n        >>> if picture:\\n        ...     st.image(picture)', metadata={'source': 'streamlit/lib/streamlit/elements/camera_input.py', 'content_type': 'functions_classes', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\n\\nx = st.camera_input(\"Label1\", help=\"help1\")\\n\\nif x is not None:\\n    st.image(x)\\n\\ny = st.camera_input(\"Label2\", help=\"help2\", disabled=True)', metadata={'source': 'streamlit/e2e/scripts/st_camera_input.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\n\\nst.text_input(\"text_input\")', metadata={'source': 'streamlit/e2e/scripts/app_hotkeys.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\nfrom streamlit import runtime\\n\\ni1 = st.text_input(\"text input 1\")\\nst.write(\\'value 1: \"\\', i1, \\'\"\\')\\n\\ni2 = st.text_input(\"text input 2\", \"default text\")\\nst.write(\\'value 2: \"\\', i2, \\'\"\\')', metadata={'source': 'streamlit/e2e/scripts/st_text_input.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\nfrom streamlit import runtime\\n\\ni1 = st.number_input(\"number input 1\")\\nst.write(\\'value 1: \"\\', i1, \\'\"\\')\\n\\ni2 = st.number_input(\"number input 2\", value=1)\\nst.write(\\'value 2: \"\\', i2, \\'\"\\')', metadata={'source': 'streamlit/e2e/scripts/st_number_input.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='from streamlit.elements.button import ButtonMixin\\nfrom streamlit.elements.camera_input import CameraInputMixin\\nfrom streamlit.elements.checkbox import CheckboxMixin\\nfrom streamlit.elements.color_picker import ColorPickerMixin', metadata={'source': 'streamlit/lib/streamlit/delta_generator.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import requests\\n\\nimport streamlit as st\\n\\n\\n@st.experimental_memo\\n# Code for: def audio():\\n\\n\\n@st.experimental_memo\\n# Code for: def video():\\n\\n\\naudio()\\nvideo()', metadata={'source': 'streamlit/e2e/scripts/st_media_replay.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='`Streamlit forum post <https://discuss.streamlit.io/t/st-video-doesnt-show-opencv-generated-mp4/3193/2>`_\\n           for more information.', metadata={'source': 'streamlit/lib/streamlit/elements/media.py', 'content_type': 'functions_classes', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st', metadata={'source': 'streamlit/e2e/scripts/st_main_menu.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='serde = CameraInputSerde()', metadata={'source': 'streamlit/lib/streamlit/elements/camera_input.py', 'content_type': 'functions_classes', 'language': 'python'}),\n",
       " Document(page_content='import streamlit.components.v1 as components\\n\\nsrc = \"http://not.a.real.url\"\\ncomponents.iframe(src, width=200, height=500, scrolling=True)', metadata={'source': 'streamlit/e2e/scripts/components_iframe.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='.. output::\\n           https://doc-select-slider.streamlitapp.com/\\n           height: 450px', metadata={'source': 'streamlit/lib/streamlit/elements/select_slider.py', 'content_type': 'functions_classes', 'language': 'python'}),\n",
       " Document(page_content='from streamlit.web.cli import main', metadata={'source': 'streamlit/lib/streamlit/__main__.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\nfrom streamlit import runtime', metadata={'source': 'streamlit/e2e/scripts/st_session_state.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\n\\nCAT_IMAGE = \"https://images.unsplash.com/photo-1552933529-e359b2477252?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=950&q=80\"', metadata={'source': 'streamlit/e2e/scripts/st_columns.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\nfrom streamlit import runtime\\n\\ns1 = st.sidebar.slider(\"Label A\", 0, 12345678, 12345678)\\nst.sidebar.write(\"Value A:\", s1)', metadata={'source': 'streamlit/e2e/scripts/st_slider.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='html = streamlit._main._html\\niframe = streamlit._main._iframe', metadata={'source': 'streamlit/lib/streamlit/components/v1/__init__.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import requests\\n\\nimport streamlit as st\\n\\nurl = \"https://www.w3schools.com/html/mov_bbb.mp4\"\\nfile = requests.get(url).content\\nst.video(file)', metadata={'source': 'streamlit/e2e/scripts/st_video.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\n\\nquery_params = st.experimental_get_query_params()\\nst.write(\"Current query string is:\", str(query_params))', metadata={'source': 'streamlit/e2e/scripts/st_experimental_get_query_params.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='from datetime import datetime, time\\n\\nimport streamlit as st\\nfrom streamlit import runtime\\n\\nw1 = st.time_input(\"Label 1\", time(8, 45))\\nst.write(\"Value 1:\", w1)\\n\\nw2 = st.time_input(\"Label 2\", datetime(2019, 7, 6, 21, 15))\\nst.write(\"Value 2:\", w2)', metadata={'source': 'streamlit/e2e/scripts/st_time_input.py', 'content_type': 'simplified_code', 'language': 'python'})]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    "    Callbacks,\n",
    ")\n",
    "qa_over_streamlit_code._get_docs(question=\"How do I get camera inputs in a streamlit app ?\", inputs={}, run_manager=CallbackManagerForChainRun.get_noop_manager())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Examples\\n        --------\\n        >>> import streamlit as st\\n        >>>\\n        >>> picture = st.camera_input(\"Take a picture\")\\n        >>>\\n        >>> if picture:\\n        ...     st.image(picture)', metadata={'source': 'streamlit/lib/streamlit/elements/camera_input.py', 'content_type': 'functions_classes', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\n\\nx = st.camera_input(\"Label1\", help=\"help1\")\\n\\nif x is not None:\\n    st.image(x)\\n\\ny = st.camera_input(\"Label2\", help=\"help2\", disabled=True)', metadata={'source': 'streamlit/e2e/scripts/st_camera_input.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\n\\nst.text_input(\"text_input\")', metadata={'source': 'streamlit/e2e/scripts/app_hotkeys.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\nfrom streamlit import runtime\\n\\ni1 = st.text_input(\"text input 1\")\\nst.write(\\'value 1: \"\\', i1, \\'\"\\')\\n\\ni2 = st.text_input(\"text input 2\", \"default text\")\\nst.write(\\'value 2: \"\\', i2, \\'\"\\')', metadata={'source': 'streamlit/e2e/scripts/st_text_input.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st\\nfrom streamlit import runtime\\n\\ni1 = st.number_input(\"number input 1\")\\nst.write(\\'value 1: \"\\', i1, \\'\"\\')\\n\\ni2 = st.number_input(\"number input 2\", value=1)\\nst.write(\\'value 2: \"\\', i2, \\'\"\\')', metadata={'source': 'streamlit/e2e/scripts/st_number_input.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='from streamlit.elements.button import ButtonMixin\\nfrom streamlit.elements.camera_input import CameraInputMixin\\nfrom streamlit.elements.checkbox import CheckboxMixin\\nfrom streamlit.elements.color_picker import ColorPickerMixin', metadata={'source': 'streamlit/lib/streamlit/delta_generator.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import requests\\n\\nimport streamlit as st\\n\\n\\n@st.experimental_memo\\n# Code for: def audio():\\n\\n\\n@st.experimental_memo\\n# Code for: def video():\\n\\n\\naudio()\\nvideo()', metadata={'source': 'streamlit/e2e/scripts/st_media_replay.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='import streamlit as st', metadata={'source': 'streamlit/e2e/scripts/st_main_menu.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='.. output::\\n           https://doc-select-slider.streamlitapp.com/\\n           height: 450px', metadata={'source': 'streamlit/lib/streamlit/elements/select_slider.py', 'content_type': 'functions_classes', 'language': 'python'}),\n",
       " Document(page_content='from streamlit.web.cli import main', metadata={'source': 'streamlit/lib/streamlit/__main__.py', 'content_type': 'simplified_code', 'language': 'python'})]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa._get_docs(question=\"How do I get camera inputs in a streamlit app ?\", inputs={}, run_manager=CallbackManagerForChainRun.get_noop_manager())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load up the Streamlit Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'docs'...\n",
      "Updating files: 100% (1110/1110), done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "if os.path.exists(\"docs\"):\n",
    "    shutil.rmtree(\"docs\")\n",
    "os.system(\"git clone https://github.com/streamlit/docs.git -branch main --depth 1 docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/248 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248/248 [00:27<00:00,  9.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='title: Welcome to Streamlit\\nfeatures:\\n  - title: Get started\\n    body: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\\n    color: violet-70\\n    icon: arrow_forward\\n    url: /library/get-started\\n    image: \"\"\\n  - title: API reference\\n    body: Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut\\n      fugit, sed quia.\\n    color: orange-70\\n    icon: dvr\\n    url: /library/api-reference\\n    image: \"\"\\n  - title: Topic guides\\n    body: Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut\\n      fugit, sed quia.\\n    color: l-blue-70\\n    icon: description\\n    url:\\n      Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit,\\n      sed quia.\\n    image: \"\"\\nwhats_new:\\n  - title: Feature title\\n    body: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Eleifend a\\n      facilisis sagittis, vitae nibh massa in facilisis et. Pretium eget non\\n      cursus purus tempus porta sodales.\\n    icon: visibility\\n    url: /library/get-started\\n    image: /img/logo.svg\\n  - title: Feature title\\n    body: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Eleifend a\\n      facilisis sagittis, vitae nibh massa in facilisis et. Pretium eget non\\n      cursus purus tempus porta sodales.\\n    url: /library/get-started\\n    icon: visibility\\n    image: \"\"\\n  - body: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Eleifend a\\n      facilisis sagittis, vitae nibh massa in facilisis et. Pretium eget non\\n      cursus purus tempus porta sodales.\\n    title: Feature title\\n    url: /library/get-started\\n    image: /img/logo.svg\\n    icon: edit\\n  - title: Feature title\\n    body: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Eleifend a\\n      facilisis sagittis, vitae nibh massa in facilisis et. Pretium eget non\\n      cursus purus tempus porta sodales.\\n    url: /library/get-started\\n    icon: edit\\n    image: \"\"\\nnews:\\n  - publish_date: April 7, 2021\\n    title: New Story 1\\n    body:\\n      Lorem ipsum dolor sit amet, consectetur adipiscing elit. In venenatis leo\\n      felis, quis egestas est dignissim vel.\\n    url: https://www.streamlit.io\\n  - publish_date: April 7, 2021\\n    title: New Story 2\\n    body:\\n      Lorem ipsum dolor sit amet, consectetur adipiscing elit. In venenatis leo\\n      felis, quis egestas est dignissim vel. Pellentesque interdum massa metus,\\n      vitae tincidunt nibh molestie consequat. Sed congue commodo bibendum.\\n      Donec iaculis velit ante, a venenatis elit dictum ut. Etiam pellentesque\\n      libero ex, vitae eleifend quam egestas et. Vivamus pulvinar libero et\\n      iaculis placerat. Suspendisse bibendum orci a turpis suscipit dictum sed\\n      convallis risus. Aliquam eros erat, hendrerit vel viverra pretium,\\n      vulputate ac dui. Nullam vitae arcu ut enim tincidunt blandit. Donec\\n      sapien mi, vulputate eu sagittis pellentesque, imperdiet elementum nisl.\\n      In dapibus quam id magna pretium finibus. Curabitur id nunc dolor.\\n      Pellentesque eu tellus vitae neque sollicitudin sodales. Integer arcu\\n      urna, rutrum vel arcu sit amet, aliquam lobortis est.\\n    url: https://www.streamlit.io\\nnext: get-started\\n\\nStreamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps - so let’s get started!', metadata={'source': 'docs/content/index.md'}),\n",
       " Document(page_content='enabled: false\\ntitle: We value your privacy.\\n\\nWe would like to use cookies to help us understand how users interact with this website. This is used, for example, to find out which parts of this site should be further improved.\\n\\nMore information can be found in our Privacy Notice.', metadata={'source': 'docs/content/gdpr-banner.md'}),\n",
       " Document(page_content='title: Installing dependencies\\nslug: /knowledge-base/dependencies\\n\\nInstalling dependencies\\n\\nModuleNotFoundError: No module named\\n\\nImportError: libGL.so.1: cannot open shared object file: No such file or directory\\n\\nERROR: No matching distribution found for\\n\\nHow to install a package not on PyPI/Conda but available on GitHub\\n\\nInstall the Snowflake Connector for Python on Streamlit Community Cloud', metadata={'source': 'docs/content/kb/dependencies/index.md'}),\n",
       " Document(page_content='title: Knowledge Base\\nslug: /knowledge-base\\n\\nKnowledge base\\n\\nThe knowledge base is a self-serve library of tips, step-by-step tutorials, and articles that answer your questions about creating and deploying Streamlit apps.\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"local_library\"\\n    bold=\"Tutorials.\"\\n    href=\"/knowledge-base/tutorials\"\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"auto_awesome\"\\n    bold=\"Using Streamlit.\"\\n    href=\"/knowledge-base/using-streamlit\"\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"build\"\\n    bold=\"Streamlit Components.\"\\n    href=\"/knowledge-base/components\"\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"downloading\"\\n    bold=\"Installing dependencies.\"\\n    href=\"/knowledge-base/dependencies\"\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"report\"\\n    bold=\"Deployment issues.\"\\n    href=\"/knowledge-base/deploy\"', metadata={'source': 'docs/content/kb/index.md'}),\n",
       " Document(page_content=\"title: ModuleNotFoundError No module named\\nslug: /knowledge-base/dependencies/module-not-found-error\\n\\nModuleNotFoundError: No module named\\n\\nProblem\\n\\nYou receive the error ModuleNotFoundError: No module named when you deploy an app on Streamlit Community Cloud.\\n\\nSolution\\n\\nThis error occurs when you import a module on Streamlit Community Cloud that isn’t included in your requirements file. Any external Python dependencies that are not distributed with a standard Python installation should be included in your requirements file.\\n\\nE.g. You will see ModuleNotFoundError: No module named 'sklearn' if you don’t include scikit-learn in your requirements file and import sklearn in your app.\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/getting-error-modulenotfounderror-no-module-named-beautifulsoup/9126\\n\\nhttps://discuss.streamlit.io/t/modulenotfounderror-no-module-named-vega-datasets/16354\", metadata={'source': 'docs/content/kb/dependencies/module-not-found-error.md'}),\n",
       " Document(page_content='title: Install the Snowflake Connector for Python on Streamlit Community Cloud\\nslug: /knowledge-base/dependencies/snowflake-connector-python-streamlit-cloud\\n\\nInstall the Snowflake Connector for Python on Streamlit Community Cloud\\n\\nThe Snowflake Connector for Python is available on PyPI and the installation instructions are found in the Snowflake documentation. When installing the connector, Snowflake recommends installing specific versions of its dependent libraries. The steps below will help you install the connector and its dependencies on Streamlit Community Cloud:\\n\\nDetermine the version of the Snowflake Connector for Python you want to install.\\n\\nDetermine the version of Python you want to use on Streamlit Community Cloud.\\n\\nTo install the connector and the dependent libraries, select the requirements file for that version of the connector and Python.\\n\\nAdd the raw GitHub URL of the requirements file to your requirements.txt file and prepend -r to the line.\\n   For example, if you want to install version 2.7.9 of the connector on Python 3.9, add the following line to your requirements.txt file:\\n\\nbash\\n   -r https://raw.githubusercontent.com/snowflakedb/snowflake-connector-python/v2.7.9/tested_requirements/requirements_39.reqs\\n\\nOn Streamlit Community Cloud, select the appropriate version of Python for your app by clicking \"Advanced settings\" before you deploy the app:\\n\\nThat\\'s it! You\\'re ready to use the Snowflake Connector for Python on Streamlit Community Cloud. ❄️🎈\\n\\nAs the Snowflake dependencies requirements files (.reqs) contain the pinned version of the connector, there is no need add a separate entry for the connector to requirements.txt.\\n\\nAdditional resources:\\n\\nInstalling the Python Connector\\n\\nUnable to Deploy streamlit app with snowflake-connector-python\\n\\nPrerequisite Python packages for the Snowflake Connector', metadata={'source': 'docs/content/kb/dependencies/snowflake-connector-python.md'}),\n",
       " Document(page_content='title: How to install a package not on PyPI/Conda but available on GitHub\\nslug: /knowledge-base/dependencies/install-package-not-pypi-conda-available-github\\n\\nHow to install a package not on PyPI/Conda but available on GitHub\\n\\nOverview\\n\\nAre you trying to deploy your app to Streamlit Community Cloud, but don\\'t know how to specify a Python dependency in your requirements file that is available on a public GitHub repo but not any package index like PyPI or Conda? If so, continue reading to find out how!\\n\\nLet\\'s suppose you want to install SomePackage and its Python dependencies from GitHub, a hosting service for the popular version control system (VCS) Git. And suppose SomePackage is found at the the following URL: https://github.com/SomePackage.git.\\n\\npip (via requirements.txt) supports installing from GitHub. This support requires a working executable to be available (for Git). It is used through a URL prefix: git+.\\n\\nSpecify the GitHub web URL\\n\\nTo install SomePackage, innclude the following in your requirements.txt file:\\n\\nbash\\ngit+https://github.com/SomePackage#egg=SomePackage\\n\\nYou can even specify a \"git ref\" such as branch name, a commit hash or a tag name, as shown in the examples below.\\n\\nSpecify a Git branch name\\n\\nInstall SomePackage by specifying a branch name such as main, master, develop, etc, in requirements.txt:\\n\\nbash\\ngit+https://github.com/SomePackage.git@main#egg=SomePackage\\n\\nSpecify a commit hash\\n\\nInstall SomePackage by specifying a commit hash in requirements.txt:\\n\\nbash\\ngit+https://github.com/SomePackage.git@eb40b4ff6f7c5c1e4366cgfg0671291bge918#egg=SomePackage\\n\\nSpecify a tag\\n\\nInstall SomePackage by specifying a tag in requirements.txt:\\n\\nbash\\ngit+https://github.com/SomePackage.git@v1.1.0#egg=SomePackage\\n\\nLimitations\\n\\nIt is currently not possible to install private packages from private GitHub repos using the URI form:\\n\\nbash\\ngit+https://{token}@github.com/user/project.git@{version}\\n\\nwhere version is a tag, a branch, or a commit. And token is a personal access token with read only permissions. Streamlit Community Cloud only supports installing public packages from public GitHub repos.', metadata={'source': 'docs/content/kb/dependencies/install-package-pypi-github.md'}),\n",
       " Document(page_content='title: ERROR No matching distribution found for\\nslug: /knowledge-base/dependencies/no-matching-distribution\\n\\nERROR: No matching distribution found for\\n\\nProblem\\n\\nYou receive the error ERROR: No matching distribution found for when you deploy an app on Streamlit Community Cloud.\\n\\nSolution\\n\\nThis error occurs when you deploy an app on Streamlit Community Cloud and have one or more of the following issues with your Python dependencies in your requirements file:\\n\\nThe package is part of the Python Standard Library. E.g. You will see ERROR: No matching distribution found for base64 if you include base64 in your requirements file, as it is part of the Python Standard Library. The solution is to not include the package in your requirements file. Only include packages in your requirements file that are not distributed with a standard Python installation.\\n\\nThe package name in your requirements file is misspelled. Double-check the package name before including it in your requirements file.\\n\\nThe package does not support the operating system on which your Streamlit app is running. E.g. You see ERROR: No matching distribution found for pywin32 while deploying to Streamlit Community Cloud. The pywin32 module provides access to many of the Windows APIs from Python. Apps deployed to Streamlit Community Cloud are executed in a Linux environment. As such, pywin32 fails to install on non-Windows systems, including on Streamlit Community Cloud. The solution is to either exclude pywin32 from your requirements file, or deploy your app on a cloud service offering Windows machines.\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/error-no-matching-distribution-found-for-base64/15758\\n\\nhttps://discuss.streamlit.io/t/error-could-not-find-a-version-that-satisfies-the-requirement-pywin32-301-from-versions-none/15343/2', metadata={'source': 'docs/content/kb/dependencies/no-matching-distribution.md'}),\n",
       " Document(page_content='title: ImportError libGL.so.1 cannot open shared object file No such file or directory\\nslug: /knowledge-base/dependencies/libgl\\n\\nImportError libGL.so.1 cannot open shared object file No such file or directory\\n\\nProblem\\n\\nYou receive the error ImportError libGL.so.1 cannot open shared object file No such file or directory when using OpenCV in your app deployed on Streamlit Community Cloud.\\n\\nSolution\\n\\nIf you use OpenCV in your app, include opencv-python-headless in your requirements file on Streamlit Community Cloud in place of opencv_contrib_python and opencv-python.\\n\\nIf opencv-python is a required (non-optional) dependency of your app or a dependency of a library used in your app, the above solution is not applicable. Instead, you can use the following solution:\\n\\nCreate a packages.txt file in your repo with the following line to install the apt-get dependency libgl:\\n\\nlibgl1', metadata={'source': 'docs/content/kb/dependencies/libgl.md'}),\n",
       " Document(page_content='title: Using Streamlit\\nslug: /knowledge-base/using-streamlit\\n\\nUsing Streamlit\\n\\nHere are some frequently asked questions about using Streamlit. If you feel something important is missing that everyone needs to know, please open an issue or submit a pull request and we\\'ll be happy to review it!\\n\\nSanity checks\\n\\nBatch elements and input widgets with st.form\\n\\nHow do I run my Streamlit script?\\n\\nHow can I make Streamlit watch for changes in other modules I\\'m importing in my app?\\n\\nWhat browsers does Streamlit support?\\n\\nWhat is the path of Streamlit’s config.toml file?\\n\\nWhere does st.file_uploader store uploaded files and when do they get deleted?\\n\\nHow do you retrieve the filename of a file uploaded with st.file_uploader?\\n\\nHow to remove \"· Streamlit\" from the app title?\\n\\nHow to download a file in Streamlit?\\n\\nHow to download a Pandas DataFrame as a CSV?\\n\\nHow can I make st.pydeck_chart use custom Mapbox styles?\\n\\nHow to insert elements out of order?\\n\\nHow to animate elements?\\n\\nAppend data to a table or chart\\n\\nHide row indices when displaying a dataframe\\n\\nHow do I upgrade to the latest version of Streamlit?\\n\\nWidget updating for every second input when using session state\\n\\nHow do I create an anchor link?\\n\\nHow do I enable camera access?\\n\\nWhy does Streamlit restrict nested st.columns?\\n\\nHow to host static files in Streamlit?\\n\\nWhat is serializable session state?', metadata={'source': 'docs/content/kb/using-streamlit/index.md'}),\n",
       " Document(page_content=\"title: Enabling camera access in your browser\\nslug: /knowledge-base/using-streamlit/enable-camera\\n\\nEnabling camera access in your browser\\n\\nStreamlit apps may include a widget to upload images directly from your computer's camera. To\\nsafeguard the users' privacy and security, browsers require users to explicitly allow access to the\\ncamera before this widget can work.\\n\\nTo learn how to enable camera access, please check the documentation for your browser:\\n\\nChrome\\n\\nSafari\\n\\nFirefox\", metadata={'source': 'docs/content/kb/using-streamlit/enable-camera.md'}),\n",
       " Document(page_content='title: Why does Streamlit restrict nested st.columns?\\nslug: /knowledge-base/using-streamlit/why-streamlit-restrict-nested-columns\\n\\nWhy does Streamlit restrict nested st.columns?\\n\\nStarting in version 1.18.0, Streamlit allows nesting st.columns inside other\\nst.columns with the following restrictions:\\n\\nIn the main area of the app, columns can be nested up to one level of nesting.\\n\\nIn the sidebar, columns cannot be nested.\\n\\nThese restrictions are in place to make Streamlit apps look good on all device sizes. Nesting columns multiple times often leads to a bad UI.\\nYou might be able to make it look good on one screen size but as soon as a user on a different screen views the app,\\nthey will have a bad experience. Some columns will be tiny, others will be way too long, and complex layouts will look out of place.\\nStreamlit tries its best to automatically resize elements to look good across devices, without any help from the developer.\\nBut for complex layouts with multiple levels of nesting, this is not possible.\\n\\nWe are always working on improving layout options though! So if you have a use case that requires a more complex layout,\\nplease open a GitHub issue, ideally with a sketch of what you want to do.', metadata={'source': 'docs/content/kb/using-streamlit/why-streamlit-restrict-nested-columns.md'}),\n",
       " Document(page_content='title: How to insert elements out of order?\\nslug: /knowledge-base/using-streamlit/insert-elements-out-of-order\\n\\nHow to insert elements out of order?\\n\\nYou can use the st.empty method as a placeholder,\\nto \"save\" a slot in your app that you can use later.\\n\\n```python\\nst.text(\\'This will appear first\\')\\n\\nAppends some text to the app.\\n\\nmy_slot1 = st.empty()\\n\\nAppends an empty slot to the app. We\\'ll use this later.\\n\\nmy_slot2 = st.empty()\\n\\nAppends another empty slot.\\n\\nst.text(\\'This will appear last\\')\\n\\nAppends some more text to the app.\\n\\nmy_slot1.text(\\'This will appear second\\')\\n\\nReplaces the first empty slot with a text string.\\n\\nmy_slot2.line_chart(np.random.randn(20, 2))\\n\\nReplaces the second empty slot with a chart.\\n\\n```', metadata={'source': 'docs/content/kb/using-streamlit/insert-elements-out-of-order.md'}),\n",
       " Document(page_content=\"title: How to animate elements?\\nslug: /knowledge-base/using-streamlit/animate-elements\\n\\nHow to animate elements?\\n\\nLet's combine some of the things you've learned to create compelling\\nanimations in your app.\\n\\n```python\\nprogress_bar = st.progress(0)\\nstatus_text = st.empty()\\nchart = st.line_chart(np.random.randn(10, 2))\\n\\nfor i in range(100):\\n    # Update progress bar.\\n    progress_bar.progress(i + 1)\\n\\nstatus_text.text('Done!')\\nst.balloons()\\n```\", metadata={'source': 'docs/content/kb/using-streamlit/animate-elements.md'}),\n",
       " Document(page_content=\"title: How can I make st.pydeck_chart use custom Mapbox styles?\\nslug: /knowledge-base/using-streamlit/pydeck-chart-custom-mapbox-styles\\n\\nHow can I make st.pydeck_chart use custom Mapbox styles?\\n\\nIf you are supplying a Mapbox token, but the resulting pydeck_chart doesn't show your custom Mapbox styles, please check that you are adding the Mapbox token to the Streamlit config.toml configuration file. Streamlit DOES NOT read Mapbox tokens from inside of a PyDeck specification (i.e. from inside of the Streamlit app). Please see this forum thread for more information.\", metadata={'source': 'docs/content/kb/using-streamlit/pydeck-chart-custom-mapbox-styles.md'}),\n",
       " Document(page_content='title: Widget updating for every second input when using session state\\nslug: /knowledge-base/using-streamlit/widget-updating-session-state\\n\\nWidget updating for every second input when using session state\\n\\nOverview\\n\\nYou are using session state to store page interactions in your app. When users interact with a widget in your app (e.g., click a button), you expect your app to update its widget states and reflect the new values. However, you notice that it doesn\\'t. Instead, users have to interact with the widget twice (e.g., click a button twice) for the app to show the correct values. What do you do now? 🤔 Let\\'s walk through the solution in the section below.\\n\\nSolution\\n\\nWhen using session state to update widgets or values in your script, you need to use the unique key you assigned to the widget, not the variable that you assigned your widget to. In the example code block below, the unique key assigned to the slider widget is slider, and the variable the widget is assigned to is slide_val.\\n\\nLet\\'s see this in an example. Say you want a user to click a button that resets a slider.\\n\\nTo have the slider\\'s value update on the button click, you need to use a callback function with the on_click parameter of st.button:\\n\\n```python\\n\\nthe callback function for the button will add 1 to the\\n\\nslider value up to 10\\n\\ndef plus_one():\\n    if st.session_state[\"slider\"] < 10:\\n        st.session_state.slider += 1\\n    else:\\n        pass\\n    return\\n\\nwhen creating the button, assign the name of your callback\\n\\nfunction to the on_click parameter\\n\\nadd_one = st.button(\"Add one to the slider\", on_click=plus_one, key=\"add_one\")\\n\\ncreate the slider\\n\\nslide_val = st.slider(\"Pick a number\", 0, 10, key=\"slider\")\\n```\\n\\nRelevant resources\\n\\nCaching Sqlite DB connection resulting in glitchy rendering of the page\\n\\nSelect all checkbox that is linked to selectbox of options', metadata={'source': 'docs/content/kb/using-streamlit/widget-updating-session-state.md'}),\n",
       " Document(page_content='title: How to download a Pandas DataFrame as a CSV?\\nslug: /knowledge-base/using-streamlit/how-download-pandas-dataframe-csv\\n\\nHow to download a Pandas DataFrame as a CSV?\\n\\nUse the st.download_button widget that is natively built into Streamlit. Check out a sample app demonstrating how you can use st.download_button to download common file formats.\\n\\nExample usage\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\ndf = pd.read_csv(\"dir/file.csv\")\\n\\n@st.experimental_memo\\ndef convert_df(df):\\n   return df.to_csv(index=False).encode(\\'utf-8\\')\\n\\ncsv = convert_df(df)\\n\\nst.download_button(\\n   \"Press to Download\",\\n   csv,\\n   \"file.csv\",\\n   \"text/csv\",\\n   key=\\'download-csv\\'\\n)\\n```\\n\\nAdditional resources:\\n\\nhttps://blog.streamlit.io/0-88-0-release-notes/\\n\\nhttps://streamlit-release-demos-0-88streamlit-app-0-88-v8ram3.streamlit.app/\\n\\nhttps://docs.streamlit.io/library/api-reference/widgets/st.download_button', metadata={'source': 'docs/content/kb/using-streamlit/how-download-pandas-dataframe-csv.md'}),\n",
       " Document(page_content='title: What is the path of Streamlit’s config.toml file?\\nslug: /knowledge-base/using-streamlit/path-streamlit-config-toml\\n\\nWhat is the path of Streamlit’s config.toml file?\\n\\nA global config file is found at ~/.streamlit/config.toml for macOS/Linux or %userprofile%/.streamlit/config.toml for Windows.\\n\\nA per-project config file can be created at $CWD/.streamlit/config.toml, where $CWD is the folder you’re running Streamlit from.\\n\\nClick here to learn more about Streamlit configuration options.', metadata={'source': 'docs/content/kb/using-streamlit/path-streamlit-config-toml.md'}),\n",
       " Document(page_content='title: How do I create an anchor link?\\nslug: /knowledge-base/using-streamlit/create-anchor-link\\n\\nHow do I create an anchor link?\\n\\nOverview\\n\\nHave you wanted to create anchors so that users of your app can directly navigate to specific sections by specifying #anchor in the URL? If so, let\\'s find out how.\\n\\nSolution\\n\\nAnchors are automatically added to header text.\\n\\nFor example, if you define a header text via the st.header() command as follows:\\n\\npython\\nst.header(\"Section 1\")\\n\\nThen you can create a link to this header using:\\n\\npython\\nst.markdown(\"[Section 1](#section-1)\")\\n\\nExamples\\n\\nDemo app: https://dataprofessor-streamlit-anchor-app-80kk8w.streamlit.app/\\n\\nGitHub repo: https://github.com/dataprofessor/streamlit/blob/main/anchor_app.py', metadata={'source': 'docs/content/kb/using-streamlit/create-anchor-link.md'}),\n",
       " Document(page_content='title: Hide row indices when displaying a dataframe\\nslug: /knowledge-base/using-streamlit/hide-row-indices-displaying-dataframe\\n\\nHide row indices when displaying a dataframe\\n\\nOverview\\n\\nStreamlit offers two ways to display a dataframe: as a static table using st.table(), and as an interactive table using st.dataframe().\\n\\nBoth options display row indices in the left-most column. To see this in action, let\\'s display a dataframe with random entries using both st.table() and st.dataframe():\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame(\\n    np.random.randn(10, 5),\\n    columns=(\"col %d\" % i for i in range(5)))\\n\\nDisplay a static table\\n\\nst.table(df)\\n\\nDisplay an interactive table\\n\\nst.dataframe(df)\\n```\\n\\nNotice how row indices are displayed to the left of the col0 column: 👇\\n\\nTo hide the column containing row indices, you can use CSS selectors to modify the visibility of the column. Before you display your dataframe, you must inject the appropriate CSS with st.markdown(), and set unsafe_allow_html=True.\\n\\nNow that you have a conceptual understanding of how to hide row indices, let\\'s implement it in code!\\n\\nHide row indices with st.table\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame(\\n    np.random.randn(10, 5),\\n    columns=(\"col %d\" % i for i in range(5)))\\n\\nCSS to inject contained in a string\\n\\nhide_table_row_index = \"\"\"\\n            \\n            thead tr th:first-child {display:none}\\n            tbody th {display:none}\\n            \\n            \"\"\"\\n\\nInject CSS with Markdown\\n\\nst.markdown(hide_table_row_index, unsafe_allow_html=True)\\n\\nDisplay a static table\\n\\nst.table(df)\\n```\\n\\nHide row indices with st.dataframe\\n\\nThe below workaround for st.dataframe does not work for streamlit>=1.10.0.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame(\\n    np.random.randn(10, 5),\\n    columns=(\"col %d\" % i for i in range(5)))\\n\\nCSS to inject contained in a string\\n\\nhide_dataframe_row_index = \"\"\"\\n            \\n            .row_heading.level0 {display:none}\\n            .blank {display:none}\\n            \\n            \"\"\"\\n\\nInject CSS with Markdown\\n\\nst.markdown(hide_dataframe_row_index, unsafe_allow_html=True)\\n\\nDisplay an interactive table\\n\\nst.dataframe(df)\\n```', metadata={'source': 'docs/content/kb/using-streamlit/hide-row-indices-display-dataframe.md'}),\n",
       " Document(page_content='title: Sanity checks\\nslug: /knowledge-base/using-streamlit/sanity-checks\\n\\nSanity checks\\n\\nIf you\\'re having problems running your Streamlit app, here are a few things to try out.\\n\\nCheck #0: Are you using a Streamlit-supported version of Python?\\n\\nStreamlit will maintain backwards-compatibility with earlier Python versions as practical,\\nguaranteeing compatibility with at least the last three minor versions of Python 3.\\n\\nAs new versions of Python are released, we will try to be compatible with the new version as soon\\nas possible, though frequently we are at the mercy of other Python packages to support these new versions as well.\\n\\nStreamlit currently supports versions 3.8, 3.9, 3.10, and 3.11 of Python.\\n\\nCheck #1: Is Streamlit running?\\n\\nOn a Mac or Linux machine, type this on the terminal:\\n\\nbash\\nps -Al | grep streamlit\\n\\nIf you don\\'t see streamlit run in the output (or streamlit hello, if that\\'s\\nthe command you ran) then the Streamlit server is not running. So re-run your command and see if the bug goes away.\\n\\nCheck #2: Is this an already-fixed Streamlit bug?\\n\\nWe try to fix bugs quickly, so many times a problem will go away when you\\nupgrade Streamlit. So the first thing to try when having an issue is upgrading\\nto the latest version of Streamlit:\\n\\nbash\\npip install --upgrade streamlit\\nstreamlit version\\n\\n...and then verify that the version number printed corresponds to the version number displayed on PyPI.\\n\\nTry reproducing the issue now. If not fixed, keep reading on.\\n\\nCheck #3: Are you running the correct Streamlit binary?\\n\\nLet\\'s check whether your Python environment is set up correctly. Edit the\\nStreamlit script where you\\'re experiencing your issue, comment everything\\nout, and add these lines instead:\\n\\npython\\nimport streamlit as st\\nst.write(st.__version__)\\n\\n...then call streamlit run on your script and make sure it says the same\\nversion as above. If not the same version, check out these\\ninstructions for some sure-fire ways to set up your\\nenvironment.\\n\\nCheck #4: Is your browser caching your app too aggressively?\\n\\nThere are two easy ways to check this:\\n\\nLoad your app in a browser then press Ctrl-Shift-R or ⌘-Shift-R to do a\\n   hard refresh (Chrome/Firefox).\\n\\nAs a test, run Streamlit on another port. This way the browser starts the\\n   page with a brand new cache. For that, pass the --server.port\\n   argument to Streamlit on the command line:\\n\\nbash\\n   streamlit run my_app.py --server.port=9876\\n\\nCheck #5: Is this a Streamlit regression?\\n\\nIf you\\'ve upgraded to the latest version of Streamlit and things aren\\'t\\nworking, you can downgrade at any time using this command:\\n\\nbash\\npip install --upgrade streamlit==1.0.0\\n\\n...where 1.0.0 is the version you\\'d like to downgrade to. See\\nChangelog for a complete list of Streamlit versions.\\n\\nCheck #6 [Windows]: Is Python added to your PATH?\\n\\nWhen installed by downloading from python.org, Python is\\nnot automatically added to the Windows system PATH. Because of this, you may get error messages\\nlike the following:\\n\\nCommand Prompt:\\n\\nbash\\nC:\\\\Users\\\\streamlit> streamlit hello\\n\\'streamlit\\' is not recognized as an internal or external command,\\noperable program or batch file.\\n\\nPowerShell:\\n\\nbash\\nPS C:\\\\Users\\\\streamlit> streamlit hello\\nstreamlit : The term \\'streamlit\\' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that\\nthe path is correct and try again.\\nAt line:1 char:1\\n+ streamlit hello\\n+ ~~~~~~~~~\\n    + CategoryInfo          : ObjectNotFound: (streamlit:String) [], CommandNotFoundException\\n    + FullyQualifiedErrorId : CommandNotFoundException\\n\\nTo resolve this issue, add Python to the Windows system PATH.\\n\\nAfter adding Python to your Windows PATH, you should then be able to follow the instructions in our Get Started section.\\n\\nCheck #7 [Windows]: Do you need Build Tools for Visual Studio installed?\\n\\nStreamlit includes pyarrow as an install dependency. Occasionally, when trying to install Streamlit from PyPI, you may see errors such as the following:\\n\\n```bash\\nUsing cached pyarrow-1.0.1.tar.gz (1.3 MB)\\n  Installing build dependencies ... error\\n  ERROR: Command errored out with exit status 1:\\n   command: \\'c:\\\\users\\\\streamlit\\\\appdata\\\\local\\\\programs\\\\python\\\\python38-32\\\\python.exe\\' \\'c:\\\\users\\\\streamlit\\\\appdata\\\\local\\\\programs\\\\python\\\\python38-32\\\\lib\\\\site-packages\\\\pip\\' install --ignore-installed --no-user --prefix \\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-s7owjrle\\\\overlay\\' --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- \\'cython >= 0.29\\' \\'numpy==1.14.5; python_version<\\'\"\\'\"\\'3.8\\'\"\\'\"\\'\\' \\'numpy==1.16.0; python_version>=\\'\"\\'\"\\'3.8\\'\"\\'\"\\'\\' setuptools setuptools_scm wheel\\n       cwd: None\\n\\nComplete output (319 lines):\\n\\nERROR: Command errored out with exit status 1: \\'c:\\\\users\\\\streamlit\\\\appdata\\\\local\\\\programs\\\\python\\\\python38-32\\\\python.exe\\' -u -c \\'import sys, setuptools, tokenize; sys.argv[0] = \\'\"\\'\"\\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-install-0jwfwx_u\\\\numpy\\\\setup.py\\'\"\\'\"\\'; file=\\'\"\\'\"\\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-install-0jwfwx_u\\\\numpy\\\\setup.py\\'\"\\'\"\\';f=getattr(tokenize, \\'\"\\'\"\\'open\\'\"\\'\"\\', open)(file);code=f.read().replace(\\'\"\\'\"\\'\\\\r\\\\n\\'\"\\'\"\\', \\'\"\\'\"\\'\\\\n\\'\"\\'\"\\');f.close();exec(compile(code, file, \\'\"\\'\"\\'exec\\'\"\\'\"\\'))\\' install --record \\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-record-eys4l2gc\\\\install-record.txt\\' --single-version-externally-managed --prefix \\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-s7owjrle\\\\overlay\\' --compile --install-headers \\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-s7owjrle\\\\overlay\\\\Include\\\\numpy\\' Check the logs for full command output.\\n\\n```\\n\\nThis error indicates that Python is trying to compile certain libraries during install, but it cannot find the proper compilers on your system,\\nas reflected by the line error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\".\\n\\nInstalling Build Tools for Visual Studio should resolve this issue.', metadata={'source': 'docs/content/kb/using-streamlit/sanity-checks.md'}),\n",
       " Document(page_content='title: How to remove \"· Streamlit\" from the app title?\\nslug: /knowledge-base/using-streamlit/remove-streamlit-app-title\\n\\nHow to remove \"· Streamlit\" from the app title?\\n\\nUsing st.set_page_config to assign the page title will not append \"· Streamlit\" to that title. E.g.:\\n\\n```python\\nimport streamlit as st\\n\\nst.set_page_config(\\n   page_title=\"Ex-stream-ly Cool App\",\\n   page_icon=\"🧊\",\\n   layout=\"wide\",\\n   initial_sidebar_state=\"expanded\",\\n)\\n```', metadata={'source': 'docs/content/kb/using-streamlit/remove-streamlit-app-title.md'}),\n",
       " Document(page_content='title: Where does st.file_uploader store uploaded files and when do they get deleted?\\nslug: /knowledge-base/using-streamlit/where-file-uploader-store-when-deleted\\n\\nWhere does st.file_uploader store uploaded files and when do they get deleted?\\n\\nWhen you upload a file using st.file_uploader, the data are copied to the Streamlit backend via the browser, and contained in a BytesIO buffer in Python memory (i.e. RAM, not disk). The data will persist in RAM until the Streamlit app re-runs from top-to-bottom, which is on each widget interaction. If you need to save the data that was uploaded between runs, then you can cache it so that Streamlit persists it across re-runs.\\n\\nAs files are stored in memory, they get deleted immediately as soon as they’re not needed anymore.\\n\\nThis means Streamlit removes a file from memory when:\\n\\nThe user uploads another file, replacing the original one\\n\\nThe user clears the file uploader\\n\\nThe user closes the browser tab where they uploaded the file\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/streamlit-sharing-fileupload-where-does-it-go/9267\\n\\nhttps://discuss.streamlit.io/t/how-to-update-the-uploaded-file-using-file-uploader/13512/', metadata={'source': 'docs/content/kb/using-streamlit/where-file-uploader-store-when-deleted.md'}),\n",
       " Document(page_content=\"title: Batch elements and input widgets with st.form\\nslug: /knowledge-base/using-streamlit/batch-elements-input-widgets-form\\n\\nBatch elements and input widgets with st.form\\n\\nLet's take a look at how to use st.form to batch elements and input widgets.\\n\\nIn Streamlit, every widget interaction causes a rerun of the app. However,\\nthere are times when you might want to interact with a couple of widgets and\\nsubmit those interactions while triggering a single re-run of the app.\\n\\nUsing st.form you can batch input widgets together and along with\\nst.form_submit_button submit the state inside these widgets with the click\\nof a single button.\\n\\n```python\\n\\nForms can be declared using the 'with' syntax\\n\\nwith st.form(key='my_form'):\\n    text_input = st.text_input(label='Enter your name')\\n    submit_button = st.form_submit_button(label='Submit')\\n```\\n\\n```python\\n\\nAlternative syntax, declare a form and use the returned object\\n\\nform = st.form(key='my_form')\\nform.text_input(label='Enter some text')\\nsubmit_button = form.form_submit_button(label='Submit')\\n```\\n\\n```python\\n\\nst.form_submit_button returns True upon form submit\\n\\nif submit_button:\\n    st.write(f'hello {name}')\\n```\\n\\nForms can appear anywhere in your app (sidebar, columns etc), but there are\\nsome constraints:\\n\\nA form cannot have interdependent widgets, i.e. the output of widget1 cannot\\n  be the input to widget2 inside a form.\\n\\nBy design, interacting with widgets inside st.form does not trigger\\n  a re-run. Because of this reason, st.button cannot be declared inside st.form.\\n\\nst.form cannot be embedded inside another st.form.\\n\\nForms must have an associated st.form_submit_button. Clicking this button\\n  triggers a re-run. Streamlit throws an error if a form does not have an\\n  associated st.form_submit_button.\", metadata={'source': 'docs/content/kb/using-streamlit/batch-elements-widgets.md'}),\n",
       " Document(page_content=\"title: How can I make Streamlit watch for changes in other modules I'm importing in my app?\\nslug: /knowledge-base/using-streamlit/streamlit-watch-changes-other-modules-importing-app\\n\\nHow can I make Streamlit watch for changes in other modules I'm importing in my app?\\n\\nBy default, Streamlit only watches modules contained in the current directory of the main app module. You can track other modules by adding the parent directory of each module to the PYTHONPATH.\\n\\nbash\\nexport PYTHONPATH=$PYTHONPATH:/path/to/module\\nstreamlit run your_script.py\", metadata={'source': 'docs/content/kb/using-streamlit/streamlit-watch-changes-other-modules-importing-app.md'}),\n",
       " Document(page_content='title: How do you retrieve the filename of a file uploaded with st.file_uploader?\\nslug: /knowledge-base/using-streamlit/retrieve-filename-uploaded\\n\\nHow do you retrieve the filename of a file uploaded with st.file_uploader?\\n\\nIf you upload a single file (i.e. accept_multiple_files=False), the filename can be retrieved by using the .name attribute on the returned UploadedFile object:\\n\\n```python\\nimport streamlit as st\\n\\nuploaded_file = st.file_uploader(\"Upload a file\")\\n\\nif uploaded_file:\\n   st.write(\"Filename: \", uploaded_file.name)\\n```\\n\\nIf you upload multiple files (i.e. accept_multiple_files=True), the individual filenames can be retrieved by using the .name attribute on each UploadedFile object in the returned list:\\n\\n```python\\nimport streamlit as st\\n\\nuploaded_files = st.file_uploader(\"Upload multiple files\", accept_multiple_files=True)\\n\\nif uploaded_files:\\n   for uploaded_file in uploaded_files:\\n       st.write(\"Filename: \", uploaded_file.name)\\n```\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/is-it-possible-to-get-uploaded-file-file-name/7586', metadata={'source': 'docs/content/kb/using-streamlit/retrieve-filename-uploaded.md'}),\n",
       " Document(page_content=\"title: How do I upgrade to the latest version of Streamlit?\\nslug: /knowledge-base/using-streamlit/how-upgrade-latest-version-streamlit\\n\\nHow do I upgrade to the latest version of Streamlit?\\n\\nWe recommend upgrading to the latest official release of Streamlit so you have access to the newest, cutting-edge features. If you haven't installed Streamlit yet, please read our Installation guide. It helps you set up your virtual environment and walks you through installing Streamlit on Windows, macOS, and Linux. Regardless of which package management tool and OS you're using, we recommend running the commands on this page in a virtual environment.\\n\\nIf you've previously installed Streamlit and want to upgrade to the latest version, here's how to do it based on your dependency manager.\\n\\nPipenv\\n\\nStreamlit's officially-supported environment manager for macOS and Linux is Pipenv.\\n\\nNavigate to the project folder containing your Pipenv environment:\\n\\nbash\\ncd myproject\\n\\nActivate that environment, upgrade Streamlit, and verify you have the lastest version:\\n\\nbash\\npipenv shell\\npip install --upgrade streamlit\\nstreamlit version\\n\\nOr if you want to use an easily-reproducible environment, replace pip with pipenvevery time you install or update a package:\\n\\nbash\\npipenv update streamlit\\npipenv run streamlit version\\n\\nConda\\n\\nActivate the conda environment where Streamlit is installed:\\n\\nbash\\nconda activate $ENVIRONMENT_NAME\\n\\nBe sure to replace$ENVIRONMENT_NAME ☝️ with the name your conda environment!\\n\\nUpdate Streamlit within the active conda environment and verify you have the lastest version:\\n\\nbash\\nconda update -c conda-forge streamlit -y\\nstreamlit version\\n\\nPoetry\\n\\nIn order to get the latest version of Streamlit with Poetry and verify you have the lastest version, run:\\n\\nbash\\npoetry update streamlit\\nstreamlit version\", metadata={'source': 'docs/content/kb/using-streamlit/upgrade-version-streamlit.md'}),\n",
       " Document(page_content='title: What is serializable session state?\\nslug: /knowledge-base/using-streamlit/serializable-session-state\\n\\nWhat is serializable session state?\\n\\nSerializable Session State\\n\\nSerialization refers to the process of converting an object or data structure into a format that can be persisted and shared, and allowing you to recover the data’s original structure. Python’s built-in pickle module serializes Python objects to a byte stream (\"pickling\") and deserializes the stream into an object (\"unpickling\").\\n\\nBy default, Streamlit’s Session State allows you to persist any Python object for the duration of the session, irrespective of the object’s pickle-serializability. This property lets you store Python primitives such as integers, floating-point numbers, complex numbers and booleans, dataframes, and even lambdas returned by functions. However, some execution environments may require serializing all data in Session State, so it may be useful to detect incompatibility during development, or when the execution environment will stop supporting it in the future.\\n\\nTo that end, Streamlit provides a runner.enforceSerializableSessionState configuration option that, when set to true, only allows pickle-serializable objects in Session State. To enable the option, either create a global or project config file with the following or use it as a command-line flag:\\n\\n```toml\\n\\n.streamlit/config.toml\\n\\n[runner]\\nenforceSerializableSessionState = true\\n```\\n\\nBy \"pickle-serializable\", we mean calling pickle.dumps(obj) should not raise a PicklingError exception. When the config option is enabled, adding unserializable data to session state should result in an exception. E.g.,\\n\\n```python\\nimport streamlit as st\\n\\ndef unserializable_data():\\n        return lambda x: x\\n\\n👇 results in an exception when enforceSerializableSessionState is on\\n\\nst.session_state.unserializable = unserializable_data()\\n```', metadata={'source': 'docs/content/kb/using-streamlit/serializable-session-state.md'}),\n",
       " Document(page_content='title: What browsers does Streamlit support?\\nslug: /knowledge-base/using-streamlit/supported-browsers\\n\\nWhat browsers does Streamlit support?\\n\\nThe latest version of Streamlit is compatible with the two most recent versions of the following browsers:\\n\\nGoogle Chrome\\n\\nFirefox\\n\\nMicrosoft Edge\\n\\nSafari\\n\\nYou may not be able to use all the latest features of Streamlit with unsupported browsers or older versions of the above browsers. Streamlit will not provide bug fixes for unsupported browsers.', metadata={'source': 'docs/content/kb/using-streamlit/supported-browsers.md'}),\n",
       " Document(page_content='title: Append data to a table or chart\\nslug: /knowledge-base/using-streamlit/append-data-table-chart\\n\\nAppend data to a table or chart\\n\\nIn Streamlit, you can not only replace entire elements in your app, but also\\nmodify the data behind those elements. Here is how:\\n\\n```python\\nimport numpy as np\\nimport time\\n\\nGet some data.\\n\\ndata = np.random.randn(10, 2)\\n\\nShow the data as a chart.\\n\\nchart = st.line_chart(data)\\n\\nWait 1 second, so the change is clearer.\\n\\ntime.sleep(1)\\n\\nGrab some more data.\\n\\ndata2 = np.random.randn(10, 2)\\n\\nAppend the new data to the existing chart.\\n\\nchart.add_rows(data2)\\n```', metadata={'source': 'docs/content/kb/using-streamlit/append-data-table-chart.md'}),\n",
       " Document(page_content=\"title: How to download a file in Streamlit?\\nslug: /knowledge-base/using-streamlit/how-download-file-streamlit\\n\\nHow to download a file in Streamlit?\\n\\nUse the st.download_button widget that is natively built into Streamlit. Check out a sample app demonstrating how you can use st.download_button to download common file formats.\\n\\nExample usage\\n\\n```python\\nimport streamlit as st\\n\\nText files\\n\\ntext_contents = '''\\nFoo, Bar\\n123, 456\\n789, 000\\n'''\\n\\nDifferent ways to use the API\\n\\nst.download_button('Download CSV', text_contents, 'text/csv')\\nst.download_button('Download CSV', text_contents)  # Defaults to 'text/plain'\\n\\nwith open('myfile.csv') as f:\\n   st.download_button('Download CSV', f)  # Defaults to 'text/plain'\\n\\n---\\n\\nBinary files\\n\\nbinary_contents = b'whatever'\\n\\nDifferent ways to use the API\\n\\nst.download_button('Download file', binary_contents)  # Defaults to 'application/octet-stream'\\n\\nwith open('myfile.zip', 'rb') as f:\\n   st.download_button('Download Zip', f, file_name='archive.zip')  # Defaults to 'application/octet-stream'\\n\\nYou can also grab the return value of the button,\\n\\njust like with any other button.\\n\\nif st.download_button(...):\\n   st.write('Thanks for downloading!')\\n```\\n\\nAdditional resources:\\n\\nhttps://blog.streamlit.io/0-88-0-release-notes/\\n\\nhttps://streamlit-release-demos-0-88streamlit-app-0-88-v8ram3.streamlit.app/\\n\\nhttps://docs.streamlit.io/library/api-reference/widgets/st.download_button\", metadata={'source': 'docs/content/kb/using-streamlit/how-download-file-streamlit.md'}),\n",
       " Document(page_content=\"title: Deployment Issues\\nslug: /knowledge-base/deploy\\n\\nDeployment-related questions and errors\\n\\nHow do I deploy Streamlit on a domain so it appears to run on a regular port (i.e. port 80)?\\n\\nHow can I deploy multiple Streamlit apps on different subdomains?\\n\\nHow do I deploy Streamlit on Heroku, AWS, Google Cloud, etc...?\\n\\nInvoking a Python subprocess in a deployed Streamlit app\\n\\nDoes Streamlit support the WSGI Protocol? (aka Can I deploy Streamlit with gunicorn?)\\n\\nArgh. This app has gone over its resource limits.\\n\\nApp is not loading when running remotely\\n\\nAuthentication without SSO\\n\\nI don't have GitHub or GSuite. How do I sign in to Streamlit Community Cloud?\\n\\nHow do I share apps with viewers outside my organization?\\n\\nUpgrade the Streamlit version of your app on Streamlit Community Cloud\\n\\nOrganizing your apps with workspaces on Streamlit Community Cloud\\n\\nHow do I increase the upload limit of st.file_uploader on Streamlit Community Cloud?\\n\\nHow do I customize my app's subdomain?\\n\\nHow to update account admin settings on Streamlit Community Cloud?\\n\\nUnable to edit or delete apps in Streamlit Community Cloud after modifying GitHub username\\n\\nHuh. This is isn't supposed to happen message after trying to log in\\n\\nHuh. This isn't supposed to happen. No valid SSO connection for domain\\n\\nView-only access to app after changing GitHub username or repository name\\n\\nLogin attempt to Streamlit Community Cloud fails with error 403\\n\\nHow to submit a support case for Streamlit Community Cloud\\n\\nHow to delete your Streamlit Community Cloud account\", metadata={'source': 'docs/content/kb/deployments/index.md'}),\n",
       " Document(page_content='site_menu:\\n  - category: Streamlit library\\n    url: /library\\n    color: violet-70\\n    icon: description\\n  - category: Streamlit library / Get started\\n    url: /library/get-started\\n  - category: Streamlit library / Get started / Installation\\n    url: /library/get-started/installation\\n  - category: Streamlit library / Get started / Main concepts\\n    url: /library/get-started/main-concepts\\n  - category: Streamlit library / Get started / Create an app\\n    url: /library/get-started/create-an-app\\n  - category: Streamlit library / Get started / Multipage apps\\n    url: /library/get-started/multipage-apps\\n  - category: Streamlit library / Get started / Multipage apps / Create a multipage app\\n    url: /library/get-started/multipage-apps/create-a-multipage-app\\n  # - category: Streamlit library / Get started / Deploy an app\\n  #   url: /library/get-started/deploy-an-app\\n  # - category: Streamlit library / Get started / App gallery\\n  #   url: https://streamlit.io/gallery\\n  - category: Streamlit library / API reference\\n    url: /library/api-reference\\n  - category: Streamlit library / API reference / Write and magic\\n    url: /library/api-reference/write-magic\\n  - category: Streamlit library / API reference / Write and magic / st.write\\n    url: /library/api-reference/write-magic/st.write\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Write and magic / magic\\n    url: /library/api-reference/write-magic/magic\\n  - category: Streamlit library / API reference / Text elements\\n    url: /library/api-reference/text\\n  - category: Streamlit library / API reference / Text elements / st.markdown\\n    url: /library/api-reference/text/st.markdown\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.title\\n    url: /library/api-reference/text/st.title\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.header\\n    url: /library/api-reference/text/st.header\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.subheader\\n    url: /library/api-reference/text/st.subheader\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.caption\\n    url: /library/api-reference/text/st.caption\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.code\\n    url: /library/api-reference/text/st.code\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.text\\n    url: /library/api-reference/text/st.text\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.latex\\n    url: /library/api-reference/text/st.latex\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.divider\\n    url: /library/api-reference/text/st.divider\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements\\n    url: /library/api-reference/data\\n  - category: Streamlit library / API reference / Data elements / st.dataframe\\n    url: /library/api-reference/data/st.dataframe\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.data_editor\\n    url: /library/api-reference/data/st.data_editor\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config\\n    url: /library/api-reference/data/st.column_config\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Column\\n    url: /library/api-reference/data/st.column_config/st.column_config.column\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Text column\\n    url: /library/api-reference/data/st.column_config/st.column_config.textcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Number column\\n    url: /library/api-reference/data/st.column_config/st.column_config.numbercolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Checkbox column\\n    url: /library/api-reference/data/st.column_config/st.column_config.checkboxcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Selectbox column\\n    url: /library/api-reference/data/st.column_config/st.column_config.selectboxcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Datetime column\\n    url: /library/api-reference/data/st.column_config/st.column_config.datetimecolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Date column\\n    url: /library/api-reference/data/st.column_config/st.column_config.datecolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Time column\\n    url: /library/api-reference/data/st.column_config/st.column_config.timecolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / List column\\n    url: /library/api-reference/data/st.column_config/st.column_config.listcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Link column\\n    url: /library/api-reference/data/st.column_config/st.column_config.linkcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Image column\\n    url: /library/api-reference/data/st.column_config/st.column_config.imagecolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Line chart column\\n    url: /library/api-reference/data/st.column_config/st.column_config.linechartcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Bar chart column\\n    url: /library/api-reference/data/st.column_config/st.column_config.barchartcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Progress column\\n    url: /library/api-reference/data/st.column_config/st.column_config.progresscolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.table\\n    url: /library/api-reference/data/st.table\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.metric\\n    url: /library/api-reference/data/st.metric\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.json\\n    url: /library/api-reference/data/st.json\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements\\n    url: /library/api-reference/charts\\n  - category: Streamlit library / API reference / Chart elements / st.line_chart\\n    url: /library/api-reference/charts/st.line_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.area_chart\\n    url: /library/api-reference/charts/st.area_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.bar_chart\\n    url: /library/api-reference/charts/st.bar_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.pyplot\\n    url: /library/api-reference/charts/st.pyplot\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.altair_chart\\n    url: /library/api-reference/charts/st.altair_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.vega_lite_chart\\n    url: /library/api-reference/charts/st.vega_lite_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.plotly_chart\\n    url: /library/api-reference/charts/st.plotly_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.bokeh_chart\\n    url: /library/api-reference/charts/st.bokeh_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.pydeck_chart\\n    url: /library/api-reference/charts/st.pydeck_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.graphviz_chart\\n    url: /library/api-reference/charts/st.graphviz_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.map\\n    url: /library/api-reference/charts/st.map\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets\\n    url: /library/api-reference/widgets\\n  - category: Streamlit library / API reference / Input widgets / st.button\\n    url: /library/api-reference/widgets/st.button\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.download_button\\n    url: /library/api-reference/widgets/st.download_button\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.checkbox\\n    url: /library/api-reference/widgets/st.checkbox\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.radio\\n    url: /library/api-reference/widgets/st.radio\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.selectbox\\n    url: /library/api-reference/widgets/st.selectbox\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.multiselect\\n    url: /library/api-reference/widgets/st.multiselect\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.slider\\n    url: /library/api-reference/widgets/st.slider\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.select_slider\\n    url: /library/api-reference/widgets/st.select_slider\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.text_input\\n    url: /library/api-reference/widgets/st.text_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.number_input\\n    url: /library/api-reference/widgets/st.number_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.text_area\\n    url: /library/api-reference/widgets/st.text_area\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.date_input\\n    url: /library/api-reference/widgets/st.date_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.time_input\\n    url: /library/api-reference/widgets/st.time_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.file_uploader\\n    url: /library/api-reference/widgets/st.file_uploader\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.camera_input\\n    url: /library/api-reference/widgets/st.camera_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.color_picker\\n    url: /library/api-reference/widgets/st.color_picker\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Media elements\\n    url: /library/api-reference/media\\n  - category: Streamlit library / API reference / Media elements / st.image\\n    url: /library/api-reference/media/st.image\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Media elements / st.audio\\n    url: /library/api-reference/media/st.audio\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Media elements / st.video\\n    url: /library/api-reference/media/st.video\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers\\n    url: /library/api-reference/layout\\n  - category: Streamlit library / API reference / Layouts and containers / st.sidebar\\n    url: /library/api-reference/layout/st.sidebar\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.columns\\n    url: /library/api-reference/layout/st.columns\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.tabs\\n    url: /library/api-reference/layout/st.tabs\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.expander\\n    url: /library/api-reference/layout/st.expander\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.container\\n    url: /library/api-reference/layout/st.container\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.empty\\n    url: /library/api-reference/layout/st.empty\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chat elements\\n    url: /library/api-reference/chat\\n  - category: Streamlit library / API reference / Chat elements / st.chat_message\\n    url: /library/api-reference/chat/st.chat_message\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chat elements / st.chat_input\\n    url: /library/api-reference/chat/st.chat_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements\\n    url: /library/api-reference/status\\n  - category: Streamlit library / API reference / Status elements / st.progress\\n    url: /library/api-reference/status/st.progress\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.spinner\\n    url: /library/api-reference/status/st.spinner\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.balloons\\n    url: /library/api-reference/status/st.balloons\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.snow\\n    url: /library/api-reference/status/st.snow\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.error\\n    url: /library/api-reference/status/st.error\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.warning\\n    url: /library/api-reference/status/st.warning\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.info\\n    url: /library/api-reference/status/st.info\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.success\\n    url: /library/api-reference/status/st.success\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.exception\\n    url: /library/api-reference/status/st.exception\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Control flow\\n    url: /library/api-reference/control-flow\\n  - category: Streamlit library / API reference / Control flow / st.stop\\n    url: /library/api-reference/control-flow/st.stop\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Control flow / st.form\\n    url: /library/api-reference/control-flow/st.form\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Control flow / st.form_submit_button\\n    url: /library/api-reference/control-flow/st.form_submit_button\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Control flow / st.experimental_rerun\\n    url: /library/api-reference/control-flow/st.experimental_rerun\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Utilities\\n    url: /library/api-reference/utilities\\n  - category: Streamlit library / API reference / Utilities / st.set_page_config\\n    url: /library/api-reference/utilities/st.set_page_config\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Utilities / st.echo\\n    url: /library/api-reference/utilities/st.echo\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Utilities / st.help\\n    url: /library/api-reference/utilities/st.help\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Utilities / st.experimental_get_query_params\\n    url: /library/api-reference/utilities/st.experimental_get_query_params\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Utilities / st.experimental_set_query_params\\n    url: /library/api-reference/utilities/st.experimental_set_query_params\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Mutate charts\\n    url: /library/api-reference/mutate\\n  - category: Streamlit library / API reference / State management\\n    url: /library/api-reference/session-state\\n  - category: Streamlit library / API reference / Performance\\n    url: /library/api-reference/performance\\n  - category: Streamlit library / API reference / Performance / st.cache_data\\n    url: /library/api-reference/performance/st.cache_data\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Performance / Clear cached data\\n    url: /library/api-reference/performance/st.cache_data.clear\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Performance / st.cache_resource\\n    url: /library/api-reference/performance/st.cache_resource\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Performance / Clear cached resources\\n    url: /library/api-reference/performance/st.cache_resource.clear\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Performance / st.cache\\n    url: /library/api-reference/performance/st.cache\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Performance / st.experimental_memo\\n    url: /library/api-reference/performance/st.experimental_memo\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Performance / Clear memo\\n    url: /library/api-reference/performance/st.experimental_memo.clear\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Performance / st.experimental_singleton\\n    url: /library/api-reference/performance/st.experimental_singleton\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Performance / Clear singleton\\n    url: /library/api-reference/performance/st.experimental_singleton.clear\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Personalization\\n    url: /library/api-reference/personalization\\n    isVersioned: false\\n  - category: Streamlit library / API reference / Personalization / st.experimental_user\\n    url: /library/api-reference/personalization/st.experimental_user\\n    isVersioned: false\\n  - category: Streamlit library / API reference / Connections and databases\\n    url: /library/api-reference/connections\\n  - category: Streamlit library / API reference / Connections and databases / st.experimental_connection\\n    url: /library/api-reference/connections/st.experimental_connection\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Connections and databases / SQLConnection\\n    url: /library/api-reference/connections/st.connections.sqlconnection\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Connections and databases / SnowparkConnection\\n    url: /library/api-reference/connections/st.connections.snowparkconnection\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Connections and databases / Connection base class\\n    url: /library/api-reference/connections/st.connections.experimentalbaseconnection\\n    isVersioned: true\\n  - category: Streamlit library / Advanced features\\n    url: /library/advanced-features\\n  - category: Streamlit library / Advanced features / ☰ App menu\\n    url: /library/advanced-features/app-menu\\n  - category: Streamlit library / Advanced features/ Command-line options\\n    url: /library/advanced-features/cli\\n  - category: Streamlit library / Advanced features/ Configuration\\n    url: /library/advanced-features/configuration\\n  - category: Streamlit library / Advanced features / Theming\\n    url: /library/advanced-features/theming\\n  - category: Streamlit library / Advanced features/ Caching\\n    url: /library/advanced-features/caching\\n  - category: Streamlit library / Advanced features/ Connecting to data\\n    url: /library/advanced-features/connecting-to-data\\n  - category: Streamlit library / Advanced features/ Optimize performance with st.cache\\n    url: /library/advanced-features/st.cache\\n    visible: false\\n  - category: Streamlit library / Advanced features/ Experimental cache primitives\\n    url: /library/advanced-features/experimental-cache-primitives\\n    visible: false\\n  - category: Streamlit library / Advanced features/ Add statefulness to apps\\n    url: /library/advanced-features/session-state\\n  - category: Streamlit library / Advanced features/ Button behavior and examples\\n    url: /library/advanced-features/button-behavior-and-examples\\n  - category: Streamlit library / Advanced features/ Dataframes\\n    url: /library/advanced-features/dataframes\\n  - category: Streamlit library / Advanced features/ Widget semantics\\n    url: /library/advanced-features/widget-semantics\\n  - category: Streamlit library / Advanced features/ Pre-release features\\n    url: /library/advanced-features/prerelease\\n  - category: Streamlit library / Advanced features/ Working with timezones\\n    url: /library/advanced-features/timezone-handling\\n  - category: Streamlit library / Advanced features/ Static file serving\\n    url: /library/advanced-features/static-file-serving\\n  - category: Streamlit library / Advanced features/ HTTPS support\\n    url: /library/advanced-features/https-support\\n  - category: Streamlit library / Advanced features/ Secrets management\\n    url: /library/advanced-features/secrets-management\\n  - category: Streamlit library / Components\\n    url: /library/components\\n  - category: Streamlit library / Components / Components API\\n    url: /library/components/components-api\\n  - category: Streamlit library / Components / Create a Component\\n    url: /library/components/create\\n  - category: Streamlit library / Components / Publish a Component\\n    url: /library/components/publish\\n  - category: Streamlit library / Components / Component gallery\\n    url: https://streamlit.io/components\\n  - category: Streamlit library / Roadmap\\n    url: https://roadmap.streamlit.app\\n  - category: Streamlit library / Changelog\\n    url: /library/changelog\\n  - category: Streamlit library / Cheat sheet\\n    url: /library/cheatsheet\\n\\ncategory: Streamlit Community Cloud\\n    url: /streamlit-community-cloud\\n    color: l-blue-70\\n    icon: cloud\\n\\ncategory: Streamlit Community Cloud / Get started\\n    url: /streamlit-community-cloud/get-started\\n\\ncategory: Streamlit Community Cloud / Get started / Deploy an app\\n    url: /streamlit-community-cloud/get-started/deploy-an-app\\n\\ncategory: Streamlit Community Cloud / Get started / Deploy an app / App dependencies\\n    url: /streamlit-community-cloud/get-started/deploy-an-app/app-dependencies\\n\\ncategory: Streamlit Community Cloud / Get started / Deploy an app / Connect to data sources\\n    url: /streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources\\n\\ncategory: Streamlit Community Cloud / Get started / Deploy an app / Connect to data sources / Secrets management\\n    url: /streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources/secrets-management\\n\\ncategory: Streamlit Community Cloud / Get started / Embed your app\\n    url: /streamlit-community-cloud/get-started/embed-your-app\\n\\ncategory: Streamlit Community Cloud / Get started / Share your app\\n    url: /streamlit-community-cloud/get-started/share-your-app\\n\\ncategory: Streamlit Community Cloud / Get started / Share your app / App indexability\\n    url: /streamlit-community-cloud/get-started/share-your-app/indexability\\n\\ncategory: Streamlit Community Cloud / Get started / Share your app / Share previews\\n    url: /streamlit-community-cloud/get-started/share-your-app/share-previews\\n\\ncategory: Streamlit Community Cloud / Get started / Manage your app\\n    url: /streamlit-community-cloud/get-started/manage-your-app\\n\\ncategory: Streamlit Community Cloud / Manage your account\\n    url: /streamlit-community-cloud/manage-your-account\\n\\ncategory: Streamlit Community Cloud / Manage your account / Update your email\\n    url: /streamlit-community-cloud/manage-your-account/update-your-email\\n\\ncategory: Streamlit Community Cloud / Manage your account / Delete your account\\n    url: /streamlit-community-cloud/manage-your-account/delete-your-account\\n  # - category: Streamlit Community Cloud / Additional features\\n  #   url: /streamlit-community-cloud/additional-features\\n\\ncategory: Streamlit Community Cloud / Trust and Security\\n    url: /streamlit-community-cloud/trust-and-security\\n\\ncategory: Streamlit Community Cloud / Troubleshooting\\n    url: /streamlit-community-cloud/troubleshooting\\n\\ncategory: Knowledge base\\n    url: /knowledge-base\\n    color: orange-70\\n    icon: school\\n\\ncategory: Knowledge base / Tutorials\\n    url: /knowledge-base/tutorials\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources\\n    url: /knowledge-base/tutorials/databases\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / AWS S3\\n    url: /knowledge-base/tutorials/databases/aws-s3\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / BigQuery\\n    url: /knowledge-base/tutorials/databases/bigquery\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Deta Base\\n    url: /knowledge-base/tutorials/databases/deta-base\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Firestore\\n    url: https://blog.streamlit.io/streamlit-firestore/\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Google Cloud Storage\\n    url: /knowledge-base/tutorials/databases/gcs\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Microsoft SQL Server\\n    url: /knowledge-base/tutorials/databases/mssql\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / MongoDB\\n    url: /knowledge-base/tutorials/databases/mongodb\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / MySQL\\n    url: /knowledge-base/tutorials/databases/mysql\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / PostgreSQL\\n    url: /knowledge-base/tutorials/databases/postgresql\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Private Google Sheet\\n    url: /knowledge-base/tutorials/databases/private-gsheet\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Public Google Sheet\\n    url: /knowledge-base/tutorials/databases/public-gsheet\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Snowflake\\n    url: /knowledge-base/tutorials/databases/snowflake\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Supabase\\n    url: /knowledge-base/tutorials/databases/supabase\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Tableau\\n    url: /knowledge-base/tutorials/databases/tableau\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / TiDB\\n    url: /knowledge-base/tutorials/databases/tidb\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / TigerGraph\\n    url: /knowledge-base/tutorials/databases/tigergraph\\n\\ncategory: Knowledge base / Tutorials / Deploy Streamlit apps\\n    url: /knowledge-base/tutorials/deploy\\n\\ncategory: Knowledge base / Tutorials / Deploy Streamlit apps / Docker\\n    url: /knowledge-base/tutorials/deploy/docker\\n\\ncategory: Knowledge base / Tutorials / Deploy Streamlit apps / Kubernetes\\n    url: /knowledge-base/tutorials/deploy/kubernetes\\n\\ncategory: Knowledge base / Tutorials / Session State basics\\n    url: /knowledge-base/tutorials/session-state\\n\\ncategory: Knowledge base / Tutorials / Build conversational apps\\n    url: /knowledge-base/tutorials/build-conversational-apps\\n\\ncategory: Knowledge base / Using Streamlit\\n    url: /knowledge-base/using-streamlit\\n\\ncategory: Knowledge base / Using Streamlit / How to animate elements?\\n    url: /knowledge-base/using-streamlit/animate-elements\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Append data to a table or chart\\n    url: /knowledge-base/using-streamlit/append-data-table-chart\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Batch elements and input widgets with st.form\\n    url: /knowledge-base/using-streamlit/batch-elements-input-widgets-form\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How do I create an anchor link?\\n    url: /knowledge-base/using-streamlit/create-anchor-link\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Enabling camera access in your browser\\n    url: /knowledge-base/using-streamlit/enable-camera\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Hide row indices when displaying a dataframe\\n    url: /knowledge-base/using-streamlit/hide-row-indices-displaying-dataframe\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How do I run my Streamlit script?\\n    url: /knowledge-base/using-streamlit/how-do-i-run-my-streamlit-script\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How to download a file in Streamlit?\\n    url: /knowledge-base/using-streamlit/how-download-file-streamlit\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How to download a Pandas DataFrame as a CSV?\\n    url: /knowledge-base/using-streamlit/how-download-pandas-dataframe-csv\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How do I upgrade to the latest version of Streamlit?\\n    url: /knowledge-base/using-streamlit/how-upgrade-latest-version-streamlit\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How to insert elements out of order?\\n    url: /knowledge-base/using-streamlit/insert-elements-out-of-order\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / What is the path of Streamlit’s config.toml file?\\n    url: /knowledge-base/using-streamlit/path-streamlit-config-toml\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How can I make st.pydeck_chart use custom Mapbox styles?\\n    url: /knowledge-base/using-streamlit/pydeck-chart-custom-mapbox-styles\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How to remove \"· Streamlit\" from the app title?\\n    url: /knowledge-base/using-streamlit/remove-streamlit-app-title\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How do you retrieve the filename of a file uploaded with st.file_uploader?\\n    url: /knowledge-base/using-streamlit/retrieve-filename-uploaded\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Sanity checks\\n    url: /knowledge-base/using-streamlit/sanity-checks\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How can I make Streamlit watch for changes in other modules I\\'m importing in my app?\\n    url: /knowledge-base/using-streamlit/streamlit-watch-changes-other-modules-importing-app\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / What browsers does Streamlit support?\\n    url: /knowledge-base/using-streamlit/supported-browsers\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Where does st.file_uploader store uploaded files and when do they get deleted?\\n    url: /knowledge-base/using-streamlit/where-file-uploader-store-when-deleted\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Widget updating for every second input when using session state\\n    url: /knowledge-base/using-streamlit/widget-updating-session-state\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Why does Streamlit restrict nested st.columns?\\n    url: /knowledge-base/using-streamlit/why-streamlit-restrict-nested-columns\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How to host static files in Streamlit?\\n    url: /knowledge-base/using-streamlit/how-host-static-files\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / What is serializable session state?\\n    url: /knowledge-base/using-streamlit/serializable-session-state\\n    visible: false\\n\\ncategory: Knowledge base / Streamlit Components\\n    url: /knowledge-base/components\\n\\ncategory: Knowledge base / Streamlit Components / How do I add a Component to the sidebar?\\n    url: /knowledge-base/components/add-component-sidebar\\n    visible: false\\n\\ncategory: Knowledge base / Streamlit Components / My Component seems to be stuttering...how do I fix that?\\n    url: /knowledge-base/components/component-blinking-stuttering-fix\\n    visible: false\\n\\ncategory: Knowledge base / Streamlit Components / How do Streamlit Components differ from functionality provided in the base Streamlit package?\\n    url: /knowledge-base/components/how-streamlit-components-differ-base-package\\n    visible: false\\n\\ncategory: Knowledge base / Streamlit Components / What types of things aren\\'t possible with Streamlit Components?\\n    url: /knowledge-base/components/not-possibe-streamlit-components\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies\\n    url: /knowledge-base/dependencies\\n\\ncategory: Knowledge base / Installing dependencies / How to install a package not on PyPI or Conda but available on GitHub\\n    url: /knowledge-base/dependencies/install-package-not-pypi-conda-available-github\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies / ImportError libGL.so.1 cannot open shared object file No such file or directory\\n    url: /knowledge-base/dependencies/libgl\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies / ModuleNotFoundError No module named\\n    url: /knowledge-base/dependencies/module-not-found-error\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies / ERROR No matching distribution found for\\n    url: /knowledge-base/dependencies/no-matching-distribution\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies / Install the Snowflake Connector for Python on Streamlit Community Cloud\\n    url: /knowledge-base/dependencies/snowflake-connector-python-streamlit-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues\\n    url: /knowledge-base/deploy\\n\\ncategory: Knowledge base / Deployment issues / Authentication without SSO\\n    url: /knowledge-base/deploy/authentication-without-sso\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How can I deploy multiple Streamlit apps on different subdomains?\\n    url: /knowledge-base/deploy/deploy-multiple-streamlit-apps-different-subdomains\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How do I deploy Streamlit on a domain so it appears to run on a regular port (i.e. port 80)?\\n    url: /knowledge-base/deploy/deploy-streamlit-domain-port-80\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How do I deploy Streamlit on Heroku, AWS, Google Cloud, etc...?\\n    url: /knowledge-base/deploy/deploy-streamlit-heroku-aws-google-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Does Streamlit support the WSGI Protocol? (aka Can I deploy Streamlit with gunicorn?)\\n    url: /knowledge-base/deploy/does-streamlit-support-wsgi-protocol\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How do I increase the upload limit of st.file_uploader on Streamlit Community Cloud?\\n    url: /knowledge-base/deploy/increase-file-uploader-limit-streamlit-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Invoking a Python subprocess in a deployed Streamlit app\\n    url: /knowledge-base/deploy/invoking-python-subprocess-deployed-streamlit-app\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Organizing your apps with workspaces on Streamlit Community Cloud\\n    url: /knowledge-base/deploy/organizing-apps-workspaces-streamlit-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / App is not loading when running remotely\\n    url: /knowledge-base/deploy/remote-start\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Argh. This app has gone over its resource limits\\n    url: /knowledge-base/deploy/resource-limits\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How do I share apps with viewers outside my organization?\\n    url: /knowledge-base/deploy/share-apps-with-viewers-outside-organization\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / I don\\'t have SSO. How do I sign in to Streamlit Community Cloud?\\n    url: /knowledge-base/deploy/sign-in-without-sso\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Upgrade the Streamlit version of your app on Streamlit Community Cloud\\n    url: /knowledge-base/deploy/upgrade-streamlit-version-on-streamlit-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Custom subdomains\\n    url: /knowledge-base/deploy/custom-subdomains\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How to update account admin settings on Streamlit Community Cloud?\\n    url: /knowledge-base/deploy/how-to-update-account-admin-settings-on-streamlit-community-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Unable to edit or delete apps in Streamlit Community Cloud after modifying GitHub username\\n    url: /knowledge-base/deploy/unable-to-edit-or-delete-apps-in-streamlit-community-cloud-after-modifying-github-username\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Huh. This is isn\\'t supposed to happen message after trying to log in\\n    url: /knowledge-base/deploy/huh-this-isnt-supposed-to-happen-message-after-trying-to-log-in\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Huh. This isn\\'t supposed to happen. No valid SSO connection for domain\\n    url: /knowledge-base/deploy/huh-this-isnt-supposed-to-happen-no-valid-sso-connection-for-domain\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / View-only access to app after changing GitHub username or repository name\\n    url: /knowledge-base/deploy/view-only-access-to-app-after-changing-github-username-or-repository-name\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Login attempt to Streamlit Community Cloud fails with error 403\\n    url: /knowledge-base/deploy/login-attempt-to-streamlit-community-cloud-fails-with-error-403\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How to submit a support case for Streamlit Community Cloud\\n    url: /knowledge-base/deploy/how-to-submit-a-support-case-for-streamlit-community-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How to delete your Streamlit Community Cloud account\\n    url: /knowledge-base/deploy/how-to-delete-your-streamlit-community-cloud-account\\n    visible: false', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content=\"title: How do I run my Streamlit script?\\nslug: /knowledge-base/using-streamlit/how-do-i-run-my-streamlit-script\\n\\nHow do I run my Streamlit script?\\n\\nWorking with Streamlit is simple. First you sprinkle a few Streamlit commands into a normal Python script, and then you run it. We list few ways to run your script, depending on your use case.\\n\\nUse streamlit run\\n\\nOnce you've created your script, say your_script.py, the easiest way to run it is with streamlit run:\\n\\nbash\\nstreamlit run your_script.py\\n\\nAs soon as you run the script as shown above, a local Streamlit server will spin up and your app will open in a new tab in your default web browser.\\n\\nPass arguments to your script\\n\\nWhen passing your script some custom arguments, they must be passed after two dashes. Otherwise the arguments get interpreted as arguments to Streamlit itself:\\n\\nbash\\nstreamlit run your_script.py [-- script args]\\n\\nPass a URL to streamlit run\\n\\nYou can also pass a URL to streamlit run! This is great when your script is hosted remotely, such as a GitHub Gist. For example:\\n\\nbash\\nstreamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n\\nRun Streamlit as a Python module\\n\\nAnother way of running Streamlit is to run it as a Python module. This is useful when configuring an IDE like PyCharm to work with Streamlit:\\n\\n```bash\\n\\nRunning\\n\\npython -m streamlit run your_script.py\\n```\\n\\n```bash\\n\\nis equivalent to:\\n\\nstreamlit run your_script.py\\n```\", metadata={'source': 'docs/content/kb/using-streamlit/how-run-my-streamlit-script.md'}),\n",
       " Document(page_content='title: View-only access to app after changing GitHub username or repository name\\nslug: /knowledge-base/deploy/view-only-access-to-app-after-changing-github-username-or-repository-name\\n\\nView-only access to app after changing GitHub username or repository name\\n\\nThis KB helps to resolve the issue of the apps becoming view-only.\\n\\nProblem\\n\\nWhen opening the dropdown menu of the apps on Streamlit Community CLoud, the options Reboot, Delete and Settings is grayed out, and there is a message \"You have view-only access to this application\".\\n\\nChanging your GitHub username or the repository name after deploying an app causes your app to become view-only.\\n\\nSolution\\n\\nPlease reach out to the Snowflake support team to delete the affected app(s) for you so you can redeploy them with your new username.', metadata={'source': 'docs/content/kb/deployments/view-only-access-to-app-after-changing-github-username-or-repository-name.md'}),\n",
       " Document(page_content=\"title: Huh. This isn't supposed to happen. No valid SSO connection for domain\\nslug: /knowledge-base/deploy/huh-this-isnt-supposed-to-happen-no-valid-sso-connection-for-domain\\n\\nHuh. This isn't supposed to happen. No valid SSO connection for domain\\n\\nThis KB helps you resolve the No valid SSO connection error while login to Streamlit Community Cloud with SSO.\\n\\nProblem\\n\\nYou have got the following screen when trying to login to your Streamlit Community Cloud account with SSO(SAML authentication):\\n\\nThis message means that you’ve logged in with both GitHub and Google in the past, but now you’re trying to log in with only GitHub.\\n\\nSolution\\n\\nYou can resolve the error by just logging in with both GitHub and Google and then, if you’d like to unlink your Google account from Streamlit Community Cloud, you can then log out via only Google.\", metadata={'source': 'docs/content/kb/deployments/huh-this-isnt-supposed-to-happen-no-valid-sso-connection-for-domain.md'}),\n",
       " Document(page_content='title: How to update account admin settings on Streamlit Community Cloud?\\nslug: /knowledge-base/deploy/how-to-update-account-admin-settings-on-streamlit-community-cloud\\n\\nHow to update account admin settings on Streamlit Community Cloud?\\n\\nSince retiring our paid Cloud plans in mid-2022, Streamlit Community Cloud no longer has account admins. Instead, app permissions are derived from GitHiub. In other words, to edit an app, you’ll need permission to make changes to the app’s GitHub repository and the changes will happen in GitHub.', metadata={'source': 'docs/content/kb/deployments/how-to-update-account-admin-settings-on-streamlit-community-cloud.md'}),\n",
       " Document(page_content='title: Organizing your apps with workspaces on Streamlit Community Cloud\\nslug: /knowledge-base/deploy/organizing-apps-workspaces-streamlit-cloud\\n\\nOrganizing your apps with workspaces on Streamlit Community Cloud\\n\\nStreamlit Community Cloud is organized into workspaces, which automatically group your apps according to the corresponding GitHub repository\\'s owner. If you are part of multiple repositories, then you will have multiple workspaces.\\n\\nPersonal workspace\\n\\nIf an app\\'s GitHub repository is owned by you, the app will appear in your personal workspace, named \"<YourGitHubHandle>\".\\n\\nOrganization workspace\\n\\nIf an app\\'s GitHub repository is owned by an organization (such as your company), the app will appear in a separate workspace, named \"<GitHubOrganizationHandle>\".\\n\\nWorkspaces with view access\\n\\nYou will also have access to any workspaces containing app(s) for which you only have view access. These apps will have a \"view-only\" tooltip when you click on their respective hamburger menus.\\n\\nSwitching between workspaces\\n\\nTo switch between workspaces, click on the workspace listed in the top right corner, then select the desired workspace name.\\n\\nIf you have further questions about workspaces on Streamlit Community Cloud, please emails us at success@streamlit.io.', metadata={'source': 'docs/content/kb/deployments/organize-apps-workspaces.md'}),\n",
       " Document(page_content='title: Invoking a Python subprocess in a deployed Streamlit app\\nslug: /knowledge-base/deploy/invoking-python-subprocess-deployed-streamlit-app\\n\\nInvoking a Python subprocess in a deployed Streamlit app\\n\\nProblem\\n\\nLet\\'s suppose you want to invoke a subprocess to run a Python script script.py in your deployed Streamlit app streamlit_app.py. For example, the machine learning library Ludwig is run using a command-line interface, or maybe you want to run a bash script or similar type of process from Python.\\n\\nYou have tried the following, but run into dependency issues for script.py, even though you have specified your Python dependencies in a requirements file:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport subprocess\\n\\nsubprocess.run([\"python\", \"script.py\"])\\n```\\n\\nSolution\\n\\nWhen you run the above code block, you will get the version of Python that is on the system path—not necessarily the Python executable installed in the virtual environment that the Streamlit code is running under.\\n\\nThe solution is to detect the Python executable directy with sys.executable:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport subprocess\\nimport sys\\n\\nsubprocess.run([f\"{sys.executable}\", \"script.py\"])\\n```\\n\\nThis ensures that script.py is running under the same Python executable as your Streamlit code—where your Python dependencies are installed.\\n\\nRelevant links\\n\\nhttps://stackoverflow.com/questions/69947867/run-portion-of-python-code-in-parallel-from-a-streamlit-app/69948545#69948545\\n\\nhttps://discuss.streamlit.io/t/modulenotfounderror-no-module-named-cv2-streamlit/18319/3?u=snehankekre\\n\\nhttps://docs.python.org/3/library/sys.html#sys.executable', metadata={'source': 'docs/content/kb/deployments/invoking-python-subprocess-deployed-streamlit-app.md'}),\n",
       " Document(page_content='title: Huh. This is isn\\'t supposed to happen message after trying to log in\\nslug: /knowledge-base/deploy/huh-this-isnt-supposed-to-happen-message-after-trying-to-log-in\\n\\nHuh. This is isn\\'t supposed to happen message after trying to log in\\n\\nThis article helps to resolve the login issue caused by email mismatching between the GitHub and the Streamlit Community Cloud.\\n\\nProblem\\n\\nYou see the following message after signing in to your Streamlit Community Cloud account:\\n\\nThis message usually indicates that our system has linked your GitHub username with an email address other than the email address you\\'re currently logged in with.\\n\\nSolution\\n\\nNo worries – all you have to do is:\\n\\nLog out of Streamlit Community Cloud completely (via both your email and GitHub accounts).\\n\\nLog in first with your email account (you can do so via either \"Continue with Google\" or \"Continue with email\").\\n\\nLog in with your GitHub account.', metadata={'source': 'docs/content/kb/deployments/huh-this-is-isnt-supposed-to-happen-message-after-trying-to-log-in.md'}),\n",
       " Document(page_content=\"title: How can I deploy multiple Streamlit apps on different subdomains?\\nslug: /knowledge-base/deploy/deploy-multiple-streamlit-apps-different-subdomains\\n\\nHow can I deploy multiple Streamlit apps on different subdomains?\\n\\nProblem\\n\\nYou want to deploy multiple Streamlit apps on different subdomains.\\n\\nSolution\\n\\nLike running your Streamlit app on more common ports such as 80, subdomains are handled by a web server like Apache or Nginx:\\n\\nSet up a web server on a machine with a public IP address, then use a DNS server to point all desired subdomains to your webserver's IP address\\n\\nConfigure your web server to route requests for each subdomain to the different ports that your Streamlit apps are running on\\n\\nCheck out these two tutorials for Apache2 and Nginx that deal with setting up a webserver to redirect subdomains to different ports:\\n\\nApache2 subdomains\\n\\nNGinx subdomains\", metadata={'source': 'docs/content/kb/deployments/deploy-multiple-streamlit-apps-different-subdomains.md'}),\n",
       " Document(page_content='title: How do I deploy Streamlit on Heroku, AWS, Google Cloud, etc...?\\nslug: /knowledge-base/deploy/deploy-streamlit-heroku-aws-google-cloud\\n\\nHow do I deploy Streamlit on Heroku, AWS, Google Cloud, etc...?\\n\\nProblem\\n\\nYou want to deploy your Streamlit app on a cloud service other than Streamlit Community Cloud.\\n\\nSolution\\n\\nDocker\\n\\nKubernetes\\n\\nWhile we work on official Streamlit deployment guides for other hosting providers, here are some user-submitted tutorials for different cloud services:\\n\\nHow to deploy Streamlit apps to Google App Engine, by Yuichiro Tachibana (Tsuchiya)\\n\\nHow to Deploy Streamlit to a Free Amazon EC2 instance, by Rahul Agarwal\\n\\nHost Streamlit on Heroku, by Maarten Grootendorst\\n\\nHost Streamlit on Azure, by Richard Peterson\\n\\nHost Streamlit on 21YunBox, by Toby Lei\\n\\nYou can find guides for other hosting providers on our community-supported deployment wiki.', metadata={'source': 'docs/content/kb/deployments/deploy-streamlit-heroku-aws-google-cloud.md'}),\n",
       " Document(page_content='title: How do I share apps with viewers outside my organization?\\nslug: /knowledge-base/deploy/share-apps-with-viewers-outside-organization\\n\\nHow do I share apps with viewers outside my organization?\\n\\nWhen you share an app with someone outside of your organization, they will receive an email inviting them to sign in with their email.\\n\\nClick here to learn more about how sign in with email works.\\n\\nViewer auth allows you to restrict the viewers of your app. To access your app, users have to authenticate using an email-based passwordless login or single sign-on (SSO). You can share your app with viewers outside your organization in two ways:\\n\\nAdding viewers from the app\\n\\nAdding viewers from your dashboard\\n\\nAdding viewers from the app\\n\\nFrom your deployed app you can easily add viewers from your developer console.\\n\\nSelect \"Manage app\" in the lower right corner.\\n\\nChoose \"Settings\" from the menu.\\n\\nAdd Viewers in Settings.\\n\\nYou can choose to allow only selected viewers based on their individual emails. Make sure to enter them as a line-separated list.\\n\\nThe viewers you have added will receive an email, like the one below, inviting them to sign in with their email.\\n\\nAdding viewers from your dashboard\\n\\nYou can also add viewers directly from your dashboard.\\n\\nOpen settings for your app\\n\\nNavigate to the app you want to add viewer to and click the hamburger icon to select \"Settings.\"\\n\\nAdd Viewers in Settings\\n\\nClick on the \"Sharing\" section in the App Settings and in the text input area, provide a line-separated list of email addresses for the users you wish to grant viewer access to your app. Click \"Save.\"\\n\\nThe viewers you have added will receive an email, like the one below, inviting them to sign in with their email.', metadata={'source': 'docs/content/kb/deployments/share-apps-with-viewers-outside-organization.md'}),\n",
       " Document(page_content=\"title: How to delete your Streamlit Community Cloud account\\nslug: /knowledge-base/deploy/how-to-delete-your-streamlit-community-cloud-account\\n\\nHow to delete your Streamlit Community Cloud account\\n\\nDeleting your Streamlit Community Cloud account is just as easy as creating it via the Settings tab. When you delete your account, your information, account, and all your hosted apps are deleted as well.\\n\\nDeleting your account is permanent and cannot be undone. Make sure you really want to delete your account and all hosted apps before proceeding.\\n\\nHow to delete your account\\n\\nFollow these steps to delete your account:\\n\\nMake sure you are logged in to Streamlit Community Cloud: https://share.streamlit.io/.\\n\\nClick 'Settings' in the top right corner of the page to go to the Settings dashboard. In the 'Account' section, click 'Delete account':\\n\\nIn the 'Delete account?' modal that appears, you will be asked to confirm that you want to delete your account by typing:\\n\\ndelete <your email address>\\n\\nType in delete followed by your email address and click 'Delete account forever':\\n\\nYou will then be logged out and your account, information, and apps will be permanently deleted.\\n\\nUpon deletion, you're shown a confirmation message that your account has been deleted, after which you will be redirected to the Streamlit Community Cloud homepage.\\n\\nIt's that simple! If you have any questions or run into issues deleting your account, please reach out to us on the Forum. We're happy to help! 🎈\", metadata={'source': 'docs/content/kb/deployments/how-to-delete-account.md'}),\n",
       " Document(page_content='title: How do I deploy Streamlit on a domain so it appears to run on a regular port (i.e. port 80)?\\nslug: /knowledge-base/deploy/deploy-streamlit-domain-port-80\\n\\nHow do I deploy Streamlit on a domain so it appears to run on a regular port (i.e. port 80)?\\n\\nProblem\\n\\nYou want to deploy a Streamlit app on a domain so it appears to run on port 80.\\n\\nSolution\\n\\nYou should use a reverse proxy to forward requests from a webserver like Apache or Nginx to the port where your Streamlit app is running. You can accomplish this in several different ways. The simplest way is to forward all requests sent to your domain so that your Streamlit app appears as the content of your website.\\n\\nAnother approach is to configure\\xa0your webserver to forward requests to designated subfolders (e.g. http://awesomestuff.net/streamlitapp) to different Streamlit apps on the same domain, as in this example config for Nginx submitted by a Streamlit community member.\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/permission-denied-in-ec2-port-80/798/3\\n\\nhttps://discuss.streamlit.io/t/how-to-use-streamlit-with-nginx/378/7', metadata={'source': 'docs/content/kb/deployments/deploy-streamlit-domain-port-80.md'}),\n",
       " Document(page_content=\"title: Does Streamlit support the WSGI Protocol? (aka Can I deploy Streamlit with gunicorn?)\\nslug: /knowledge-base/deploy/does-streamlit-support-wsgi-protocol\\n\\nDoes Streamlit support the WSGI Protocol? (aka Can I deploy Streamlit with gunicorn?)\\n\\nProblem\\n\\nYou're not sure whether your Streamlit app can be deployed with gunicorn.\\n\\nSolution\\n\\nStreamlit does not support the WSGI protocol at this time, so deploying Streamlit with (for example) gunicorn is not currently possible. Check out this forum thread regarding deploying Streamlit in a gunicorn-like manner to see how other users have accomplished this.\", metadata={'source': 'docs/content/kb/deployments/does-streamlit-support-wsgi-protocol.md'}),\n",
       " Document(page_content=\"title: Upgrade the Streamlit version of your app on Streamlit Community Cloud\\nslug: /knowledge-base/deploy/upgrade-streamlit-version-on-streamlit-cloud\\n\\nUpgrade the Streamlit version of your app on Streamlit Community Cloud\\n\\nWant to use a cool new Streamlit feature but your app on Streamlit Community Cloud is running an old version of the Streamlit library? If that's you, don't worry! All you need to do is upgrade your app's Streamlit version. Here are five ways to do this, based on how your app manages dependencies:\\n\\nNo dependency file\\n\\nWhen there is no dependency file in the repo, the app will always use the same Streamlit version that existed when the app was first deployed on Streamlit Community Cloud; even if you reboot the app! In other words, Streamlit Community Cloud automatically pins the version for you so that the app does not break suddenly when rebooted in the future.\\n\\nYou may want to avoid getting into this situation if your app depends on a specific version of Streamlit. That is why we encourage you to use a dependency file and pin your desired version of Streamlit. We cover this in more detail in the sections below.\\n\\nrequirements.txt\\n\\nIf the Streamlit version is not pinned (i.e., the requirements file contains a line with streamlit and nothing else):\\n\\nReboot the app as described above.\\n\\nIf the Streamlit version is pinned (e.g., streamlit==1.4.0):\\n\\nAdapt the pinned version in the requirements file and push it to GitHub.\\n\\nThe app on Streamlit Community Cloud will reboot automatically as soon as it detects these changes.\\n\\npipenv/poetry/conda\\n\\nIf you use any of these dependency managers, you probably know what you need to do. 😉\\n\\nOn your local computer, run the command to update the Streamlit package:\\n\\npipenv update streamlit or\\n\\npoetry update streamlit or\\n\\nWith an activated conda environment, run:\\npip install -U streamlit and\\nconda env export\\n\\nThen push any changes to Pipfile.lock or poetry.lock or environment.yml to GitHub.\\n\\nThe app on Streamlit Community Cloud will reboot automatically as soon as it detects these changes.\", metadata={'source': 'docs/content/kb/deployments/upgrade-streamlit-version.md'}),\n",
       " Document(page_content='title: App is not loading when running remotely\\nslug: /knowledge-base/deploy/remote-start\\n\\nApp is not loading when running remotely\\n\\nBelow are a few common errors that occur when users spin up their own solution\\nto host a Streamlit app remotely.\\n\\nTo learn about a deceptively simple way to host Streamlit apps that avoids all\\nthe issues below, check out Streamlit Community Cloud.\\n\\nSymptom #1: The app never loads\\n\\nWhen you enter the app\\'s URL in a browser and all you see is a blank page, a\\n\"Page not found\" error, a \"Connection refused\" error, or anything like that,\\nfirst check that Streamlit is actually running on the remote server. On a Linux\\nserver you can SSH into it and then run:\\n\\nbash\\nps -Al | grep streamlit\\n\\nIf you see Streamlit running, the most likely culprit is the Streamlit port not\\nbeing exposed. The fix depends on your exact setup. Below are three example\\nfixes:\\n\\nTry port 80: Some hosts expose port 80 by default. To\\n  set Streamlit to use that port, start Streamlit with the --server.port\\n  option:\\n\\nbash\\n  streamlit run my_app.py --server.port=80\\n\\nAWS EC2 server: First, click on your instance in the AWS Console.\\n  Then scroll down and click on Security Groups → Inbound → Edit. Next, add\\n  a Custom TCP rule that allows the Port Range 8501 with Source\\n  0.0.0.0/0.\\n\\nOther types of server: Check the firewall settings.\\n\\nIf that still doesn\\'t solve the problem, try running a simple HTTP server\\ninstead of Streamlit, and seeing if that works correctly. If it does, then\\nyou know the problem lies somewhere in your Streamlit app or configuration (in\\nwhich case you should ask for help in our\\nforums!) If not, then it\\'s definitely unrelated\\nto Streamlit.\\n\\nHow to start a simple HTTP server:\\n\\nbash\\npython -m http.server [port]\\n\\nSymptom #2: The app says \"Please wait...\" forever\\n\\nIf when you try to load your app in a browser you see a blue box in the center\\nof the page with the text \"Please wait...\", the underlying cause is likely one\\nof the following:\\n\\nMisconfigured CORS\\n  protection.\\n\\nServer is stripping headers from the Websocket connection, thereby breaking\\n  compression.\\n\\nTo diagnose the issue, try temporarily disabling CORS protection by running\\nStreamlit with the --server.enableCORS flag set to false:\\n\\nbash\\nstreamlit run my_app.py --server.enableCORS=false\\n\\nIf this fixes your issue, you should re-enable CORS protection and then set\\nbrowser.serverPort and browser.serverAddress to the URL and port of your\\nStreamlit app.\\n\\nIf the issue persists, try disabling websocket compression by running Streamlit with the\\n--server.enableWebsocketCompression flag set to false\\n\\nbash\\nstreamlit run my_app.py --server.enableWebsocketCompression=false\\n\\nIf this fixes your issue, your server setup is likely stripping the\\nSec-WebSocket-Extensions HTTP header that is used to negotiate Websocket compression.\\n\\nCompression is not required for Streamlit to work, but it\\'s strongly recommended as it\\nimproves performance. If you\\'d like to turn it back on, you\\'ll need to find which part\\nof your infrastructure is stripping the Sec-WebSocket-Extensions HTTP header and\\nchange that behavior.\\n\\nSymptom #3: Unable to upload files when running in multiple replicas\\n\\nIf the file uploader widget returns an error with status code 403, this is probably\\ndue to a misconfiguration in your app\\'s\\nXSRF protection logic.\\n\\nTo diagnose the issue, try temporarily disabling XSRF protection by running Streamlit\\nwith the --server.enableXsrfProtection flag set to false:\\n\\nbash\\nstreamlit run my_app.py --server.enableXsrfProtection=false\\n\\nIf this fixes your issue, you should re-enable XSRF protection and then\\nconfigure your app to use the same secret across every replica by setting the\\nserver.cookieSecret config option to the same hard-to-guess string everywhere.', metadata={'source': 'docs/content/kb/deployments/remote-start.md'}),\n",
       " Document(page_content='title: I don\\'t have GitHub or GSuite. How do I sign in to Streamlit Community Cloud?\\nslug: /knowledge-base/deploy/sign-in-without-sso\\n\\nI don\\'t have GitHub or GSuite. How do I sign in to Streamlit Community Cloud?\\n\\nIf you don\\'t have GitHub or GSuite accounts, you can sign in with your email address! Visit share.streamlit.io, enter the email address you used to sign up for Streamlit Community Cloud, and click the \"Continue with email\" button.\\n\\nOnce you do so, you will see a confirmation message (like the one below) asking you to check your email.\\n\\nCheck your inbox for an email from Streamlit, with the subject \"Sign in to Streamlit Community Cloud\". Click the link in the email to sign in to Streamlit. Note that this link will expire in 15 minutes and can only be used once.\\n\\nOnce you click the link in your email, you will be taken to your Streamlit Community Cloud workspace!🎈', metadata={'source': 'docs/content/kb/deployments/sign-in-email.md'}),\n",
       " Document(page_content='title: Argh. This app has gone over its resource limits\\nslug: /knowledge-base/deploy/resource-limits\\n\\nArgh. This app has gone over its resource limits\\n\\nSorry! It means you\\'ve hit the resource limits of your Streamlit Community Cloud account.\\n\\nThere are a few things you can change in your app to make it less resource-hungry:\\n\\nReboot your app (temporary fix)\\n\\nUse st.cache_data or st.cache_resource to load models or data only once\\n\\nRestrict the cache size with ttl or max_entries\\n\\nMove big datasets to a database\\n\\nProfile your app\\'s memory usage\\n\\nCheck out our blog post on “Common app problems: Resource limits\" for more in-depth tips prevent your app from hitting the resource limits of the Streamlit Community Cloud.\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/common-app-problems-resource-limits/16969\\n\\nhttps://blog.streamlit.io/common-app-problems-resource-limits/\\n\\nWe offer free resource increases only to support nonprofits or educational organizations on a case-by-case basis. If you are a nonprofit or educational organization, please complete this form and we will review your submission as soon as possible.\\n\\nOnce the increase is completed, you will receive an email from the Streamlit marketing team with a confirmation that the increase has been applied.', metadata={'source': 'docs/content/kb/deployments/resource-limits.md'}),\n",
       " Document(page_content='title: Login attempt to Streamlit Community Cloud fails with error 403\\nslug: /knowledge-base/deploy/login-attempt-to-streamlit-community-cloud-fails-with-error-403\\n\\nLogin attempt to Streamlit Community Cloud fails with error 403\\n\\nProblem\\n\\nStreamlit Community Cloud has monitoring jobs to detect malicious users using the platform for crypto mining. These jobs sometimes result in false positives and a normal user starts getting error 403 against a login attempt.\\n\\nSolution\\n\\nPlease contact Support by providing your GitHub username for help referring to this article.', metadata={'source': 'docs/content/kb/deployments/login-attempt-to-streamlit-community-cloud-fails-with-error-403.md'}),\n",
       " Document(page_content='title: Custom subdomains\\nslug: /knowledge-base/deploy/custom-subdomains\\n\\nCustom subdomains\\n\\nOnce you\\'ve deployed your app on Community Cloud, it\\'s given an automatically generated subdomain that follows a structure based on your GitHub repo. This subdomain is unique to your app and can be used to share your app with others. However, the default subdomain is not always the most memorable or easy to share. E.g. the following is a bit of a mouthful!\\n\\nhttps://streamlit-demo-self-driving-streamlit-app-8jya0g.streamlit.app\\n\\nYou can instead set up a custom subdomain to make your app easier to share. You can customize your subdomain to reflect your app content, personal branding, or whatever you’d like. The URL will appear as:\\n\\n<your-custom-subdomain>.streamlit.app\\n\\nTo customize your app subdomain from the dashboard:\\n\\nClick the \"︙\" overflow menu to the app\\'s right and select \"Settings\".\\n\\nView the \"General\" tab in the App settings modal. Your app\\'s unique subdomain will appear here.\\n\\nPick a custom subdomain between 6 and 63 characters in length for your app\\'s URL and hit \"Save\".\\n\\nIt\\'s that simple! You can then access your app by visiting your custom subdomain URL 🎉.\\n\\nIf a custom subdomain is not available (e.g. because it\\'s already taken), you\\'ll see an error message like this:', metadata={'source': 'docs/content/kb/deployments/custom-subdomains.md'}),\n",
       " Document(page_content='title: How to submit a support case for Streamlit Community Cloud\\nslug: /knowledge-base/deploy/how-to-submit-a-support-case-for-streamlit-community-cloud\\n\\nHow to submit a support case for Streamlit Community Cloud\\n\\nThis article describes the steps to submit a support request to Snowflake for Streamlit Community Cloud.\\n\\nFor Snowflake customers, a support case can be submitted via the support portal on Snowsight.\\n\\nNavigate to https://community.snowflake.com/s/ in your browser.\\n\\nEnsure you are registered.\\n\\na. If you are already a registered user, enter your Snowflake Community username and password into the login form. Click LOG IN.\\n\\nb. If you are not a registered user, click \"Not a member?\". Complete the form on the next screen and follow the instructions to reset your password. Return to the original \"SUBMIT A CASE\" page and log in to your account.\\n\\nScroll down to the first main section of the page (past the search bar) and locate the Support dropdown menu. Select the \"SUBMIT A CASE\" link.\\n\\nSelect the option \"I am a Streamlit Community Cloud User\"\\n\\nHit the button \"Next\" to open the case description page.\\n\\nPlease fill out your request and submit the support case.\\n\\nYou should receive a confirmation email with the case number.\\n\\nA Snowflake Support engineer will follow up directly with the next steps to resolve your case. All communication will be through email.', metadata={'source': 'docs/content/kb/deployments/how-to-submit-a-support-case-for-streamlit-community-cloud.md'}),\n",
       " Document(page_content=\"title: Streamlit Components\\nslug: /knowledge-base/components\\n\\nStreamlit Components\\n\\nBelow are some selected questions we've received about Streamlit Components. If you don't find your question here, take a look on the Streamlit community forum via the Components tag.\\n\\nHow do Streamlit Components differ from functionality provided in the base Streamlit package?\\n\\nWhat types of things aren't possible with Streamlit Components?\\n\\nHow do I add a Component to the sidebar?\\n\\nMy Component seems to be blinking/stuttering...how do I fix that?\", metadata={'source': 'docs/content/kb/components/index.md'}),\n",
       " Document(page_content='title: Unable to edit or delete apps in Streamlit Community Cloud after modifying GitHub username\\nslug: /knowledge-base/deploy/unable-to-edit-or-delete-apps-in-streamlit-community-cloud-after-modifying-github-username\\n\\nUnable to edit or delete apps in Streamlit Community Cloud after modifying GitHub username\\n\\nProblem\\n\\nAfter updating the GitHub username, apps cannot be edited or deleted from Streamlit Community Cloud.\\n\\nSolution\\n\\nSupport can delete your old applications that were deployed before updating the GitHub username.\\n\\nOnce deleted, you can redeploy the applications using the new GitHub username.\\n\\nPlease contact Support with a list of the URLs for the apps you need to be deleted.', metadata={'source': 'docs/content/kb/deployments/unable-to-edit-or-delete-apps-in-streamlit-community-cloud-after-modifying-github-username.md'}),\n",
       " Document(page_content=\"title: How do I increase the upload limit of st.file_uploader on Streamlit Community Cloud?\\nslug: /knowledge-base/deploy/increase-file-uploader-limit-streamlit-cloud\\n\\nHow do I increase the upload limit of st.file_uploader on Streamlit Community Cloud?\\n\\nOverview\\n\\nBy default, files uploaded using st.file_uploader() are limited to 200MB. You can configure this using the server.maxUploadSize config option.\\n\\nStreamlit provides four different ways to set configuration options:\\n\\nIn a global config file at ~/.streamlit/config.toml for macOS/Linux or %userprofile%/.streamlit/config.toml for Windows:\\n   toml\\n   [server]\\n   maxUploadSize = 200\\n\\nIn a per-project config file at $CWD/.streamlit/config.toml, where $CWD is the folder you're running Streamlit from.\\n\\nThrough STREAMLIT_* environment variables, such as:\\n   bash\\n   export STREAMLIT_SERVER_MAX_UPLOAD_SIZE=200\\n\\nAs flags on the command line when running streamlit run:\\n   bash\\n   streamlit run your_script.py --server.maxUploadSize 200\\n\\nWhich of the four options should you choose for an app deployed to Streamlit Community Cloud? 🤔\\n\\nSolution\\n\\nWhen deploying your app to Streamlit Community Cloud, you should use option 1. Namely, set the maxUploadSize config option in a global config file (.streamlit/config.toml) uploaded to your app's GitHub repo. 🎈\\n\\nFor example, to increase the upload limit to 400MB, upload a .streamlit/config.toml file containing the following lines to your app's GitHub repo:\\n\\ntoml\\n[server]\\nmaxUploadSize = 400\\n\\nRelevant resources\\n\\nStreamlit drag and drop capping at 200MB, need workaround\\n\\nFile uploader widget API\\n\\nHow to set Streamlit configuration options\", metadata={'source': 'docs/content/kb/deployments/increase-upload-limit-cloud.md'}),\n",
       " Document(page_content='title: My Component seems to be blinking/stuttering...how do I fix that?\\nslug: /knowledge-base/components/component-blinking-stuttering-fix\\n\\nMy Component seems to be blinking/stuttering...how do I fix that?\\n\\nCurrently, no automatic debouncing of Component updates is performed within Streamlit. The Component creator themselves can decide to rate-limit the updates they send back to Streamlit.', metadata={'source': 'docs/content/kb/components/component-blinking-stuttering-fix.md'}),\n",
       " Document(page_content='title: How do I add a Component to the sidebar?\\nslug: /knowledge-base/components/add-component-sidebar\\n\\nHow do I add a Component to the sidebar?\\n\\nYou can add a component to st.sidebar using the with syntax. For example:\\n\\npython\\nwith st.sidebar:\\n    my_component(greeting=\"hello\")\\n\\nIn fact, you can add your component to any layout container (eg st.columns, st.expander), using the with syntax!\\n\\npython\\ncol1, col2 = st.columns(2)\\nwith col2:\\n    my_component(greeting=\"hello\")', metadata={'source': 'docs/content/kb/components/add-component-sidebar.md'}),\n",
       " Document(page_content='title: How do Streamlit Components differ from functionality provided in the base Streamlit package?\\nslug: /knowledge-base/components/how-streamlit-components-differ-base-package\\n\\nHow do Streamlit Components differ from functionality provided in the base Streamlit package?\\n\\nStreamlit Components are wrapped up in an iframe, which gives you the ability to do whatever you want (within the iframe) using any web technology you like.\\n\\nThere is a strict message protocol between Components and Streamlit, which makes possible for Components to act as widgets. As Streamlit Components are wrapped in iframe, they cannot modify their parent’s DOM (a.k.a the Streamlit report), which ensures that Streamlit is always secure even with user-written components.', metadata={'source': 'docs/content/kb/components/how-streamlit-components-differ-base-package.md'}),\n",
       " Document(page_content=\"title: What types of things aren't possible with Streamlit Components?\\nslug: /knowledge-base/components/not-possibe-streamlit-components\\n\\nWhat types of things aren't possible with Streamlit Components?\\n\\nBecause each Streamlit Component gets mounted into its own sandboxed iframe, this implies a few limitations on what is possible with Components:\\n\\nCan't communicate with other Components: Components can’t contain (or otherwise communicate with) other components, so Components cannot be used to build something like grid_layout\\n\\nCan't modify CSS: A Component can’t modify the CSS that the rest of the Streamlit app uses, so you can't create something like dark_mode\\n\\nCan't add/remove elements: A Component can’t add or remove other elements of a Streamlit app, so you couldn't make something like remove_streamlit_hamburger_menu\", metadata={'source': 'docs/content/kb/components/not-possibe-streamlit-components.md'}),\n",
       " Document(page_content='title: Tutorials\\nslug: /knowledge-base/tutorials\\n\\nTutorials\\n\\nOur tutorials include step-by-step examples of building different types of apps in Streamlit.\\n\\nConnect to data sources\\n\\nSession State basics\\n\\nDeploy Streamlit apps\\n\\nBuild conversational apps', metadata={'source': 'docs/content/kb/tutorials/index.md'}),\n",
       " Document(page_content='title: Authentication without SSO\\nslug: /knowledge-base/deploy/authentication-without-sso\\n\\nAuthentication without SSO\\n\\nIntroduction\\n\\nWant to secure your Streamlit app with passwords, but cannot implement single sign-on? We got you covered! This guide shows you two simple techniques for adding basic authentication to your Streamlit app, using secrets management.\\n\\nWhile this technique adds some level of security, it is NOT comparable to proper authentication with an SSO provider.\\n\\nOption 1: One global password for all users\\n\\nThis is the easiest option! Your app will ask for a password that\\'s shared between all users. It will be stored in the app secrets using secrets management. If you want to change this password or revoke a user\\'s access, you will need to change it for everyone. If you want to have one password per user instead, jump to Option 2 below.\\n\\nStep 1: Add the password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root dir. Create this file if it doesn\\'t exist yet and add your password to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\npassword = \"streamlit123\"\\n```\\n\\nBe sure to add this file to your .gitignore so you don\\'t commit your secrets!\\n\\nStep 2: Copy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at secrets management.\\n\\nStep 3: Ask for the password in your Streamlit app\\n\\nCopy the code below to your Streamlit app, insert your normal app code in the if statement at the bottom, and run it:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\ndef check_password():\\n    \"\"\"Returns True if the user had the correct password.\"\"\"\\n\\nif check_password():\\n    st.write(\"Here goes your normal Streamlit app...\")\\n    st.button(\"Click me\")\\n```\\n\\nIf everything worked out, your app should look like this:\\n\\nOption 2: Individual password for each user\\n\\nThis option allows you to set a username and password for each user of your app. Like in Option 1, both values will be stored in the app secrets using secrets management.\\n\\nStep 1: Add usernames & passwords to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root dir. Create this file if it doesn\\'t exist yet and add the usernames & password to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[passwords]\\n\\nFollow the rule: username = \"password\"\\n\\nalice_foo = \"streamlit123\"\\nbob_bar = \"mycrazypw\"\\n```\\n\\nBe sure to add this file to your .gitignore so you don\\'t commit your secrets!\\n\\nAlternatively, you could set up and manage usernames & passwords via a spreadsheet or database. To use secrets to securely connect to Google Sheets, AWS, and other data providers, read our tutorials on how to Connect Streamlit to data sources.\\n\\nStep 2: Copy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at secrets management.\\n\\nStep 3: Ask for username & password in your Streamlit app\\n\\nCopy the code below to your Streamlit app, insert your normal app code in the if statement at the bottom, and run it:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\ndef check_password():\\n    \"\"\"Returns True if the user had a correct password.\"\"\"\\n\\nif check_password():\\n    st.write(\"Here goes your normal Streamlit app...\")\\n    st.button(\"Click me\")\\n```\\n\\nIf everything worked out, your app should look like this:', metadata={'source': 'docs/content/kb/deployments/authentication-without-sso.md'}),\n",
       " Document(page_content='title: Session State basics\\nslug: /knowledge-base/tutorials/session-state\\n\\nSession State basics\\n\\nCheck out this Session State basics tutorial video by Streamlit Developer Advocate Dr. Marisa Smith to get started:', metadata={'source': 'docs/content/kb/tutorials/session-state.md'}),\n",
       " Document(page_content='title: Deploy Streamlit apps\\nslug: /knowledge-base/tutorials/deploy\\n\\nDeploy Streamlit apps\\n\\nThis sections contains step-by-step guides on how to deploy Streamlit apps to various cloud platforms and services. We have deployment guides for:\\n\\nStreamlit Community Cloud\\n\\nDocker\\n\\nKubernetes\\n\\nWhile we work on official Streamlit deployment guides for other hosting providers, here are some user-submitted tutorials for different cloud services:\\n\\nHow to deploy Streamlit apps to Google App Engine, by Yuichiro Tachibana (Tsuchiya)\\n\\nHow to Deploy Streamlit to a Free Amazon EC2 instance, by Rahul Agarwal\\n\\nHost Streamlit on Heroku, by Maarten Grootendorst\\n\\nHost Streamlit on Azure, by Richard Peterson\\n\\nHost Streamlit on 21YunBox, by Toby Lei\\n\\nCommunity-supported deployment wiki.', metadata={'source': 'docs/content/kb/tutorials/deploy/index.md'}),\n",
       " Document(page_content=\"title: Connect to data sources\\nslug: /knowledge-base/tutorials/databases\\n\\nConnect Streamlit to data sources\\n\\nThese step-by-step guides demonstrate how to connect Streamlit apps to various databases & APIs.\\nThey use Streamlit's secrets management and\\ncaching to provide secure and fast data access.\\n\\nAWS S3\\n\\nBigQuery\\n\\nDeta Base\\n\\nFirestore (blog)\\n\\nGoogle Cloud Storage\\n\\nMicrosoft SQL Server\\n\\nMongoDB\\n\\nMySQL\\n\\nPostgreSQL\\n\\nPrivate Google Sheet\\n\\nPublic Google Sheet\\n\\nSnowflake\\n\\nSupabase\\n\\nTableau\\n\\nTiDB\\n\\nTigerGraph\", metadata={'source': 'docs/content/kb/tutorials/databases/index.md'}),\n",
       " Document(page_content='title: Deploy Streamlit using Kubernetes\\nslug: /knowledge-base/tutorials/deploy/kubernetes\\n\\nDeploy Streamlit using Kubernetes\\n\\nIntroduction\\n\\nSo you have an amazing app and you want to start sharing it with other people, what do you do? You have a few options. First, where do you want to run your Streamlit app, and how do you want to access it?\\n\\nOn your corporate network\\xa0- Most corporate networks are closed to the outside world. You typically use a VPN to log onto your corporate network and access resources there. You could run your Streamlit app on a server in your corporate network for security reasons, to ensure that only folks internal to your company can access it.\\n\\nOn the cloud\\xa0- If you\\'d like to access your Streamlit app from outside of a corporate network, or share your app with folks outside of your home network or laptop, you might choose this option. In this case, it\\'ll depend on your hosting provider. We have community-submitted guides from Heroku, AWS, and other providers.\\n\\nWherever you decide to deploy your app, you will first need to containerize it. This guide walks you through using Kubernetes to deploy your app. If you prefer Docker see Deploy Streamlit using Docker.\\n\\nPrerequisites\\n\\nInstall Docker Engine\\n\\nInstall the gcloud CLI\\n\\nInstall Docker Engine\\n\\nIf you haven\\'t already done so, install Docker on your server. Docker provides\\xa0.deb and\\xa0.rpm packages from many Linux distributions, including:\\n\\nDebian\\n\\nUbuntu\\n\\nVerify that Docker Engine is installed correctly by running the\\xa0hello-world Docker image:\\n\\nbash\\nsudo docker run hello-world\\n\\nFollow Docker\\'s official post-installation steps for Linux to run Docker as a non-root user, so that you don\\'t have to preface the docker command with sudo.\\n\\nInstall the gcloud CLI\\n\\nIn this guide, we will orchestrate Docker containers with Kubernetes and host docker images on the Google Container Registry (GCR). As GCR is a Google-supported Docker registry, we need to register\\xa0gcloud as the Docker credential helper.\\n\\nFollow the official documentation to Install the gcloud CLI and initialize it.\\n\\nCreate a Docker container\\n\\nWe need to create a docker container which contains all the dependencies and the application code. Below you can see the entrypoint, i.e. the command run when the container starts, and the Dockerfile definition.\\n\\nCreate an entrypoint script\\n\\nCreate a run.sh script containing the following:\\n\\n```bash\\n\\n!/bin/bash\\n\\nAPP_PID=\\nstopRunningProcess() {\\n    # Based on https://linuxconfig.org/how-to-propagate-a-signal-to-child-processes-from-a-bash-script\\n    if test ! \"${APP_PID}\" = \\'\\' && ps -p ${APP_PID} > /dev/null ; then\\n       > /proc/1/fd/1 echo \"Stopping ${COMMAND_PATH} which is running with process ID ${APP_PID}\"\\n\\ntrap stopRunningProcess EXIT TERM\\n\\nsource ${VIRTUAL_ENV}/bin/activate\\n\\nstreamlit run ${HOME}/app/streamlit_app.py &\\nAPP_ID=${!}\\n\\nwait ${APP_ID}\\n```\\n\\nCreate a Dockerfile\\n\\nDockerfile reference. The\\n\\ndocker build command builds an image from a\\n\\ndocker run command first creates a container over the specified image, and then starts it using the specified command.\\n\\nHere\\'s an example Dockerfile that you can add to the root of your directory.\\n\\n```docker\\nFROM python:3.8-slim\\n\\nRUN groupadd --gid 1000 appuser \\\\\\n    && useradd --uid 1000 --gid 1000 -ms /bin/bash appuser\\n\\nRUN pip3 install --no-cache-dir --upgrade \\\\\\n    pip \\\\\\n    virtualenv\\n\\nRUN apt-get update && apt-get install -y \\\\\\n    build-essential \\\\\\n    software-properties-common \\\\\\n    git\\n\\nUSER appuser\\nWORKDIR /home/appuser\\n\\nRUN git clone https://github.com/streamlit/streamlit-example.git app\\n\\nENV VIRTUAL_ENV=/home/appuser/venv\\nRUN virtualenv ${VIRTUAL_ENV}\\nRUN . ${VIRTUAL_ENV}/bin/activate && pip install -r app/requirements.txt\\n\\nEXPOSE 8501\\n\\nCOPY run.sh /home/appuser\\nENTRYPOINT [\"./run.sh\"]\\n```\\n\\nAs mentioned in Development flow, for Streamlit version 1.10.0 and higher, Streamlit apps cannot be run from the root directory of Linux distributions. Your main script should live in a directory other than the root directory. If you try to run a Streamlit app from the root directory, Streamlit will throw a FileNotFoundError: [Errno 2] No such file or directory error. For more information, see GitHub issue #5239.\\n\\nIf you are using Streamlit version 1.10.0 or higher, you must set the WORKDIR to a directory other than the root directory. For example, you can set the WORKDIR to /home/appuser as shown in the example Dockerfile above.\\n\\nBuild a Docker image\\n\\nPut the above files (run.sh and Dockerfile) in the same folder and build the docker image:\\n\\ndocker\\ndocker build --platform linux/amd64 -t gcr.io/$GCP_PROJECT_ID/k8s-streamlit:test .\\n\\nReplace $GCP_PROJECT_ID in the above command with the name of your Google Cloud project.\\n\\nUpload the Docker image to a container registry\\n\\nThe next step is to upload the Docker image to a container registry. In this example, we will use the Google Container Registry (GCR). Start by enabling the Container Registry API. Sign in to Google Cloud and navigate to your project’s Container Registry and click Enable.\\n\\nWe can now build the Docker image from the previous step and push it to our project’s GCR. Be sure to replace $GCP_PROJECT_ID in the docker push command with the name of your project:\\n\\nbash\\ngcloud auth configure-docker\\ndocker push gcr.io/$GCP_PROJECT_ID/k8s-streamlit:test\\n\\nCreate a Kubernetes deployment\\n\\nFor this step you will need a:\\n\\nRunning Kubernetes service\\n\\nCustom domain for which you can generate a TLS certificate\\n\\nDNS service where you can configure your custom domain to point to the application IP\\n\\nAs the image was uploaded to the container registry in the previous step, we can run it in Kubernetes using the below configurations.\\n\\nInstall and run Kubernetes\\n\\nMake sure your Kubernetes client, kubectl, is installed and running on your machine.\\n\\nConfigure a Google OAuth Client and oauth2-proxy\\n\\nFor configuring the Google OAuth Client, please see Google Auth Provider. Configure oauth2-proxy to use the desired OAuth Provider Configuration and update the oath2-proxy config in the config map.\\n\\nThe below configuration contains a ouath2-proxy sidecar container which handles the authentication with Google. You can learn more from the oauth2-proxy repository.\\n\\nCreate a Kubernetes configuration file\\n\\nCreate a YAML configuration file named k8s-streamlit.yaml:\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: streamlit-configmap\\ndata:\\n  oauth2-proxy.cfg: |-\\n    http_address = \"0.0.0.0:4180\"\\n    upstreams = [\"http://127.0.0.1:8501/\"]\\n    email_domains = [\"*\"]\\n    client_id = \"\"\\n    client_secret = \"\"\\n    cookie_secret = \"<16, 24, or 32 bytes>\"\\n    redirect_url =\\n\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: streamlit-deployment\\n  labels:\\n    app: streamlit\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: streamlit\\n  template:\\n    metadata:\\n      labels:\\n        app: streamlit\\n    spec:\\n      containers:\\n        - name: oauth2-proxy\\n          image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\\n          args: [\"--config\", \"/etc/oauth2-proxy/oauth2-proxy.cfg\"]\\n          ports:\\n            - containerPort: 4180\\n          livenessProbe:\\n            httpGet:\\n              path: /ping\\n              port: 4180\\n              scheme: HTTP\\n          readinessProbe:\\n            httpGet:\\n              path: /ping\\n              port: 4180\\n              scheme: HTTP\\n          volumeMounts:\\n            - mountPath: \"/etc/oauth2-proxy\"\\n              name: oauth2-config\\n        - name: streamlit\\n          image: gcr.io/GCP_PROJECT_ID/k8s-streamlit:test\\n          imagePullPolicy: Always\\n          ports:\\n            - containerPort: 8501\\n          livenessProbe:\\n            httpGet:\\n              path: /_stcore/health\\n              port: 8501\\n              scheme: HTTP\\n            timeoutSeconds: 1\\n          readinessProbe:\\n            httpGet:\\n              path: /_stcore/health\\n              port: 8501\\n              scheme: HTTP\\n            timeoutSeconds: 1\\n          resources:\\n            limits:\\n              cpu: 1\\n              memory: 2Gi\\n            requests:\\n              cpu: 100m\\n              memory: 745Mi\\n      volumes:\\n        - name: oauth2-config\\n          configMap:\\n            name: streamlit-configmap\\n\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: streamlit-service\\nspec:\\n  type: LoadBalancer\\n  selector:\\n    app: streamlit\\n  ports:\\n    - name: streamlit-port\\n      protocol: TCP\\n      port: 80\\n      targetPort: 4180\\n```\\n\\nWhile the above configurations can be copied verbatim, you will have to configure the oauth2-proxy yourself and use the correct GOOGLE_CLIENT_ID, GOOGLE_CLIENT_ID, GCP_PROJECT_ID, and REDIRECT_URL.\\n\\nNow create the configuration from the file in Kubernetes with the kubectl create command:\\n\\nbash\\nkubctl create -f k8s-streamlit.yaml\\n\\nSet up TLS support\\n\\nSince you are using the Google authentication, you will need to set up TLS support. Find out how in TLS Configuration.\\n\\nVerify the deployment\\n\\nOnce the deployment and the service are created, we need to wait a couple of minutes for the public IP address to become available. We can check when that is ready by running:\\n\\nbash\\nkubectl get service streamlit-service -o jsonpath=\\'{.status.loadBalancer.ingress[0].ip}\\'\\n\\nAfter the public IP is assigned, you will need to configure in your DNS service an A record pointing to the above IP address.', metadata={'source': 'docs/content/kb/tutorials/deploy/kubernetes.md'}),\n",
       " Document(page_content='title: Connect Streamlit to a public Google Sheet\\nslug: /knowledge-base/tutorials/databases/public-gsheet\\n\\nConnect Streamlit to a public Google Sheet\\n\\nIntroduction\\n\\nThis guide explains how to securely access a public Google Sheet from Streamlit Community Cloud. It uses the pandas library and Streamlit\\'s secrets management.\\n\\nThis method requires you to enable link sharing for your Google Sheet. While the sharing link will not appear in your code (and actually acts as sort of a password!), someone with the link can get all the data in the Sheet. If you don\\'t want this, follow the (more complicated) guide Connect Streamlit to a private Google Sheet.\\n\\nCreate a Google Sheet and turn on link sharing\\n\\nIf you already have a Sheet that you want to access, feel free to skip to the next\\nstep.\\n\\nAdd the Sheets URL to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the share link of your Google Sheet to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\npublic_gsheets_url = \"https://docs.google.com/spreadsheets/d/xxxxxxx/edit#gid=0\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport pandas as pd\\nimport streamlit as st\\n\\nRead in data from the Google Sheet.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef load_data(sheets_url):\\n    csv_url = sheets_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\\n    return pd.read_csv(csv_url)\\n\\ndf = load_data(st.secrets[\"public_gsheets_url\"])\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/public-gsheet.md'}),\n",
       " Document(page_content='title: Deploy Streamlit using Docker\\nslug: /knowledge-base/tutorials/deploy/docker\\n\\nDeploy Streamlit using Docker\\n\\nIntroduction\\n\\nSo you have an amazing app and you want to start sharing it with other people, what do you do? You have a few options. First, where do you want to run your Streamlit app, and how do you want to access it?\\n\\nOn your corporate network\\xa0- Most corporate networks are closed to the outside world. You typically use a VPN to log onto your corporate network and access resources there. You could run your Streamlit app on a server in your corporate network for security reasons, to ensure that only folks internal to your company can access it.\\n\\nOn the cloud\\xa0- If you\\'d like to access your Streamlit app from outside of a corporate network, or share your app with folks outside of your home network or laptop, you might choose this option. In this case, it\\'ll depend on your hosting provider. We have community-submitted guides from Heroku, AWS, and other providers.\\n\\nWherever you decide to deploy your app, you will first need to containerize it. This guide walks you through using Docker to deploy your app. If you prefer Kubernetes see Deploy Streamlit using Kubernetes.\\n\\nPrerequisites\\n\\nInstall Docker Engine\\n\\nCheck network port accessibility\\n\\nInstall Docker Engine\\n\\nIf you haven\\'t already done so, install Docker on your server. Docker provides\\xa0.deb and\\xa0.rpm packages from many Linux distributions, including:\\n\\nDebian\\n\\nUbuntu\\n\\nVerify that Docker Engine is installed correctly by running the\\xa0hello-world Docker image:\\n\\nbash\\nsudo docker run hello-world\\n\\nFollow Docker\\'s official post-installation steps for Linux to run Docker as a non-root user, so that you don\\'t have to preface the docker command with sudo.\\n\\nCheck network port accessibility\\n\\nAs you and your users are behind your corporate VPN, you need to make sure all of you can access a certain network port. Let\\'s say port 8501, as it is the default port used by Streamlit. Contact your IT team and request access to port 8501 for you and your users.\\n\\nCreate a Dockerfile\\n\\nDockerfile reference. The\\n\\ndocker build command builds an image from a\\n\\ndocker run command first creates a container over the specified image, and then starts it using the specified command.\\n\\nHere\\'s an example Dockerfile that you can add to the root of your directory. i.e. in /app/\\n\\n```docker\\n\\napp/Dockerfile\\n\\nFROM python:3.9-slim\\n\\nWORKDIR /app\\n\\nRUN apt-get update && apt-get install -y \\\\\\n    build-essential \\\\\\n    curl \\\\\\n    software-properties-common \\\\\\n    git \\\\\\n    && rm -rf /var/lib/apt/lists/*\\n\\nRUN git clone https://github.com/streamlit/streamlit-example.git .\\n\\nRUN pip3 install -r requirements.txt\\n\\nEXPOSE 8501\\n\\nHEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\\n\\nENTRYPOINT [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\\n```\\n\\nDockerfile walkthrough\\n\\nLet’s walk through each line of the Dockerfile :\\n\\nA Dockerfile must start with a\\xa0FROM instruction. It sets the Base Image (think OS) for the container:\\n\\ndocker\\n   FROM python:3.9-slim\\n\\nDocker has a number of official Docker base images based on various Linux distributions. They also have base images that come with language-specific modules such as Python. The python images come in many flavors, each designed for a specific use case. Here, we use the python:3.9-slim image which is a lightweight image that comes with the latest version of Python 3.9.\\n\\nYou can also use your own base image, provided the image you use contains a supported version of Python for Streamlit. There is no one-size-fits-all approach to using any specific base image, nor is there an official Streamlit-specific base image.\\n\\nThe\\xa0WORKDIR instruction sets the working directory for any\\xa0RUN,\\xa0CMD,\\xa0ENTRYPOINT,\\xa0COPY and\\xa0ADD instructions that follow it in the\\xa0Dockerfile . Let’s set it to app/ :\\n\\ndocker\\n   WORKDIR /app\\n\\nAs mentioned in Development flow, for Streamlit version 1.10.0 and higher, Streamlit apps cannot be run from the root directory of Linux distributions. Your main script should live in a directory other than the root directory. If you try to run a Streamlit app from the root directory, Streamlit will throw a FileNotFoundError: [Errno 2] No such file or directory error. For more information, see GitHub issue #5239.\\n\\nIf you are using Streamlit version 1.10.0 or higher, you must set the WORKDIR to a directory other than the root directory. For example, you can set the WORKDIR to /app as shown in the example Dockerfile above.\\n\\nInstall git so that we can clone the app code from a remote repo:\\n\\ndocker\\n   RUN apt-get update && apt-get install -y \\\\\\n       build-essential \\\\\\n       curl \\\\\\n       software-properties-common \\\\\\n       git \\\\\\n       && rm -rf /var/lib/apt/lists/*\\n\\nClone your code that lives in a remote repo to WORKDIR:\\n\\na. If your code is in a public repo:\\n\\ndocker\\n   RUN git clone https://github.com/streamlit/streamlit-example.git .\\n\\nOnce cloned, the directory of WORKDIR will look like the following:\\n\\nbash\\n   app/\\n   - requirements.txt\\n   - streamlit_app.py\\n\\nwhere requirements.txt file contains all your Python dependencies. E.g\\n\\naltair\\n   pandas\\n   streamlit\\n\\nand streamlit_app.py is your main script. E.g.\\n\\n```python\\n   from collections import namedtuple\\n   import altair as alt\\n   import math\\n   import pandas as pd\\n   import streamlit as st\\n\\n\"\"\"\\n   # Welcome to Streamlit!\\n\\nEdit /streamlit_app.py to customize this app to your heart\\'s desire :heart:\\n\\nIf you have any questions, checkout our documentation and community\\n   forums.\\n\\nIn the meantime, below is an example of what you can do with just a few lines of code:\\n   \"\"\"\\n\\nwith st.echo(code_location=\\'below\\'):\\n      total_points = st.slider(\"Number of points in spiral\", 1, 5000, 2000)\\n      num_turns = st.slider(\"Number of turns in spiral\", 1, 100, 9)\\n\\n```\\n\\nb. If your code is in a private repo, please read Using SSH to access private data in builds and modify the Dockerfile accordingly -- to install an SSH client, download the public key for github.com, and clone your private repo. If you use an alternative VCS such as GitLab or Bitbucket, please consult the documentation for that VCS on how to copy your code to the WORKDIR of the Dockerfile.\\n\\nc. If your code lives in the same directory as the Dockerfile, copy all your app files from your server into the container, including streamlit_app.py, requirements.txt, etc, by replacing the git clone line with:\\n\\ndocker\\n   COPY . .\\n\\nMore generally, the idea is copy your app code from wherever it may live on your server into the container. If the code is not in the same directory as the Dockerfile, modify the above command to include the path to the code.\\n\\nInstall your app’s Python dependencies from the cloned requirements.txt in the container:\\n\\ndocker\\n   RUN pip3 install -r requirements.txt\\n\\nThe\\xa0EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. Your container needs to listen to Streamlit’s (default) port 8501:\\n\\ndocker\\n   EXPOSE 8501\\n\\nThe\\xa0HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. Your container needs to listen to Streamlit’s (default) port 8501:\\n\\ndocker\\n   HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\\n\\nAn\\xa0ENTRYPOINT\\xa0allows you to configure a container that will run as an executable. Here, it also contains the entire streamlit run command for your app, so you don’t have to call it from the command line:\\n\\ndocker\\n   ENTRYPOINT [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\\n\\nBuild a Docker image\\n\\nThe\\xa0docker build command builds an image from a\\xa0Dockerfile . Run the following command from the app/ directory on your server to build the image:\\n\\ndocker\\ndocker build -t streamlit .\\n\\nThe -t flag is used to tag the image. Here, we have tagged the image streamlit. If you run:\\n\\ndocker\\ndocker images\\n\\nYou should see a streamlit image under the REPOSITORY column. For example:\\n\\nREPOSITORY   TAG       IMAGE ID       CREATED              SIZE\\nstreamlit    latest    70b0759a094d   About a minute ago   1.02GB\\n\\nRun the Docker container\\n\\nNow that you have built the image, you can run the container by executing:\\n\\ndocker\\ndocker run -p 8501:8501 streamlit\\n\\nThe -p flag publishes the container’s port 8501 to your server’s 8501 port.\\n\\nIf all went well, you should see an output similar to the following:\\n\\n```\\ndocker run -p 8501:8501 streamlit\\n\\nYou can now view your Streamlit app in your browser.\\n\\nURL: http://0.0.0.0:8501\\n```\\n\\nTo view your app, users can browse to http://0.0.0.0:8501 or http://localhost:8501\\n\\nBased on your server\\'s network configuration, you could map to port 80/443 so that users can view your app using the server IP or hostname. For example: http://your-server-ip:80 or http://your-hostname:443.', metadata={'source': 'docs/content/kb/tutorials/deploy/docker.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Snowflake\\nslug: /knowledge-base/tutorials/databases/snowflake\\n\\nConnect Streamlit to Snowflake\\n\\nIntroduction\\n\\nThis guide explains how to securely access a Snowflake database from Streamlit. It uses st.experimental_connection, the Snowpark Python library and Streamlit\\'s secrets management. The below example code will only work on Streamlit version >= 1.22, when st.experimental_connection was added.\\n\\nSkip to the bottom for information about connecting using Snowflake Connector for Python.\\n\\nCreate a Snowflake database\\n\\nIf you already have a database that you want to use, feel free to skip to the next step.\\n\\nFirst, sign up for Snowflake and log into the Snowflake web interface (note down your username, password, and account identifier!):\\n\\nEnter the following queries into the SQL editor in the Worksheets page to create a database and a table with some example values:\\n\\n```sql\\nCREATE DATABASE PETS;\\n\\nCREATE TABLE MYTABLE (\\n    NAME            varchar(80),\\n    PET             varchar(80)\\n);\\n\\nINSERT INTO MYTABLE VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\');\\n\\nSELECT * FROM MYTABLE;\\n```\\n\\nBefore you execute the queries, first determine which Snowflake UI / web interface you\\'re using. The examples below use Snowsight. You can also use Classic Console Worksheets or any other means of running Snowflake SQL statements.\\n\\nExecute queries in a Worksheet\\n\\nTo execute the queries in a Worksheet, highlight or select all the queries with your mouse, and click the play button in the top right corner.\\n\\nBe sure to highlight or select all the queries (lines 1-10) before clicking the play button.\\n\\nOnce you have executed the queries, you should see a preview of the table in the Results panel at the bottom of the page. Additionally, you should see your newly created database and schema by expanding the accordion on the left side of the page. Lastly, the warehouse name is displayed on the button to the left of the Share button.\\n\\nMake sure to note down the name of your warehouse, database, and schema. ☝️\\n\\nInstall snowflake-snowpark-python\\n\\nYou can find the instructions and prerequisites for installing snowflake-snowpark-python in the Snowpark Developer Guide.\\n\\nbash\\npip install \"snowflake-snowpark-python[pandas]\"\\n\\nParticular prerequisites to highlight:\\n\\nCurrently, only python 3.8 is supported.\\n\\nEnsure you have the correct pyarrow version installed for your version of snowflake-snowpark-python. When in doubt, try uninstalling pyarrow before installing snowflake-snowpark-python.\\n\\nAdd connection parameters to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app’s root directory. Learn more about Streamlit secrets management here. Create this file if it doesn’t exist yet and add your Snowflake username, password, account identifier, and the name of your warehouse, database, and schema as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowpark]\\naccount = \"xxx\"\\nuser = \"xxx\"\\npassword = \"xxx\"\\nrole = \"xxx\"\\nwarehouse = \"xxx\"\\ndatabase = \"xxx\"\\nschema = \"xxx\"\\nclient_session_keep_alive = true\\n```\\n\\nIf you created the database from the previous step, the names of your database and schema are PETS and PUBLIC, respectively. Streamlit will also use Snowflake config and credentials from a SnowSQL config file if available.\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the query to use the name of your table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'snowpark\\')\\n\\nPerform query.\\n\\ndf = conn.query(\\'SELECT * from mytable;\\', ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.NAME} has a :{row.PET}:\")\\n```\\n\\nSee st.experimental_connection above? This handles secrets retrieval, setup, query caching and retries. By default, query() results are cached without expiring. In this case, we set ttl=600 to ensure the query result is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:\\n\\nUsing a Snowpark Session\\n\\nThe same SnowparkConnection used above also provides access to the Snowpark Session for DataFrame-style operations that run natively inside Snowflake. Using this approach, you can rewrite the app above as follows:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'snowpark\\')\\n\\nLoad the table as a dataframe using the Snowpark Session.\\n\\n@st.cache_data\\ndef load_table():\\n    with conn.safe_session() as session:\\n        return session.table(\\'mytable\\').to_pandas()\\n\\ndf = load_table()\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.NAME} has a :{row.PET}:\")\\n```\\n\\nThis example uses with conn.safe_session() to provide thread safety. conn.session also works directly, but does not guarantee thread safety. If everything worked out (and you used the example table we created above), your app should look the same as the screenshot from the first example above.\\n\\nUsing the Snowflake Connector for Python\\n\\nIn some cases, you may prefer to use the Snowflake Connector for Python instead of Snowpark Python. Streamlit supports this natively through the SQLConnection and the snowflake-sqlalchemy library.\\n\\nbash\\npip install snowflake-sqlalchemy\\n\\nInstalling snowflake-sqlalchemy will also install all necessary dependencies.\\n\\nConfiguring credentials follows the SQLConnection format which is slightly different. See the Snowflake SQLAlchemy Configuration Parameters documentation for more details.\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowflake]\\nurl = \"snowflake://:@//?warehouse=&role=\"\\n```\\n\\nAlternatively, specify connection parameters like authenticator or key pair authentication using create_engine_kwargs, as shown below.\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowflake]\\nurl = \"snowflake://@/\"\\n\\n[connections.snowflake.create_engine_kwargs.connect_args]\\nauthenticator = \"externalbrowser\"\\nwarehouse = \"xxx\"\\nrole = \"xxx\"\\nclient_session_keep_alive = true\\n```\\n\\nInitializing and using the connection in your app is similar. Note that SQLConnection.query() supports extra arguments like params and chunksize which may be useful for more advanced apps.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'snowflake\\', type=\\'sql\\')\\n\\nPerform query.\\n\\ndf = conn.query(\\'SELECT * from mytable;\\', ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```\\n\\nIf everything worked out (and you used the example table we created above), your app should look the same as the screenshot from the first example above.\\n\\nConnecting to Snowflake from Community Cloud\\n\\nThis tutorial assumes a local Streamlit app, however you can also connect to Snowflake from apps hosted in Community Cloud. The main additional steps are:\\n\\nInclude information about dependencies using a requirements.txt file with snowflake-snowpark-python and any other dependencies.\\n\\nAdd your secrets to your Community Cloud app.\\n\\nFor apps using snowflake-snowpark-python, you should also ensure the app is running on python 3.8.', metadata={'source': 'docs/content/kb/tutorials/databases/snowflake.md'}),\n",
       " Document(page_content='title: Build conversational apps\\nslug: /knowledge-base/tutorials/build-conversational-apps\\n\\nBuild conversational apps\\n\\nIntroduction\\n\\nThe advent of large language models like GPT has revolutionized the ease of developing chat-based applications. Streamlit offers several Chat elements, enabling you to build Graphical User Interfaces (GUIs) for conversational agents or chatbots. Leveraging session state along with these elements allows you to construct anything from a basic chatbot to a more advanced, ChatGPT-like experience using purely Python code.\\n\\nIn this tutorial, we\\'ll start by walking through Streamlit\\'s chat elements, st.chat_message and st.chat_input. Then we\\'ll proceed to construct three distinct applications, each showcasing an increasing level of complexity and functionality:\\n\\nFirst, we\\'ll Build a bot that mirrors your input to get a feel for the chat elements and how they work. We\\'ll also introduce session state and how it can be used to store the chat history. This section will serve as a foundation for the rest of the tutorial.\\n\\nNext, you\\'ll learn how to Build a simple chatbot GUI with streaming.\\n\\nFinally, we\\'ll Build a ChatGPT-like app that leverages session state to remember conversational context, all within less than 50 lines of code.\\n\\nHere\\'s a sneak peek of the simple chatbot GUI with streaming we\\'ll build in this tutorial:\\n\\nPlay around with the above demo to get a feel for what we\\'ll build in this tutorial. A few things to note:\\n\\nThere\\'s a chat input at the bottom of the screen that\\'s always visible. It contains some placeholder text. You can type in a message and press Enter or click the run button to send it.\\n\\nWhen you enter a message, it appears as a chat message in the container above. The container is scrollable, so you can scroll up to see previous messages. A default avatar is displayed to your messages\\' left.\\n\\nThe assistant responds to your messages with a random message from a list of responses. The responses are streamed to the frontend and are displayed with a different default avatar.\\n\\nBefore we start building, let\\'s take a closer look at the chat elements we\\'ll use.\\n\\nChat elements\\n\\nStreamlit offers several commands to help you build conversational apps. These chat elements are designed to be used in conjunction with each other, but you can also use them separately.\\n\\nst.chat_message lets you insert a chat message container into the app so you can display messages from the user or the app. Chat containers can contain other Streamlit elements, including charts, tables, text, and more. st.chat_input lets you display a chat input widget so the user can type in a message.\\n\\nst.chat_message\\n\\nst.chat_message lets you insert a multi-element chat message container into your app. The returned container can contain any Streamlit element, including charts, tables, text, and more. To add elements to the returned container, you can use with notation.\\n\\nst.chat_message\\'s first parameter is the name of the message author, which can be either \"user\" or \"assistant\" to enable preset styling and avatars, like in the demo above. You can also pass in a custom string to use as the author name. Currently, the name is not shown in the UI but is only set as an accessibility label. For accessibility reasons, you should not use an empty string.\\n\\nHere\\'s an minimal example of how to use st.chat_message to display a welcome message:\\n\\n```python\\nimport streamlit as st\\n\\nwith st.chat_message(\"user\"):\\n    st.write(\"Hello 👋\")\\n```\\n\\nNotice the message is displayed with a default avatar and styling since we passed in \"user\" as the author name. You can also pass in \"assistant\" as the author name to use a different default avatar and styling, or pass in a custom name and avatar. See the API reference for more details.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\n\\nwith st.chat_message(\"assistant\"):\\n    st.write(\"Hello human\")\\n    st.bar_chart(np.random.randn(30, 3))\\n```\\n\\nWhile we\\'ve used the preferred with notation in the above examples, you can also just call methods directly in the returned objects. The below example is equivalent to the one above:\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\n\\nmessage = st.chat_message(\"assistant\")\\nmessage.write(\"Hello human\")\\nmessage.bar_chart(np.random.randn(30, 3))\\n```\\n\\nSo far, we\\'ve displayed predefined messages. But what if we want to display messages based on user input?\\n\\nst.chat_input\\n\\nst.chat_input lets you display a chat input widget so the user can type in a message. The returned value is the user\\'s input, which is None if the user hasn\\'t sent a message yet. You can also pass in a default prompt to display in the input widget. Here\\'s an example of how to use st.chat_input to display a chat input widget and show the user\\'s input:\\n\\n```python\\nimport streamlit as st\\n\\nprompt = st.chat_input(\"Say something\")\\nif prompt:\\n    st.write(f\"User has sent the following prompt: {prompt}\")\\n```\\n\\nPretty straightforward, right? Now let\\'s combine st.chat_message and st.chat_input to build a bot the mirrors or echoes your input.\\n\\nBuild a bot that mirrors your input\\n\\nIn this section, we\\'ll build a bot that mirrors or echoes your input. More specifically, the bot will respond to your input with the same message. We\\'ll use st.chat_message to display the user\\'s input and st.chat_input to accept user input. We\\'ll also use session state to store the chat history so we can display it in the chat message container.\\n\\nFirst, let\\'s think about the different components we\\'ll need to build our bot:\\n\\nTwo chat message containers to display messages from the user and the bot, respectively.\\n\\nA chat input widget so the user can type in a message.\\n\\nA way to store the chat history so we can display it in the chat message containers. We can use a list to store the messages, and append to it every time the user or bot sends a message. Each entry in the list will be a dictionary with the following keys: role (the author of the message), and content (the message content).\\n\\n```python\\nimport streamlit as st\\n\\nst.title(\"Echo Bot\")\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n```\\n\\nIn the above snippet, we\\'ve added a title to our app and a for loop to iterate through the chat history and display each message in the chat message container (with the author role and message content). We\\'ve also added a check to see if the messages key is in st.session_state. If it\\'s not, we initialize it to an empty list. This is because we\\'ll be adding messages to the list later on, and we don\\'t want to overwrite the list every time the app reruns.\\n\\nNow let\\'s accept user input with st.chat_input, display the user\\'s message in the chat message container, and add it to the chat history.\\n\\n```python\\n\\nReact to user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Display user message in chat message container\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n```\\n\\nWe used the := operator to assign the user\\'s input to the prompt variable and checked if it\\'s not None in the same line. If the user has sent a message, we display the message in the chat message container and append it to the chat history.\\n\\nAll that\\'s left to do is add the chatbot\\'s responses within the if block. We\\'ll use the same logic as before to display the bot\\'s response (which is just the user\\'s prompt) in the chat message container and add it to the history.\\n\\n```python\\nresponse = f\"Echo: {prompt}\"\\n\\nDisplay assistant response in chat message container\\n\\nwith st.chat_message(\"assistant\"):\\n    st.markdown(response)\\n\\nAdd assistant response to chat history\\n\\nst.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\\n```\\n\\nPutting it all together, here\\'s the full code for our simple chatbot GUI and the result:\\n\\n```python\\nimport streamlit as st\\n\\nst.title(\"Echo Bot\")\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nReact to user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Display user message in chat message container\\n    st.chat_message(\"user\").markdown(prompt)\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n\\n```\\n\\nWhile the above example is very simple, it\\'s a good starting point for building more complex conversational apps. Notice how the bot responds instantly to your input. In the next section, we\\'ll add a delay to simulate the bot \"thinking\" before responding.\\n\\nBuild a simple chatbot GUI with streaming\\n\\nIn this section, we\\'ll build a simple chatbot GUI that responds to user input with a random message from a list of pre-determind responses. In the next section, we\\'ll convert this simple toy example into a ChatGPT-like experience using OpenAI.\\n\\nJust like previously, we still require the same components to build our chatbot. Two chat message containers to display messages from the user and the bot, respectively. A chat input widget so the user can type in a message. And a way to store the chat history so we can display it in the chat message containers.\\n\\nLet\\'s just copy the code from the previous section and add a few tweaks to it.\\n\\n```python\\nimport streamlit as st\\nimport random\\nimport time\\n\\nst.title(\"Simple chat\")\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nAccept user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Display user message in chat message container\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n```\\n\\nThe only difference so far is we\\'ve changed the title of our app and added imports for random and time. We\\'ll use random to randomly select a response from a list of responses and time to add a delay to simulate the chatbot \"thinking\" before responding.\\n\\nAll that\\'s left to do is add the chatbot\\'s responses within the if block. We\\'ll use a list of responses and randomly select one to display. We\\'ll also add a delay to simulate the chatbot \"thinking\" before responding (or stream its response).\\n\\n```python\\n\\nDisplay assistant response in chat message container\\n\\nwith st.chat_message(\"assistant\"):\\n    message_placeholder = st.empty()\\n    full_response = \"\"\\n    assistant_response = random.choice(\\n        [\\n            \"Hello there! How can I assist you today?\",\\n            \"Hi, human! Is there anything I can help you with?\",\\n            \"Do you need help?\",\\n        ]\\n    )\\n    # Simulate stream of response with milliseconds delay\\n    for chunk in assistant_response.split():\\n        full_response += chunk + \" \"\\n        time.sleep(0.05)\\n        # Add a blinking cursor to simulate typing\\n        message_placeholder.markdown(full_response + \"▌\")\\n    message_placeholder.markdown(full_response)\\n\\nAdd assistant response to chat history\\n\\nst.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\\n```\\n\\nAbove, we\\'ve added a placeholder to display the chatbot\\'s response. We\\'ve also added a for loop to iterate through the response and display it one word at a time. We\\'ve added a delay of 0.05 seconds between each word to simulate the chatbot \"thinking\" before responding. Finally, we append the chatbot\\'s response to the chat history. As you\\'ve probably guessed, this is a naive implementation of streaming. We\\'ll see how to implement streaming with OpenAI in the next section.\\n\\nPutting it all together, here\\'s the full code for our simple chatbot GUI and the result:\\n\\n```python\\nimport streamlit as st\\nimport random\\nimport time\\n\\nst.title(\"Simple chat\")\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nAccept user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n    # Display user message in chat message container\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n\\n```\\n\\nPlay around with the above demo to get a feel for what we\\'ve built. It\\'s a very simple chatbot GUI, but it has all the components of a more sophisticated chatbot. In the next section, we\\'ll see how to build a ChatGPT-like app using OpenAI.\\n\\nBuild a ChatGPT-like app\\n\\nNow that you\\'ve understood the basics of Streamlit\\'s chat elements, let\\'s make a few tweaks to it to build our own ChatGPT-like app. You\\'ll need to install the OpenAI Python library and get an API key to follow along.\\n\\nInstall dependencies\\n\\nFirst let\\'s install the dependencies we\\'ll need for this section:\\n\\nbash\\npip install openai streamlit\\n\\nAdd OpenAI API key to Streamlit secrets\\n\\nNext, let\\'s add our OpenAI API key to Streamlit secrets. We do this by creating .streamlit/secrets.toml file in our project directory and adding the following lines to it:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nOPENAI_API_KEY = \"YOUR_API_KEY\"\\n```\\n\\nWrite the app\\n\\nNow let\\'s write the app. We\\'ll use the same code as before, but we\\'ll replace the list of responses with a call to the OpenAI API. We\\'ll also add a few more tweaks to make the app more ChatGPT-like.\\n\\n```python\\nimport streamlit as st\\nimport openai\\n\\nst.title(\"ChatGPT-like clone\")\\n\\nSet OpenAI API key from Streamlit secrets\\n\\nopenai.api_key = st.secrets[\"OPENAI_API_KEY\"]\\n\\nSet a default model\\n\\nif \"openai_model\" not in st.session_state:\\n    st.session_state[\"openai_model\"] = \"gpt-3.5-turbo\"\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nAccept user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n    # Display user message in chat message container\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    # Display assistant response in chat message container\\n    with st.chat_message(\"assistant\"):\\n        message_placeholder = st.empty()\\n        full_response = \"\"\\n```\\n\\nAll that\\'s changed is that we\\'ve added a default model to st.session_state and set our OpenAI API key from Streamlit secrets. Here\\'s where it gets interesting. We can replace our logic from earlier to emulate streaming predetermind responses with the model\\'s responses from OpenAI:\\n\\npython\\n    for response in openai.ChatCompletion.create(\\n        model=st.session_state[\"openai_model\"],\\n        messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in st.session_state.messages],\\n        stream=True,\\n    ):\\n        full_response += response.choices[0].delta.get(\"content\", \"\")\\n        message_placeholder.markdown(full_response + \"▌\")\\n    message_placeholder.markdown(full_response)\\nst.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\\n\\nAbove, we\\'ve replaced the list of responses with a call to openai.ChatCompletion.create. We\\'ve set stream=True to stream the responses to the frontend. In the API call, we pass the model name we hardcoded in session state and pass the chat history as a list of messages. We also pass the role and content of each message in the chat history. Finally, OpenAI returns a stream of responses (split into chunks of tokens), which we iterate through and display each chunk.\\n\\nPutting it all together, here\\'s the full code for our ChatGPT-like app and the result:\\n\\n```python\\nimport openai\\nimport streamlit as st\\n\\nst.title(\"ChatGPT-like clone\")\\n\\nopenai.api_key = st.secrets[\"OPENAI_API_KEY\"]\\n\\nif \"openai_model\" not in st.session_state:\\n    st.session_state[\"openai_model\"] = \"gpt-3.5-turbo\"\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n\\n```\\n\\nCongratulations! You\\'ve built your own ChatGPT-like app in less than 50 lines of code.\\n\\nWe\\'re very excited to see what you\\'ll build with Streamlit\\'s chat elements. Experiment with different models and tweak the code to build your own conversational apps. If you build something cool, let us know on the Forum or check out some other Generative AI apps for inspiration. 🎈', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Google BigQuery\\nslug: /knowledge-base/tutorials/databases/bigquery\\n\\nConnect Streamlit to Google BigQuery\\n\\nIntroduction\\n\\nThis guide explains how to securely access a BigQuery database from Streamlit Community Cloud. It uses the\\ngoogle-cloud-bigquery library and\\nStreamlit\\'s secrets management.\\n\\nCreate a BigQuery database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFor this example, we will use one of the sample datasets from BigQuery (namely the shakespeare table). If you want to create a new dataset instead, follow Google\\'s quickstart guide.\\n\\nEnable the BigQuery API\\n\\nProgrammatic access to BigQuery is controlled through Google Cloud Platform. Create an account or sign in and head over to the APIs & Services dashboard (select or create a project if asked). As shown below, search for the BigQuery API and enable it:\\n\\nCreate a service account & key file\\n\\nTo use the BigQuery API from Streamlit Community Cloud, you need a Google Cloud Platform service account (a special account type for programmatic data access). Go to the Service Accounts page and create an account with the Viewer permission (this will let the account access data but not change it):\\n\\nIf the button CREATE SERVICE ACCOUNT is gray, you don\\'t have the correct permissions. Ask the\\nadmin of your Google Cloud project for help.\\n\\nAfter clicking DONE, you should be back on the service accounts overview. Create a JSON key file for the new account and download it:\\n\\nAdd the key file to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root\\ndirectory. Create this file if it doesn\\'t exist yet and add the content of the key file you just\\ndownloaded to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[gcp_service_account]\\ntype = \"service_account\"\\nproject_id = \"xxx\"\\nprivate_key_id = \"xxx\"\\nprivate_key = \"xxx\"\\nclient_email = \"xxx\"\\nclient_id = \"xxx\"\\nauth_uri = \"https://accounts.google.com/o/oauth2/auth\"\\ntoken_uri = \"https://oauth2.googleapis.com/token\"\\nauth_provider_x509_cert_url = \"https://www.googleapis.com/oauth2/v1/certs\"\\nclient_x509_cert_url = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd google-cloud-bigquery to your requirements file\\n\\nAdd the google-cloud-bigquery package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ngoogle-cloud-bigquery==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the query if you don\\'t use the sample table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom google.oauth2 import service_account\\nfrom google.cloud import bigquery\\n\\nCreate API client.\\n\\ncredentials = service_account.Credentials.from_service_account_info(\\n    st.secrets[\"gcp_service_account\"]\\n)\\nclient = bigquery.Client(credentials=credentials)\\n\\nPerform query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query(query):\\n    query_job = client.query(query)\\n    rows_raw = query_job.result()\\n    # Convert to list of dicts. Required for st.cache_data to hash the return value.\\n    rows = [dict(row) for row in rows_raw]\\n    return rows\\n\\nrows = run_query(\"SELECT word FROM bigquery-public-data.samples.shakespeare LIMIT 10\")\\n\\nPrint results.\\n\\nst.write(\"Some wise words from Shakespeare:\")\\nfor row in rows:\\n    st.write(\"✍️ \" + row[\\'word\\'])\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nAlternatively, you can use pandas to read from BigQuery right into a dataframe! Follow all the above steps, install the pandas-gbq library (don\\'t forget to add it to requirements.txt!), and call pandas.read_gbq(query, credentials=credentials). More info in the pandas docs.\\n\\nIf everything worked out (and you used the sample table), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/bigquery.md'}),\n",
       " Document(page_content='title: Connect Streamlit to MongoDB\\nslug: /knowledge-base/tutorials/databases/mongodb\\n\\nConnect Streamlit to MongoDB\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote MongoDB database from Streamlit Community Cloud. It uses the PyMongo library and Streamlit\\'s secrets management.\\n\\nCreate a MongoDB Database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, follow the official tutorials to install MongoDB, set up authentication (note down the username and password!), and connect to the MongoDB instance. Once you are connected, open the mongo shell and enter the following two commands to create a collection with some example values:\\n\\nsql\\nuse mydb\\ndb.mycollection.insertMany([{\"name\" : \"Mary\", \"pet\": \"dog\"}, {\"name\" : \"John\", \"pet\": \"cat\"}, {\"name\" : \"Robert\", \"pet\": \"bird\"}])\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the database information as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[mongo]\\nhost = \"localhost\"\\nport = 27017\\nusername = \"xxx\"\\npassword = \"xxx\"\\n```\\n\\nWhen copying your app secrets to Streamlit Community Cloud, be sure to replace the values of host, port, username, and password with those of your remote MongoDB database!\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd PyMongo to your requirements file\\n\\nAdd the PyMongo package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\npymongo==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the name of your database and collection.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport pymongo\\n\\nInitialize connection.\\n\\nUses st.cache_resource to only run once.\\n\\n@st.cache_resource\\ndef init_connection():\\n    return pymongo.MongoClient(**st.secrets[\"mongo\"])\\n\\nclient = init_connection()\\n\\nPull data from the collection.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef get_data():\\n    db = client.mydb\\n    items = db.mycollection.find()\\n    items = list(items)  # make hashable for st.cache_data\\n    return items\\n\\nitems = get_data()\\n\\nPrint results.\\n\\nfor item in items:\\n    st.write(f\"{item[\\'name\\']} has a :{item[\\'pet\\']}:\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example data we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/mongodb.md'}),\n",
       " Document(page_content='title: Connect Streamlit to AWS S3\\nslug: /knowledge-base/tutorials/databases/aws-s3\\n\\nConnect Streamlit to AWS S3\\n\\nIntroduction\\n\\nThis guide explains how to securely access files on AWS S3 from Streamlit Community Cloud. It uses Streamlit FilesConnection, the s3fs library and optionally Streamlit\\'s secrets management.\\n\\nCreate an S3 bucket and add a file\\n\\nIf you already have a bucket that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, sign up for AWS or log in. Go to the S3 console and create a new bucket:\\n\\nNavigate to the upload section of your new bucket:\\n\\nAnd note down the \"AWS Region\" for later. In this example, it\\'s us-east-1, but it may differ for you.\\n\\nNext, upload the following CSV file, which contains some example data:\\n\\nmyfile.csv\\n\\nCreate access keys\\n\\nGo to the AWS console, create access keys as shown below and copy the \"Access Key ID\" and \"Secret Access Key\":\\n\\nAccess keys created as a root user have wide-ranging permissions. In order to make your AWS account\\nmore secure, you should consider creating an IAM account with restricted permissions and using its\\naccess keys. More information here.\\n\\nSet up your AWS credentials locally\\n\\nStreamlit FilesConnection and s3fs will read and use your existing AWS credentials and configuration if available - such as from an ~/.aws/credentials file or environment variables.\\n\\nIf you don\\'t already have this set up, or plan to host the app on Streamlit Community Cloud, you should specify the credentials from a file .streamlit/secrets.toml in your app\\'s root directory or your home directory. Create this file if it doesn\\'t exist yet and add to it the access key ID, access key secret, and the AWS default region you noted down earlier, as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nAWS_ACCESS_KEY_ID = \"xxx\"\\nAWS_SECRET_ACCESS_KEY = \"xxx\"\\nAWS_DEFAULT_REGION = \"xxx\"\\n```\\n\\nBe sure to replace xxx above with the values you noted down earlier, and add this file to .gitignore so you don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nTo host your app on Streamlit Community Cloud, you will need to pass your credentials to your deployed app via secrets. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml above into the text area. More information is available at Secrets Management.\\n\\nAdd FilesConnection and s3fs to your requirements file\\n\\nAdd the FilesConnection and s3fs packages to your requirements.txt file, preferably pinning the versions (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ns3fs==x.x.x\\n\\nDirect pypi install coming soon\\n\\ngit+https://github.com/streamlit/files-connection\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the name of your bucket and file. Note that Streamlit automatically turns the access keys from your secrets file into environment variables, where s3fs searches for them by default.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom st_files_connection import FilesConnection\\n\\nCreate connection object and retrieve file contents.\\n\\nSpecify input format is a csv and to cache the result for 600 seconds.\\n\\nconn = st.experimental_connection(\\'s3\\', type=FilesConnection)\\ndf = conn.read(\"testbucket-jrieke/myfile.csv\", input_format=\"csv\", ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.Owner} has a :{row.Pet}:\")\\n```\\n\\nSee st.experimental_connection above? This handles secrets retrieval, setup, result caching and retries. By default, read() results are cached without expiring. In this case, we set ttl=600 to ensure the file contents is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example file given above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/aws-s3.md'}),\n",
       " Document(page_content='title: Connect Streamlit to a private Google Sheet\\nslug: /knowledge-base/tutorials/databases/private-gsheet\\n\\nConnect Streamlit to a private Google Sheet\\n\\nIntroduction\\n\\nThis guide explains how to securely access a private Google Sheet from Streamlit Community Cloud. It uses the gsheetsdb library and Streamlit\\'s secrets management.\\n\\nIf you are fine with enabling link sharing for your Google Sheet (i.e. everyone with the link can view it), the guide Connect Streamlit to a public Google Sheet shows a simpler method of doing this. If your Sheet contains sensitive information and you cannot enable link sharing, keep on reading.\\n\\nCreate a Google Sheet\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nEnable the Sheets API\\n\\nProgrammatic access to Google Sheets is controlled through Google Cloud Platform. Create an account or sign in and head over to the APIs & Services dashboard (select or create a project if asked). As shown below, search for the Sheets API and enable it:\\n\\nCreate a service account & key file\\n\\nTo use the Sheets API from Streamlit Community Cloud, you need a Google Cloud Platform service account (a special account type for programmatic data access). Go to the Service Accounts page and create an account with the Viewer permission (this will let the account access data but not change it):\\n\\nThe button CREATE SERVICE ACCOUNT is gray, you don\\'t have the correct permissions. Ask the admin of your Google Cloud project for help.\\n\\nAfter clicking DONE, you should be back on the service accounts overview. First, note down the email address of the account you just created (important for next step!). Then, create a JSON key file for the new account and download it:\\n\\nShare the Google Sheet with the service account\\n\\nBy default, the service account you just created cannot access your Google Sheet. To give it access, click on the Share button in the Google Sheet, add the email of the service account (noted down in step 2), and choose the correct permission (if you just want to read the data, Viewer is enough):\\n\\nAdd the key file to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the URL of your Google Sheet plus the content of the key file you downloaded to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nprivate_gsheets_url = \"https://docs.google.com/spreadsheets/d/12345/edit?usp=sharing\"\\n\\n[gcp_service_account]\\ntype = \"service_account\"\\nproject_id = \"xxx\"\\nprivate_key_id = \"xxx\"\\nprivate_key = \"xxx\"\\nclient_email = \"xxx\"\\nclient_id = \"xxx\"\\nauth_uri = \"https://accounts.google.com/o/oauth2/auth\"\\ntoken_uri = \"https://oauth2.googleapis.com/token\"\\nauth_provider_x509_cert_url = \"https://www.googleapis.com/oauth2/v1/certs\"\\nclient_x509_cert_url = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd gsheetsdb to your requirements file\\n\\nAdd the gsheetsdb package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ngsheetsdb==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom google.oauth2 import service_account\\nfrom gsheetsdb import connect\\n\\nCreate a connection object.\\n\\ncredentials = service_account.Credentials.from_service_account_info(\\n    st.secrets[\"gcp_service_account\"],\\n    scopes=[\\n        \"https://www.googleapis.com/auth/spreadsheets\",\\n    ],\\n)\\nconn = connect(credentials=credentials)\\n\\nPerform SQL query on the Google Sheet.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query(query):\\n    rows = conn.execute(query, headers=1)\\n    rows = rows.fetchall()\\n    return rows\\n\\nsheet_url = st.secrets[\"private_gsheets_url\"]\\nrows = run_query(f\\'SELECT * FROM \"{sheet_url}\"\\')\\n\\nPrint results.\\n\\nfor row in rows:\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/private-gsheet.md'}),\n",
       " Document(page_content='title: Connect Streamlit to PostgreSQL\\nslug: /knowledge-base/tutorials/databases/postgresql\\n\\nConnect Streamlit to PostgreSQL\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote PostgreSQL database from Streamlit Community Cloud. It uses the psycopg2 library and Streamlit\\'s secrets management.\\n\\nCreate a PostgreSQL database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, follow this tutorial to install PostgreSQL and create a database (note down the database name, username, and password!). Open the SQL Shell (psql) and enter the following two commands to create a table with some example values:\\n\\n```sql\\nCREATE TABLE mytable (\\n    name            varchar(80),\\n    pet             varchar(80)\\n);\\n\\nINSERT INTO mytable VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\');\\n```\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the name, user, and password of your database as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[postgres]\\nhost = \"localhost\"\\nport = 5432\\ndbname = \"xxx\"\\nuser = \"xxx\"\\npassword = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd psycopg2 to your requirements file\\n\\nAdd the psycopg2 package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\npsycopg2-binary==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt query to use the name of your table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport psycopg2\\n\\nInitialize connection.\\n\\nUses st.cache_resource to only run once.\\n\\n@st.cache_resource\\ndef init_connection():\\n    return psycopg2.connect(**st.secrets[\"postgres\"])\\n\\nconn = init_connection()\\n\\nPerform query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query(query):\\n    with conn.cursor() as cur:\\n        cur.execute(query)\\n        return cur.fetchall()\\n\\nrows = run_query(\"SELECT * from mytable;\")\\n\\nPrint results.\\n\\nfor row in rows:\\n    st.write(f\"{row[0]} has a :{row[1]}:\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/postgresql.md'}),\n",
       " Document(page_content='title: Connect Streamlit to MySQL\\nslug: /knowledge-base/tutorials/databases/mysql\\n\\nConnect Streamlit to MySQL\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote MySQL database from Streamlit Community Cloud. It uses st.experimental_connection and Streamlit\\'s secrets management. The below example code will only work on Streamlit version >= 1.22, when st.experimental_connection was added.\\n\\nCreate a MySQL database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, follow this tutorial to install MySQL and start the MySQL server (note down the username and password!). Once your MySQL server is up and running, connect to it with the mysql client and enter the following commands to create a database and a table with some example values:\\n\\n```sql\\nCREATE DATABASE pets;\\n\\nUSE pets;\\n\\nCREATE TABLE mytable (\\n    name            varchar(80),\\n    pet             varchar(80)\\n);\\n\\nINSERT INTO mytable VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\');\\n```\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Learn more about Streamlit secrets management here. Create this file if it doesn\\'t exist yet and add the database name, user, and password of your MySQL server as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.mysql]\\ndialect = \"mysql\"\\nhost = \"localhost\"\\nport = 3306\\ndatabase = \"xxx\"\\nusername = \"xxx\"\\npassword = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd dependencies to your requirements file\\n\\nAdd the mysqlclient and SQLAlchemy packages to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\nmysqlclient==x.x.x\\nSQLAlchemy==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt query to use the name of your table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'mysql\\', type=\\'sql\\')\\n\\nPerform query.\\n\\ndf = conn.query(\\'SELECT * from mytable;\\', ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```\\n\\nSee st.experimental_connection above? This handles secrets retrieval, setup, query caching and retries. By default, query() results are cached without expiring. In this case, we set ttl=600 to ensure the query result is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/mysql.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Microsoft SQL Server\\nslug: /knowledge-base/tutorials/databases/mssql\\n\\nConnect Streamlit to Microsoft SQL Server\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote Microsoft SQL Server database from Streamlit Community Cloud. It uses the pyodbc library and Streamlit\\'s secrets management.\\n\\nCreate an SQL Server database\\n\\nIf you already have a remote database that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, follow the Microsoft documentation to install SQL Server and the sqlcmd Utility. They have detailed installation guides on how to:\\n\\nInstall SQL Server on Windows\\n\\nInstall on Red Hat Enterprise Linux\\n\\nInstall on SUSE Linux Enterprise Server\\n\\nInstall on Ubuntu\\n\\nRun on Docker\\n\\nProvision a SQL VM in Azure\\n\\nOnce you have SQL Server installed, note down your SQL Server name, username, and password during setup.\\n\\nConnect locally\\n\\nIf you are connecting locally, use sqlcmd to connect to your new local SQL Server instance.\\n\\nIn your terminal, run the following command:\\n\\nbash\\n   sqlcmd -S localhost -U SA -P \\'<YourPassword>\\'\\n\\nAs you are connecting locally, the SQL Server name is localhost, the username is SA, and the password is the one you provided during the SA account setup.\\n\\nYou should see a sqlcmd command prompt 1>, if successful.\\n\\nIf you run into a connection failure, review Microsoft\\'s connection troubleshooting recommendations for your OS (Linux & Windows).\\n\\nWhen connecting remotely, the SQL Server name is the machine name or IP address. You might also need to open the SQL Server TCP port (default 1433) on your firewall.\\n\\nCreate a SQL Server database\\n\\nBy now, you have SQL Server running and have connected to it with sqlcmd! 🥳 Let\\'s put it to use by creating a database containing a table with some example values.\\n\\nFrom the sqlcmd command prompt, run the following Transact-SQL command to create a test database mydb:\\n\\nsql\\n   CREATE DATABASE mydb\\n\\nTo execute the above command, type GO on a new line:\\n\\nsql\\n   GO\\n\\nInsert some data\\n\\nNext create a new table, mytable, in the mydb database with three columns and two rows.\\n\\nSwitch to the new mydb database:\\n\\nsql\\n   USE mydb\\n\\nCreate a new table with the following schema:\\n\\nsql\\n   CREATE TABLE mytable (name varchar(80), pet varchar(80))\\n\\nInsert some data into the table:\\n\\nsql\\n   INSERT INTO mytable VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\')\\n\\nType GO to execute the above commands:\\n\\nsql\\n   GO\\n\\nTo end your sqlcmd session, type QUIT on a new line.\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the SQL Server name, database name, username, and password as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nserver = \"localhost\"\\ndatabase = \"mydb\"\\nusername = \"SA\"\\npassword = \"xxx\"\\n```\\n\\nWhen copying your app secrets to Streamlit Community Cloud, be sure to replace the values of server, database, username, and password with those of your remote SQL Server!\\n\\nAnd add this file to .gitignore and don\\'t commit it to your GitHub repo.\\n\\nCopy your app secrets to Streamlit Community Cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd pyodbc to your requirements file\\n\\nTo connect to SQL Server locally with Streamlit, you need to pip install pyodbc, in addition to the Microsoft ODBC driver you installed during the SQL Server installation.\\n\\nOn Streamlit Cloud, we have built-in support for SQL Server. On popular demand, we directly added SQL Server tools including the ODBC drivers and the executables sqlcmd and bcp to the container image for Cloud apps, so you don\\'t need to install them.\\n\\nAll you need to do is add the pyodbc Python package to your requirements.txt file, and you\\'re ready to go! 🎈\\n\\n```bash\\n\\nrequirements.txt\\n\\npyodbc==x.x.x\\n```\\n\\nReplace x.x.x ☝️ with the version of pyodbc you want installed on Cloud.\\n\\nAt this time, Streamlit Community Cloud does not support Azure Active Directory authentication. We will update this tutorial when we add support for Azure Active Directory.\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt query to use the name of your table.\\n\\n```python\\nimport streamlit as st\\nimport pyodbc\\n\\nInitialize connection.\\n\\nUses st.cache_resource to only run once.\\n\\n@st.cache_resource\\ndef init_connection():\\n    return pyodbc.connect(\\n        \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\"\\n        + st.secrets[\"server\"]\\n        + \";DATABASE=\"\\n        + st.secrets[\"database\"]\\n        + \";UID=\"\\n        + st.secrets[\"username\"]\\n        + \";PWD=\"\\n        + st.secrets[\"password\"]\\n    )\\n\\nconn = init_connection()\\n\\nPerform query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query(query):\\n    with conn.cursor() as cur:\\n        cur.execute(query)\\n        return cur.fetchall()\\n\\nrows = run_query(\"SELECT * from mytable;\")\\n\\nPrint results.\\n\\nfor row in rows:\\n    st.write(f\"{row[0]} has a :{row[1]}:\")\\n\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/mssql.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Deta Base\\nslug: /knowledge-base/tutorials/databases/deta-base\\n\\nConnect Streamlit to Deta Base\\n\\nIntroduction\\n\\nThis guide explains how to securely access and write to a Deta Base database from Streamlit Community Cloud. Deta Base is a fully-managed and fast NoSQL database with a focus on end-user simplicity. The data is stored in your own \"personal cloud\" on Deta Space. This guide uses the Deta Python SDK for Deta Base and Streamlit\\'s Secrets Management.\\n\\nCreate an account and sign in to Deta Space\\n\\nFirst, you need to create a Deta Space account for using Deta Base. Make sure the \"Developer Mode\" option is enabled when signing up. Once you have an account, sign in to Deta Space. After signing in, open the Collections app by clicking on it.\\n\\nDeta Collections is a pre-installed app on Space that stores different types of data that can be connected to other apps or services.\\n\\nNow click on the Get Started button and then click on the Create Collection button after giving your Collection a name.\\n\\nAfter that, click on the Collection Settings option in the top corner, which will show the modal for creating a Data Key. Click on the Create New Data Key button, then give your key a name, and click the Generate button. Copy the key shown to your clipboard by clicking on the copy button.\\n\\nData Keys allow you to read and manipulate data within your Collections.\\n\\nBe sure to store your Data Key securely. It is shown only once, and you will need it to connect to your Deta Base.\\n\\nAdd Data Key to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the Data Key (from the previous step) of your Deta Base as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\ndata_key = \"xxx\"\\n```\\n\\nReplace xxx above ☝️ with your Data Key from the previous step.\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd deta to your requirements file\\n\\nAdd the deta Python SDK for Deta Base to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ndeta==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. The example app below writes data from a Streamlit form to a Deta Base database example-db.\\n\\n```python\\nimport streamlit as st\\nfrom deta import Deta\\n\\nData to be written to Deta Base\\n\\nwith st.form(\"form\"):\\n    name = st.text_input(\"Your name\")\\n    age = st.number_input(\"Your age\")\\n    submitted = st.form_submit_button(\"Store in database\")\\n\\nConnect to Deta Base with your Data Key\\n\\ndeta = Deta(st.secrets[\"data_key\"])\\n\\nCreate a new database \"example-db\"\\n\\nIf you need a new database, just use another name.\\n\\ndb = deta.Base(\"example-db\")\\n\\nIf the user clicked the submit button,\\n\\nwrite the data from the form to the database.\\n\\nYou can store any data you want here. Just modify that dictionary below (the entries between the {}).\\n\\nif submitted:\\n    db.put({\"name\": name, \"age\": age})\\n\\n\"---\"\\n\"Here\\'s everything stored in the database:\"\\n\\nThis reads all items from the database and displays them to your app.\\n\\ndb_content is a list of dictionaries. You can do everything you want with it.\\n\\ndb_content = db.fetch().items\\nst.write(db_content)\\n```\\n\\nIf everything worked out (and you used the example we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/detabase.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Supabase\\nslug: /knowledge-base/tutorials/databases/supabase\\n\\nConnect Streamlit to Supabase\\n\\nIntroduction\\n\\nThis guide explains how to securely access a Supabase instance from Streamlit Community Cloud. It uses the Supabase Python Client Library and Streamlit\\'s secrets management. Supabase is the open source Firebase alternative and is based on PostgreSQL.\\n\\nSign in to Supabase and create a project\\n\\nFirst, head over to Supabase and sign up for a free account using your GitHub.\\n\\nOnce you\\'re signed in, you can create a project.\\n\\nYour screen should look like this once your project has been created:\\n\\nMake sure to note down your Project API Key and Project URL highlighted in the above screenshot. ☝️\\n\\nYou will need these to connect to your Supabase instance from Streamlit.\\n\\nCreate a Supabase database\\n\\nNow that you have a project, you can create a database and populate it with some sample data. To do so, click on the SQL editor button on the same project page, followed by the New query button in the SQL editor.\\n\\nIn the SQL editor, enter the following queries to create a database and a table with some example values:\\n\\n```sql\\nCREATE TABLE mytable (\\n    name            varchar(80),\\n    pet             varchar(80)\\n);\\n\\nINSERT INTO mytable VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\');\\n```\\n\\nClick Run to execute the queries. To verify that the queries were executed successfully, click on the Table Editor button on the left menu, followed by your newly created table mytable.\\n\\nWith your Supabase database created, you can now connect to it from Streamlit!\\n\\nAdd Supabase Project URL and API key to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the supabase_url and supabase_key here:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nsupabase_url = \"xxxx\"\\nsupabase_key = \"xxxx\"\\n```\\n\\nReplace xxxx above with your Project URL and API key from Step 1.\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd supabase to your requirements file\\n\\nAdd the supabase Python Client Library to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\nsupabase==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom supabase import create_client, Client\\n\\nInitialize connection.\\n\\nUses st.cache_resource to only run once.\\n\\n@st.cache_resource\\ndef init_connection():\\n    url = st.secrets[\"supabase_url\"]\\n    key = st.secrets[\"supabase_key\"]\\n    return create_client(url, key)\\n\\nsupabase = init_connection()\\n\\nPerform query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query():\\n    return supabase.table(\"mytable\").select(\"*\").execute()\\n\\nrows = run_query()\\n\\nPrint results.\\n\\nfor row in rows.data:\\n    st.write(f\"{row[\\'name\\']} has a :{row[\\'pet\\']}:\")\\n\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:\\n\\nAs Supabase uses PostgresSQL under the hood, you can also connect to Supabase by using the connection string Supabase provides under Settings > Databases. From there, you can refer to the PostgresSQL tutorial to connect to your database.', metadata={'source': 'docs/content/kb/tutorials/databases/supabase.md'}),\n",
       " Document(page_content='title: Connect Streamlit to TigerGraph\\nslug: /knowledge-base/tutorials/databases/tigergraph\\n\\nConnect Streamlit to TigerGraph\\n\\nIntroduction\\n\\nThis guide explains how to securely access a TigerGraph database from Streamlit Community Cloud. It uses the pyTigerGraph library and Streamlit\\'s secrets management.\\n\\nCreate a TigerGraph Cloud Database\\n\\nFirst, follow the official tutorials to create a TigerGraph instance in TigerGraph Cloud, either as a blog or a video. Note your username, password, and subdomain.\\n\\nFor this tutorial, we will be using the COVID-19 starter kit. When setting up your solution, select the “COVID-19 Analysis\" option.\\n\\nOnce it is started, ensure your data is downloaded and queries are installed.\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app’s root directory. Create this file if it doesn’t exist yet and add your TigerGraph Cloud instance username, password, graph name, and subdomain as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[tigergraph]\\nhost = \"https://xxx.i.tgcloud.io/\"\\nusername = \"xxx\"\\npassword = \"xxx\"\\ngraphname = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd PyTigerGraph to your requirements file\\n\\nAdd the pyTigerGraph package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\npyTigerGraph==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the name of your graph and query.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport pyTigerGraph as tg\\n\\nInitialize connection.\\n\\nconn = tg.TigerGraphConnection(**st.secrets[\"tigergraph\"])\\nconn.apiToken = conn.getToken(conn.createSecret())\\n\\nPull data from the graph by running the \"mostDirectInfections\" query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef get_data():\\n    most_infections = conn.runInstalledQuery(\"mostDirectInfections\")[0][\"Answer\"][0]\\n    return most_infections[\"v_id\"], most_infections[\"attributes\"]\\n\\nitems = get_data()\\n\\nPrint results.\\n\\nst.title(f\"Patient {items[0]} has the most direct infections\")\\nfor key, val in items[1].items():\\n    st.write(f\"Patient {items[0]}\\'s {key} is {val}.\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example data we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/tigergraph.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Tableau\\nslug: /knowledge-base/tutorials/databases/tableau\\n\\nConnect Streamlit to Tableau\\n\\nIntroduction\\n\\nThis guide explains how to securely access data on Tableau from Streamlit Community Cloud. It uses the tableauserverclient library and Streamlit\\'s secrets management.\\n\\nCreate a Tableau site\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFor simplicity, we are using the cloud version of Tableau here but this guide works equally well for self-hosted deployments. First, sign up for Tableau Online or log in. Create a workbook or run one of the example workbooks under \"Dashboard Starters\".\\n\\nCreate personal access tokens\\n\\nWhile the Tableau API allows authentication via username and password, you should use personal access tokens for a production app.\\n\\nGo to your Tableau Online homepage, create an access token and note down the token name and secret.\\n\\nPersonal access tokens will expire if not used after 15 consecutive days.\\n\\nAdd token to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add your token, the site name you created during setup, and the URL of your Tableau server like below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[tableau]\\ntoken_name = \"xxx\"\\ntoken_secret = \"xxx\"\\nserver_url = \"https://abc01.online.tableau.com/\"\\nsite_id = \"streamlitexample\"  # in your site\\'s URL behind the server_url\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd tableauserverclient to your requirements file\\n\\nAdd the tableauserverclient package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ntableauserverclient==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Note that this code just shows a few options of data you can get – explore the tableauserverclient library to find more!\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport tableauserverclient as TSC\\n\\nSet up connection.\\n\\ntableau_auth = TSC.PersonalAccessTokenAuth(\\n    st.secrets[\"tableau\"][\"token_name\"],\\n    st.secrets[\"tableau\"][\"personal_access_token\"],\\n    st.secrets[\"tableau\"][\"site_id\"],\\n)\\nserver = TSC.Server(st.secrets[\"tableau\"][\"server_url\"], use_server_version=True)\\n\\nGet various data.\\n\\nExplore the tableauserverclient library for more options.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query():\\n    with server.auth.sign_in(tableau_auth):\\n\\nworkbooks_names, views_names, view_name, view_image, view_csv = run_query()\\n\\nPrint results.\\n\\nst.subheader(\"📓 Workbooks\")\\nst.write(\"Found the following workbooks:\", \", \".join(workbooks_names))\\n\\nst.subheader(\"👁️ Views\")\\nst.write(\\n    f\"Workbook {workbooks_names[0]} has the following views:\",\\n    \", \".join(views_names),\\n)\\n\\nst.subheader(\"🖼️ Image\")\\nst.write(f\"Here\\'s what view {view_name} looks like:\")\\nst.image(view_image, width=300)\\n\\nst.subheader(\"📊 Data\")\\nst.write(f\"And here\\'s the data for view {view_name}:\")\\nst.write(pd.read_csv(StringIO(view_csv)))\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out, your app should look like this (can differ based on your workbooks):', metadata={'source': 'docs/content/kb/tutorials/databases/tableau.md'}),\n",
       " Document(page_content=\"title: Streamlit Community Cloud\\nslug: /streamlit-community-cloud\\n\\nWelcome to Streamlit Community Cloud\\n\\nStreamlit's Community Cloud is an open and free platform for the community to deploy, discover, and share Streamlit apps and code with each other. If you're just getting started and have not yet built your first Streamlit app, check out the main Get started page first. When you're ready to share it, create a Community Cloud account and you can launch your app in just a few minutes! Deploy, manage, and share your apps with the world, directly from Streamlit — all for free.\\n\\nInterested in our security model? Check out our Trust and Security page.\\n\\nQuestions? Reach out to us on the Community forum!\", metadata={'source': 'docs/content/streamlit-cloud/index.md'}),\n",
       " Document(page_content='title: Connect Streamlit to TiDB\\nslug: /knowledge-base/tutorials/databases/tidb\\n\\nConnect Streamlit to TiDB\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote TiDB database from Streamlit Community Cloud. It uses st.experimental_connection and Streamlit\\'s secrets management. The below example code will only work on Streamlit version >= 1.22, when st.experimental_connection was added.\\n\\nTiDB is an open-source, MySQL-compatible database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. TiDB Cloud is a fully managed cloud database service that simplifies the deployment and management of TiDB databases for developers.\\n\\nSign in to TiDB Cloud and create a cluster\\n\\nFirst, head over to TiDB Cloud and sign up for a free account, using either Google, GitHub, Microsoft or E-mail:\\n\\nOnce you\\'ve signed in, you will already have a TiDB cluster:\\n\\nYou can create more clusters if you want to. Click the cluster name to enter cluster overview page:\\n\\nThen click Connect to easily get the connection arguments to access the cluster. On the popup, click Create password to set the password.\\n\\nMake sure to note down the password. It won\\'t be available on TiDB Cloud after this step.\\n\\nCreate a TiDB database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nOnce your TiDB cluster is up and running, connect to it with the mysql client(or with Chat2Query tab on the console) and enter the following commands to create a database and a table with some example values:\\n\\n```sql\\nCREATE DATABASE pets;\\n\\nUSE pets;\\n\\nCREATE TABLE mytable (\\n    name            varchar(80),\\n    pet             varchar(80)\\n);\\n\\nINSERT INTO mytable VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\');\\n```\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Learn more about Streamlit secrets management here. Create this file if it doesn\\'t exist yet and add host, username and password of your TiDB cluster as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.tidb]\\ndialect = \"mysql\"\\nhost = \"\"\\nport = 4000\\ndatabase = \"pets\"\\nusername = \"\"\\npassword = \"\"\\n```\\n\\nWhen copying your app secrets to Streamlit Community Cloud, be sure to replace the values of host, username and password with those of your remote TiDB cluster!\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd dependencies to your requirements file\\n\\nAdd the mysqlclient and SQLAlchemy packages to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\nmysqlclient==x.x.x\\nSQLAlchemy==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt query to use the name of your table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'tidb\\', type=\\'sql\\')\\n\\nPerform query.\\n\\ndf = conn.query(\\'SELECT * from mytable;\\', ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```\\n\\nSee st.experimental_connection above? This handles secrets retrieval, setup, query caching and retries. By default, query() results are cached without expiring. In this case, we set ttl=600 to ensure the query result is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:\\n\\nConnect with PyMySQL\\n\\nOther than mysqlclient, PyMySQL is another popular MySQL Python client. To use PyMySQL, first you need to adapt your requirements file:\\n\\n```bash\\n\\nrequirements.txt\\n\\nPyMySQL==x.x.x\\nSQLAlchemy==x.x.x\\n```\\n\\nThen adapt your secrets file:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.tidb]\\ndialect = \"mysql\"\\ndriver = \"pymysql\"\\nhost = \"\"\\nport = 4000\\ndatabase = \"pets\"\\nusername = \"\"\\npassword = \"\"\\ncreate_engine_kwargs = { connect_args = { ssl = { ca = \"\" }}}\\n```', metadata={'source': 'docs/content/kb/tutorials/databases/tidb.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Google Cloud Storage\\nslug: /knowledge-base/tutorials/databases/gcs\\n\\nConnect Streamlit to Google Cloud Storage\\n\\nIntroduction\\n\\nThis guide explains how to securely access files on Google Cloud Storage from Streamlit Community Cloud. It uses Streamlit FilesConnection, the gcsfs library and Streamlit\\'s secrets management.\\n\\nCreate a Google Cloud Storage bucket and add a file\\n\\nIf you already have a bucket that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, sign up for Google Cloud Platform or log in. Go to the Google Cloud Storage console and create a new bucket.\\n\\nNavigate to the upload section of your new bucket:\\n\\nAnd upload the following CSV file, which contains some example data:\\n\\nmyfile.csv\\n\\nEnable the Google Cloud Storage API\\n\\nThe Google Cloud Storage API is enabled by default when you create a project through the Google Cloud Console or CLI. Feel free to skip to the next step.\\n\\nIf you do need to enable the API for programmatic access in your project, head over to the APIs & Services dashboard (select or create a project if asked). Search for the Cloud Storage API and enable it. The screenshot below has a blue \"Manage\" button and indicates the \"API is enabled\" which means no further action needs to be taken. This is very likely what you have since the API is enabled by default. However, if that is not what you see and you have an \"Enable\" button, you\\'ll need to enable the API:\\n\\nCreate a service account and key file\\n\\nTo use the Google Cloud Storage API from Streamlit, you need a Google Cloud Platform service account (a special type for programmatic data access). Go to the Service Accounts page and create an account with Viewer permission.\\n\\nIf the button CREATE SERVICE ACCOUNT is gray, you don\\'t have the correct permissions. Ask the\\nadmin of your Google Cloud project for help.\\n\\nAfter clicking DONE, you should be back on the service accounts overview. Create a JSON key file for the new account and download it:\\n\\nAdd the key to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the access key to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.gcs]\\ntype = \"service_account\"\\nproject_id = \"xxx\"\\nprivate_key_id = \"xxx\"\\nprivate_key = \"xxx\"\\nclient_email = \"xxx\"\\nclient_id = \"xxx\"\\nauth_uri = \"https://accounts.google.com/o/oauth2/auth\"\\ntoken_uri = \"https://oauth2.googleapis.com/token\"\\nauth_provider_x509_cert_url = \"https://www.googleapis.com/oauth2/v1/certs\"\\nclient_x509_cert_url = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd FilesConnection and gcsfs to your requirements file\\n\\nAdd the FilesConnection and gcsfs packages to your requirements.txt file, preferably pinning the versions (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ngcsfs==x.x.x\\n\\nDirect pypi install coming soon\\n\\ngit+https://github.com/streamlit/files-connection\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the name of your bucket and file. Note that Streamlit automatically turns the access keys from your secrets file into environment variables.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom st_files_connection import FilesConnection\\n\\nCreate connection object and retrieve file contents.\\n\\nSpecify input format is a csv and to cache the result for 600 seconds.\\n\\nconn = st.experimental_connection(\\'gcs\\', type=FilesConnection)\\ndf = conn.read(\"streamlit-bucket/myfile.csv\", input_format=\"csv\", ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.Owner} has a :{row.Pet}:\")\\n```\\n\\nSee st.experimental_connection above? This handles secrets retrieval, setup, result caching and retries. By default, read() results are cached without expiring. In this case, we set ttl=600 to ensure the file contents is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example file given above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/gcs.md'}),\n",
       " Document(page_content='title: Manage your account\\nslug: /streamlit-community-cloud/manage-your-account\\n\\nManage your account\\n\\nYou can update the email associated with your account or delete your account entirely through \"Settings.\" When using Streamlit Community Cloud, you have two kinds of logins: a primary identity (email) and source control (GitHub). Your primary identity allows other users to share private apps with you. Your source control identity allows you to deploy apps from GitHub repositories and manage them through the Streamlit Community Cloud dashboard.\\n\\nAccess your account settings\\n\\nTo manage your account, sign in to https://share.streamlit.io and click \"Settings\" in the top right corner.\\n\\nDoing so opens your \"Workspace settings,\" where you can update your email or delete your account.\\n\\nYou can find the detailed steps for each option in Update your email and Delete your account.', metadata={'source': 'docs/content/streamlit-cloud/manage-your-account/index.md'}),\n",
       " Document(page_content='title: Update your email\\nslug: /streamlit-community-cloud/manage-your-account/update-your-email\\n\\nUpdate your email\\n\\nIf you wish to update your email on Streamlit Community Cloud, you can do so via \"Settings.\" Updating your email changes the primary identity of your account. Once updated, if your account\\'s email is associated with a Google account, you can Sign in with Google OAuth. Otherwise, you have the alternative to Sign in with Email. The latter involves typing in your email, after which we\\'ll send you a unique link (valid for 15 minutes) to that email. Click the link in the email to sign in to Streamlit.\\n\\nHow to update your email\\n\\nSign in to Streamlit Community Cloud: https://share.streamlit.io/\\n\\nClick \"Settings\" in the page\\'s top-right corner.\\n\\nClick \"Update email\" within the \"Linked accounts\" section.\\n\\nEnter your new email and click \"Update email.\"\\n\\nYou\\'ll see a confirmation dialog asking you to check your email for a confirmation link. Click \"Done.\"\\n\\nYour account settings will show \"Update pending\" until you complete the next step.\\n\\nCheck your inbox for an email from Streamlit containing a \"Change email\" button and a confirmation link. This one-time link expires in 15 minutes. Click either one to confirm your new email address for Streamlit Community Cloud. Before doing so, ensure you access the link from the same browser session where you are logged in to Streamlit Community Cloud.\\n\\nIf you access the confirmation link from a browser session where you are not logged in to Streamlit Community Cloud, the confirmation link will not complete the process. You will be prompted to sign in. If you try to sign in with your new email, you will create a second account instead. See Troubleshooting.\\n\\nA confirmation will display to confirm your email update is complete! 🎈\\n\\nResend your confirmation link\\n\\nIf your confirmation link expires, don\\'t worry! You can resend it by following these steps:\\n\\nSign in to Streamlit Community Cloud: https://share.streamlit.io/ and click \"Settings\" in the page\\'s top-right corner.\\n\\nClick \"Update pending\"\\n\\nClick \"Resend email\"\\n\\nContinue from step 4 of How to update your email.\\n\\nTroubleshooting\\n\\nIf you click the confirmation link in a browser session where you are not signed in, you will be informed that \"Sign in is required.\" If you try to sign in with your new email, you will create a second account instead. You cannot resend your confirmation link while you have this second account. If you accidentally created a second account, you can Delete your account, then Resend your confirmation link from your first account.', metadata={'source': 'docs/content/streamlit-cloud/manage-your-account/update-your-email.md'}),\n",
       " Document(page_content='title: Streamlit Trust and Security\\nslug: /streamlit-community-cloud/trust-and-security\\n\\nStreamlit Trust and Security\\n\\nStreamlit is a framework that turns Python scripts into interactive apps, giving data scientists the ability to quickly create data and model-based apps for the entire company.\\n\\nA simple Streamlit app is:\\n\\npython\\nimport streamlit as st\\nnumber = st.slider(\"Pick a number: \", min_value=1, max_value=10)\\nst.text(\"Your number is \" + str(number))\\n\\nWhen you streamlit run my_app.py, you start a web server that runs the interactive application on your local computer at http://localhost:8501. This is great for local development. When you want to share with your colleagues, Streamlit Community Cloud enables you to deploy and run these applications in the cloud. Streamlit Community Cloud handles all the details of scaling, reliability, and security as well as providing you an interface for easily managing your deployed apps.\\n\\nThis document is an overview of how we provide best-in-industry security for you. We\\'ll cover all the important areas in the lifecycle of your data:\\n\\nProduct Security: how we ensure only you can create and view apps that access your data\\n\\nNetwork and Application Security: how we ensure your data is protected when it is in our cloud\\n\\nOngoing Operations: how we stay good stewards of security best practices\\n\\nProduct Security\\n\\nSSO\\n\\nAll access and sign-ins to Streamlit are conducted via an SSO provider: GitHub and GSuite. We do not store customer passwords.\\n\\nCredential Storage\\n\\nWe encrypt sensitive customer data (e.g. secrets, authentication tokens) at-rest with AES256 as described in Google\\'s documentation.\\n\\nPermissions and Role-Based Access Control\\n\\nOur permission levels inherit from the permissions you have assigned in GitHub. Users with write access to a GitHub repository for a given app will be able to make changes in the Streamlit administrative console.\\n\\nOnly users with admin access to a repository are able to deploy and delete apps.\\n\\nNetwork and Application Security\\n\\nData Hosting\\n\\nOur physical infrastructure is hosted and managed within Google Cloud Platform (GCP) using their secure data centers. Streamlit leverages many of the platform\\'s built-in security, privacy, and redundancy features. GCP continually monitors its data centers for risk and undergoes assessments to ensure compliance with industry standards. GCP\\'s data centers have numerous accreditations, including ISO-27001, SOC 1 and SOC 2.\\n\\nVirtual Private Cloud\\n\\nAll of our servers are within a virtual private cloud (VPC) with firewalls and network access control lists (ACLs) to allow external access to a select few API endpoints; all other internal services are only accessible within the VPC.\\n\\nEncryption\\n\\nAll Streamlit apps are served entirely over HTTPS. All data sent to or from Streamlit over the public internet is encrypted in transit using 256-bit encryption. Our API and application endpoints are TLS only (v1.2). We use only strong cipher suites and HTTP Strict Transport Security (HSTS) to ensure browsers interact with Streamlit apps over HTTPS. We also encrypt data at rest using AES-256.\\n\\nPermissions and Authentication\\n\\nAccess to customer data is limited to authorized employees who require it for their job. We run a zero-trust corporate network so there are no corporate resources or additional privileges gained from being on Streamlit\\'s internal network. We utilize single sign-on, 2-factor authentication (2FA), and enforce strong password policies to ensure access to all cloud-related services are protected.\\n\\nIncident Response\\n\\nWe have an internal protocol for handling security events which includes escalation procedures, rapid mitigation, and documented post-mortems. We notify customers promptly and publicize security advisories at https://streamlit.io/advisories.\\n\\nPenetration Testing\\n\\nStreamlit uses third-party security tools to scan for vulnerabilities on a regular basis. Our security partners conduct periodic, intensive penetration tests on the Streamlit platform. Our product development team immediately responds to any identified issues or potential vulnerabilities to ensure the quality and security of Streamlit applications.\\n\\nSecurity and Compliance Programs\\n\\nPeople\\n\\nBackground Checks\\n\\nAll Streamlit employees go through a thorough background check before hiring.\\n\\nTraining\\n\\nWe take a least-privilege approach to the access and handling of data. While we retain a minimal amount of customer data and limit internal access on a need-to-know basis, all employees are required to review related security policies and are trained on proper data handling to ensure they uphold our strict commitment to the privacy and security of your data.\\n\\nConfidentiality\\n\\nAll employees sign a confidentiality agreement before they start at Streamlit.\\n\\nVulnerability Control\\n\\nVulnerability Management\\n\\nWe keep our systems up-to-date with the latest security patches and continuously monitor for new vulnerabilities through compliance and security mailing lists. This includes automatic scanning of our code repositories for vulnerable dependencies.\\n\\nIf you have further questions about Community Cloud and our security approach, please reach out to us on the Community forum.', metadata={'source': 'docs/content/streamlit-cloud/security-model.md'}),\n",
       " Document(page_content='title: Delete your account\\nslug: /streamlit-community-cloud/manage-your-account/delete-your-account\\n\\nDelete your account\\n\\nDeleting your Streamlit Community Cloud account is just as easy as creating it via \"Settings\". When you delete your account, your information, account, and all your hosted apps are deleted as well.\\n\\nDeleting your account is permanent and cannot be undone. Make sure you really want to delete your account and all hosted apps before proceeding.\\n\\nHow to delete your account\\n\\nFollow these steps to delete your account:\\n\\nSign in to Streamlit Community Cloud: https://share.streamlit.io/\\n\\nClick \"Settings\" in the top right corner of the page to go to the Settings dashboard. In the \\'Account\\' section, click \\'Delete account\\':\\n\\nIn the \"Delete account?\" dialog that appears, you will be asked to confirm that you want to delete your account by typing:\\n\\ndelete <your email address>\\n\\nType in delete followed by your email address and click \"Delete account forever\":\\n\\nYou will then be logged out and your account, information, and apps will be permanently deleted.\\n\\nUpon deletion, you\\'re shown a confirmation message that your account has been deleted, after which you will be redirected to the Streamlit Community Cloud homepage.\\n\\nIt\\'s that simple! If you have any questions or run into issues deleting your account, please reach out to us on the Forum. We\\'re happy to help! 🎈', metadata={'source': 'docs/content/streamlit-cloud/manage-your-account/delete-your-account.md'}),\n",
       " Document(page_content='title: Troubleshooting\\nslug: /streamlit-community-cloud/troubleshooting\\n\\nTroubleshooting\\n\\nSorry to hear you\\'re having issues! Please take a look at some frequently asked questions and issues below. If you cannot find an answer to your issue, please post on our Community forum so that our engineers or community members can help you.\\n\\nTable of contents\\n\\nGeneral help\\n\\nDeploying apps\\n\\nSharing and accessing apps\\n\\nData and app security\\n\\nGitHub integration\\n\\nLimitations and known issues\\n\\nGeneral help\\n\\nHow can I get help with my app?\\n\\nIf you have any questions, feedback, run into any issues, or need to reach us, you can ask on our Community forum. This is best suited for any questions related to the open source library and Community Cloud - debugging code, deployment, resource limits, etc.\\n\\nDeploying apps\\n\\nMy repo isn\\'t showing on the Deploy page\\n\\nIt\\'s possible it just isn\\'t showing up even though it is already there. Try typing it in. If we don\\'t recognize it, you\\'ll see the message below with a link to click and give access.\\n\\nIf for some reason that doesn\\'t work, please try logging out and back in again to make sure the change took effect. And if that doesn\\'t work - please let us know and we\\'ll get you sorted!\\n\\nIt won\\'t let me deploy the app\\n\\nTo deploy an app for the first time you must have admin-level access to the repo in GitHub. Please check with your administrator to make sure you have that access. If not, please ask them to deploy for the first time (we need this in order to establish webhooks for continuous integration) and from there you can then push updates to the app.\\n\\nI need to set a specific Python version for my app\\n\\nWhen deploying an app, under advanced settings, you can choose which version of Python you wish your app to use.\\n\\nHow do I store files locally?\\n\\nIf you want to store your data locally as opposed to in a database, you can store the file in your GitHub repository. Streamlit is just python, so you can read the file using:\\n\\npandas.read_csv(\"data.csv\") or open(\"data.csv\")\\n\\nIf you have really big or binary data that you change frequently, and git is feeling slow, you might want to check out Git Large File Store (LFS) as a better way to store large files in GitHub. You don\\'t need to make any changes to your app to start using it. If your GitHub repo uses LFS, it will now just work with Streamlit.\\n\\nMy app is running into issues while deploying\\n\\nCheck your Cloud logs by clicking on the \"Manage app\" expander in the bottom right corner of your screen. Often the trouble is due to a dependency not being declared. See here for more information on dependency management.\\n\\nIf that\\'s not the issue, then please send the logs and warning you are seeing to our Community forum and we\\'ll help get you sorted!\\n\\nMy app is hitting resource limits / my app is running very slowly\\n\\nIf your app is running slowly or you\\'re hitting the \\'Argh\\' page, we first highly recommend going through and implementing the suggestions in the following blog posts to prevent your app from hitting the resource limits and to detect if your Streamlit app leaks memory:\\n\\nCommon app problems: Resource limits\\n\\n3 steps to fix app memory leaks\\n\\nIf you\\'re still having issues, click here to learn more about resource limits.\\n\\nCan I get a custom URL for my app?\\n\\nYes! You can find instructions for setting a custom subdomain here.\\n\\nSharing and accessing apps\\n\\nI don\\'t have SSO. How do I sign in to Streamlit?\\n\\nDon\\'t have SSO? No problem! You can sign in to Streamlit with your email address. Click here for step-by-step instructions on how to sign in with email.\\n\\nHow do I add viewers to my Streamlit apps?\\n\\nViewer auth allows you to restrict the viewers of your private app. To access your app, users have to authenticate using an email-based passwordless login or Google OAuth. To learn more about how to share your public and private apps with viewers, click here.\\n\\nDo viewers need access to the GitHub repo?\\n\\nNope! You only need access to the GitHub repo if you want to push changes to the app.\\n\\nWhat will unauthorized/logged out viewers see when they view my app?\\n\\nA 404 error is displayed to unauthorized viewers to avoid providing any unnecessary information about your app to unintended viewers. Users who satisfy any of the following conditions will see a 404 error when attempting to view your app after you have configured viewer auth:\\n\\nUser is not logged in with Google SSO.\\n\\nUser is not included in the list of viewers provided in the app settings.\\n\\nUser lacks read access to your app\\'s GitHub repo.\\n\\nUser has read access to your app\\'s GitHub repo but is not enrolled in Community Cloud.\\n\\nI\\'ve added someone to the viewer list but they still see a 404 error when attempting to view the app\\n\\nIf a user is still seeing a 404 error after their email address has been added to the viewer list, we recommend that you:\\n\\nCheck that the user did not log into a different Google account via Single Sign-On (if you have added their work email address to the viewer list, ask the user to check that they are not logged into their personal Google account, and vice versa).\\n\\nCheck that the user has navigated to the correct URL.\\n\\nCheck that the user\\'s email address has been entered correctly in the viewer list.\\n\\nReach out on our Community forum and we will be happy to help.\\n\\nData and app security\\n\\nHow will Streamlit secure my data?\\n\\nStreamlit takes a number of industry best-practice measures to ensure your code, data, and apps are all secure. Read more in our Trust and Security memo.\\n\\nHow do I set up SSO for my organization?\\n\\nCommunity Cloud uses Google OAuth, by default. If you use Google for authentication you\\'re all set.\\n\\nBilling and administration\\n\\nThe Community Cloud is a free service. You don\\'t have to worry about setting up billing or being charged.\\n\\nGitHub integration\\n\\nWhy does Streamlit require additional OAuth scope?\\n\\nIn order to deploy your app, Streamlit requires access to your app\\'s source code in GitHub and also the ability to manage the public keys associated with the repositories. The default GitHub OAuth scopes are sufficient to work with apps in public GitHub repositories. However, in order to work with apps in private GitHub repositories, Streamlit requires the additional repo OAuth scope from GitHub. We recognize that this scope provides Streamlit with extra permissions that we do not really need, and which, as people who prize security, we\\'d rather not even be granted. Alas, we need to work with the APIs we are provided by GitHub.\\n\\nAfter deploying my private-repo app, I received an email from GitHub saying a new public key was added to my repo. Is this expected?\\n\\nThis is the expected behavior. When you try to deploy an app that lives in a private repo, Streamlit Community Cloud needs to get access to that repo somehow. For this, we create a read-only GitHub Deploy Key then access your repo using a public SSH key. When we set this up, GitHub notifies admins of the repo that the key was created as a security measure.\\n\\nWhat happens when a user\\'s permissions change on GitHub?\\n\\nOnce a user is added to a repository on GitHub, it will take at most 15 minutes before they can deploy the app on Cloud. If a user is removed from a repository on GitHub, it will take at most 15 minutes before their permissions to manage the app from that repository are revoked.\\n\\nLimitations and known issues\\n\\nHere are some limitations and known issues that we\\'re actively working to resolve. If you find an issue\\xa0please let us know!\\n\\nWhen you print something to the Cloud logs, you may need to do a\\xa0sys.stdout.flush()\\xa0before it shows up.\\n\\nMatplotlib\\xa0doesn\\'t work well with threads. So if you\\'re using Matplotlib you should wrap your code with locks as shown in the snippet below. This Matplotlib bug is more prominent when you share your app apps since you\\'re more likely to get more concurrent users then.\\n\\n```python\\n  from matplotlib.backends.backend_agg import RendererAgg\\n  _lock = RendererAgg.lock\\n\\nwith _lock:\\n    fig.title(\\'This is a figure)\\')\\n    fig.plot([1,20,3,40])\\n    st.pyplot(fig)\\n  ```\\n\\nAll apps are hosted in the United States. This is currently not configurable.', metadata={'source': 'docs/content/streamlit-cloud/troubleshooting.md'}),\n",
       " Document(page_content='title: Embed your app\\nslug: /streamlit-community-cloud/get-started/embed-your-app\\n\\nEmbed your app\\n\\nEmbedding Streamlit Community Cloud apps enriches your content by integrating interactive, data-driven applications directly within your pages. Whether you\\'re writing a blog post, a technical document, or sharing resources on platforms like Medium, Notion, or even StackOverflow, embedding Streamlit apps adds a dynamic component to your content. This allows your audience to interact with your ideas, rather than merely reading about them or looking at screenshots.\\n\\nStreamlit Community Cloud supports both iframe and oEmbed methods for embedding public apps. This flexibility enables you to share your apps across a wide array of platforms, broadening your app\\'s visibility and impact. In this guide, we\\'ll cover how to use both methods effectively to share your Streamlit apps with the world.\\n\\nEmbedding with iframes\\n\\nStreamlit Community Cloud supports embedding\\xa0public\\xa0apps using the subdomain scheme. To embed a public app, add the query parameter\\xa0/?embed=true\\xa0to the end of the\\xa0*streamlit.app\\xa0URL.\\n\\nFor example, say you want to embed the 30DaysOfStreamlit app. The URL to include in your iframe is: https://30days.streamlit.app/?embed=true:\\n\\n```javascript\\n\\n```\\n\\nThere will be no official support for embedding private apps.\\n\\nIn addition to allowing you to embed apps via iframes, the\\xa0?embed=true\\xa0query parameter also does the following:\\n\\nRemoves the toolbar with the hamburger menu\\n\\nRemoves the padding at the top and bottom of the app\\n\\nRemoves the footer\\n\\nRemoves the colored line from the top of the app\\n\\nFor granular control over the embedding behavior, Streamlit allows you to specify one or more instances of the ?embed_options query parameter (e.g. to show the toolbar, open the app in dark theme, etc). Click here for a full list of Embed options.\\n\\nEmbedding with oEmbed\\n\\nThe new oEmbed support allows for a simpler embedding experience. You can now directly drop a Streamlit app\\'s URL into a Medium, Ghost, or Notion page (or any site that supports oEmbed or embed.ly, which includes over 700 content providers), and the embedded app will automatically appear. This helps Streamlit Community Cloud apps seamlessly integrate into these platforms, improving the visibility and accessibility of your apps.\\n\\nExample\\n\\nWhen creating content in a Notion page, Medium article, or Ghost blog, you only need to paste the app\\'s URL and hit Enter. The app will then render automatically at that spot in your content. Use the same URL as for iframe embedding, but without the ?embed=true query parameter.\\n\\nhttps://30days.streamlit.app/\\n\\nHere\\'s an example of @chrieke\\'s Prettymapp app embedded in a Medium article:\\n\\nEnsure the platform hosting the embedded Streamlit app supports oEmbed or embed.ly.\\n\\nKey Sites for oEmbed\\n\\noEmbed should work out of the box for several platforms including but not limited to:\\n\\nMedium\\n\\nNotion\\n\\nLooker\\n\\nTableau\\n\\nGhost\\n\\nDiscourse\\n\\nStackOverflow\\n\\nW3\\n\\nReddit\\n\\nPlease check the specific platform\\'s documentation to verify support for oEmbed.\\n\\niframe versus oEmbed\\n\\nThe only noteworthy differences between the methods is that iframing allows you customize the app\\'s embedding behavior (e.g. showing the toolbar, opening the app in dark theme, etc) using the various ?embed_options described in the next section.\\n\\nEmbed options\\n\\nWhen Embedding with iframes, Streamlit allows you to specify one or more instances of the ?embed_options query parameter for granular control over the embedding behavior. The supported values for\\xa0?embed_options\\xa0are listed below:\\n\\nShow the toolbar at the top right of the app (menu, running man, ...).\\n\\njavascript\\n   /?embed=true&embed_options=show_toolbar\\n\\nShow padding at the top and bottom of the app.\\n\\njavascript\\n   /?embed=true&embed_options=show_padding\\n\\nShow the footer reading \"Made with Streamlit.\"\\n\\njavascript\\n   /?embed=true&embed_options=show_footer\\n\\nShow the colored line at the top of the app.\\n\\njavascript\\n   /?embed=true&embed_options=show_colored_line\\n\\nDisable scrolling for the main body of the app. (The sidebar will still be scrollable.)\\n\\njavascript\\n   /?embed=true&embed_options=disable_scrolling\\n\\nOpen the app with light theme.\\n\\njavascript\\n   /?embed=true&embed_options=light_theme\\n\\nOpen the app with dark theme.\\n\\njavascript\\n   /?embed=true&embed_options=dark_theme\\n\\nYou can also combine the params:\\n\\njavascript\\n/?embed=true&embed_options=show_toolbar&embed_options=show_padding&embed_options=show_footer&embed_options=show_colored_line&embed_options=disable_scrolling\\n\\nBoth\\xa0?embed\\xa0and\\xa0?embed_options\\xa0are invisible to\\xa0st.experimental_get_query_params and\\xa0st.experimental_set_query_params. The former ignores the embed query parameters and does not return them, while the latter disallows setting embed query parameters.', metadata={'source': 'docs/content/streamlit-cloud/get-started/embed-your-app.md'}),\n",
       " Document(page_content='title: Share your app\\nslug: /streamlit-community-cloud/get-started/share-your-app\\n\\nShare your app\\n\\nNow that your app is deployed you can easily share it and collaborate on it. But first, let\\'s take a moment and do a little joy dance for getting that app deployed! 🕺💃\\n\\nYour app is now live at that fixed URL, so go wild and share it with whomever you want. Your app will inherit permissions from your GitHub repo, meaning that if your repo is private your app will be private and if your repo is public your app will be public. If you want to change that you can simply do so from the app menu.\\n\\nSharing private apps\\n\\nSharing public apps\\n\\nAdding developers\\n\\nThere are three primary ways to share your app with viewers. You can either directly add viewers from the in-app share menu, or do so from the Cloud logs menu, or from your app dashboard.\\n\\nSharing private apps\\n\\nBy default all apps deployed from private source code are private to the developers in the workspace. Your apps will not be visible to anyone else unless you grant them explicit permission. You can grant permission either in your workspace or from the app itself.\\n\\nWhat is viewer auth?\\n\\nViewer auth allows you to restrict the viewers of your app. To access your app, users have to authenticate using an email-based passwordless login or Google OAuth.\\n\\nConfiguring single sign-on\\n\\nGoogle OAuth is enabled by default, so if you use Google, you\\'re good to go.\\n\\nOnce you have added someone\\'s email address to your app\\'s viewer list, that person will be able to sign in via Google Single Sign-On and view your app. They will also receive an email inviting them to view your app. If they are already logged in with that account in their browser (the usual case for most people) then they will automatically be able to view the app. If they are not logged in, or they have not been given access, then they will see a page asking them to first log in.\\n\\nHaving trouble granting access? Is a viewer having trouble logging on? See our troubleshooting section for help.\\n\\nAdding viewers from the in-app share menu\\n\\nYou can add viewers from the in-app share menu by clicking the \"Share\" button in the top right corner of your deployed app.\\n\\nClick \"Share\" in the top right corner.\\n\\nEnter the email addresses of the viewers.\\n\\nClick \"Invite\".\\n\\nIt\\'s that easy! The viewers you have added will receive an email inviting them to visit your app. The most recently added viewers will appear at the top of the list in the in-app share menu.\\n\\nTo remove a viewer, simply hover over their email address and click \"X\" that appears to the right:\\n\\nDevelopers, invited viewers, and members of your workspace can all see the in-app share menu, read the list of viewers, and add and remove viewers.\\n\\nOnly developers are allowed to toggle whether the app is public or private. App viewers don\\'t have permission to change this setting.\\n\\nAdding viewers from the Cloud logs menu\\n\\nFrom your deployed app you can easily add viewers from your Cloud logs menu.\\n\\nSelect \"Manage app\" in the lower right corner.\\n\\nChoose \"Settings\" from the menu.\\n\\nAdd Viewers in Settings.\\n\\nYou can choose to allow only selected viewers based on their individual emails. Make sure to enter them as a line-separated list.\\n\\nAdding viewers from the app dashboard\\n\\nYou can also add viewers directly from your dashboard.\\n\\nOpen settings for your app\\n\\nNavigate to the app you want to add viewer to and click the hamburger icon to select \"Settings.\"\\n\\nAdd Viewers in Settings\\n\\nClick on the \"Sharing\" section in the App Settings and in the text input area, provide a line-separated list of email addresses for the users you wish to grant viewer access to your app. Click \"Save.\"\\n\\nSharing public apps\\n\\nFrom your deployed app you can click on the \"☰\" menu on the top right and select \\'Share this app\\' to post it directly into social media or to share with the community on our Community forum. We\\'d love to see what you make and perhaps feature your app as our app of the month ❤️.\\n\\nAdd a GitHub badge\\n\\nTo help others find and play with your Streamlit app, you can add Streamlit\\'s GitHub badge to your repo. Below is an example of what the badge looks like. Clicking on the badge takes you to, in this case, Streamlit\\'s Face-GAN Demo.\\n\\nOnce you deploy your app, you can embed this badge right into your GitHub README.md by adding the following Markdown:\\n\\nmarkdown\\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://<your-custom-subdomain>.streamlit.app)\\n\\nBe sure to replace https://<your-custom-subdomain>.streamlit.app with the URL of your deployed app!\\n\\nAdding developers\\n\\nInviting other developers is simple, just invite them to your GitHub repository so that you can code on apps together, and then have them log in to share.streamlit.io. If you are working as a team, you likely are already in the same repos, so skip step 1 and go straight to having them log into share.streamlit.io\\n\\nStreamlit Community Cloud inherits developer permissions from GitHub, so when your teammates log in, they will automatically view the workspaces you share. From there you can all deploy, manage, and share apps together.\\n\\nPushing new code\\n\\nYou can also collaborate with other developers by having multiple contributors pushing to the same GitHub repo. Whenever anyone on the team updates the code on GitHub, the app will also automatically update for you!\\n\\nIf you want to try out something new while still keeping your original app running, just create a new branch, make some changes, and deploy a new version of the Streamlit app.\\n\\nFinding app code\\n\\nEvery deployed app has its GitHub source code linked in the \"☰\" menu on the top right. So if you are looking to understand the code of another Streamlit app, you can navigate to the GitHub page from there and read or fork the app.', metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/index.md'}),\n",
       " Document(page_content='title: Get started\\nslug: /streamlit-community-cloud/get-started\\n\\nGet started\\n\\nWelcome to Streamlit Community Cloud! First things first, before you get started with Streamlit Community Cloud, you need to have a Streamlit app to deploy. If you haven\\'t built one yet, read our Get started docs or start with an Example app. Either way, it only takes a few minutes to create your first app.\\n\\nHow Streamlit Community Cloud works\\n\\nStreamlit Community Cloud is a workspace for your team to deploy, manage, and collaborate on your Streamlit apps. You connect your Streamlit Community Cloud account directly to your GitHub repository (public or private) and then Streamlit Community Cloud launches the apps directly from the code you\\'ve stored on GitHub. Most apps will launch in only a few minutes, and any time you update the code on GitHub, your app will automatically update for you. This creates a fast iteration cycle for your deployed apps, so that developers and viewers of apps can rapidly prototype, explore, and update apps.\\n\\nUnder the hood Streamlit Community Cloud handles all of the containerization, authentication, scaling, security and everything else so that all you need to worry about is creating the app. Maintaining Streamlit apps is easy. Containers get the latest security patches, are actively monitored for container health. We are also building the capability to observe and monitor apps.\\n\\nGetting started\\n\\nGetting your workspace set up with Streamlit Community Cloud only takes a few minutes.\\n\\nSign up for Streamlit Community Cloud\\n\\nLog in to your account\\n\\nConnect your Streamlit Community Cloud account to GitHub\\n\\nExplore your Streamlit Community Cloud workspace\\n\\nInvite other developers on your team\\n\\nSign up for Streamlit Community Cloud\\n\\nStreamlit\\'s Community Cloud allows you to deploy, manage, and share your apps with the world, directly from Streamlit — all for free. Sign up on the Community Cloud homepage.\\n\\nOnce you\\'ve signed up, login to share.streamlit.io and follow the steps below.\\n\\nLog in to share.streamlit.io\\n\\nYou can login to Streamlit Community Cloud with:\\n\\nGoogle\\n\\nGitHub\\n\\nEmail based sign-in link: These are single-use links that are valid for up to 15 minutes.\\n\\nIf you\\'re a developer, we recommend starting with GitHub the first time you login. You can later setup your account to login using Google.\\n\\nIf you\\'re sharing your app, your app\\'s users can use any of the above methods to login.\\n\\nSign in with Google\\n\\nVisit share.streamlit.io and click the \"Continue with Google\" button.\\n\\nOn the next page, choose an account to sign in with and enter your Google account credentials.\\n\\nOnce you have signed in to Google, you will be taken to your Streamlit Community Cloud workspace!🎈\\n\\nSign in with GitHub\\n\\nVisit share.streamlit.io and click the \"Continue with GitHub\" button.\\n\\nOn the next page, enter your GitHub credentials to sign in.\\n\\nOnce you have signed in to GitHub, you will be taken to your Streamlit Community Cloud workspace!🎈\\n\\nSign in with Email\\n\\nIf you don\\'t have SSO, you can sign in with your email address! Visit share.streamlit.io, enter the email address you used to sign up for Streamlit Community Cloud, and click the \"Continue with email\" button.\\n\\nOnce you do so, you will see a confirmation message (like the one below) asking you to check your email.\\n\\nCheck your inbox for an email from Streamlit, with the subject \"Sign in to Streamlit Community Cloud\". Click the link in the email to sign in to Streamlit. Note that this link will expire in 15 minutes and can only be used once.\\n\\nOnce you click the link in your email, you will be taken to your Streamlit Community Cloud workspace!🎈\\n\\nConnect your GitHub account\\n\\nNext you need to authorize Streamlit to connect to your GitHub account. This lets your Streamlit Community Cloud workspace launch apps directly from the app files you store in your repos, as well as let the system check for updates to those app files so that your apps can automatically update. You will see two different authorization screens to give this access. Click \"authorize\" on both. Questions about GitHub permissions? Read more here!\\n\\nYou must have admin permissions to your repo in order to deploy apps. If you don\\'t have admin access, talk to your IT team or manager about helping you set up your Streamlit Community Cloud account or reach out to us on the Community forum.\\n\\nOnce a user is added to a repository on GitHub, it will take at most 15 minutes before they can deploy the app on Cloud. If a user is removed from a repository on GitHub, it will take at most 15 minutes before their permissions to manage the app from that repository are revoked.\\n\\nExplore your Streamlit Community Cloud workspace\\n\\nCongrats! You are now logged in and ready to go. If you are joining someone else\\'s workspace you may already see apps populated in your workspace. If not, then you need to deploy an app! Check out our next section on how to deploy an app. And if you need an app to deploy check out our example apps that include apps for machine learning, data science, and business use cases.\\n\\nYou may also find that you already have multiple Streamlit Community Cloud workspaces. Streamlit Community Cloud automatically groups your apps according to the corresponding GitHub repository\\'s owner. In the upper right corner you can see the workspaces you have access to. If your team has already launched apps, then you will see those apps in your workspace. Read more about workspaces here.\\n\\nInvite other developers to your workspace\\n\\nInviting other developers is simple, just invite them to your GitHub repository so that you can code on apps together, and then have them log in to share.streamlit.io. If you are working as a team, you likely are already in the same repos, so skip step 1 and go straight to having them log into share.streamlit.io\\n\\nStreamlit Community Cloud inherits developer permissions from GitHub, so when your teammates log in, they will automatically view the workspaces you share. From there you can all deploy, manage, and share apps together.\\n\\nAnd remember, whenever anyone on the team updates the code on GitHub, the app will also automatically update for you!', metadata={'source': 'docs/content/streamlit-cloud/get-started/index.md'}),\n",
       " Document(page_content='title: App indexability\\nslug: /streamlit-community-cloud/get-started/share-your-app/indexability\\n\\nApp indexability\\n\\nWhen you deploy a public app to Community Cloud, it is automatically indexed by search engines like Google and Bing on a weekly basis. 🎈 This means that anyone can find your app by searching for its custom subdomain (e.g. https://traingenerator.streamlit.app) or by searching for the app\\'s title.\\n\\nGet the most out of app indexability\\n\\nHere are some tips to help you get the most out of app indexability:\\n\\nMake sure your app is public\\n\\nChoose a custom subdomain early\\n\\nChoose a descriptive app title\\n\\nCustomize your app\\'s meta description\\n\\nMake sure your app is public\\n\\nAll public apps hosted on Community Cloud are indexed by search engines. If your app is private, it will not be indexed by search engines. To make your private app public, read Share your app.\\n\\nChoose a custom subdomain early\\n\\nCommunity Cloud automatically generates a random subdomain for your app based on the structure of the app\\'s GitHub repo. To learn more about app URLs, see Your app URL. However, subdomains are customizable! Custom subdomains modify your app URLs to reflect your app content, personal branding, or whatever you’d like. Read more about custom subdomains in Custom subdomains.\\n\\nBy choosing a custom subdomain, you can use it to help people find your app. For example, if you\\'re deploying an app that generates training data, you might choose a subdomain like traingenerator.streamlit.app. This makes it easy for people to find your app by searching for \"training generator\" or \"train generator streamlit app\"\\n\\nWe recommend choosing a custom subdomain early, right after you deploy your app. This ensures that your app is indexed by search engines using your custom subdomain, rather than the automatically generated one. If you choose a custom subdomain later, your app may first be indexed by search engines using the default subdomain. This means that your app will be indexed multiple times, once using the default subdomain (which will lead to a 404) and once using your custom subdomain. This can confuse users who are searching for your app.\\n\\nChoose a descriptive app title\\n\\nThe meta title of your app is the text that appears in search engine results. It is also the text that appears in the browser tab when your app is open. By default, the meta title of your app is the same as the title of your app. However, you can customize the meta title of your app by setting the st.set_page_config parameter page_title to a custom string. For example:\\n\\npython\\nst.set_page_config(page_title=\"Traingenerator\")\\n\\nThis will change the meta title of your app to \"Traingenerator.\" This makes it easier for people to find your app by searching for \"Traingenerator\" or \"train generator streamlit app\":\\n\\nCustomize your app\\'s meta description\\n\\nMeta descriptions are the short descriptions that appear in search engine results. Search engines use the meta description to help users understand what your app is about.\\n\\nFrom our observations, search engines seem to favor the content in both st.header and st.text over st.title. If you put a description at the top of your app under st.header or st.text, there’s a good chance search engines will use this for the meta description.\\n\\nWhat does my indexed app look like?\\n\\nIf you\\'re curious about what your app looks like in search engine results, you can type the following into Google Search:\\n\\nsite:<your-app-url>\\n\\nExample: site:traingenerator.streamlit.app\\n\\nWhat if I don\\'t want my app to be indexed?\\n\\nIf you don\\'t want your app to be indexed by search engines, you can make it private. Read Share your app to learn more about making your app private. Note: each workspace can only have one private app. If you want to make your app private, you must first delete any other private apps in your workspace.\\n\\nThat said, Community Cloud is an open and free platform for the community to deploy, discover, and share Streamlit apps and code with each other. As such, we encourage you to make your app public so that it can be indexed by search engines and discovered by other Streamlit users and community members.\\n\\nHave questions or feedback?\\n\\nIf you run into issues with app indexability or have any questions or feedback about it, we’d love to hear from you! Please post on our Community forum so that our team and community members can help you. 🤗', metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/indexability.md'}),\n",
       " Document(page_content='title: Share previews\\nslug: /streamlit-community-cloud/get-started/share-your-app/share-previews\\n\\nShare previews\\n\\nSocial media sites generate a card with a title, preview image, and description when you share a link. This feature is called a \"share preview.\" In the same way, when you share a link to a public Streamlit app on social media, a share preview is also generated. Here\\'s an example of a share preview for a public Streamlit app posted on Twitter:\\n\\nShare previews are generated only for public apps deployed on Community Cloud.\\n\\nTitles\\n\\nThe title is the text that appears at the top of the share preview. The text also appears in the browser tab when you visit the app. You should set the title to something that will make sense to your app\\'s audience and describe what the app does. It is best practice to keep the title concise, ideally under 60 characters.\\n\\nThere are two ways to set the title of a share preview:\\n\\nSet the page_title parameter in st.set_page_config() to your desired title. E.g.:\\n\\n```python\\n   import streamlit as st\\n\\nst.set_page_config(page_title=\"My App\")\\n\\n# ... rest of your app\\n   ```\\n\\nIf you don\\'t set the page_title parameter, the title of the share preview will be the name of your app\\'s GitHub repository. E.g., if you don\\'t set the page_title parameter in st.set_page_config(), the title of the share preview for an app hosted on GitHub at https://github.com/jrieke/traingenerator will be \"traingenerator\".\\n\\nDescriptions\\n\\nThe description is the text that appears below the title in the share preview. The description should summarize what the app does and ideally should be under 100 characters.\\n\\nStreamlit pulls the description from the README in the app\\'s GitHub repository. If there is no README, the description will default to:\\n\\nThis app was built in Streamlit! Check it out and visit https://streamlit.io for more awesome community apps. 🎈\\n\\nIf you want your share previews to look great and want users to share your app and click on your links, you should write a good description in the README of your app’s GitHub repository.\\n\\nPreview images\\n\\nCommunity Cloud takes a screenshot of your app once a day and uses it as the preview image, unlike titles and descriptions, which are pulled directly from your app\\'s code or GitHub repository. This screenshot may take up to 24 hours to update.\\n\\nSwitching your app from public to private\\n\\nIf you initially made your app public and later decided to make it private, we will stop generating share previews for the app. However, it may take up to 24 hours for the share previews to stop appearing.', metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/share-previews.md'}),\n",
       " Document(page_content='title: Manage your app\\nslug: /streamlit-community-cloud/get-started/manage-your-app\\n\\nManage your app\\n\\nYou can manage your app directly from the deployed app in your developer view or you can log in to your app dashboard at\\xa0share.streamlit.io\\xa0to view, deploy, delete, reboot, or favorite an app.\\n\\nManage apps from your developer view\\n\\nManage apps from your app dashboard\\n\\nManage apps in GitHub\\n\\nApp resources and limits\\n\\nApp favoriting\\n\\nAnalytics Modal\\n\\nManage apps from your developer view\\n\\nOnce you have deployed an app you will have a developer view for that app.\\n\\nDeveloper view\\n\\nClick on the bottom right where it says \"Manage app\" to view your Cloud logs and other settings.\\n\\nCloud logs\\n\\nOnce you\\'ve clicked on \"Manage app\", you will be able to view your app\\'s logs. This is your primary place to troubleshoot any issues with your app.\\n\\nYou can also click on the \"︙\" overflow menu at the bottom of the Cloud logs to view other options for your app including the ability to download logs, reboot the app, delete the app, navigate to settings (which includes managing viewer access and app secrets), go to your app dashboard, go to documentation, contact support, or sign out.\\n\\nManage apps from your app dashboard\\n\\nWhen you first log into\\xa0share.streamlit.io\\xa0you will land on your app dashboard, which gives you a list of all your deployed apps. This list does include apps deployed by other developers in your workspace, since you\\'re all managers of those apps. Such apps are indicated with an icon like this one:\\n\\nApp workspaces\\n\\nStreamlit Community Cloud is organized into workspaces, which automatically group your apps according to the corresponding GitHub repository\\'s owner. If you are part of multiple repositories, then you will have multiple workspaces.\\n\\nIf an app\\'s GitHub repository is owned by you, the app will appear in your personal workspace, named \"<YourGitHubHandle>\".\\n\\nIf an app\\'s GitHub repository is owned by an organization (such as your company), the app will appear in a separate workspace, named \"<GitHubOrganizationHandle>\".\\n\\nYou will also have access to any workspaces containing app(s) for which you only have view access. These apps will have a \"view-only\" tooltip when you click on their respective hamburger menus.\\n\\nTo switch between workspaces, click on the workspace listed in the top right corner, then select the desired workspace name.\\n\\nReboot an app\\n\\nIf your app needs a hard reboot, click on the \"︙\" overflow menu to the right of the app and click to Reboot. This will interrupt any user that may currently be using that app. It may also take a few minutes for your app to re-deploy, and in that time you — and anyone visiting the app — will see the \\'Your app is in the oven\\' screen.\\n\\nApp settings\\n\\nThe app settings let you pick a custom subdomain for your app, manage viewers of your apps and secrets of your apps. Click on the links to lean more about these features.\\n\\nManage apps in GitHub\\n\\nUpdate your app\\n\\nYour GitHub repository is the source for the app, so that means that any time you push an update to your repo you\\'ll see it reflected in the app in almost real time. Try it out!\\n\\nStreamlit also smartly detects whether you touched your dependencies, in which case it will automatically do a full redeploy for you—which will take a little more time. But since most updates don\\'t involve dependency changes, you should usually see your app update in real time.\\n\\nAdd or remove dependencies\\n\\nYou can add/remove dependencies at any point by updating\\xa0requirements.txt\\xa0(Python deps) or\\xa0packages.txt\\xa0(Debian deps) and doing a\\xa0git push\\xa0to your remote repo. This will cause Streamlit to detect there was a change in its dependencies, which will automatically trigger its installation.\\n\\nIt is best practice to pin your Streamlit version in\\xa0requirements.txt. Otherwise, the version may be auto-upgraded at any point without your knowledge, which could lead to undesired results (e.g. when we deprecate a feature in Streamlit).\\n\\nApp resources and limits\\n\\nResource limits\\n\\nAll Community Cloud users have access to the same resources and are subject to the same limits (1 GB of RAM).\\nIf your app is running slowly or you\\'re hitting the \\'Argh\\' page, we first highly recommend going through and implementing the suggestions in the following blog posts to prevent your app from hitting the resource limits and to detect if your Streamlit app leaks memory:\\n\\nCommon app problems: Resource limits\\n\\n3 steps to fix app memory leaks\\n\\nDeveloper view\\n\\nIf your app exceeds its resource limits, you will see one of the following messages when you visit your app. If your app uses an older version of Streamlit (<1.1.0) without memory fixes, you will see the message on the left. If your app uses a newer version of Streamlit (>=1.1.0), you will see the message on the right:\\n\\nSimilarly, you will receive one of the following two emails from alert@streamlit.io with the subject \"Your Streamlit app has gone over its resource limits 🤯\":\\n\\nNon-developer view\\n\\nIf your app exceeds its resource limits, users with view-only access will see one of the following messages when they visit your app. They will see the message on the left if your app uses an older version of Streamlit (<1.1.0) without memory fixes, and the message on the right if your app uses a newer version of Streamlit (>=1.1.0). Viewers have the option to notify you when the app exceeds its resource limits:\\n\\nApp hibernation\\n\\nPrivate apps will not hibernate, but public Community Cloud apps without traffic for 7 consecutive days will automatically go to sleep. This is done to alleviate resources and allow the best communal use of the platform! Here are some need to know\\'s about how this works:\\n\\nAs the app developer, you will receive an email after 5 days of no traffic on your app.\\n\\nIf you would like to keep your app awake, you have one of two choices:\\n\\nVisit the app (create traffic).\\n\\nPush a commit to the app (this can be empty!).\\n\\nIf left alone the app will go to sleep at the 7 day mark (2 days after you receive the email). When someone visits the app after this, they will see the sleeping page:\\n\\nTo wake the app up, press the \"Yes, get this app back up!\" button. This can be done by\\xa0anyone\\xa0who wants to view the app, not just the app developer!\\n\\nYou can also wake apps through your Streamlit Community Cloud dashboard. You will know which apps are sleeping because a moon icon will appear next to the app settings. To wake an app from the dashboard, click the moon.\\n\\nApp favoriting\\n\\nStreamlit Community Cloud supports a \"favorite\" feature that lets you quickly access your apps from the app dashboard. Favorited apps appear at the top of the app dashboard with a yellow star (⭐) beside them. You can favorite and unfavorite apps in any workspace to which you have access.\\n\\nFavorites are specific to your account. Other members of your workspace cannot see which apps you have favorited.\\n\\nFavorite an app from your app dashboard\\n\\nThere are two ways to favorite an app from the app dashboard:\\n\\nHover over an app and click the star (☆) that appears.\\n\\nClick on the \"︙\" overflow menu to the app\\'s right and click to Favorite.\\n\\nTo unfavorite an app, either hover over the app and click the star (⭐) again, or click on the \"︙\" overflow menu to the app\\'s right and click to Unfavorite.\\n\\nIn-app favoriting\\n\\nYou can also favorite an app from right within the app! Currently, in-app favoriting is available for apps that use Streamlit v1.4.0 or later. Note that in-app favoriting is not available on apps in your workspaces for which you only have view access.\\n\\nWhen viewing any app in your workspace, click the star (☆) in the top-right corner of the app, besides the \"☰\" hamburger menu.\\n\\nTo unfavorite an app, click the star (⭐) again.\\n\\nClick here to learn more about upgrading the Streamlit version of your app on Streamlit Community Cloud.\\n\\nAnalytics Modal\\n\\nOnce you have access to a Streamlit workspace, you have access to 2 types of analytics:\\n\\nWorkspace analytics: shows you how many viewers in total have visited all the apps in your workspace.\\n\\nApp viewers: shows you who has recently viewed your workspace’s individual apps and when.\\n\\nThe Analytics Modal is visible to everyone with access to your workspace, including admins, developers, or anyone with viewer access to a workspace.\\n\\nWorkspace analytics\\n\\nStreamlit Community Cloud enables you to view analytics data for all apps in your workspace in one central dashboard. At a glance, you get an overview of how active your workspace is and how popular your apps are.\\n\\nTo view your Workspace analytics:\\n\\nSelect the \"Analytics\" option on the dashboard header\\n\\nView the \"Workspace\" tab in the Analytics modal\\n\\nYou\\'re presented with a graph that you can hover over to see the number of users who have viewed at least one app in your workspace that month. This viewers count includes apps that anyone in your workspace created.\\n\\nSolid lines indicate fully-complete months on the dashboard, while dotted lines indicate the current in-progress month.\\n\\nViewers data on your dashboard starts from April 2022 and onward. April 2022 data was our first month comprehensively tracking user analytics in Streamlit workspaces, and our tracking is even more refined starting in May 2022 and onward.\\n\\nApp viewers\\n\\nIn addition to a general overview of the activity of your workspace and the popularity of your apps, Streamlit Community Cloud allows you to drill down to the level of individual apps and understand their viewership better.\\n\\nAs an app developer or a viewer with access to a given workspace, you can see who has viewed a given app and when. Specifically, you can see the total viewers count of your app (since April 2022 and onward), the most recent unique viewers (capped up to your last 20 viewers), and a relative timestamp of their last view.\\n\\nThere are three ways to access the app viewers data:\\n\\nFrom the app dashboard, click the \"︙\" overflow menu to the app\\'s right and select Analytics:\\n\\nDoing so opens the \"App viewers\" tab of the \"Analytics\" modal.\\n\\nThe dropdown selects your app by default and displays:\\n\\nThe total (all time) number of unique viewers for the app.\\n\\nA list of the most recent viewers\\' names and a relative timestamp of their last view, sorted by the time since the last view (newest first).\\n\\nClick the \"Analytics\" option on the dashboard header and select the \"App viewers\" tab:\\n\\nDoing so opens the \"App viewers\" tab of the \"Analytics\" modal.\\n\\nThe first app in your workspace is pre-selected in the dropdown by default. You can select the app you want to see the analytics for by clicking the corresponding app in the dropdown.\\n\\nYou can also access app viewer analytics from right within individual apps! This is a capability if you have GitHub push access for a given app. Just view any app in your workspace as a developer, click the \"︙\" overflow menu at the bottom of the Cloud logs and select \"Analytics\":\\n\\nApp viewers for public vs private apps\\n\\nFor public apps, we anonymize all viewers outside your workspace to protect their privacy and display anonymous viewers as random pseudonyms. You\\'ll still be able to see the identities of fellow members in your workspace, though.\\n\\nMeanwhile, for private apps that are only accessible to your own workspace\\'s viewers, you will be able to see the specific users who recently viewed your apps.\\n\\nAdditionally, you may occasionally see anonymous users in private apps. Rest assured, these anonymous users do have authorized view access granted by you or your workspace members.\\n\\nCommon reasons why users show up anonymously are:\\n\\nThe app was previously public\\n\\nGiven viewer viewed app in April 2022, when the Streamlit team was honing user identification for this feature\\n\\nGiven viewer disconnected their SSO and GitHub accounts previously\\n\\nSee Streamlit\\'s general Privacy Notice.', metadata={'source': 'docs/content/streamlit-cloud/get-started/manage-your-app.md'}),\n",
       " Document(page_content=\"title: App dependencies\\nslug: /streamlit-community-cloud/get-started/deploy-an-app/app-dependencies\\n\\nApp dependencies\\n\\nThe main reason that apps fail to build properly is because Streamlit Community Cloud can't find your dependencies! So make sure you:\\n\\nAdd a\\xa0requirements file\\xa0for Python dependencies.\\n\\n(optional) Add a\\xa0packages.txt\\xa0file to manage any external dependencies (i.e Linux dependencies outside Python environment).\\n\\nPython requirements files should be placed either in the root of your repository or in the same\\ndirectory as your Streamlit app.\\n\\nAdd Python dependencies\\n\\nStreamlit looks at your requirements file's filename to determine which Python dependency manager to use in the order below. Streamlit will stop and install the first requirements file found.\\n\\ndocs\\n\\ndocs\\n\\ndocs\\n\\ndocs\\n\\nOnly include packages in your requirements file that are not distributed with a standard Python\\ninstallation. If any of the modules from base Python\\nare included in the requirements file, you will get an error when you try to deploy. Additionally, we recommend that you\\nuse the latest version of Streamlit to ensure full Streamlit Community Cloud functionality. Be sure to take note of\\nStreamlit's current requirements\\nfor package compatibility when planning your environment, especially protobuf>=3.20,<5.\\n\\nYou should only use one requirements file for your app. If you include more than one (e.g.\\nrequirements.txt and Pipfile). Streamlit will first look in the directory of your Streamlit app;\\nhowever, if no requirements file is found, Streamlit will then look at the root of the repo.\\n\\napt-get dependencies\\n\\nIf packages.txt exists in the root directory of your repository we automatically detect it, parse it, and install the listed packages as described below. You can read more about apt-get in their docs.\\n\\nAdd apt-get dependencies to\\xa0packages.txt, one package name per line. For example:\\n\\nbash\\n    freeglut3-dev\\n    libgtk2.0-dev\", metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/app-dependencies.md'}),\n",
       " Document(page_content='title: Deploy an app\\nslug: /streamlit-community-cloud/get-started/deploy-an-app\\n\\nDeploy an app\\n\\nStreamlit Community Cloud lets you deploy your apps in just one click, and most apps will deploy in only a few minutes. If you don\\'t have an app ready to deploy, fork or clone one of our example apps — you can find apps for machine learning, data visualization, data exploration, A/B testing and more.\\n\\nIf you want to deploy your app on a different cloud service, check out the Deploy Streamlit apps article in our Knowledge Base.\\n\\nAdd your app to GitHub\\n\\nStreamlit Community Cloud launches apps directly from your GitHub repo, so your app code and dependencies need to be on GitHub before you try to deploy the app. See App dependencies for more information.\\n\\nOptionally, add a configuration file\\n\\nStreamlit allows you to optionally set configuration options via four different methods. Among other things, you can use custom configs to customize your app\\'s theme, enable logging, or set the port on which your app runs. For more information, see\\xa0Configuration\\xa0and\\xa0Theming. On Streamlit Community Cloud, however, you can only set configuration options via a configuration file in your GitHub repo.\\n\\nSpecifically, you can add a configuration file to the root (top-level) directory of your repo: create a\\xa0.streamlit\\xa0folder, and then add a\\xa0config.toml\\xa0file to that folder. E.g., if your app is in a repo called\\xa0my-app, you would add a file called\\xa0my-app/.streamlit/config.toml. Say you want to set the theme of your app to \"dark\". You would add the following to your\\xa0.streamlit/config.toml\\xa0file:\\n\\ntoml\\n[theme]\\nbase=\"dark\"\\n\\nThere can be only one configuration file, regardless of the number of apps in the repo.\\n\\nDeploy your app\\n\\nTo deploy an app, click \"New app\" from the upper right corner of your workspace.\\n\\nFill in your repo, branch, and file path. As a shortcut, you can also click \"Paste GitHub URL\". Optionally, you can specify a custom subdomain. In the example below, the app would be deployed to https://red-balloon.streamlit.app/. You can always set or change your subdomain later. See more about custom subdomains at the end of this page.\\n\\nAdvanced settings for deployment\\n\\nIf you are connecting to a data source or want to select a Python version for your app, you can do that by clicking \"Advanced settings\" before you deploy the app.\\n\\nYou can connect to private data sources by using secrets management. Read more on how to connect to data sources.\\n\\nStreamlit Community Cloud supports Python 3.8 - Python 3.11, and defaults to version 3.9. You can select a version of your choice from the \"Python version\" dropdown in the \"Advanced settings\" modal.\\n\\nWatch your app launch\\n\\nYour app is now deploying and you can watch while it launches. Most apps take only a couple of minutes to deploy, but if your app has a lot of dependencies it may take some time to deploy the first time. After the initial deployment, any change that does not touch your dependencies should show up immediately.\\n\\nThe Cloud logs on the right hand side are only viewable to the developer and is how you can grab logs and debug any issues with the app. Learn more about Cloud logs.\\n\\nYour app URL\\n\\nThat\\'s it — you\\'re done! Your app now has a unique subdomain URL that you can share with others. Click here to read about how to share your app with viewers.\\n\\nUnique subdomains\\n\\nIf a custom subdomain was not set, an app URL follows a structure based on your GitHub repo. The URL begins with your GitHub username or organization owning your repo, followed by your repo name, app path, and a short hash. If you deploy from a branch other than main or master, the URL also includes the branch name.\\n\\nbash\\nhttps://[GitHub username or organization]-[repo name]-[app path]-[branch name]-[short hash].streamlit.app\\n\\nFor example, this is an app deployed from the streamlit organization. The repo is demo-self-driving and the app name is streamlit_app.py in the root directory. The branch name is master and therefore not included.\\n\\nbash\\nhttps://streamlit-demo-self-driving-streamlit-app-8jya0g.streamlit.app\\n\\nCustom subdomains\\n\\nThe default subdomain is not always the most memorable or easy to share. That\\'s why you can also set a custom domain for your app. The URL will appear as:\\n\\nbash\\nhttps://<your-custom-subdomain>.streamlit.app\\n\\nTo view or customize your app subdomain from the dashboard:\\n\\nClick the \"︙\" overflow menu to the app\\'s right and select \"Settings\".\\n\\nView the \"General\" tab in the App settings modal. Your app\\'s unique subdomain will appear here.\\n\\nPick a custom subdomain between 6 and 63 characters in length for your app\\'s URL and hit \"Save\".\\n\\nIt\\'s that simple! You can then access your app by visiting your custom subdomain URL 🎉.\\n\\nIf a custom subdomain is not available (e.g. because it\\'s already taken), you\\'ll see an error message like this:\\n\\nEmbed apps\\n\\nDocumentation for embedding apps has moved to Embed your app. Please update your bookmarks.', metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/index.md'}),\n",
       " Document(page_content='title: Streamlit Library\\nslug: /library\\n\\nStreamlit Library', metadata={'source': 'docs/content/library/index.md'}),\n",
       " Document(page_content='title: Secrets management\\nslug: /streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources/secrets-management\\n\\nSecrets management\\n\\nIntroduction\\n\\nIt\\'s generally considered bad practice to store unencrypted secrets in a git repository. If your application needs access to sensitive credentials the recommended solution is to store those credentials in a file that is not committed to the repository and to pass them as environment variables.\\n\\nSecrets Management allows you to store secrets securely and access them in your Streamlit app as environment variables.\\n\\nHow to use Secrets Management\\n\\nDeploy an app and set up secrets\\n\\nGo to http://share.streamlit.io/ and click \"New app\" to deploy a new app with secrets.\\n\\nClick \"Advanced settings...\"\\n\\nYou will see a modal appear with an input box for your secrets.\\n\\nProvide your secrets in the \"Secrets\" field using TOML format. For example:\\n\\n```toml\\n   # Everything in this section will be available as an environment variable\\n   db_username = \"Jane\"\\n   db_password = \"12345qwerty\"\\n\\n# You can also add other sections if you like.\\n   # The contents of sections as shown below will not become environment variables,\\n   # but they\\'ll be easily accessible from within Streamlit anyway as we show\\n   # later in this doc.\\n   [my_cool_secrets]\\n   things_i_like = [\"Streamlit\", \"Python\"]\\n   ```\\n\\nUse secrets in your app\\n\\nAccess your secrets as environment variables or by querying the st.secrets dict. For example, if you enter the secrets from the section above, the code below shows you how you can access them within your Streamlit app.\\n\\n```python\\nimport streamlit as st\\n\\nEverything is accessible via the st.secrets dict:\\n\\nst.write(\"DB username:\", st.secrets[\"db_username\"])\\nst.write(\"DB password:\", st.secrets[\"db_password\"])\\nst.write(\"My cool secrets:\", st.secrets[\"my_cool_secrets\"][\"things_i_like\"])\\n\\nAnd the root-level secrets are also accessible as environment variables:\\n\\nimport os\\n\\nst.write(\\n    \"Has environment variables been set:\",\\n    os.environ[\"db_username\"] == st.secrets[\"db_username\"],\\n)\\n```\\n\\nYou can access st.secrets via attribute notation (e.g. st.secrets.key), in addition to key notation (e.g. st.secrets[\"key\"])—like st.session_state.\\n\\nYou can even use TOML sections to compactly pass multiple secrets as a single attribute.\\n\\nConsider the following secrets:\\n\\ntoml\\n[db_credentials]\\nusername = \"my_username\"\\npassword = \"my_password\"\\n\\nRather than passing each secret as attributes in a function, you can more compactly pass the section to achieve the same result. See the notional code below which uses the secrets above:\\n\\n```python\\n\\nVerbose version\\n\\nmy_db.connect(username=st.secrets.db_credentials.username, password=st.secrets.db_credentials.password)\\n\\nFar more compact version!\\n\\nmy_db.connect(**st.secrets.db_credentials)\\n```\\n\\nEdit your app\\'s secrets\\n\\nGo to https://share.streamlit.io/\\n\\nOpen the menu for your app, and click \"Settings\".\\n\\nYou will see a modal appear. Click on the \"Secrets\" section and edit your secrets.\\n\\nAfter you edit your secrets, click \"Save\". It might take a minute for the update to be propagated to your app, but the new values will be reflected when the app re-runs.\\n\\nDevelop locally with secrets\\n\\nWhen developing your app locally, add a file called secrets.toml in a folder called .streamlit at the root of your app repo, and copy/paste your secrets into that file. Further instructions are available in the Streamlit library Secrets management documentation.\\n\\nBe sure to add this file to your .gitignore so you don\\'t commit your secrets!', metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/connect-data-sources/secrets-management.md'}),\n",
       " Document(page_content=\"title: Connect to data sources\\nslug: /streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources\\n\\nConnect data sources\\n\\nYour app probably connects to some data source, and it's important to make sure that connection is secure. That data might just be a csv that you have in your GitHub repo, but in many cases it'll be a private data source you connect with via API, on a cloud service, or maybe in your company's VPN.\\n\\nStreamlit has one primary way of securely connecting to private data:\\n\\nSecrets management: securely store secrets like API keys and TOML files that you can then access as environment variables in your app.\\n\\nWe also have a series of guides on how to connect to:\\n\\nAWS S3\\n\\nBigQuery\\n\\nDeta Base\\n\\nFirestore (blog)\\n\\nGoogle Cloud Storage\\n\\nMicrosoft SQL Server\\n\\nMongoDB\\n\\nMySQL\\n\\nPostgreSQL\\n\\nPrivate Google Sheet\\n\\nPublic Google Sheet\\n\\nSnowflake\\n\\nSupabase\\n\\nTableau\\n\\nTiDB\\n\\nTigerGraph\\n\\nTrouble connecting to data? Need a different way to securely connect? Reach out on our Community forum to chat through options!\", metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/connect-data-sources/index.md'}),\n",
       " Document(page_content=\"title: Advanced features\\nslug: /library/advanced-features\\n\\nAdvanced features\\n\\nThis section gives you background on how different parts of Streamlit work.\\n\\n☰ App menu\\n\\nStreamlit provides a configurable menu within your app to access convenient tools for developers and viewers. These options can modify the appearance of your app while running.\\n\\nModify your app's theme while running\\n\\nRecord a screencast of your app\\n\\nDeploy a local app to Streamlit Community Cloud\\n\\nCustomize or hide the app menu\\n\\nCommand-line options\\n\\nWhen you install Streamlit, a command-line (CLI) tool gets installed as well. The purpose of this tool is to run Streamlit apps, change Streamlit configuration options, and help you diagnose and fix issues.\\n\\nWhat is the command-line interface (CLI)?\\n\\nHow to run Streamlit apps from the CLI?\\n\\nView Streamlit version from the CLI?\\n\\nView documentation from the CLI\\n\\nClear cache from the CLI\\n\\nStreamlit configuration\\n\\nStreamlit provides four different ways to set configuration options. Learn how to use each of them to change the behavior of Streamlit.\\n\\nHow to set configuration options?\\n\\nOpt out of telemetry collection\\n\\nView all configuration options\\n\\nTheming\\n\\nThis section provides examples of how Streamlit page elements are affected by the various theme config options.\\n\\nprimaryColor\\n\\nbackgroundcolor\\n\\nsecondarybackgroundcolor\\n\\ntextcolor\\n\\nfont\\n\\nbase\\n\\nCaching\\n\\nThe Streamlit cache allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations. To cache a function in Streamlit, you need to decorate it with one of two decorators: st.cache_data and st.cache_resource.\\n\\nMinimal example\\n\\nBasic usage\\n\\nst.cache_data\\n\\nst.cache_resource\\n\\nDeciding which caching decorator to use\\n\\nAdvanced usage\\n\\nExcluding input parameters\\n\\nControlling cache size and duration\\n\\nCustomizing the spinner\\n\\nUsing Streamlit commands in cached functions\\n\\nMutation and concurrency issues\\n\\nMigrating from st.cache\\n\\nAdd statefulness to apps\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks.\\n\\nWhat is Session State?\\n\\nHow to initialize Session State items?\\n\\nHow to read and update Session State items?\\n\\nHow to use callbacks in Session State?\\n\\nHow to use args and kwargs in callbacks?\\n\\nHow to use callbacks in forms?\\n\\nHow is Session State related to Widget State?\\n\\nCaveats and limitations\\n\\nPre-release features\\n\\nAt Streamlit, we like to move quick while keeping things stable. In our latest effort to move even faster without sacrificing stability, we're offering our bold and fearless users two ways to try out Streamlit's bleeding-edge features.\\n\\nExperimental features\\n\\nNightly releases\\n\\nSecrets management\\n\\nThis section provides examples of how to use secrets management to store and retrieve sensitive information in your Streamlit app.\\n\\nDevelop locally and set up secrets\\n\\nUse secrets in your app\\n\\nError handling\\n\\nUse secrets on Streamlit Community Cloud\\n\\nWorking with timezones\\n\\nWorking with timezones can be tricky. This section provides a high-level description of how to handle timezones in Streamlit to avoid unexpected behavior.\\n\\nOverview\\n\\nHow Streamlit handles timezones\\n\\ndatetime instance without a timezone (naive)\\n\\ndatetime instance with a timezone\\n\\nAdvanced notes on widget behavior\\n\\nWidgets are magical and often work how you want. But they can have surprising behavior in some situations. This section provides is a high-level, abstract description of widget behavior, including some common edge-cases.\", metadata={'source': 'docs/content/library/advanced-features/index.md'}),\n",
       " Document(page_content='title: Experimental cache primitives\\nslug: /library/advanced-features/experimental-cache-primitives\\n\\nThe experimental cache primitives described on this page were deprecated in version 1.18.0. Use st.cache_data or st.cache_resource instead. Learn more in Caching.\\n\\nExperimental cache primitives\\n\\nOverview\\n\\nStreamlit\\'s unique execution model is a part of what makes it a joy to use: your code executes from top to bottom like a simple script for every interaction. There\\'s no need to think about models, views, controllers, or anything of the sort.\\n\\nWhenever your code re-executes, a decorator called @st.cache—which is a powerful primitive for memoization and state storage capabilities—provides a caching mechanism that allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations.\\n\\nHowever, we\\'ve found that @st.cache is hard to use and not fast. You\\'re either faced with cryptic errors like InternalHashError or UnhashableTypeError. Or you need to understand concepts like hash_funcs and allow_output_mutation.\\n\\nOur solutions include two new primitives: st.experimental_memo and st.experimental_singleton. They\\'re conceptually simpler and much, much faster. In some of our internal tests on caching large dataframes, @st.experimental_memo has outperformed @st.cache by an order of magnitude. That\\'s over 10X faster! 🚀\\n\\nLet\\'s take a look at the use-cases these two experimental APIs serve, and how they\\'re a significant improvement over @st.cache.\\n\\nProblem\\n\\n@st.cache was serving the following use-cases:\\n\\nStoring computation results given different kinds of inputs. In Computer Science literature, this is called memoization.\\n\\nInitializing an object exactly once, and reusing that same instance on each rerun for the Streamlit server\\'s lifetime. This is called the singleton pattern.\\n\\nStoring global state to be shared and modified across multiple Streamlit sessions (and, since Streamlit is threaded, you need to pay special attention to thread-safety).\\n\\nAs a result of @st.cache trying to cover too many use-cases under a single unified API, it\\'s both slow and complex.\\n\\nSolution\\n\\nWhile @st.cache tries to solve two very different problems simultaneously (caching data and sharing global singleton objects), these new primitives simplify things by dividing the problem across two different APIs. As a result, they are faster and simpler.\\n\\n@st.experimental_memo\\n\\nUse @st.experimental_memo to store expensive computation which can be \"cached\" or \"memoized\" in the traditional sense. It has almost the exact same API as the existing @st.cache, so you can often blindly replace one for the other:\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_memo\\ndef factorial(n):\\n    if n < 1:\\n        return 1\\n    return n * factorial(n - 1)\\n\\nf10 = factorial(10)\\nf9 = factorial(9)  # Returns instantly!\\n```\\n\\nProperties\\n\\nUnlike @st.cache, this returns cached items by value, not by reference. This means that you no longer have to worry about accidentally mutating the items stored in the cache. Behind the scenes, this is done by using Python\\'s pickle() function to serialize/deserialize cached values.\\n\\nAlthough this uses a custom hashing solution for generating cache keys (like @st.cache), it does not use hash_funcs as an escape hatch for unhashable parameters. Instead, we allow you to ignore unhashable parameters (e.g. database connections) by prefixing them with an underscore.\\n\\nFor example:\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nfrom sqlalchemy.orm import sessionmaker\\n\\n@st.experimental_memo\\ndef get_page(_sessionmaker, page_size, page):\\n    \"\"\"Retrieve rows from the RNA database, and cache them.\\n\\n```\\n\\n@st.experimental_singleton\\n\\n@st.experimental_singleton is a key-value store that\\'s shared across all sessions of a Streamlit app. It\\'s great for storing heavyweight singleton objects across sessions (like TensorFlow/Torch/Keras sessions and/or database connections).\\n\\nExample usage:\\n\\n```python\\nimport streamlit as st\\nfrom sqlalchemy.orm import sessionmaker\\n\\n@st.experimental_singleton\\ndef get_db_sessionmaker():\\n    # This is for illustration purposes only\\n    DB_URL = \"your-db-url\"\\n    engine = create_engine(DB_URL)\\n    return sessionmaker(engine)\\n\\ndbsm = get_db_sessionmaker()\\n```\\n\\nHow this compares to @st.cache:\\n\\nLike @st.cache, this returns items by reference.\\n\\nYou can return any object type, including objects that are not serializable.\\n\\nUnlike @st.cache, this decorator does not have additional logic to check whether you are unexpectedly mutating the cached object. That logic was slow and produced confusing error messages. So, instead, we\\'re hoping that by calling this decorator \"singleton,\" we\\'re nudging you to the correct behavior.\\n\\nThis does not follow the computation graph.\\n\\nYou don\\'t have to worry about hash_funcs! Just prefix your arguments with an underscore to ignore them.\\n\\nSingleton objects can be used concurrently by every user connected to your app, and you are responsible for ensuring that @st.singleton objects are thread-safe. (Most objects you\\'d want to stick inside an @st.singleton annotation are probably already safe—but you should verify this.)\\n\\nWhich to use: memo or singleton?\\n\\nFor example:\\n\\nDataframe computation (pandas, numpy, etc): this is data—use memo\\n\\nStoring downloaded data: memo\\n\\nCalculating pi to n digits: memo\\n\\nTensorflow session: this is a non-data object—use singleton\\n\\nDatabase connection: singleton\\n\\nClear memo and singleton caches procedurally\\n\\nYou can clear caches of functions decorated with @st.experimental_memo and @st.experimental_singleton in code. For example, you can do the following:\\n\\n```python\\n@st.experimental_memo\\ndef square(x):\\n    return x**2\\n\\nif st.button(\"Clear Square\"):\\n    # Clear square\\'s memoized values:\\n    square.clear()\\n\\nif st.button(\"Clear All\"):\\n    # Clear values from all memoized functions:\\n    st.experimental_memo.clear()\\n```\\n\\nPressing the \"Clear Square\" button will clear square()\\'s memoized values. Pressing the \"Clear All\" button will clear memoized values from all functions decorated with @st.experimental_memo.\\n\\nIn summary:\\n\\nAny function annotated with @st.experimental_memo or @st.experimental_singleton gets its own clear() function automatically.\\n\\nAdditionally, you can use st.experimental_memo.clear() and st.experimental_singleton.clear() to clear all memo and singleton caches, respectively.\\n\\nThe commands are experimental, so they\\'re governed by our experimental API process.\\n\\nThese specialized memoization and singleton commands represent a big step in Streamlit\\'s evolution, with the potential to entirely replace @st.cache at some point in 2022.\\n\\nYes, today you may use @st.cache for storing data you pulled in from a database connection (for a Tensorflow session, for caching the results of a long computation like changing the datetime values on a pandas dataframe, etc.). But these are very different things, so we made two new functions that will make it much faster! 💨\\n\\nPlease help us out by testing these commands in real apps and leaving comments in the Streamlit forums.', metadata={'source': 'docs/content/library/advanced-features/experimental-cache-primitives.md'}),\n",
       " Document(page_content='title: Cheat sheet\\nslug: /library/cheatsheet\\n\\nCheat Sheet\\n\\nThis is a summary of the docs, as of Streamlit v1.24.0.\\n\\nInstall & Import\\n\\n```python\\nstreamlit run first_app.py\\n\\nImport convention\\n\\nimport streamlit as st\\n```\\n\\nCommand line\\n\\npython\\nstreamlit --help\\nstreamlit run your_script.py\\nstreamlit hello\\nstreamlit config show\\nstreamlit cache clear\\nstreamlit docs\\nstreamlit --version\\n\\nPre-release features\\n\\npython\\npip uninstall streamlit\\npip install streamlit-nightly --upgrade\\n\\nLearn more about experimental features\\n\\nMagic commands\\n\\n```python\\n\\nMagic commands implicitly\\n\\ncall st.write().\\n\\n\\'This is some Markdown*\\'\\nmy_variable\\n\\'dataframe:\\', my_data_frame\\n\\n```\\n\\nDisplay text\\n\\npython\\nst.text(\\'Fixed width text\\')\\nst.markdown(\\'_Markdown_\\') # see *\\nst.latex(r\\'\\'\\' e^{i\\\\pi} + 1 = 0 \\'\\'\\')\\nst.write(\\'Most objects\\') # df, err, func, keras!\\nst.write([\\'st\\', \\'is <\\', 3]) # see *\\nst.title(\\'My title\\')\\nst.header(\\'My header\\')\\nst.subheader(\\'My sub\\')\\nst.code(\\'for i in range(8): foo()\\')\\n* optional kwarg unsafe_allow_html = True\\n\\nDisplay data\\n\\n```python\\nst.dataframe(my_dataframe)\\nst.table(data.iloc[0:10])\\nst.json({\\'foo\\':\\'bar\\',\\'fu\\':\\'ba\\'})\\nst.metric(\\'My metric\\', 42, 2)\\n\\n```\\n\\nDisplay media\\n\\npython\\nst.image(\\'./header.png\\')\\nst.audio(data)\\nst.video(data)\\n\\nAdd widgets to sidebar\\n\\n```python\\n\\nJust add it after st.sidebar:\\n\\na = st.sidebar.radio(\\'Select one:\\', [1, 2])\\n\\nOr use \"with\" notation:\\n\\nwith st.sidebar:\\n  st.radio(\\'Select one:\\', [1, 2])\\n```\\n\\nColumns\\n\\n```python\\n\\nTwo equal columns:\\n\\ncol1, col2 = st.columns(2)\\ncol1.write(\"This is column 1\")\\ncol2.write(\"This is column 2\")\\n\\nThree different columns:\\n\\ncol1, col2, col3 = st.columns([3, 1, 1])\\n\\ncol1 is larger.\\n\\nYou can also use \"with\" notation:\\n\\nwith col1:\\n  st.radio(\\'Select one:\\', [1, 2])\\n```\\n\\nTabs\\n\\n```python\\n\\nInsert containers separated into tabs:\\n\\ntab1, tab2 = st.tabs([\"Tab 1\", \"Tab2\"])\\ntab1.write(\"this is tab 1\")\\ntab2.write(\"this is tab 2\")\\n\\nYou can also use \"with\" notation:\\n\\nwith tab1:\\n  st.radio(\\'Select one:\\', [1, 2])\\n```\\n\\nControl flow\\n\\n```python\\n\\nStop execution immediately:\\n\\nst.stop()\\n\\nRerun script immediately:\\n\\nst.experimental_rerun()\\n\\nGroup multiple widgets:\\n\\nwith st.form(key=\\'my_form\\'):\\n  username = st.text_input(\\'Username\\')\\n  password = st.text_input(\\'Password\\')\\n  st.form_submit_button(\\'Login\\')\\n```\\n\\nDisplay interactive widgets\\n\\n```python\\nst.button(\\'Click me\\')\\nst.data_editor(\\'Edit data\\', data)\\nst.checkbox(\\'I agree\\')\\nst.radio(\\'Pick one\\', [\\'cats\\', \\'dogs\\'])\\nst.selectbox(\\'Pick one\\', [\\'cats\\', \\'dogs\\'])\\nst.multiselect(\\'Buy\\', [\\'milk\\', \\'apples\\', \\'potatoes\\'])\\nst.slider(\\'Pick a number\\', 0, 100)\\nst.select_slider(\\'Pick a size\\', [\\'S\\', \\'M\\', \\'L\\'])\\nst.text_input(\\'First name\\')\\nst.number_input(\\'Pick a number\\', 0, 10)\\nst.text_area(\\'Text to translate\\')\\nst.date_input(\\'Your birthday\\')\\nst.time_input(\\'Meeting time\\')\\nst.file_uploader(\\'Upload a CSV\\')\\nst.download_button(\\'Download file\\', data)\\nst.camera_input(\"Take a picture\")\\nst.color_picker(\\'Pick a color\\')\\n\\nUse widgets\\' returned values in variables:\\n\\nfor i in range(int(st.number_input(\\'Num:\\'))):\\n  foo()\\nif st.sidebar.selectbox(\\'I:\\',[\\'f\\']) == \\'f\\':\\n  b()\\nmy_slider_val = st.slider(\\'Quinn Mallory\\', 1, 88)\\nst.write(slider_val)\\n\\nDisable widgets to remove interactivity:\\n\\nst.slider(\\'Pick a number\\', 0, 100, disabled=True)\\n```\\n\\nBuild chat-based apps\\n\\n```python\\n\\nInsert a chat message container.\\n\\nwith st.chat_message(\"user\"):\\n   st.write(\"Hello 👋\")\\n   st.line_chart(np.random.randn(30, 3))\\n\\nDisplay a chat input widget.\\n\\nst.chat_input(\"Say something\")\\n```\\n\\nLearn how to build chat-based apps\\n\\nMutate data\\n\\n```python\\n\\nAdd rows to a dataframe after\\n\\nshowing it.\\n\\nelement = st.dataframe(df1)\\nelement.add_rows(df2)\\n\\nAdd rows to a chart after\\n\\nshowing it.\\n\\nelement = st.line_chart(df1)\\nelement.add_rows(df2)\\n```\\n\\nDisplay code\\n\\n```python\\n\\nwith st.echo():\\n  st.write(\\'Code will be executed and printed\\')\\n```\\n\\nPlaceholders, help, and options\\n\\n```python\\n\\nReplace any single element.\\n\\nelement = st.empty()\\nelement.line_chart(...)\\nelement.text_input(...)  # Replaces previous.\\n\\nInsert out of order.\\n\\nelements = st.container()\\nelements.line_chart(...)\\nst.write(\"Hello\")\\nelements.text_input(...)  # Appears above \"Hello\".\\n\\nst.help(pandas.DataFrame)\\nst.get_option(key)\\nst.set_option(key, value)\\nst.set_page_config(layout=\\'wide\\')\\nst.experimental_get_query_params()\\nst.experimental_set_query_params(**params)\\n```\\n\\nConnect to data sources\\n\\n```python\\nst.experimental_connection(\\'pets_db\\', type=\\'sql\\')\\nconn = st.experimental_connection(\\'sql\\')\\nconn = st.experimental_connection(\\'snowpark\\')\\n\\nclass MyConnection(ExperimentalBaseConnection[myconn.MyConnection]):\\n   def _connect(self, kwargs) -> MyConnection:\\n       return myconn.connect(self._secrets, **kwargs)\\n   def query(self, query):\\n      return self._instance.query(query)\\n```\\n\\nOptimize performance\\n\\nCache data objects\\n\\n```python\\n\\nE.g. Dataframe computation, storing downloaded data, etc.\\n\\n@st.cache_data\\n... def foo(bar):\\n...   # Do something expensive and return data\\n...   return data\\n\\nExecutes foo\\n\\nd1 = foo(ref1)\\n\\nDoes not execute foo\\n\\nReturns cached item by value, d1 == d2\\n\\nd2 = foo(ref1)\\n\\nDifferent arg, so function foo executes\\n\\nd3 = foo(ref2)\\n\\nClear all cached entries for this function\\n\\nfoo.clear()\\n\\nClear values from all in-memory or on-disk cached functions\\n\\nst.cache_data.clear()\\n```\\n\\nCache global resources\\n\\n```python\\n\\nE.g. TensorFlow session, database connection, etc.\\n\\n@st.cache_resource\\n... def foo(bar):\\n...   # Create and return a non-data object\\n...   return session\\n\\nExecutes foo\\n\\ns1 = foo(ref1)\\n\\nDoes not execute foo\\n\\nReturns cached item by reference, s1 == s2\\n\\ns2 = foo(ref1)\\n\\nDifferent arg, so function foo executes\\n\\ns3 = foo(ref2)\\n\\nClear all cached entries for this function\\n\\nfoo.clear()\\n\\nClear all global resources from cache\\n\\nst.cache_resource.clear()\\n```\\n\\nDeprecated caching\\n\\n```python\\n\\n@st.cache\\n... def foo(bar):\\n...   # Do something expensive in here...\\n...   return data\\n\\nExecutes foo\\n\\nd1 = foo(ref1)\\n\\nDoes not execute foo\\n\\nReturns cached item by reference, d1 == d2\\n\\nd2 = foo(ref1)\\n\\nDifferent arg, so function foo executes\\n\\nd3 = foo(ref2)\\n```\\n\\nDisplay progress and status\\n\\n```python\\n\\nwith st.spinner(text=\\'In progress\\'):\\n  time.sleep(5)\\n  st.success(\\'Done\\')\\n\\nst.progress(progress_variable_1_to_100)\\nst.balloons()\\nst.snow()\\nst.error(\\'Error message\\')\\nst.warning(\\'Warning message\\')\\nst.info(\\'Info message\\')\\nst.success(\\'Success message\\')\\nst.exception(e)\\n```\\n\\nPersonalize apps for users\\n\\n```python\\n\\nShow different content based on the user\\'s email address.\\n\\nif st.user.email == \\'jane@email.com\\':\\n   display_jane_content()\\nelif st.user.email == \\'adam@foocorp.io\\':\\n   display_adam_content()\\nelse:\\n   st.write(\"Please contact us to get access!\")\\n```', metadata={'source': 'docs/content/library/api-cheat-sheet.md'}),\n",
       " Document(page_content='title: Static file serving\\nslug: /library/advanced-features/static-file-serving\\n\\nStatic file serving\\n\\nStreamlit apps can host and serve small, static media files to support media embedding use cases that\\nwon\\'t work with the normal media elements.\\n\\nTo enable this feature, set enableStaticServing = true under [server] in your config file,\\nor environment variable STREAMLIT_SERVER_ENABLE_STATIC_SERVING=true.\\n\\nMedia stored in the folder ./static/ relative to the running app file is served at path\\napp/static/[filename], such as http://localhost:8501/app/static/cat.png.\\n\\nDetails on usage\\n\\nFiles with the following extensions will be served normally: \".jpg\", \".jpeg\", \".png\", \".gif\". Any other\\n  file will be sent with header Content-Type:text/plain which will cause browsers to render in plain text.\\n  This is included for security - other file types that need to render should be hosted outside the app.\\n\\nStreamlit also sets X-Content-Type-Options:nosniff for all files rendered from the static directory.\\n\\nFor apps running on Streamlit Community Cloud:\\n\\nFiles available in the Github repo will always be served. Any files generated while the app is running,\\n    such as based on user interaction (file upload, etc), are not guaranteed to persist across user sessions.\\n\\nApps which store and serve many files, or large files, may run into resource limits and be shut down.\\n\\nExample usage\\n\\nPut an image cat.png in the folder ./static/\\n\\nAdd enableStaticServing = true under [server] in your .streamlit/config.toml\\n\\nAny media in the ./static/ folder is served at the relative URL like app/static/cat.png\\n\\n```toml\\n\\n.streamlit/config.toml\\n\\n[server]\\nenableStaticServing = true\\n```\\n\\n```python\\n\\napp.py\\n\\nimport streamlit as st\\n\\nwith st.echo():\\n    st.title(\"CAT\")\\n\\n```\\n\\nAdditional resources:\\n\\nhttps://docs.streamlit.io/library/advanced-features/configuration\\n\\nhttps://static-file-serving.streamlit.app/', metadata={'source': 'docs/content/library/advanced-features/static-file-serving.md'}),\n",
       " Document(page_content='title: Add statefulness to apps\\nslug: /library/advanced-features/session-state\\n\\nAdd statefulness to apps\\n\\nWhat is State?\\n\\nWe define access to a Streamlit app in a browser tab as a session. For each browser tab that connects to the Streamlit server, a new session is created. Streamlit reruns your script from top to bottom every time you interact with your app. Each reruns takes place in a blank slate: no variables are shared between runs.\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks. Session state also persists across apps inside a multipage app.\\n\\nIn this guide, we will illustrate the usage of Session State and Callbacks as we build a stateful Counter app.\\n\\nFor details on the Session State and Callbacks API, please refer to our Session State API Reference Guide.\\n\\nAlso, check out this Session State basics tutorial video by Streamlit Developer Advocate Dr. Marisa Smith to get started:\\n\\nBuild a Counter\\n\\nLet\\'s call our script counter.py. It initializes a count variable and has a button to increment the value stored in the count variable:\\n\\n```python\\nimport streamlit as st\\n\\nst.title(\\'Counter Example\\')\\ncount = 0\\n\\nincrement = st.button(\\'Increment\\')\\nif increment:\\n    count += 1\\n\\nst.write(\\'Count = \\', count)\\n```\\n\\nNo matter how many times we press the Increment button in the above app, the count remains at 1. Let\\'s understand why:\\n\\nEach time we press the Increment button, Streamlit reruns counter.py from top to bottom, and with every run, count gets initialized to 0 .\\n\\nPressing Increment subsequently adds 1 to 0, thus count=1 no matter how many times we press Increment.\\n\\nAs we\\'ll see later, we can avoid this issue by storing count as a Session State variable. By doing so, we\\'re indicating to Streamlit that it should maintain the value stored inside a Session State variable across app reruns.\\n\\nLet\\'s learn more about the API to use Session State.\\n\\nInitialization\\n\\nThe Session State API follows a field-based API, which is very similar to Python dictionaries:\\n\\n```python\\nimport streamlit as st\\n\\nCheck if \\'key\\' already exists in session_state\\n\\nIf not, then initialize it\\n\\nif \\'key\\' not in st.session_state:\\n    st.session_state[\\'key\\'] = \\'value\\'\\n\\nSession State also supports the attribute based syntax\\n\\nif \\'key\\' not in st.session_state:\\n    st.session_state.key = \\'value\\'\\n```\\n\\nReads and updates\\n\\nRead the value of an item in Session State by passing the item to st.write :\\n\\n```python\\nimport streamlit as st\\n\\nif \\'key\\' not in st.session_state:\\n    st.session_state[\\'key\\'] = \\'value\\'\\n\\nReads\\n\\nst.write(st.session_state.key)\\n\\nOutputs: value\\n\\n```\\n\\nUpdate an item in Session State by assigning it a value:\\n\\n```python\\nimport streamlit as st\\n\\nif \\'key\\' not in st.session_state:\\n    st.session_state[\\'key\\'] = \\'value\\'\\n\\nUpdates\\n\\nst.session_state.key = \\'value2\\'     # Attribute API\\nst.session_state[\\'key\\'] = \\'value2\\'  # Dictionary like API\\n```\\n\\nStreamlit throws an exception if an uninitialized variable is accessed:\\n\\n```python\\nimport streamlit as st\\n\\nst.write(st.session_state[\\'value\\'])\\n\\nThrows an exception!\\n\\n```\\n\\nLet\\'s now take a look at a few examples that illustrate how to add Session State to our Counter app.\\n\\nExample 1: Add Session State\\n\\nNow that we\\'ve got a hang of the Session State API, let\\'s update our Counter app to use Session State:\\n\\n```python\\nimport streamlit as st\\n\\nst.title(\\'Counter Example\\')\\nif \\'count\\' not in st.session_state:\\n    st.session_state.count = 0\\n\\nincrement = st.button(\\'Increment\\')\\nif increment:\\n    st.session_state.count += 1\\n\\nst.write(\\'Count = \\', st.session_state.count)\\n```\\n\\nAs you can see in the above example, pressing the Increment button updates the count each time.\\n\\nExample 2: Session State and Callbacks\\n\\nNow that we\\'ve built a basic Counter app using Session State, let\\'s move on to something a little more complex. The next example uses Callbacks with Session State.\\n\\nSession State API Reference Guide.\\n\\n```python\\nimport streamlit as st\\n\\nst.title(\\'Counter Example using Callbacks\\')\\nif \\'count\\' not in st.session_state:\\n    st.session_state.count = 0\\n\\ndef increment_counter():\\n    st.session_state.count += 1\\n\\nst.button(\\'Increment\\', on_click=increment_counter)\\n\\nst.write(\\'Count = \\', st.session_state.count)\\n```\\n\\nNow, pressing the Increment button updates the count each time by calling the increment_counter() function.\\n\\nExample 3: Use args and kwargs in Callbacks\\n\\nCallbacks also support passing arguments using the args parameter in a widget:\\n\\n```python\\nimport streamlit as st\\n\\nst.title(\\'Counter Example using Callbacks with args\\')\\nif \\'count\\' not in st.session_state:\\n    st.session_state.count = 0\\n\\nincrement_value = st.number_input(\\'Enter a value\\', value=0, step=1)\\n\\ndef increment_counter(increment_value):\\n    st.session_state.count += increment_value\\n\\nincrement = st.button(\\'Increment\\', on_click=increment_counter,\\n    args=(increment_value, ))\\n\\nst.write(\\'Count = \\', st.session_state.count)\\n```\\n\\nAdditionally, we can also use the kwargs parameter in a widget to pass named arguments to the callback function as shown below:\\n\\n```python\\nimport streamlit as st\\n\\nst.title(\\'Counter Example using Callbacks with kwargs\\')\\nif \\'count\\' not in st.session_state:\\n    st.session_state.count = 0\\n\\ndef increment_counter(increment_value=0):\\n    st.session_state.count += increment_value\\n\\ndef decrement_counter(decrement_value=0):\\n    st.session_state.count -= decrement_value\\n\\nst.button(\\'Increment\\', on_click=increment_counter,\\n    kwargs=dict(increment_value=5))\\n\\nst.button(\\'Decrement\\', on_click=decrement_counter,\\n    kwargs=dict(decrement_value=1))\\n\\nst.write(\\'Count = \\', st.session_state.count)\\n```\\n\\nExample 4: Forms and Callbacks\\n\\nSay we now want to not only increment the count, but also store when it was last updated. We illustrate doing this using Callbacks and st.form:\\n\\n```python\\nimport streamlit as st\\nimport datetime\\n\\nst.title(\\'Counter Example\\')\\nif \\'count\\' not in st.session_state:\\n    st.session_state.count = 0\\n    st.session_state.last_updated = datetime.time(0,0)\\n\\ndef update_counter():\\n    st.session_state.count += st.session_state.increment_value\\n    st.session_state.last_updated = st.session_state.update_time\\n\\nwith st.form(key=\\'my_form\\'):\\n    st.time_input(label=\\'Enter the time\\', value=datetime.datetime.now().time(), key=\\'update_time\\')\\n    st.number_input(\\'Enter a value\\', value=0, step=1, key=\\'increment_value\\')\\n    submit = st.form_submit_button(label=\\'Update\\', on_click=update_counter)\\n\\nst.write(\\'Current Count = \\', st.session_state.count)\\nst.write(\\'Last Updated = \\', st.session_state.last_updated)\\n```\\n\\nAdvanced concepts\\n\\nSession State and Widget State association\\n\\nSession State provides the functionality to store variables across reruns. Widget state (i.e. the value of a widget) is also stored in a session.\\n\\nFor simplicity, we have unified this information in one place. i.e. the Session State. This convenience feature makes it super easy to read or write to the widget\\'s state anywhere in the app\\'s code. Session State variables mirror the widget value using the key argument.\\n\\nWe illustrate this with the following example. Let\\'s say we have an app with a slider to represent temperature in Celsius. We can set and get the value of the temperature widget by using the Session State API, as follows:\\n\\n```python\\nimport streamlit as st\\n\\nif \"celsius\" not in st.session_state:\\n    # set the initial default value of the slider widget\\n    st.session_state.celsius = 50.0\\n\\nst.slider(\\n    \"Temperature in Celsius\",\\n    min_value=-100.0,\\n    max_value=100.0,\\n    key=\"celsius\"\\n)\\n\\nThis will get the value of the slider widget\\n\\nst.write(st.session_state.celsius)\\n```\\n\\nThere is a limitation to setting widget values using the Session State API.\\n\\nStreamlit does not allow setting widget values via the Session State API for st.button and st.file_uploader.\\n\\nThe following example will raise a StreamlitAPIException on trying to set the state of st.button via the Session State API:\\n\\n```python\\nimport streamlit as st\\n\\nif \\'my_button\\' not in st.session_state:\\n    st.session_state.my_button = True\\n    # Streamlit will raise an Exception on trying to set the state of button\\n\\nst.button(\\'Submit\\', key=\\'my_button\\')\\n```\\n\\nSerializable Session State\\n\\nSerialization refers to the process of converting an object or data structure into a format that can be persisted and shared, and allowing you to recover the data’s original structure. Python’s built-in pickle module serializes Python objects to a byte stream (\"pickling\") and deserializes the stream into an object (\"unpickling\").\\n\\nBy default, Streamlit’s Session State allows you to persist any Python object for the duration of the session, irrespective of the object’s pickle-serializability. This property lets you store Python primitives such as integers, floating-point numbers, complex numbers and booleans, dataframes, and even lambdas returned by functions. However, some execution environments may require serializing all data in Session State, so it may be useful to detect incompatibility during development, or when the execution environment will stop supporting it in the future.\\n\\nTo that end, Streamlit provides a runner.enforceSerializableSessionState configuration option that, when set to true, only allows pickle-serializable objects in Session State. To enable the option, either create a global or project config file with the following or use it as a command-line flag:\\n\\n```toml\\n\\n.streamlit/config.toml\\n\\n[runner]\\nenforceSerializableSessionState = true\\n```\\n\\nBy \"pickle-serializable\", we mean calling pickle.dumps(obj) should not raise a PicklingError exception. When the config option is enabled, adding unserializable data to session state should result in an exception. E.g.,\\n\\n```python\\nimport streamlit as st\\n\\ndef unserializable_data():\\n        return lambda x: x\\n\\n👇 results in an exception when enforceSerializableSessionState is on\\n\\nst.session_state.unserializable = unserializable_data()\\n```\\n\\nCaveats and limitations\\n\\nHere are some limitations to keep in mind when using Session State:\\n\\nSession State exists for as long as the tab is open and connected to the Streamlit server. As soon as you close the tab, everything stored in Session State is lost.\\n\\nSession State is not persisted. If the Streamlit server crashes, then everything stored in Session State gets wiped\\n\\nFor caveats and limitations with the Session State API, please see the API limitations.', metadata={'source': 'docs/content/library/advanced-features/session-state.md'}),\n",
       " Document(page_content='title: Working with timezones\\nslug: /library/advanced-features/timezone-handling\\n\\nWorking with timezones\\n\\nIn general, working with timezones can be tricky. Your Streamlit app users are not necessarily in the same timezone as the server running your app. It is especially true of public apps, where anyone in the world (in any timezone) can access your app. As such, it is crucial to understand how Streamlit handles timezones, so you can avoid unexpected behavior when displaying datetime information.\\n\\nHow Streamlit handles timezones\\n\\nStreamlit always shows datetime information on the frontend with the same information as its corresponding datetime instance in the backend. I.e., date or time information does not automatically adjust to the users\\' timezone. We distinguish between the following two cases:\\n\\ndatetime instance without a timezone (naive)\\n\\nWhen you provide a datetime instance without specifying a timezone, the frontend shows the datetime instance without timezone information. For example (this also applies to other widgets like st.dataframe):\\n\\n```python\\nimport streamlit as st\\nfrom datetime import datetime\\n\\nst.write(datetime(2020, 1, 10, 10, 30))\\n\\nOutputs: 2020-01-10 10:30:00\\n\\n```\\n\\nUsers of the above app always see the output as 2020-01-10 10:30:00.\\n\\ndatetime instance with a timezone\\n\\nWhen you provide a datetime instance and specify a timezone, the frontend shows the datetime instance in that same timezone. For example (this also applies to other widgets like st.dataframe):\\n\\n```python\\nimport streamlit as st\\nfrom datetime import datetime\\nimport pytz\\n\\nst.write(datetime(2020, 1, 10, 10, 30, tzinfo=pytz.timezone(\"EST\")))\\n\\nOutputs: 2020-01-10 10:30:00-05:00\\n\\n```\\n\\nUsers of the above app always see the output as 2020-01-10 10:30:00-05:00.\\n\\nIn both cases, neither the date nor time information automatically adjusts to the users\\' timezone on the frontend. What users see is identical to the corresponding datetime instance in the backend. It is currently not possible to automatically adjust the date or time information to the timezone of the users viewing the app.\\n\\nThe legacy version of the st.dataframe has issues with timezones. We do not plan to roll out additional fixes or enhancements for the legacy dataframe. If you need stable timezone support, please consider switching to the arrow serialization by changing the config setting, config.dataFrameSerialization = \"arrow\".', metadata={'source': 'docs/content/library/advanced-features/timezone-handling.md'}),\n",
       " Document(page_content='title: Secrets management\\nslug: /library/advanced-features/secrets-management\\n\\nSecrets management\\n\\nStoring unencrypted secrets in a git repository is a bad practice. For applications that require access to sensitive credentials, the recommended solution is to store those credentials outside the repository - such as using a credentials file not committed to the repository or passing them as environment variables.\\n\\nStreamlit provides native file-based secrets management to easily store and securely access your secrets in your Streamlit app.\\n\\nExisting secrets management tools, such as dotenv files, AWS credentials files, Google Cloud Secret Manager, or Hashicorp Vault, will work fine in Streamlit. We just add native secrets management for times when it\\'s useful.\\n\\nHow to use secrets management\\n\\nDevelop locally and set up secrets\\n\\nStreamlit provides two ways to set up secrets locally using\\xa0TOML\\xa0format:\\n\\nIn a global secrets file at ~/.streamlit/secrets.toml for macOS/Linux or %userprofile%/.streamlit/secrets.toml for Windows:\\n\\n```toml\\n   # Everything in this section will be available as an environment variable\\n   db_username = \"Jane\"\\n   db_password = \"mypassword\"\\n\\n# You can also add other sections if you like.\\n   # The contents of sections as shown below will not become environment variables,\\n   # but they\\'ll be easily accessible from within Streamlit anyway as we show\\n   # later in this doc.\\n   [my_other_secrets]\\n   things_i_like = [\"Streamlit\", \"Python\"]\\n   ```\\n\\nIf you use the global secrets file, you don\\'t have to duplicate secrets across several project-level files if multiple Streamlit apps share the same secrets.\\n\\nIn a per-project secrets file at $CWD/.streamlit/secrets.toml, where $CWD is the folder you\\'re running Streamlit from. If both a global secrets file and a per-project secrets file exist, secrets in the per-project file overwrite those defined in the global file.\\n\\nAdd this file to your .gitignore so you don\\'t commit your secrets!\\n\\nUse secrets in your app\\n\\nAccess your secrets by querying the\\xa0st.secrets\\xa0dict, or as environment variables. For example, if you enter the secrets from the section above, the code below shows you how to access them within your Streamlit app.\\n\\n```python\\nimport streamlit as st\\n\\nEverything is accessible via the st.secrets dict:\\n\\nst.write(\"DB username:\", st.secrets[\"db_username\"])\\nst.write(\"DB password:\", st.secrets[\"db_password\"])\\n\\nAnd the root-level secrets are also accessible as environment variables:\\n\\nimport os\\n\\nst.write(\\n    \"Has environment variables been set:\",\\n    os.environ[\"db_username\"] == st.secrets[\"db_username\"],\\n)\\n```\\n\\nYou can access st.secrets via attribute notation (e.g. st.secrets.key), in addition to key notation (e.g. st.secrets[\"key\"]) — like st.session_state.\\n\\nYou can even compactly use TOML sections to pass multiple secrets as a single attribute. Consider the following secrets:\\n\\ntoml\\n[db_credentials]\\nusername = \"my_username\"\\npassword = \"my_password\"\\n\\nRather than passing each secret as attributes in a function, you can more compactly pass the section to achieve the same result. See the notional code below, which uses the secrets above:\\n\\n```python\\n\\nVerbose version\\n\\nmy_db.connect(username=st.secrets.db_credentials.username, password=st.secrets.db_credentials.password)\\n\\nFar more compact version!\\n\\nmy_db.connect(**st.secrets.db_credentials)\\n```\\n\\nError handling\\n\\nHere are some common errors you might encounter when using secrets management.\\n\\nIf a .streamlit/secrets.toml is created while the app is running, the server needs to be restarted for changes to be reflected in the app.\\n\\nIf you try accessing a secret, but no secrets.toml file exists, Streamlit will raise a FileNotFoundError exception:\\n\\nIf you try accessing a secret that doesn\\'t exist, Streamlit will raise a KeyError exception:\\n\\n```python\\n  import streamlit as st\\n\\nst.write(st.secrets[\"nonexistent_key\"])\\n  ```\\n\\nUse secrets on Streamlit Community Cloud\\n\\nWhen you deploy your app to Streamlit Community Cloud, you can use the same secrets management workflow as you would locally. However, you\\'ll need to also set up your secrets in the Community Cloud Secrets Management console. Learn how to do so via the Cloud-specific Secrets management documentation.', metadata={'source': 'docs/content/library/advanced-features/secrets-management.md'}),\n",
       " Document(page_content='title: Changelog\\nslug: /library/changelog\\n\\nChangelog\\n\\nThis page lists highlights, bug fixes, and known issues for official Streamlit releases. If you\\'re looking for information about nightly releases, beta features, or experimental features, see Try pre-release features.\\n\\nTo upgrade to the latest version of Streamlit, run:\\n\\nbash\\npip install --upgrade streamlit\\n\\nVersion 1.24.0\\n\\nRelease date: June 27, 2023\\n\\nHighlights\\n\\n💬 Introducing st.chat_message and st.chat_input — two new chat elements that let you build conversational apps. Learn how to use these features in your LLM-powered chat apps in our tutorial.\\n\\n💾\\xa0Streamlit\\'s caching decorators now allow you to customize Streamlit\\'s hashing of input parameters with the keyword-only argument hash_funcs.\\n\\nNotable Changes\\n\\n🐍\\xa0We\\'ve deprecated support for Python 3.7 in the core library and Streamlit Community Cloud (#6868).\\n\\n📅\\xa0st.cache_data and st.cache_resource can hash timezone-aware datetime objects (#6812, #6690, #5110).\\n\\nOther Changes\\n\\n✨\\xa0Visual design tweaks to Streamlit\\'s input widgets (#6817).\\n\\n🐛\\xa0Bug fix: st.write pretty-prints dataclasses using st.help (#6750).\\n\\n🪲\\xa0Bug fix: st.button\\'s height is consistent with that of other widgets (#6738).\\n\\n🐜\\xa0Bug fix: Upgraded the react-range frontend dependency to fix the memory usage of sliders (#6764, #5436). Thanks @wolfd!\\n\\n🐝\\xa0Bug fix: Pydantic validators no longer result in exceptions on app reruns (#6664, #3218).\\n\\n🐞\\xa0Bug fix: streamlit config show honors newlines (#6758, #2868).\\n\\n🪰\\xa0Bug fix: Fixed a race condition to ensure Streamlit reruns the latest code when the file changes (#6884).\\n\\n🦋\\xa0Bug fix: Apps no longer rerun when users click anchor links (#6834, #6500).\\n\\n🕸️\\xa0Bug fix: Added robust out-of-bounds checks for min_value and max_value in st.number_input (#6847, #6797).\\n\\nVersion 1.23.0\\n\\nRelease date: June 1, 2023\\n\\nHighlights\\n\\n✂️ Announcing the general availability of st.data_editor, a widget that allows you to edit DataFrames and many other data structures in a table-like UI. Breaking change: the data editor\\'s representation used in st.session_state was altered. Find out more about the new format in Access edited data.\\n\\n⚙️ Introducing the Column configuration API with a suite of methods to configure the display and editing behavior of st.dataframe and st.data_editor columns (e.g. their title, visibility, type, or format). Keep an eye out for a detailed blog post and in-depth documentation upcoming in the next two weeks.\\n\\n🔌 Learn to use st.experimental_connection to create and manage data connections in your apps with the new Connecting to data docs and video tutorial.\\n\\nNotable Changes\\n\\n📊\\xa0Streamlit now supports Protobuf 4 and Altair 5 (#6215, #6618, #5626, #6622).\\n\\n☎️ st.dataframe and st.data_editor can hide index columns with hide_index, specify the display order of columns with column_order, and disable editing for individual columns with the disabled parameter.\\n\\n⏱️ The ttl parameter in st.cache_data and st.cache_resource accepts formatted strings, so you can simply say ttl=\"30d\", ttl=\"1h30m\" and any other combination of w, d, h, m, s supported by Pandas\\'s Timedelta constructor (#6560).\\n\\n📂 st.file_uploader now interprets the type parameter more accurately. For example, \"jpg\" or \".jpg\" now accept both \"jpg\" and \"jpeg\" extensions. This functionality has also been extended to \"mpeg/mpg\", \"tiff/tif\", \"html/htm\", and \"mpeg4/mp4\".\\n\\n🤫\\xa0The new global.disableWidgetStateDuplicationWarning configuration option allows the silencing of warnings triggered by setting widget default values and keyed session state values concurrently (#3605, #6640). Thanks, @antonAce!\\n\\nOther Changes\\n\\n🏃\\u200d♀️Improved startup time by lazy loading some dependencies (#6531).\\n\\n👋 Removed st.beta_* and st.experimental_show due to deprecation and low-use (#6558)\\n\\n🚀\\xa0Further improvements to st.dataframe and st.data_editor:\\n\\nImproved editing on mobile devices for the data editor (#6548).\\n\\nAll editable columns have an icon in their column header and support tooltips (#6550, #6561).\\n\\nEnable editing for columns containing datetime, date, or time values (#6025).\\n\\nNew input validation options for columns in the data editor, such as max_chars and validate for text columns, and min_value, max_value and step for number columns (#6563).\\n\\nImproved type parsing capabilities in the data editor (#6551).\\n\\nUnified missing values to None in returned data structures (#6544).\\n\\nA warning is shown in cells when integers exceed the maximum safe value of (2^53) -1 (#6311, #6549).\\n\\nPrevented editing the sessions state by showing a warning (#6634).\\n\\nFixed issues with list columns sometimes breaking the frontend (#6644).\\n\\nFixed a display issue with index columns using category dtype (#6680, #6598).\\n\\nFixed an issue that prevented a rerun when adding empty rows (#6598).\\n\\nUnified the behavior between st.data_editor and st.dataframe related to auto-hiding the index column(s) based on the input data (#6659, #6598)\\n\\n🛡️\\xa0Streamlit\\'s Security Policy can be found in its GitHub repository (#6666).\\n\\n🤏 Documented the integer size limit for st.number_input and st.slider (#6724).\\n\\n🐍\\xa0The majority of Streamlit\\'s Python dependencies have set a maximum allowable version, with the standard upper limit set to the next major version, but not inclusive of it (#6691).\\n\\n💅\\xa0UI design improvements to in-app modals (#6688).\\n\\n🐞\\xa0Bug fix: st.date_input\\'s date selector is equally visible in dark mode (#6072, #6630).\\n\\n🐜\\xa0Bug fix: the sidebar navigation expansion indicator in multipage apps is restored (#6731).\\n\\n🐛\\xa0Bug fix: The docstring and exception message for st.set_page_config have been updated to clarify that this command can be invoked once for each page within a multipage app, rather than once per entire app (#6594).\\n\\n🐝\\xa0Bug fix: st.json\\xa0no longer collapses multiple spaces in both keys and values with single space when rendered (#6657, #6663).\\n\\nVersion 1.22.0\\n\\nRelease date: April 27, 2023\\n\\nHighlights\\n\\n🔌\\xa0Introducing st.experimental_connection: Easily connect your app to data sources and APIs using our new connection feature. Find more details in the API reference, and stay tuned for an upcoming blog post and in-depth documentation! In the meantime, explore our updated MySQL and Snowflake connection tutorials for examples of this feature.\\n\\nNotable Changes\\n\\n🐼\\xa0Streamlit now supports Pandas 2.0 (#6413, #6378, #6507). Thanks, connortann!\\n\\n🍔\\xa0Customize the visibility of items in the toolbar, options menu, and the settings dialog using the client.toolbarMode config option (#6174).\\n\\n🪵\\xa0Streamlit logs now reside in the \"streamlit\" namespace instead of the root logger, enabling app developers to better manage log handling (#3978, #6377).\\n\\nOther Changes\\n\\n🔏\\xa0CLI parameters can no longer be used to set sensitive configuration values (#6376).\\n\\n🤖\\xa0Improved the debugging experience by reducing log noise (#6391).\\n\\n🐞\\xa0Bug fix:\\xa0@st.cache_data decorated functions support UUID objects as parameters (#6440, #6459).\\n\\n🐛\\xa0Bug fix: Tabbing through buttons and other elements now displays a red border only when focused, not when clicked (#6373).\\n\\n🪲\\xa0Bug fix: st.multiselect\\'s clear icon is larger and includes a hover effect (#6471).\\n\\n🐜\\xa0Bug fix: Custom theme font settings no longer apply to code blocks (#6484, #6535).\\n\\n©️\\xa0Bug fix: st.code\\'s copy-to-clipboard button appears when you hover on code blocks (#6490, #6498).\\n\\nVersion 1.21.0\\n\\nRelease date: April 6, 2023\\n\\nHighlights\\n\\n📏 Introducing st.divider — a command that displays a horizontal line in your app. Learn how to use this command in its API reference.\\n\\n🔏 Streamlit now supports the use of a global secrets.toml file, in addition to a project-level file, to easily store and securely access your secrets. Learn more in Secrets management.\\n\\n🚀 st.help has been revamped to show more information about object methods, attributes, classes, and more, which is great for debugging (#5857, #6382)!\\n\\nNotable Changes\\n\\n🪜 st.time_input supports adding a stepping interval with the keyword-only step parameter (#6071).\\n\\n❓ Most text elements can include tooltips with the help parameter (#6043).\\n\\n↔️ st.pyplot has a use_container_width parameter to set the chart to the container width (now all chart elements support this parameter) (#6067).\\n\\n👩\\u200d💻 st.code supports optionally displaying line numbers to the code block\\'s left with the boolean line_numbers parameter (#5756, #6042).\\n\\n⚓ Anchors in header elements can be turned off by setting anchor=False (#6158).\\n\\nOther Changes\\n\\n🐼\\xa0st.table and st.dataframe support pandas.Period, and number and boolean types in categorical columns (#2547, #5429, #5329, #6248).\\n\\n🕸️\\xa0Added .webp to the list of allowed static file extensions (#6331)\\n\\n🐞\\xa0Bug fix: stop script execution on websocket close to immediately clear session information (#6166, #6204).\\n\\n🐜\\xa0Bug fixes: updated allowed/disallowed label markdown behavior such that unsupported elements are unwrapped and only their children (text contents) render (#5872, #6036, #6054, #6163).\\n\\n🪲\\xa0Bug fixes: don\\'t push browser history states on rerun, use HTTPS to load external resources in streamlit hello, and make the browser back button work for multipage apps (#5292, #6266, #6232). Thanks, whitphx!\\n\\n🐝\\xa0Bug fix: avoid showing emoji on non-UTF-8 terminals. (#2284, #6088). Thanks, kcarnold!\\n\\n📁\\xa0Bug fix: override default use of\\xa0File System Access API for\\xa0react-dropzone so that st.file_uploader\\'s File Selection Dialog only shows file types corresponding to those included in the type parameter (#6176, #6315).\\n\\n💾\\xa0Bug fix: make the .clear() method on cache-decorated functions work (#6310, #6321).\\n\\n🏃\\xa0Bug fix: st.experimental_get_query_params doesn\\'t need reruns to work (#6347, #6348). Thanks, PaleNeutron!\\n\\n🐛\\xa0Bug fix: CachedStFunctionWarning mentions experimental_allow_widgets instead of the deprecated suppress_st_warning (#6216, #6217).\\n\\nVersion 1.20.0\\n\\nRelease date: March 09, 2023\\n\\nNotable Changes\\n\\n🔐\\xa0Added support for configuring SSL to\\xa0serve apps directly over HTTPS\\xa0(#5969).\\n\\n🖼️\\xa0Granular control over app embedding behavior with the /?embed and /?embed_options query parameters. Learn how to use this feature in our docs (#6011, #6019).\\n\\n⚡\\xa0Enabled the runner.fastReruns configuration option by default to make apps much more responsive to user interaction (#6200).\\n\\nOther Changes\\n\\n🍔\\xa0Cleaned up the hamburger menu by removing the least used options (#6080).\\n\\n🖨️\\xa0Design changes to ensure apps being printed or saved as a PDF look good (#6180).\\n\\n🐞\\xa0Bug fix: improved dtypes checking in st.experimental_data_editor (#6185, #6188).\\n\\n🐛\\xa0Bug fix: properly position st.metric\\'s help tooltip when not inside columns (#6168).\\n\\n🪲\\xa0Bug fix: regression in retrieving messages from the server\\'s ForwardMsgCache (#6210).\\n\\n🌀\\xa0Bug fix: st.cache_data docstring for the show_spinner param now lists str as a supported type (#6207, #6213).\\n\\n⏱️\\xa0Made ping and websocket timeouts far more forgiving (#6212).\\n\\n🗺️\\xa0st.map and st.pydeck_chart docs state that Streamlit\\'s Mapbox token will not work indefinitely (#6143).\\n\\nVersion 1.19.0\\n\\nRelease date: February 23, 2023\\n\\nHighlights\\n\\n✂️\\xa0Introducing st.experimental_data_editor, a widget that allows you to edit DataFrames and many other data structures in a table-like UI. Read more in our documentation and blog post.\\n\\nOther Changes\\n\\n✨ Streamlit\\'s GitHub README got a new look (#6016).\\n\\n🌚\\xa0Improved readability of styled dataframe cells in dark mode (#6060, #6098).\\n\\n🐛\\xa0Bug fix: make apps work again in the latest versions of Safari, and in Chrome with third-party cookies blocked (#6092, #6094, #6087, #6100).\\n\\n🐞\\xa0Bug fix: refer to new cache primitives in the “Clear cache\" dialog and error messages (#6082, #6128).\\n\\n🐝\\xa0Bug fix: properly cache class member functions and instance methods (#6109, #6114).\\n\\n🐜\\xa0Bug fix: regression in st.metric tooltip position (#6093, #6129).\\n\\n🪲\\xa0Bug fix: allow fullscreen button to show for dataframes, charts, etc, in expander (#6083, #6148).\\n\\nVersion 1.18.0\\n\\nRelease date: February 09, 2023\\n\\nHighlights\\n\\n🎊\\xa0Introducing\\xa0@st.cache_data\\xa0and\\xa0@st.cache_resource\\xa0— two new caching commands to replace\\xa0st.cache! Check out our\\xa0blog post\\xa0and\\xa0documentation\\xa0for more information.\\n\\nNotable Changes\\n\\n🪆\\xa0st.columns supports up to one level of column nesting (i.e., columns inside columns) in the main area of the app.\\n\\n⏳\\xa0st.progress supports adding a message to display above the progress bar with the text keyword parameter.\\n\\n↔️ st.button has an optional\\xa0use_container_width\\xa0parameter to allow you to stretch buttons across the full container width.\\n\\n🐍 We formally added support for Python 3.11.\\n\\n🖨️\\xa0Save your app as a PDF via the “Print\" option in your app\\'s hamburger menu.\\n\\n🛎️\\xa0Apps can serve small, static media files via the enableStaticServing config option. See our documentation on how to use this feature and our demo app for an example.\\n\\nOther Changes\\n\\n🏁\\xa0All Streamlit endpoints (including /healthz) have been renamed to have a consistent pattern and avoid any clashes with reserved endpoints of GCP (notably Cloud Run and App Engine) (#5534).\\n\\n⚡\\xa0Improved caching performance when multiple sessions access an uncomputed cached value simultaneously (#6017).\\n\\n🚧\\xa0Streamlit only displays deprecation warnings in the browser when the client.showErrorDetails config option is set to True. Deprecation warnings always get logged to the console, regardless of whether they\\'re displayed in-browser (#5945).\\n\\n🏓\\xa0Refactored the st.dataframe internals to improve dataframe handling and conversion, such as detecting more types, converting key-value dicts to dataframes, and more (#6026, #6023).\\n\\n💽 The behavior of widget labels when they are passed unsupported Markdown elements is documented (#5978).\\n\\n📊\\xa0Bug fix: Plotly improvements — upgraded multiple frontend dependencies, including Plotly, to the latest version to properly redraw cached charts, make Plotly mapbox animations work, and allow users to update the figure layout when using the Streamlit theme (#5885, #5967, #6055).\\n\\n📶\\xa0Bug fix: allow browser tabs that transiently disconnect (due to a network blip, load balancer timeout, etc.) to avoid losing all of their state (#5856).\\n\\n📱 Bug fix: the keyboard is hidden on mobile when st.selectbox and st.multiselect have less than 10 options (#5979).\\n\\n🐝\\xa0Bug fix: design tweaks to st.metric, st.multiselect, st.tabs , and menu items to prevent label overflow and scrolling issues, especially with small viewport sizes (#5933, #6034).\\n\\n🐞\\xa0Bug fix: switched to a functioning Twemoji URL from which page favicons are loaded in st.set_page_config (#5943).\\n\\n✍️ More type hints (#5986). Thanks, harahu!\\n\\nVersion 1.17.0\\n\\nRelease date: January 12, 2023\\n\\nNotable Changes\\n\\n🪄\\xa0@st.experimental_singleton supports an optional validate parameter that accepts a validation function for cached data and is called each time the cached value is accessed.\\n\\n💾\\xa0 @st.experimental_memo\\'s persist parameter can also accept booleans.\\n\\nOther Changes\\n\\n📟\\xa0Multipage apps exclude __init__.py from the page selector (#5890).\\n\\n📐\\xa0The iframes of embedded apps have the ability to dynamically resize their height (#5894).\\n\\n🐞\\xa0Bug fix: thumb values of range sliders respect the container width (#5913).\\n\\n🪲\\xa0Bug fix: all examples in docstrings of Streamlit commands contain relevant imports to make them reproducible (#5877).\\n\\nVersion 1.16.0\\n\\nRelease date: December 14, 2022\\n\\nHighlights\\n\\n👩\\u200d🎨\\xa0Introducing a new Streamlit theme for Altair, Plotly, and Vega-Lite charts! Check out our blog post for more information.\\n\\n🎨\\xa0Streamlit now supports colored text in all commands that accept Markdown, including st.markdown, st.header, and more. Learn more in our documentation.\\n\\nNotable Changes\\n\\n🔁\\xa0Functions cached with st.experimental_memo or st.experimental_singleton can contain Streamlit media elements and forms.\\n\\n⛄\\xa0All Streamlit commands that accept pandas DataFrames as input also support Snowpark and PySpark DataFrames.\\n\\n🏷\\xa0st.checkbox and st.metric can customize how to hide their labels with the label_visibility parameter.\\n\\nOther Changes\\n\\n🗺️\\xa0st.map improvements: support for upper case columns and better exception messages (#5679, #5792).\\n\\n🐞\\xa0Bug fix: st.plotly_chart respects the figure\\'s height attribute and the use_container_width parameter (#5779).\\n\\n🪲\\xa0Bug fix: all commands with the icon parameter such as st.error, st.warning, etc, can contain emojis with variant selectors (#5583).\\n\\n🐝\\xa0Bug fix: prevent st.camera_input from jittering when resizing the browser window (#5661).\\n\\n🐜\\xa0Bug fix: update exception layout to avoid overflow of stack traces (#5700).\\n\\nVersion 1.15.0\\n\\nRelease date: November 17, 2022\\n\\nNotable Changes\\n\\n💅\\xa0Widget labels can contain inline Markdown. See our docs and demo app for more info.\\n\\n🎵 st.audio now supports playing audio data passed in as NumPy arrays with the keyword-only sample_rate parameter.\\n\\n🔁\\xa0Functions cached with st.experimental_memo or st.experimental_singleton can contain Streamlit widgets using the experimental_allow_widgets parameter. This allows caching checkboxes, sliders, radio buttons, and more!\\n\\nOther Changes\\n\\n👩\\u200d🎨\\xa0Design tweak to prevent jittering in sliders (#5612).\\n\\n🐛\\xa0Bug fix: links in headers are red, not blue (#5609).\\n\\n🐞\\xa0Bug fix: properly resize Plotly charts when exiting fullscreen (#5645).\\n\\n🐝: Bug fix: don\\'t accidentally trigger st.balloons and st.snow (#5401).\\n\\nVersion 1.14.0\\n\\nRelease date: October 27, 2022\\n\\nHighlights\\n\\n🎨\\xa0st.button and st.form_submit_button support designating buttons as \"primary\" (for additional emphasis) or \"secondary\" (for normal buttons) with the type keyword-only parameter.\\n\\nNotable Changes\\n\\n🤏\\xa0st.multiselect has a keyword-only max_selections parameter to limit the number of options that can be selected at a time.\\n\\n📄\\xa0st.form_submit_button now has the disabled parameter that removes interactivity.\\n\\nOther Changes\\n\\n🏓\\xa0st.dataframe and st.table accept categorical intervals as input (#5395).\\n\\n⚡\\xa0Performance improvements to Plotly charts (#5542).\\n\\n🪲\\xa0Bug fix: st.download_button supports non-latin1 characters in filenames (#5465).\\n\\n🐞\\xa0Bug fix: Allow st.image to render a local GIF as a GIF, not as a static PNG (#5438).\\n\\n📱\\xa0Design tweaks to the sidebar in multipage apps (#5538, #5445, #5559).\\n\\n📊\\xa0Improvements to the axis configuration for built-in charts (#5412).\\n\\n🔧\\xa0Memo and singleton improvements: support text values for show_spinner, use datetime.timedelta objects as ttl parameter value, properly hash PIL images and Enum classes, show better error messages when returning unevaluated dataframes (#5447, #5413, #5504, #5426, #5515).\\n\\n🔍\\xa0Zoom buttons in maps created with st.map and st.pydeck_chart use light or dark style based on the app\\'s theme (#5479).\\n\\n🗜\\xa0Websocket headers from the current session\\'s incoming WebSocket request can be obtained from a new \"internal\" (i.e.: subject to change without deprecation) API (#5457).\\n\\n📝\\xa0Improve the text that gets printed when you first install and use Streamlit (#5473).\\n\\nVersion 1.13.0\\n\\nRelease date: September 22, 2022\\n\\nNotable Changes\\n\\n🏷\\xa0Widgets can customize how to hide their labels with the label_visibility parameter.\\n\\n🔍 st.map adds zoom buttons to the map by default.\\n\\n↔️\\xa0st.dataframe\\xa0supports the\\xa0use_container_width\\xa0parameter to stretch across the full container width.\\n\\n🪄 Improvements to\\xa0st.dataframe\\xa0sizing: Column width calculation respects column headers, supports double click between column headers to autosize, better fullscreen support, and fixes the issue with the\\xa0width\\xa0parameter.\\n\\nOther Changes\\n\\n⌨️ st.time_input allows for keyboard-only input (#5194).\\n\\n💿 st.memo will warn the user when using\\xa0ttl\\xa0and\\xa0persist\\xa0keyword argument together (#5032).\\n\\n🔢\\xa0st.number_input returns consistent type after rerun (#5359).\\n\\n🚒\\xa0st.sidebar UI fixes including a fix for scrollbars in Firefox browsers (#5157, #5324).\\n\\n👩\\u200d💻\\xa0Improvements to usage metrics to guide API development.\\n\\n✍️\\xa0More type hints! (#5191, #5192, #5242, #5243, #5244, #5245, #5246) Thanks harahu!\\n\\nVersion 1.12.0\\n\\nRelease date: August 11, 2022\\n\\nHighlights\\n\\n📊\\xa0Built-in charts (e.g. st.line_chart) get a brand-new look and parameters x and y! Check out our blog post for more information.\\n\\nNotable Changes\\n\\n⏯\\xa0Functions cached with st.experimental_memo or st.experimental_singleton can now contain static st commands. This allows caching text, charts, dataframes, and more!\\n\\n↔️\\xa0The sidebar is now resizable via drag and drop.\\n\\n☎️\\xa0st.info, st.success, st.error, and st.warning got a redesign and have a new keyword-only parameter: icon.\\n\\nOther Changes\\n\\n🎚️\\xa0st.select_slider correctly handles all floats now (#4973, #4978).\\n\\n🔢\\xa0st.multi_select can take values from enums (#4987).\\n\\n🍊\\xa0st.slider range values can now be set through st.session_state (#5007).\\n\\n🎨\\xa0st.progress got a redesign (#5011, #5086).\\n\\n🔘\\xa0st.radio better deals with list-like dataframes (#5021).\\n\\n🧞\\u200d♂️\\xa0st.cache properly handles JSON files now (#5023).\\n\\n⚓️ Headers render markdown now when the anchor parameter is set (#5038).\\n\\n🗻\\xa0st.image can now load SVGs from Inkscape (#5040).\\n\\n🗺️\\xa0st.map and st.pydeck_chart use light or dark style based on the app\\'s theme (#5074, #5108).\\n\\n🎈\\xa0Clicks on elements below\\xa0st.balloons and st.snow don\\'t get blocked anymore (#5098).\\n\\n🔝\\xa0Embedded apps have lower top padding (#5111).\\n\\n💅\\xa0Adjusted padding and alignment for widgets, charts, and dataframes (#4995, #5061, #5081).\\n\\n✍️\\xa0More type hints! (#4926, #4932, #4933)\\n\\nVersion 1.11.0\\n\\nRelease date: July 14, 2022\\n\\nHighlights\\n\\n🗂\\xa0Introducing st.tabs to have tab containers in your app. See our documentation on how to use this feature.\\n\\nNotable Changes\\n\\nℹ️\\xa0st.metric supports tooltips with the help keyword parameter.\\n\\n🚇\\xa0st.columns supports setting the gap size between columns with the gap keyword parameter.\\n\\nOther Changes\\n\\n💅\\xa0Design tweaks to st.selectbox, st.expander, st.spinner (#4801).\\n\\n📱\\xa0The sidebar will close when users select a page from the navigation menu on mobile devices (#4851).\\n\\n🧠\\xa0st.memo supports dataclasses! (#4850)\\n\\n🏎\\xa0Bug fix for a race condition that destroyed widget state with rapid interaction (#4882).\\n\\n🏓\\xa0st.table presents overflowing content to be scrollable when placed inside columns and expanders (#4934).\\n\\n🐍\\xa0Types: More updated type annotations across Streamlit! (#4808, #4809, #4856)\\n\\nVersion 1.10.0\\n\\nRelease date: June 2, 2022\\n\\nHighlights\\n\\n📖 Introducing native support for multipage apps! Check out our blog post and try out our new streamlit hello.\\n\\nNotable Changes\\n\\n✨ st.dataframe has been redesigned.\\n\\n🔘 st.radio has a horizontal keyword-only parameter to display options horizontally.\\n\\n⚠️ Streamlit Community Cloud will support richer exception formatting.\\n\\n🏂 Get user information on private apps using st.experimental_user.\\n\\nOther Changes\\n\\n📊 Upgraded Vega-Lite library to support even more interactive charting improvements. See their release notes to find out more. (#4751).\\n\\n📈 st.vega_lite_chart will respond to updates, particularly in response to input widgets (#4736).\\n\\n💬 st.markdown with long text will always wrap (#4696).\\n\\n📦 Support for PDM (#4724).\\n\\n✍️ Types: Updated type annotations across Streamlit! (#4679, #4680, #4681, #4682, #4683, #4684, #4685, #4686, #4687, #4688, #4690, #4703, #4704, #4705, #4706, #4707, #4708, #4710, #4723, #4733).\\n\\nVersion 1.9.0\\n\\nRelease date: May 4, 2022\\n\\nNotable Changes\\n\\n🪗 st.json now supports a keyword-only argument, expanded on whether the JSON should be expanded by default (defaults to True).\\n\\n🏃\\u200d♀️ More performance improvements from reducing redundant work each script run.\\n\\nOther Changes\\n\\n🏇 Widgets when disabled is set/unset will maintain its value (#4527).\\n\\n🧪 Experimental feature to increase the speed of reruns using configuration runner.fastReruns. See #4628 for the known issues in enabling this feature.\\n\\n🗺️ DataFrame timestamps support UTC offset (in addition to time zone notation) (#4669).\\n\\nVersion 1.8.0\\n\\nRelease date: March 24, 2022\\n\\nNotable Changes\\n\\n🏃\\u200d♀️\\xa0Dataframes should see performance improvements (#4463).\\n\\nOther Changes\\n\\n🕰\\xa0st.slider handles timezones better by removing timezone conversions on the backend (#4348).\\n\\n👩\\u200d🎨\\xa0Design improvements to our header (#4496).\\n\\nVersion 1.7.0\\n\\nRelease date: March 3, 2022\\n\\nHighlights\\n\\nIntroducing st.snow, celebrating our acquisition by Snowflake! See more information in our blog post.\\n\\nVersion 1.6.0\\n\\nRelease date: Feb 24, 2022\\n\\nOther Changes\\n\\n🗜\\xa0WebSocket compression is now disabled by default, which will improve CPU and latency performance for large dataframes. You can use the\\xa0server.enableWebsocketCompression configuration option to re-enable it if you find the increased network traffic more impactful.\\n\\n☑️\\xa0🔘\\xa0Radio and checkboxes improve focus on Keyboard navigation (#4308).\\n\\nVersion 1.5.0\\n\\nRelease date: Jan 27, 2022\\n\\nNotable Changes\\n\\n🌟 Favicon defaults to a PNG to allow for transparency (#4272).\\n\\n🚦 Select Slider Widget now has the disabled parameter that removes interactivity (completing all of our widgets) (#4314).\\n\\nOther Changes\\n\\n🔤 Improvements to our markdown library to provide better support for HTML (specifically nested HTML) (#4221).\\n\\n📖 Expanders maintain their expanded state better when multiple expanders are present (#4290).\\n\\n🗳 Improved file uploader and camera input to call its on_change handler only when necessary (#4270).\\n\\nVersion 1.4.0\\n\\nRelease date: Jan 13, 2022\\n\\nHighlights\\n\\n📸 Introducing st.camera_input for uploading images straight from your camera.\\n\\nNotable Changes\\n\\n🚦 Widgets now have the disabled parameter that removes interactivity.\\n\\n🚮 Clear st.experimental_memo and st.experimental_singleton programmatically by using the clear() method on a cached function.\\n\\n📨 Developers can now configure the maximum size of a message to accommodate larger messages within the Streamlit application. See server.maxMessageSize.\\n\\n🐍 We formally added support for Python 3.10.\\n\\nOther Changes\\n\\n😵\\u200d💫 Calling str or repr on threading.current_thread() does not cause a RecursionError (#4172).\\n\\n📹 Gracefully stop screencast recording when user removes permission to record (#4180).\\n\\n🌇 Better scale images by using a higher-quality image bilinear resampling algorithm (#4159).\\n\\nVersion 1.3.0\\n\\nRelease date: Dec 16, 2021\\n\\nNotable Changes\\n\\n💯 Support for NumPy values in st.metric.\\n\\n🌐 Support for Mesh Layers in PyDeck.\\n\\n📊 Updated Plotly chart version to support the latest features.\\n\\n🏀 st.spinner element has visual animated spinner.\\n\\n🍰 st.caption supports HTML in text with unsafe_allow_html parameter.\\n\\nOther Changes\\n\\n🪲 Bug fix: Allow st.session_state to be used to set number_input values with no warning (#4047).\\n\\n🪲 Bug fix: Fix footer alignment in wide mode (#4035).\\n\\n🐞 Bug fix: Better support for Graphviz and Bokeh charts in containers (columns, expanders, etc.) (#4039).\\n\\n🐞 Bug fix: Support inline data values in Vega-Lite (#4070).\\n\\n✍️ Types: Updated type annotations for experimental memo and singleton decorators.\\n\\n✍️ Types: Improved type annotations for st.selectbox, st.select_slider, st.radio, st.number_input, and st.multiselect.\\n\\nVersion 1.2.0\\n\\nRelease date: Nov 11, 2021\\n\\nNotable Changes\\n\\n✏️\\xa0st.text_input\\xa0and st.text_area now have a\\xa0placeholder\\xa0parameter to display text when the field is empty.\\n\\n📏 Viewers can now resize the input box in st.text_area.\\n\\n📁 Streamlit can auto-reload when files in sub-directories change.\\n\\n🌈 We\\'ve upgraded Bokeh support to 2.4.1! We recommend updating your Bokeh library to 2.4.1 to maintain functionality. Going forward, we\\'ll let you know if there\\'s a mismatch in your Bokeh version via an error prompt.\\n\\n🔒 Developers can access secrets via attribute notation (e.g. st.secrets.key vs st.secrets[\"key\"]) just like session state.\\n\\n✍️ Publish type annotations according to PEP 561. Users now get type annotations for Streamlit when running mypy (#4025).\\n\\nOther Changes\\n\\n👀 Visual fixes (#3863, #3995, #3926, #3975).\\n\\n🍔 Fixes to the hamburger menu (#3968).\\n\\n🖨️ Ability to print session state (#3970).\\n\\nVersion 1.1.0\\n\\nRelease date: Oct 21, 2021\\n\\nHighlights\\n\\n🧠 Memory improvements: Streamlit apps allocate way less memory over time now.\\n\\nNotable Changes\\n\\n♻️ Apps automatically rerun now when the content of secrets.toml changes (before this you had to refresh the page manually).\\n\\nOther Changes\\n\\n🔗 Redirected some links to our brand-new docs site, e.g. in exceptions.\\n\\n🪲 Bug fix: Allow initialization of range slider with session state (#3586).\\n\\n🐞 Bug fix: Refresh chart when using add_rows with datetime index (#3653).\\n\\n✍️ Added some more type annotation in our codebase (#3908).\\n\\nVersion 1.0.0\\n\\nRelease date: Oct 5, 2021\\n\\nHighlights\\n\\n🎈Announcing Streamlit 1.0! To read more about check out our 1.0 blog post.\\n\\nOther Changes\\n\\n🐞 Fixed an issue where using df.dtypes to show datatypes for a DF fails while using Arrow (#3709), Image captions stay within image width and are readable (#3530).\\n\\nVersion 0.89.0\\n\\nRelease date: Sep 22, 2021\\n\\nHighlights\\n\\n💰 Introducing st.experimental_memo and experimental_singleton, a new primitive for caching! See our blog post.\\n\\n🍔 Streamlit allows developers to configure their hamburger menu to be more user-centric.\\n\\nNotable Changes\\n\\n💅 We updated our UI to a more polished look with a new font.\\n\\n🎨 We now support theme.base in the theme object when it\\'s sent to custom components.\\n\\n🧠 We\\'ve modified session state to reset widgets if any of their arguments changed even if they provide a key.\\n\\nSome widget behavior may have changed, but we believe this change makes the most sense. We have added a section to our documentation describing how they behave.\\n\\nOther Changes\\n\\n🐞 Bug fixes: Support svgs from a URL (#3809) and that do not start with <svg> tag (#3789).\\n\\nVersion 0.88.0\\n\\nRelease date: Sep 2, 2021\\n\\nHighlights\\n\\n⬇️ Introducing st.download_button, a new button widget for easily downloading files.\\n\\nNotable Changes\\n\\n🛑 We made changes to improve the redacted exception experience on Streamlit Community Cloud. When client.showErrorDetails=true exceptions display the Error Type and the Traceback, but redact the actual error text to prevent data leaks.\\n\\nVersion 0.87.0\\n\\nRelease date: Aug 19, 2021\\n\\nHighlights\\n\\n🔢 Introducing st.metric, an API for displaying KPIs. Check out the demo app showcasing the functionality.\\n\\nOther Changes\\n\\n🐞 Bug Fixes: File uploader retains state upon expander closing (#3557), setIn Error with st.empty (#3659), Missing IFrame embeds in docs (#3706), Fix error writing certain PNG files (#3597).\\n\\nVersion 0.86.0\\n\\nRelease date: Aug 5, 2021\\n\\nHighlights\\n\\n🎓 Our layout primitives are graduating from beta! You can now use st.columns, st.container and st.expander without the beta_ prefix.\\n\\nNotable Changes\\n\\n📱 When using st.columns, columns will stack vertically when viewport size <640px so that column layout on smaller viewports is consistent and cleaner. (#3594).\\n\\nOther Changes\\n\\n🐞 Bug fixes: Fixed st.date_input crashes if its empty (#3194), Opening files with utf-8(#3022), st.select_slider resets its state upon interaction (#3600).\\n\\nVersion 0.85.0\\n\\nRelease date: Jul 22, 2021\\n\\nHighlights\\n\\n🏹 Streamlit now uses Apache Arrow for serializing data frames when they are sent from Streamlit server to the front end. See our blog post.\\n\\n(Users who wish to continue using the legacy data frame serialization can do so by setting the dataFrameSerialization config option to \"legacy\" in their config.toml).\\n\\nOther Changes\\n\\n🐞 Bug fixes: Unresponsive pydeck example (#3395), JSON parse error message (#2324), Tooltips rendering (#3300), Colorpicker not working on Streamlit Sharing (#2689).\\n\\nVersion 0.84.0\\n\\nRelease date: Jul 1, 2021\\n\\nHighlights\\n\\n🧠 Introducing st.session_state and widget callbacks to allow you to add statefulness to your apps. Check out the blog post\\n\\nNotable Changes\\n\\n🪄 st.text_input now has an autocomplete parameter to allow password managers to be used\\n\\nOther Changes\\n\\nUsing st.set_page_config to assign the page title no longer appends “Streamlit\" to that title (#3467)\\n\\nNumberInput: disable plus/minus buttons when the widget is already at its max (or min) value (#3493)\\n\\nVersion 0.83.0\\n\\nRelease date: Jun 17, 2021\\n\\nHighlights\\n\\n🛣️ Updates to Streamlit docs to include step-by-step guides which demonstrate how to connect Streamlit apps to various databases & APIs\\n\\nNotable Changes\\n\\n📄 st.form now has a clear_on_submit parameter which \"resets\" all the form\\'s widgets when the form is submitted.\\n\\nOther Changes\\n\\nFixed bugs regarding file encodings (#3320, #3108, #2731)\\n\\nVersion 0.82.0\\n\\nRelease date: May 13, 2021\\n\\nNotable Changes\\n\\n♻️ Improvements to memory management by forcing garbage collection between script runs.\\n\\nVersion 0.81.1\\n\\nRelease date: Apr 29, 2021\\n\\nHighlights\\n\\n📝 Introducing st.form and st.form_submit_button to allow you to batch input widgets. Check out our blog post\\n\\n🔤 Introducing st.caption so you can add explainer text anywhere in you apps.\\n\\n🎨 Updates to Theming, including ability to build a theme that inherits from any of our default themes.\\n\\n🚀 Improvements to deployment experience to Streamlit sharing from the app menu.\\n\\nOther changes\\n\\nSupport for binary files in Custom Components (#3144)\\n\\nVersion 0.80.0\\n\\nRelease date: Apr 8, 2021\\n\\nHighlights\\n\\n🔐 Streamlit now support Secrets management for apps deployed to Streamlit Sharing!\\n\\n⚓️ Titles and headers now come with automatically generated anchor links. Just hover over any title and click the 🔗 to get the link!\\n\\nOther changes\\n\\nAdded allow-downloads capability to custom components (#3040)\\n\\nFixed markdown tables in dark theme (#3020)\\n\\nImproved color picker widget in the Custom Theme dialog (#2970)\\n\\nVersion 0.79.0\\n\\nRelease date: Mar 18, 2021\\n\\nHighlights\\n\\n🌈 Introducing support for custom themes. Check out our blog post\\n\\n🌚 This release also introduces dark mode!\\n\\n🛠️ Support for tooltips on all input widgets\\n\\nOther changes\\n\\nFixed bugs regarding file encodings (#1936, #2606) and caching functions (#2728)\\n\\nVersion 0.78.0\\n\\nRelease date: Mar 4, 2021\\n\\nFeatures\\n\\nIf you\\'re in the Streamlit for Teams beta, we made a few updates to how secrets work. Check the beta docs for more info!\\n\\nDataframes now displays timezones for all DateTime and Time columns, and shows the time with the timezone applied, rather than in UTC\\n\\nNotable Bug Fixes\\n\\nVarious improvement to column alignment in st.beta_columns\\n\\nRemoved the long-deprecated format param from st.image, and replaced with output_format.\\n\\nVersion 0.77.0\\n\\nRelease date: Feb 23, 2021\\n\\nFeatures\\n\\nAdded a new config option client.showErrorDetails allowing the developer to control the granularity of error messages. This is useful for when you deploy an app, and want to conceal from your users potentially-sensitive information contained in tracebacks.\\n\\nNotable bug fixes\\n\\nFixed bug where st.image wasn\\'t rendering certain kinds of SVGs correctly.\\n\\nFixed regression where the current value of an st.slider was only shown on hover.\\n\\nVersion 0.76.0\\n\\nRelease date: February 4, 2021\\n\\nNotable Changes\\n\\n🎨 st.color_picker is now out of beta. This means the old beta_color_picker function, which was marked as deprecated for the past 3 months, has now been replaced with color_picker.\\n\\n🐍 Display a warning when a Streamlit script is run directly as python script.py.\\n\\nst.image\\'s use_column_width now defaults to an auto option which will resize the image to the column width if the image exceeds the column width.\\n\\n✂️ Fixed bugs (2437 and 2247) with content getting cut off within a st.beta_expander\\n\\n📜 Fixed a bug in st.dataframe where the scrollbar overlapped with the contents in the last column.\\n\\n💾 Fixed a bug for st.file_uploader where file data returned was not the most recently uploaded file.\\n\\n➕ Fixed bugs (2086 and 2556) where some LaTeX commands were not rendering correctly.\\n\\nVersion 0.75.0\\n\\nRelease date: January 21, 2021\\n\\nNotable Changes\\n\\n🕳 st.empty\\n  previously would clear the component at the end of the script. It has now been\\n  updated to clear the component instantly.\\n\\n🛹 Previously in wide mode, we had thin margins around the webpage. This has\\n  now been increased to provide a better visual experience.\\n\\nVersion 0.74.0\\n\\nRelease date: January 6, 2021\\n\\nNotable Changes\\n\\n💾 st.file_uploader. has been stabilized and the deprecation warning\\n  and associated configuration option (deprecation.showfileUploaderEncoding) has been removed.\\n\\n📊 st.bokeh_chart is no longer duplicated when the page loads.\\n\\n🎈 Fixed page icon to support emojis with variants (i.e. 🤦\\u200d♀️ vs 🤦🏼\\u200d♀️) or dashes (i.e 🌙 - crescent-moon).\\n\\nVersion 0.73.0\\n\\nRelease date: December 17, 2020\\n\\nNotable Changes\\n\\n🐍 Streamlit can now be installed on Python 3.9. Streamlit components are not\\n  yet compatible with Python 3.9 and must use version 3.8 or earlier.\\n\\n🧱 Streamlit Components now allows same origin, enabling features provided by\\n  the browser such as a webcam component.\\n\\n🐙 Fix Streamlit sharing deploy experience for users running on Git versions\\n  2.7.0 or earlier.\\n\\n🧰 Handle unexpected closing of uploaded files for st.file_uploader.\\n\\nVersion 0.72.0\\n\\nRelease date: December 2, 2020\\n\\nNotable Changes\\n\\n🌈 Establish a framework for theming and migrate existing components.\\n\\n📱 Improve the sidebar experience for mobile devices.\\n\\n🧰 Update st.file_uploader to reduce reruns.\\n\\nVersion 0.71.0\\n\\nRelease date: November 11, 2020\\n\\nNotable Changes\\n\\n📁 Updated st.file_uploader\\n  to automatically reset buffer on app reruns.\\n\\n📊 Optimize the default rendering of charts and reduce issues with the initial render.\\n\\nVersion 0.70.0\\n\\nRelease date: October 28, 2020\\n\\nNotable Changes\\n\\n🧪 st.set_page_config and st.color_picker have now been moved into the\\n  Streamlit namespace. These will be removed from beta January 28th, 2021. Learn\\n  more about our beta process here.\\n\\n📊 Improve display of bar charts for discrete values.\\n\\nVersion 0.69.0\\n\\nRelease date: October 15, 2020\\n\\nHighlights:\\n\\n🎁 Introducing Streamlit sharing, the best way to deploy, manage, and share your public Streamlit apps—for free. Read more about it on our blog post or sign up here!\\n\\nAdded st.experimental_rerun to programatically re-run your app. Thanks SimonBiggs!\\n\\nNotable Changes\\n\\n📹 Better support across browsers for start and stop times for st.video.\\n\\n🖼 Bug fix for intermittently failing media files\\n\\n📦 Bug fix for custom components compatibility with Safari. Make sure to upgrade to the latest streamlit-component-lib.\\n\\nVersion 0.68.0\\n\\nRelease date: October 8, 2020\\n\\nHighlights:\\n\\n⌗ Introducing new layout options for Streamlit! Move aside, vertical layout.\\n  Make a little space for... horizontal layout! Check out our\\n  blog post.\\n\\n💾 File uploader redesigned with new functionality for multiple files uploads\\n  and better support for working with uploaded files. This may cause breaking\\n  changes. Please see the new api in our\\n  documentation\\n\\nNotable Changes\\n\\n🎈 st.balloon has gotten a facelift with nicer balloons and smoother animations.\\n\\n🚨 Breaking Change: Following the deprecation of st.deck_gl_chart in\\n  January 2020, we have now removed the API completely. Please use\\n  st.pydeck_chart instead.\\n\\n🚨 Breaking Change: Following the deprecation of width and height for\\n  st.altair_chart, st.graphviz_chart, st.plotly_chart, and\\n  st.vega_lite_chart in January 2020, we have now removed the args completely.\\n  Please set the width and height in the respective charting library.\\n\\nVersion 0.67.0\\n\\nRelease date: September 16, 2020\\n\\nHighlights:\\n\\n🦷 Streamlit Components can now return bytes to your Streamlit App. To create a\\n  component that returns bytes, make sure to upgrade to the latest\\n  streamlit-component-lib.\\n\\nNotable Changes\\n\\n📈 Deprecation warning: Beginning December 1st, 2020 st.pyplot() will require a figure to\\n  be provided. To disable the deprecation warning, please set deprecation.showPyplotGlobalUse\\n  to False\\n\\n🎚 st.multiselect and st.select are now lightning fast when working with large datasets. Thanks masa3141!\\n\\nVersion 0.66.0\\n\\nRelease date: September 1, 2020\\n\\nHighlights:\\n\\n✏️ st.write is now available for use in the sidebar!\\n\\n🎚 A slider for distinct or non-numerical values is now available with st.select_slider.\\n\\n⌗ Streamlit Components can now return dataframes to your Streamlit App. Check out our SelectableDataTable example.\\n\\n📦 The Streamlit Components library used in our Streamlit Component template is\\n  now available as a npm package (streamlit-component-lib) to simplify future upgrades to the latest version.\\n  Existing components do not need to migrate.\\n\\nNotable Changes\\n\\n🐼 Support StringDtype from pandas version 1.0.0\\n\\n🧦 Support for running Streamlit on Unix sockets\\n\\nVersion 0.65.0\\n\\nRelease date: August 12, 2020\\n\\nHighlights:\\n\\n⚙️ Ability to set page title, favicon, sidebar state, and wide mode via st.beta_set_page_config(). See our documentation for details.\\n\\n📝 Add stateful behaviors through the use of query parameters with st.experimental_set_query_params and st.experimental_get_query_params. Thanks @zhaoooyue!\\n\\n🐼 Improved pandas dataframe support for st.radio, st.selectbox, and st.multiselect.\\n\\n🛑 Break out of your Streamlit app with st.stop.\\n\\n🖼 Inline SVG support for st.image.\\n\\nCallouts:\\n\\n🚨Deprecation Warning: The st.image parameter format has been renamed to output_format.\\n\\nVersion 0.64.0\\n\\nRelease date: July 23, 2020\\n\\nHighlights:\\n\\n📊 Default matplotlib to display charts with a tight layout. To disable this,\\n  set bbox_inches to None, inches as a string, or a Bbox\\n\\n🗃 Deprecation warning for automatic encoding on st.file_uploader\\n\\n🙈 If gatherUserStats is False, do not even load the Segment library.\\n  Thanks @tanmaylaud!\\n\\nVersion 0.63.0\\n\\nRelease date: July 13, 2020\\n\\nHighlights:\\n\\n🧩 Support for Streamlit Components!!! See\\n  documentation for more info.\\n\\n🕗 Support for datetimes in\\n  st.slider. And, of course, just\\n  like any other value you use in st.slider, you can also pass in two-element lists to get a\\n  datetime range slider.\\n\\nVersion 0.62.0\\n\\nRelease date: June 21, 2020\\n\\nHighlights:\\n\\n📨 Ability to turn websocket compression on/off via the config option\\n  server.enableWebsocketCompression. This is useful if your server strips HTTP headers and you do\\n  not have access to change that behavior.\\n\\n🗝️ Out-of-the-box support for CSRF protection using the\\n  Cookie-to-header token\\n  technique. This means that if you\\'re serving your Streamlit app from multiple replicas you\\'ll need\\n  to configure them to to use the same cookie secret with the server.cookieSecret config option.\\n  To turn XSRF protection off, set server.enableXsrfProtection=false.\\n\\nNotable bug fixes:\\n\\n🖼️ Added a grace period to the image cache expiration logic in order to fix multiple related bugs\\n  where images sent with st.image or st.pyplot were sometimes missing.\\n\\nVersion 0.61.0\\n\\nRelease date: June 2, 2020\\n\\nHighlights:\\n\\n📅 Support for date ranges in st.date_picker. See\\n  docs\\n  for more info, but the TLDR is: just pass a list/tuple as the default date and it will be\\n  interpreted as a range.\\n\\n🗣️ You can now choose whether st.echo prints the code above or below the output of the echoed\\n  block. To learn more, refer to the code_location argument in the\\n  docs.\\n\\n📦 Improved @st.cache support for Keras models and Tensorflow saved_models.\\n\\nVersion 0.60.0\\n\\nRelease date: May 18, 2020\\n\\nHighlights:\\n\\n↕️ Ability to set the height of an st.text_area with the height argument\\n  (expressed in pixels). See\\n  docs for more.\\n\\n🔡 Ability to set the maximimum number of characters allowed in st.text_area\\n  or st.text_input. Check out the max_chars argument in the\\n  docs.\\n\\n🗺️ Better DeckGL support for the H3 geospatial indexing\\n  system. So now you can use things like H3HexagonLayer in\\n  st.pydeck_chart.\\n\\n📦 Improved @st.cache support for PyTorch TensorBase and Model.\\n\\nVersion 0.59.0\\n\\nRelease date: May 05, 2020\\n\\nHighlights:\\n\\n🎨 New color-picker widget! Use it with\\n  st.beta_color_picker()\\n\\n🧪 Introducing st.beta_* and st.experimental_* function prefixes, for faster\\n  Streamlit feature releases. See\\n  docs for more info.\\n\\n📦 Improved @st.cache support for SQL Alchemy objects, CompiledFFI, PyTorch\\n  Tensors, and builtins.mappingproxy.\\n\\nVersion 0.58.0\\n\\nRelease date: April 22, 2020\\n\\nHighlights:\\n\\n💼 Made st.selectbox filtering case-insensitive.\\n\\n㈬ Better support for Tensorflow sessions in @st.cache.\\n\\n📊 Changed behavior of st.pyplot to auto-clear the figure only when using\\n  the global Matplotlib figure (i.e. only when calling st.pyplot() rather\\n  than st.pyplot(fig)).\\n\\nVersion 0.57.0\\n\\nRelease date: March 26, 2020\\n\\nHighlights:\\n\\n⏲️ Ability to set expiration options for @st.cache\\'ed functions by setting\\n  the max_entries and ttl arguments. See\\n  docs.\\n\\n🆙 Improved the machinery behind st.file_uploader, so it\\'s much more\\n  performant now! Also increased the default upload limit to 200MB\\n  (configurable via server.max_upload_size).\\n\\n🔒 The server.address config option now binds the server to that address\\n  for added security.\\n\\n📄 Even more details added to error messages for @st.cache for easier\\n  debugging.\\n\\nVersion 0.56.0\\n\\nRelease date: February 15, 2020\\n\\nHighlights:\\n\\n📄 Improved error messages for st.cache. The errors now also point to the new\\n  caching docs we just released. Read more\\n  here!\\n\\nBreaking changes:\\n\\n🐍 As announced last month,\\n  Streamlit no longer supports Python 2. To use Streamlit you\\'ll need\\n  Python 3.5 or above.\\n\\nVersion 0.55.0\\n\\nRelease date: February 4, 2020\\n\\nHighlights:\\n\\n📺 Ability to record screencasts directly from Streamlit! This allows\\n  you to easily record and share explanations about your models, analyses,\\n  data, etc. Just click ☰ then \"Record a screencast\". Give it a try!\\n\\nVersion 0.54.0\\n\\nRelease date: January 29, 2020\\n\\nHighlights:\\n\\n⌨️ Support for password fields! Just pass type=\"password\" to\\n  st.text_input().\\n\\nNotable fixes:\\n\\n✳️ Numerous st.cache improvements, including better support for complex objects.\\n\\n🗣️ Fixed cross-talk in sidebar between multiple users.\\n\\nBreaking changes:\\n\\nIf you\\'re using the SessionState hack Gist, you should re-download it!\\n  Depending on which hack you\\'re using, here are some links to save you some\\n  time:\\n\\nSessionState.py\\n\\nst_state_patch.py\\n\\nVersion 0.53.0\\n\\nRelease date: January 14, 2020\\n\\nHighlights:\\n\\n🗺️ Support for all DeckGL features! Just use\\n  Pydeck instead of\\n  st.deck_gl_chart.\\n  To do that, simply pass a PyDeck object to\\n  st.pydeck_chart,\\n  st.write,\\n  or magic.\\n\\nNote that as a preview release things may change in the near future.\\n  Looking forward to hearing input from the community before we stabilize the\\n  API!\\n\\nThe goals is for this to replace st.deck_gl_chart, since it\\n  is does everything the old API did and much more!\\n\\n🆕 Better handling of Streamlit upgrades while developing. We now auto-reload\\n  the browser tab if the app it is displaying uses a newer version of Streamlit\\n  than the one the tab is running.\\n\\n👑 New favicon, with our new logo!\\n\\nNotable fixes:\\n\\nMagic now works correctly in Python 3.8. It no longer causes\\n  docstrings to render in your app.\\n\\nBreaking changes:\\n\\nUpdated how we calculate the default width and height of all chart types.\\n  We now leave chart sizing up to your charting library itself, so please refer\\n  to the library\\'s documentation.\\n\\nAs a result, the width and height arguments have been deprecated\\n  from most chart commands, and use_container_width has been introduced\\n  everywhere to allow you to make charts fill as much horizontal space as\\n  possible (this used to be the default).\\n\\nVersion 0.52.0\\n\\nRelease date: December 20, 2019\\n\\nHighlights:\\n\\n📤 Preview release of the file uploader widget. To try it out just call\\n  st.file_uploader!\\n\\nNote that as a preview release things may change in the near future.\\n  Looking forward to hearing input from the community before we stabilize the\\n  API!\\n\\n👋 Support for emoji codes in\\n  st.write and st.markdown! Try it out with st.write(\"Hello :wave:\").\\n\\nBreaking changes:\\n\\n🧹 st.pyplot now clears figures by default, since that\\'s what you want 99% of\\n  the time. This allows you to create two or more Matplotlib charts without\\n  having to call\\n  pyplot.clf\\n  every time. If you want to turn this behavior off, use\\n  st.pyplot(clear_figure=False)\\n\\n📣 st.cache no longer checks for input mutations. This is the first change\\n  of our ongoing effort to simplify the caching system and prepare Streamlit\\n  for the launch of other caching primitives like Session State!\\n\\nVersion 0.51.0\\n\\nRelease date: November 30, 2019\\n\\nHighlights:\\n\\n🐕 You can now tweak the behavior of the file watcher with the config option server.fileWatcherType. Use it to switch between:\\n\\nauto (default) : Streamlit will attempt to use the watchdog module, and\\n    falls back to polling if watchdog is not available.\\n\\nwatchdog : Force Streamlit to use the watchdog module.\\n\\npoll : Force Streamlit to always use polling.\\n\\nnone : Streamlit will not watch files.\\n\\nNotable bug fixes:\\n\\nFix the \"keyPrefix\" option in static report sharing #724\\n\\nAdd support for getColorX and getTargetColorX to DeckGL Chart #718\\n\\nFixing Tornado on Windows + Python 3.8 #682\\n\\nFall back on webbrowser if xdg-open is not installed on Linux #701\\n\\nFixing number input spin buttons for Firefox #683\\n\\nFixing CTRL+ENTER on Windows #699\\n\\nDo not automatically create credential file when in headless mode #467\\n\\nVersion 0.50.1\\n\\nRelease date: November 10, 2019\\n\\nHighlights:\\n\\n👩\\u200d🎓 SymPy support and ability to draw mathematical expressions using LaTeX! See\\n  st.latex,\\n  st.markdown,\\n  and\\n  st.write.\\n\\n🌄 You can now set config options using environment variables. For example,\\n  export STREAMLIT_SERVER_PORT=9876.\\n\\n🐱 Ability to call streamlit run directly with Github and Gist URLs. No\\n  need to grab the \"raw\" URL first!\\n\\n📃 Cleaner exception stack traces. We now remove all Streamlit-specific code\\n  from stack traces originating from the user\\'s app.\\n\\nVersion 0.49.0\\n\\nRelease date: October 23, 2019\\n\\nHighlights:\\n\\n💯 New input widget for entering numbers with the keyboard: st.number_input()\\n\\n📺 Audio/video improvements: ability to load from a URL, to embed YouTube\\n  videos, and to set the start position.\\n\\n🤝 You can now (once again) share static snapshots of your apps to S3! See\\n  the S3 section of streamlit config show to set it up. Then share from\\n  top-right menu.\\n\\n⚙️ Use server.baseUrlPath config option to set Streamlit\\'s URL to something\\n  like http://domain.com/customPath.\\n\\nNotable bug fixes:\\n\\nFixes numerous Windows bugs, including Issues\\n  #339 and\\n  #401.\\n\\nVersion 0.48.0\\n\\nRelease date: October 12, 2019\\n\\nHighlights:\\n\\n🔧 Ability to set config options as command line flags or in a local config file.\\n\\n↕️ You can now maximize charts and images!\\n\\n⚡ Streamlit is now much faster when writing data in quick succession to your app.\\n\\n✳️ Ability to blacklist folder globs from \"run on save\" and @st.cache hashing.\\n\\n🎛️ Improved handling of widget state when Python file is modified.\\n\\n🙈 Improved HTML support in st.write and st.markdown. HTML is still unsafe, though!\\n\\nNotable bug fixes:\\n\\nFixes @st.cache bug related to having your Python environment on current\\n  working directory. Issue #242\\n\\nFixes loading of root url / on Windows. Issue #244\\n\\nVersion 0.47.0\\n\\nRelease date: October 1, 2019\\n\\nHighlights:\\n\\n🌄 New hello.py showing off 4 glorious Streamlit apps. Try it out!\\n\\n🔄 Streamlit now automatically selects an unused port when 8501 is already in use.\\n\\n🎁 Sidebar support is now out of beta! Just start any command with st.sidebar. instead of st.\\n\\n⚡ Performance improvements: we added a cache to our websocket layer so we no longer re-send data to the browser when it hasn\\'t changed between runs\\n\\n📈 Our \"native\" charts st.line_chart, st.area_chart and st.bar_chart now use Altair behind the scenes\\n\\n🔫 Improved widgets: custom st.slider labels; default values in multiselect\\n\\n🕵️\\u200d♀️ The filesystem watcher now ignores hidden folders and virtual environments\\n\\n💅 Plus lots of polish around caching and widget state management\\n\\nBreaking change:\\n\\n🛡️ We have temporarily disabled support for sharing static \"snapshots\" of Streamlit apps. Now that we\\'re no longer in a limited-access beta, we need to make sure sharing is well thought through and abides by laws like the DMCA. But we\\'re working on a solution!\\n\\nVersion 0.46.0\\n\\nRelease date: September 19, 2019\\n\\nHighlights:\\n\\n✨ Magic commands! Use st.write without typing st.write. See\\n  https://docs.streamlit.io/en/latest/api.html#magic-commands\\n\\n🎛️ New st.multiselect widget.\\n\\n🐍 Fixed numerous install issues so now you can use pip install streamlit\\n  even in Conda! We\\'ve therefore deactivated our Conda repo.\\n\\n🐞 Multiple bug fixes and additional polish in preparation for our launch!\\n\\nBreaking change:\\n\\n🛡️ HTML tags are now blacklisted in st.write/st.markdown by default. More\\n  information and a temporary work-around at:\\n  https://github.com/streamlit/streamlit/issues/152\\n\\nVersion 0.45.0\\n\\nRelease date: August 28, 2019\\n\\nHighlights:\\n\\n😱 Experimental support for sidebar! Let us know if you want to be a beta\\n  tester.\\n\\n🎁 Completely redesigned st.cache! Much more performant, has a cleaner API,\\n  support for caching functions called by @st.cached functions,\\n  user-friendly error messages, and much more!\\n\\n🖼️ Lightning fast st.image, ability to choose between JPEG and PNG\\n  compression, and between RGB and BGR (for OpenCV).\\n\\n💡 Smarter API for st.slider, st.selectbox, and st.radio.\\n\\n🤖 Automatically fixes the Matplotlib backend -- no need to edit .matplotlibrc\\n\\nVersion 0.44.0\\n\\nRelease date: July 28, 2019\\n\\nHighlights:\\n\\n⚡ Lightning-fast reconnect when you do a ctrl-c/rerun on your Streamlit code\\n\\n📣 Useful error messages when the connection fails\\n\\n💎 Fixed multiple bugs and improved polish of our newly-released interactive widgets\\n\\nVersion 0.43.0\\n\\nRelease date: July 9, 2019\\n\\nHighlights:\\n\\n⚡ Support for interactive widgets! 🎈🎉\\n\\nVersion 0.42.0\\n\\nRelease date: July 1, 2019\\n\\nHighlights:\\n\\n💾 Ability to save Vega-Lite and Altair charts to SVG or PNG\\n\\n🐇 We now cache JS files in your browser for faster loading\\n\\n⛔ Improvements to error-handling inside Streamlit apps\\n\\nVersion 0.41.0\\n\\nRelease date: June 24, 2019\\n\\nHighlights:\\n\\n📈 Greatly improved our support for named datasets in Vega-Lite and Altair\\n\\n🙄 Added ability to ignore certain folders when watching for file changes. See the server.folderWatchBlacklist config option.\\n\\n☔ More robust against syntax errors on the user\\'s script and imported modules\\n\\nVersion 0.40.0\\n\\nRelease date: June 10, 2019\\n\\nHighlights:\\n\\nStreamlit is more than 10x faster. Just save and watch your analyses update instantly.\\n\\nWe changed how you run Streamlit apps:\\n  $ streamlit run your_script.py [script args]\\n\\nUnlike the previous versions of Streamlit, streamlit run [script] [script args] creates a server (now you don\\'t need to worry if the proxy is up). To kill the server, all you need to do is hit Ctrl+c.\\n\\nWhy is this so much faster?\\n\\nNow, Streamlit keeps a single Python session running until you kill the server. This means that Streamlit can re-run your code without kicking off a new process; imported libraries are cached to memory. An added bonus is that st.cache now caches to memory instead of to disk.\\n\\nWhat happens if I run Streamlit the old way?\\n\\nIf you run $ python your_script.py the script will execute from top to bottom, but won\\'t produce a Streamlit app.\\n\\nWhat are the limitations of the new architecture?\\n\\nTo switch Streamlit apps, first you have to kill the Streamlit server with Ctrl-c. Then, you can use streamlit run to generate the next app.\\n\\nStreamlit only works when used inside Python files, not interactively from the Python REPL.\\n\\nWhat else do I need to know?\\n\\nThe strings we print to the command line when liveSave is on have been cleaned up. You may need to adjust any RegEx that depends on those.\\n\\nA number of config options have been renamed:\\n\\nWhat if something breaks?\\n\\nIf the new Streamlit isn\\'t working, please let us know by Slack or email. You can downgrade at any time with these commands:\\n\\nbash\\npip install --upgrade streamlit==0.37\\n\\nbash\\nconda install streamlit=0.37\\n\\nWhat\\'s next?\\n\\nThank you for staying with us on this journey! This version of Streamlit lays the foundation for interactive widgets, a new feature of Streamlit we\\'re really excited to share with you in the next few months.\\n\\nVersion 0.36.0\\n\\nRelease date: May 03, 2019\\n\\nHighlights\\n\\n🚣\\u200d♀️ st.progress() now also accepts floats from 0.0–1.0\\n\\n🤯 Improved rendering of long headers in DataFrames\\n\\n🔐 Shared apps now default to HTTPS\\n\\nVersion 0.35.0\\n\\nRelease date: April 26, 2019\\n\\nHighlights\\n\\n📷 Bokeh support! Check out docs for st.bokeh_chart\\n\\n⚡️ Improved the size and load time of saved apps\\n\\n⚾️ Implemented better error-catching throughout the codebase', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='title: Dataframes\\nslug: /library/advanced-features/dataframes\\n\\nDataframes\\n\\nDataframes are a great way to display and edit data in a tabular format. Working with Pandas DataFrames and other tabular data structures is key to data science workflows. If developers and data scientists want to display this data in Streamlit, they have multiple options: st.dataframe and st.data_editor. If you want to solely display data in a table-like UI, st.dataframe is the way to go. If you want to interactively edit data, use st.data_editor. We explore the use cases and advantages of each option in the following sections.\\n\\nDisplay dataframes with st.dataframe\\n\\nStreamlit can display dataframes in a table-like UI via st.dataframe :\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\ndf = pd.DataFrame(\\n    [\\n        {\"command\": \"st.selectbox\", \"rating\": 4, \"is_widget\": True},\\n        {\"command\": \"st.balloons\", \"rating\": 5, \"is_widget\": False},\\n        {\"command\": \"st.time_input\", \"rating\": 3, \"is_widget\": True},\\n    ]\\n)\\n\\nst.dataframe(df, use_container_width=True)\\n```\\n\\nAdditional UI features\\n\\nst.dataframe also provides some additional functionality by using glide-data-grid under the hood:\\n\\nColumn sorting: sort columns by clicking on their headers.\\n\\nColumn resizing: resize columns by dragging and dropping column header borders.\\n\\nTable resizing: resize tables by dragging and dropping the bottom right corner.\\n\\nSearch: search through data by clicking a table, using hotkeys (⌘ Cmd + F\\xa0or\\xa0Ctrl + F) to bring up the search bar, and using the search bar to filter data.\\n\\nCopy to clipboard: select one or multiple cells, copy them to the clipboard and paste them into your favorite spreadsheet software.\\n\\nTry out all the addition UI features using the embedded app from the prior section.\\n\\nIn addition to Pandas DataFrames, st.dataframe also supports other common Python types, e.g., list, dict, or numpy array. It also supports Snowpark and PySpark DataFrames, which allow you to lazily evaluate and pull data from databases. This can be useful for working with large datasets.\\n\\nEdit data with st.data_editor\\n\\nStreamlit supports editable dataframes via the st.data_editor command. Check out its API in st.data_editor. It shows the dataframe in a table, similar to st.dataframe. But in contrast to st.dataframe, this table isn\\'t static! The user can click on cells and edit them. The edited data is then returned on the Python side. Here\\'s an example:\\n\\n```python\\ndf = pd.DataFrame(\\n    [\\n        {\"command\": \"st.selectbox\", \"rating\": 4, \"is_widget\": True},\\n        {\"command\": \"st.balloons\", \"rating\": 5, \"is_widget\": False},\\n        {\"command\": \"st.time_input\", \"rating\": 3, \"is_widget\": True},\\n    ]\\n)\\n\\ndf = load_data()\\nedited_df = st.data_editor(df) # 👈 An editable dataframe\\n\\nfavorite_command = edited_df.loc[edited_df[\"rating\"].idxmax()][\"command\"]\\nst.markdown(f\"Your favorite command is {favorite_command} 🎈\")\\n```\\n\\nTry it out by double-clicking on any cell. You\\'ll notice you can edit all cell values. Try editing the values in the rating column and observe how the text output at the bottom changes:\\n\\nst.data_editor also supports a few additional things:\\n\\nCopy and paste support from and to Excel and Google Sheets.\\n\\nAdd and delete rows. You can do this by setting num_rows= \"dynamic\" when calling st.data_editor. This will allow users to add and delete rows as needed.\\n\\nAccess edited data. Only access the individual edits instead of the entire edited data structure via session state.\\n\\nBulk edits (similar to Excel, just drag a handle to edit neighboring cells).\\n\\nAutomatic input validation, a strong data type support. e.g. There\\'s no way to enter letters into a number cell and many other configurable input validation options. e.g. min-/max-value.\\n\\nEdit common data structures such as lists, dicts, NumPy ndarray, etc.\\n\\nCopy and paste support\\n\\nThe data editor supports pasting in tabular data from Google Sheets, Excel, Notion, and many other similar tools. You can also copy-paste data between\\xa0st.data_editor instances. This functionality, powered by the Clipboard API, can be a huge time saver for users who need to work with data across multiple platforms. To try it out:\\n\\nCopy data from\\xa0this Google Sheets document\\xa0to clipboard\\n\\nSelect any cell in the\\xa0name\\xa0column of the table below and paste it in (via\\xa0ctrl/cmd + v).\\n\\nEvery cell of the pasted data will be evaluated individually and inserted into the cells if the data is compatible with the column type. E.g., pasting in non-numerical text data into a number column will be ignored.\\n\\nDid you notice that although the initial dataframe had just five rows, pasting all those rows from the spreadsheet added additional rows to the dataframe? 👀\\xa0Let\\'s find out how that works in the next section.\\n\\nIf you embed your apps with iframes, you\\'ll need to allow the iframe to access the clipboard if you want to use the copy-paste functionality. To do so, give the iframe clipboard-write and clipboard-read permissions. E.g.\\n\\n```javascript\\n\\n```\\n\\nAs developers, ensure the app is served with a valid, trusted certificate when using TLS. If users encounter issues with copying and pasting data, direct them to check if their browser has activated clipboard access permissions for the Streamlit application, either when prompted or through the browser\\'s site settings.\\n\\nAdd and delete rows\\n\\nWith st.data_editor, viewers can add or delete rows via the table UI. This mode can be activated by setting the\\xa0num_rows parameter to\\xa0\"dynamic\". E.g.\\n\\npython\\nedited_df = st.data_editor(df, num_rows=\"dynamic\")\\n\\nTo add new rows, scroll to the bottom-most row and click on the “+\" sign in any cell.\\n\\nTo delete rows, select one or more rows and press the delete key on your keyboard.\\n\\nAccess edited data\\n\\nSometimes, it is more convenient to know which cells have been changed rather than getting the entire edited dataframe back. Streamlit makes this easy through the use of session state. If a key parameter is set, Streamlit will store any changes made to the dataframe in the session state.\\n\\nThis snippet shows how you can access changed data using session state:\\n\\npython\\nst.data_editor(df, key=\"data_editor\") # 👈 Set a key\\nst.write(\"Here\\'s the session state:\")\\nst.write(st.session_state[\"data_editor\"]) # 👈 Access the edited data\\n\\nIn this code snippet, the key parameter is set to \"data_editor\". Any changes made to the data in the st.data_editor instance will be tracked by Streamlit and stored in session state under the key \"data_editor\".\\n\\nAfter the data editor is created, the contents of the \"data_editor\" key in session state are printed to the screen using st.write(st.session_state[\"data_editor\"]). This allows you to see the changes made to the original dataframe without having to return the entire dataframe from the data editor.\\n\\nThis can be useful when working with large dataframes and you only need to know which cells have changed, rather than the entire edited dataframe.\\n\\nUse all we\\'ve learned so far and apply them to the above embedded app. Try editing cells, adding new rows, and deleting rows.\\n\\nNotice how edits to the table are reflected in session state: when you make any edits, a rerun is triggered which sends the edits to the backend via st.data_editor\\'s keyed widget state. Its widget state is a JSON object containing three properties: edited_rows, added_rows, and deleted rows:.\\n\\nedited_rows is a dictionary containing all edits. Keys are zero-based row indices and values are dictionaries that map column names to edits (e.g. {0: {\"col1\": ..., \"col2\": ...}}).\\n\\nadded_rows is a list of newly added rows. Each value is a dictionary with the same format as above (e.g. [{\"col1\": ..., \"col2\": ...}]).\\n\\ndeleted_rows is a list of row numbers that have been deleted from the table (e.g. [0, 2]).\\n\\nBulk edits\\n\\nThe data editor includes a feature that allows for bulk editing of cells. Similar to Excel, you can drag a handle across a selection of cells to edit their values in bulk. You can even apply commonly used keyboard shortcuts in spreadsheet software. This is useful when you need to make the same change across multiple cells, rather than editing each cell individually:\\n\\nAutomatic input validation\\n\\nColumn configuration API. Such as\\n\\ntext columns, or\\n\\nnumber columns. You can also set\\n\\nEdit common data structures\\n\\nEditing doesn\\'t just work for Pandas DataFrames! You can also edit lists, tuples, sets, dictionaries, NumPy arrays, or Snowpark & PySpark DataFrames. Most data types will be returned in their original format. But some types (e.g. Snowpark and PySpark) are converted to Pandas DataFrames. To learn about all the supported types, read the st.data_editor API.\\n\\nE.g. you can easily let the user add items to a list:\\n\\npython\\nedited_list = st.data_editor([\"red\", \"green\", \"blue\"], num_rows= \"dynamic\")\\nst.write(\"Here are all the colors you entered:\")\\nst.write(edited_list)\\n\\nOr numpy arrays:\\n\\n```python\\nimport numpy as np\\n\\nst.data_editor(np.array([\\n    [\"st.text_area\", \"widget\", 4.92],\\n    [\"st.markdown\", \"element\", 47.22]\\n]))\\n```\\n\\nOr lists of records:\\n\\npython\\nst.data_editor([\\n    {\"name\": \"st.text_area\", \"type\": \"widget\"},\\n    {\"name\": \"st.markdown\", \"type\": \"element\"},\\n])\\n\\nOr dictionaries and many more types!\\n\\npython\\nst.data_editor({\\n    \"st.text_area\": \"widget\",\\n    \"st.markdown\": \"element\"\\n})\\n\\nConfiguring columns\\n\\nYou are able configure the display and editing behavior of columns via st.dataframe and st.data_editor via the Column configuration API. We have developed the API to let you add images, charts, and clickable URLs in dataframe and data editor columns. Additionally, you can make individual columns editable, set columns as categorical and specify which options they can take, hide the index of the dataframe, and much more.\\n\\nColumn configuration\\n\\nWhen working with data in Streamlit, the st.column_config class is a powerful tool for configuring data display and interaction. Specifically designed for the column_config parameter in st.dataframe and st.data_editor, it provides a suite of methods to tailor your columns to various data types - from simple text and numbers to lists, URLs, images, and more.\\n\\nWhether it\\'s translating temporal data into user-friendly formats or utilizing charts and progress bars for clearer data visualization, column configuration not only provides the user with an enriched data viewing experience but also ensures that you\\'re equipped with the tools to present and interact with your data, just the way you want it.\\n\\nColumn\\n\\nConfigure a generic column.\\n\\npython\\nColumn(\"Streamlit Widgets\", width=\"medium\", help=\"Streamlit **widget** commands 🎈\")\\n\\nText column\\n\\nConfigure a text column.\\n\\npython\\nTextColumn(\"Widgets\", max_chars=50, validate=\"^st\\\\.[a-z_]+$\")\\n\\nNumber column\\n\\nConfigure a number column.\\n\\npython\\nNumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nCheckbox column\\n\\nConfigure a checkbox column.\\n\\npython\\nCheckboxColumn(\"Your favorite?\", help=\"Select your **favorite** widgets\")\\n\\nSelectbox column\\n\\nConfigure a selectbox column.\\n\\npython\\nSelectboxColumn(\"App Category\", options=[\"🤖 LLM\", \"📈 Data Viz\"])\\n\\nDatetime column\\n\\nConfigure a datetime column.\\n\\npython\\nDatetimeColumn(\"Appointment\", min_value=datetime(2023, 6, 1), format=\"D MMM YYYY, h:mm a\")\\n\\nDate column\\n\\nConfigure a date column.\\n\\npython\\nDateColumn(\"Birthday\", max_value=date(2005, 1, 1), format=\"DD.MM.YYYY\")\\n\\nTime column\\n\\nConfigure a time column.\\n\\npython\\nTimeColumn(\"Appointment\", min_value=time(8, 0, 0), format=\"hh:mm a\")\\n\\nList column\\n\\nConfigure a list column.\\n\\npython\\nListColumn(\"Sales (last 6 months)\", width=\"medium\")\\n\\nLink column\\n\\nConfigure a link column.\\n\\npython\\nLinkColumn(\"Trending apps\", max_chars=100, validate=\"^https://.*$\")\\n\\nImage column\\n\\nConfigure an image column.\\n\\npython\\nImageColumn(\"Preview Image\", help=\"The preview screenshots\")\\n\\nLine chart column\\n\\nConfigure a line chart column.\\n\\npython\\nLineChartColumn(\"Sales (last 6 months)\" y_min=0, y_max=100)\\n\\nBar chart column\\n\\nConfigure a bar chart column.\\n\\npython\\nBarChartColumn(\"Marketing spend\" y_min=0, y_max=100)\\n\\nProgress column\\n\\nConfigure a progress column.\\n\\npython\\nProgressColumn(\"Sales volume\", min_value=0, max_value=1000, format=\"$%f\")\\n\\nWe will release in-depth documentation and a blog post on how to configure columns in the next two weeks. Keep at an eye out for updates on this page and the Streamlit blog.\\n\\nThere are techniques you can use with Pandas today to render columns as checkboxes, selectboxes, and change the type of columns, that don\\'t involve the column configuration API. We explore these techniques in the following sections.\\n\\nBoolean columns (checkboxes)\\n\\nTo render columns as checkboxes and clickable checkboxes in st.dataframe and st.data_editor, respectively, set the type of the Pandas column as bool.\\n\\nHere’s an example of creating a Pandas DataFrame with column A containing boolean values. When we display it using st.dataframe, the boolean values are rendered as checkboxes, where True and False values are checked and unchecked, respectively.\\n\\n```python\\nimport pandas as pd\\n\\ncreate a dataframe with a boolean column\\n\\ndf = pd.DataFrame({\"A\": [True, False, True, False]})\\n\\nshow the dataframe with checkboxes\\n\\nst.dataframe(df)\\n```\\n\\nNotice you cannot change their values from the frontend. To let users check and uncheck values, we display the dataframe with st.data_editor instead:\\n\\n```python\\nimport pandas as pd\\n\\ncreate a dataframe with a boolean column\\n\\ndf = pd.DataFrame({\"A\": [True, False, True, False]})\\n\\nshow the data editor with checkboxes\\n\\nst.data_editor(df)\\n```\\n\\nCategorical columns (selectboxes)\\n\\nTo render columns as selectboxes with st.data_editor, set the type of the Pandas column as category:\\n\\n```python\\nimport pandas as pd\\n\\ndf = pd.DataFrame(\\n    {\"command\": [\"st.selectbox\", \"st.slider\", \"st.balloons\", \"st.time_input\"]}\\n)\\ndf[\"command\"] = df[\"command\"].astype(\"category\")\\n\\nedited_df = st.data_editor(df)\\n```\\n\\nIn some cases, you may want users to select categories that aren’t in the original Pandas DataFrame. Let’s say we use df from above. Currently, the command column can take on four unique values. What should we do if we want users to see additional options such as st.button and st.radio?\\n\\nTo add additional categories to the selection, use pandas.Series.cat.add_categories:\\n\\n```python\\nimport pandas as pd\\n\\ndf = pd.DataFrame(\\n    {\"command\": [\"st.selectbox\", \"st.slider\", \"st.balloons\", \"st.time_input\"]}\\n)\\ndf[\"command\"] = (\\n    df[\"command\"].astype(\"category\").cat.add_categories([\"st.button\", \"st.radio\"])\\n)\\n\\nedited_df = st.data_editor(df)\\n```\\n\\nHandling large datasets\\n\\nst.dataframe and st.data_editor have been designed to theoretically handle tables with millions of rows thanks to their highly performant implementation using the glide-data-grid library and HTML canvas. However, the maximum amount of data that an app can realistically handle will depend on several other factors, including:\\n\\nThe maximum size of WebSocket messages: Streamlit\\'s WebSocket messages are configurable via the server.maxMessageSize config option, which limits the amount of data that can be transferred via the WebSocket connection at once.\\n\\nThe server memory: The amount of data that your app can handle will also depend on the amount of memory available on your server. If the server\\'s memory is exceeded, the app may become slow or unresponsive.\\n\\nThe user\\'s browser memory: Since all the data needs to be transferred to the user\\'s browser for rendering, the amount of memory available on the user\\'s device can also affect the app\\'s performance. If the browser\\'s memory is exceeded, it may crash or become unresponsive.\\n\\nIn addition to these factors, a slow network connection can also significantly slow down apps that handle large datasets.\\n\\nWhen handling large datasets with more than 150,000 rows, Streamlit applies additional optimizations and disables column sorting. This can help to reduce the amount of data that needs to be processed at once and improve the app\\'s performance.\\n\\nLimitations\\n\\nWhile Streamlit\\'s data editing capabilities offer a lot of functionality, there are some limitations to be aware of:\\n\\nEditing is enabled for a limited set of column types (TextColumn, NumberColumn, LinkColumn, CheckboxColumn, SelectboxColumn, DateColumn, TimeColumn, and DatetimeColumn). We are actively working on supporting editing for other column types as well, such as images, lists, and charts.\\n\\nEditing of Pandas DataFrames only supports the following index types:\\xa0RangeIndex, (string)\\xa0Index,\\xa0Float64Index,\\xa0Int64Index, and\\xa0UInt64Index.\\n\\nSome actions like deleting rows or searching data can only be triggered via keyboard hotkeys.\\n\\nWe are working to fix the above limitations in future releases, so keep an eye out for updates.', metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content=\"title: Widget semantics\\nslug: /library/advanced-features/widget-semantics\\n\\nAdvanced notes on widget behavior\\n\\nWidgets are magical and often work how you want. But they can have surprising behavior in some situations. Here is a high-level, abstract description of widget behavior, including some common edge-cases:\\n\\nIf you call a widget function before the widget state exists, the widget state defaults to a value. This value depends on the widget and its arguments.\\n\\nA widget function call returns the current widget state value. The return value is a simple Python type, and the exact type depends on the widget and its arguments.\\n\\nWidget states depend on a particular session (browser connection). The actions of one user do not affect the widgets of any other user.\\n\\nA widget's identity depends on the arguments passed to the widget function. If those change, the call will create a new widget (with a default value, per 1).\\n\\nIf you don't call a widget function in a script run, we neither store the widget state nor render the widget. If you call a widget function with the same arguments later, Streamlit treats it as a new widget.\\n\\n4 and 5 are the most likely to be surprising and may pose a problem for some application designs. When you want to persist widget state for recreating a widget, use Session State to work around 5.\", metadata={'source': 'docs/content/library/advanced-features/advanced-widget-behavior.md'}),\n",
       " Document(page_content=\"title: Command-line options\\nslug: /library/advanced-features/cli\\n\\nCommand-line interface\\n\\nWhen you install Streamlit, a command-line (CLI) tool gets installed\\nas well. The purpose of this tool is to run Streamlit apps, change Streamlit configuration options,\\nand help you diagnose and fix issues.\\n\\nTo see all of the supported commands:\\n\\nbash\\nstreamlit --help\\n\\nRun Streamlit apps\\n\\nbash\\nstreamlit run your_script.py [-- script args]\\n\\nRuns your app. At any time you can stop the server with Ctrl+c.\\n\\nWhen passing your script some custom arguments, they must be passed after\\ntwo dashes. Otherwise the arguments get interpreted as arguments to Streamlit\\nitself.\\n\\nTo see the Streamlit 'Hello, World!' example app, run streamlit hello.\\n\\nView Streamlit version\\n\\nTo see what version of Streamlit is installed, just type:\\n\\nbash\\nstreamlit version\\n\\nView documentation\\n\\nbash\\nstreamlit docs\\n\\nOpens the Streamlit documentation (i.e. this website) in a web browser.\\n\\nClear cache\\n\\nbash\\nstreamlit cache clear\\n\\nClears persisted files from the on-disk Streamlit cache, if\\npresent.\\n\\nView all configuration options\\n\\nAs described in Configuration, Streamlit has several\\nconfiguration options. To view them all, including their current values, just type:\\n\\nbash\\nstreamlit config show\", metadata={'source': 'docs/content/library/advanced-features/cli.md'}),\n",
       " Document(page_content='title: Connecting to data\\nslug: /library/advanced-features/connecting-to-data\\n\\nConnecting to data\\n\\nMost Streamlit apps need some kind of data or API access to be useful - either retrieving data to view or saving the results of some user action. This data or API is often part of some remote service, database, or other data source.\\n\\nAnything you can do with Python, including data connections, will generally work in Streamlit. Streamlit\\'s tutorials are a great starting place for many data sources. However:\\n\\nConnecting to data in a Python application is often tedious and annoying.\\n\\nThere are specific considerations for connecting to data from streamlit apps, such as caching and secrets management.\\n\\nStreamlit provides st.experimental_connection() to more easily connect your Streamlit apps to data and APIs with just a few lines of code. This page provides a basic example of using the feature and then focuses on advanced usage.\\n\\nFor a comprehensive overview of this feature, check out this video tutorial by Joshua Carroll, Streamlit\\'s Product Manager for Developer Experience. You\\'ll learn about the feature\\'s utility in creating and managing data connections within your apps by using real-world examples.\\n\\nBasic usage\\n\\nFor basic startup and usage examples, read up on the relevant data source tutorial or our blog post introducing st.experimental_connection. Streamlit has built-in connections to SQL dialects and Snowflake Snowpark. We also maintain installable connections for Cloud File Storage and Google Sheets.\\n\\nIf you are just starting, the best way to learn is to pick a data source you can access and get a minimal example working from one of the pages above 👆. Here, we will provide an ultra-minimal usage example for using a SQLite database. From there, the rest of this page will focus on advanced usage.\\n\\nA simple starting point - using a local SQLite database\\n\\nA local SQLite database could be useful for your app\\'s semi-persistent data storage.\\n\\nCommunity Cloud apps do not guarantee the persistence of local file storage, so the platform may delete data stored using this technique at any time.\\n\\nTo see the example below running live, check out the interactive demo below:\\n\\nStep 1: Install prerequisite library - SQLAlchemy\\n\\nAll SQLConnections in Streamlit use SQLAlchemy. For most other SQL dialects, you also need to install the driver. But the SQLite driver ships with python3, so it isn\\'t necessary.\\n\\nbash\\npip install SQLAlchemy==1.4.0\\n\\nStep 2: Set a database URL in your Streamlit secrets.toml file\\n\\nCreate a directory and file .streamlit/secrets.toml in the same directory your app will run from. Add the following to the file.\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.pets_db]\\nurl = \"sqlite:///pets.db\"\\n```\\n\\nStep 3: Use the connection in your app\\n\\nThe following app creates a connection to the database, uses it to create a table and insert some data, then queries the data back and displays it in a data frame.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nCreate the SQL connection to pets_db as specified in your secrets file.\\n\\nconn = st.experimental_connection(\\'pets_db\\', type=\\'sql\\')\\n\\nInsert some data with conn.session.\\n\\nwith conn.session as s:\\n    s.execute(\\'CREATE TABLE IF NOT EXISTS pet_owners (person TEXT, pet TEXT);\\')\\n    s.execute(\\'DELETE FROM pet_owners;\\')\\n    pet_owners = {\\'jerry\\': \\'fish\\', \\'barbara\\': \\'cat\\', \\'alex\\': \\'puppy\\'}\\n    for k in pet_owners:\\n        s.execute(\\n            \\'INSERT INTO pet_owners (person, pet) VALUES (:owner, :pet);\\',\\n            params=dict(owner=k, pet=pet_owners[k])\\n        )\\n    s.commit()\\n\\nQuery and display the data you inserted\\n\\npet_owners = conn.query(\\'select * from pet_owners\\')\\nst.dataframe(pet_owners)\\n```\\n\\nIn this example, we didn\\'t set a ttl= value on the call to conn.query(), meaning Streamlit caches the result indefinitely as long as the app server runs.\\n\\nNow, on to more advanced topics! 🚀\\n\\nAdvanced topics\\n\\nGlobal secrets, managing multiple apps and multiple data stores\\n\\nStreamlit supports a global secrets file specified in the user\\'s home directory, such as ~/.streamlit/secrets.toml. If you build or manage multiple apps, we recommend using a global credential or secret file for local development across apps. With this approach, you only need to set up and manage your credentials in one place, and connecting a new app to your existing data sources is effectively a one-liner. It also reduces the risk of accidentally checking in your credentials to git since they don\\'t need to exist in the project repository.\\n\\nFor cases where you have multiple similar data sources that you connect to during local development (such as a local vs. staging database), you can define different connection sections in your secrets or credentials file for different environments and then decide which to use at runtime. st.experimental_connection supports this with the name=env:<MY_NAME_VARIABLE> syntax.\\n\\nE.g., say I have a local and a staging MySQL database and want to connect my app to either at different times. I could create a global secrets file like this:\\n\\n```toml\\n\\n~/.streamlit/secrets.toml\\n\\n[connections.local]\\nurl = \"mysql://me:****@localhost:3306/local_db\"\\n\\n[connections.staging]\\nurl = \"mysql://jdoe:**@staging.acmecorp.com:3306/staging_db\"\\n```\\n\\nThen I can configure my app connection to take its name from a specified environment variable\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nconn = st.experimental_connection(\"env:DB_CONN\", \"sql\")\\ndf = conn.query(\"select * from mytable\")\\n\\n...\\n\\n```\\n\\nNow I can specify whether to connect to local or staging at runtime by setting the DB_CONN environment variable.\\n\\n```bash\\n\\nconnect to local\\n\\nDB_CONN=local streamlit run streamlit_app.py\\n\\nconnect to staging\\n\\nDB_CONN=staging streamlit run streamlit_app.py\\n```\\n\\nAdvanced SQLConnection configuration\\n\\nThe SQLConnection configuration uses SQLAlchemy create_engine() function. It will take a single URL argument or attempt to construct a URL from several parts (username, database, host, and so on) using SQLAlchemy.engine.URL.create().\\n\\nSeveral popular SQLAlchemy dialects, such as Snowflake and Google BigQuery, can be configured using additional arguments to create_engine() besides the URL. These can be passed as **kwargs to the st.experimental_connection call directly or specified in an additional secrets section called create_engine_kwargs.\\n\\nE.g. snowflake-sqlalchemy takes an additional connect_args argument as a dictionary for configuration that isn’t supported in the URL. These could be specified as follows:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowflake]\\nurl = \"snowflake://@/\"\\n\\n[connections.snowflake.create_engine_kwargs.connect_args]\\nauthenticator = \"externalbrowser\"\\nwarehouse = \"xxx\"\\nrole = \"xxx\"\\n```\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nurl and connect_args from secrets.toml above are picked up and used here\\n\\nconn = st.experimental_connection(\"snowflake\", \"sql\")\\n\\n...\\n\\n```\\n\\nAlternatively, this could be specified entirely in **kwargs.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nsecrets.toml is not needed\\n\\nconn = st.experimental_connection(\\n    \"snowflake\",\\n    \"sql\",\\n    url = \"snowflake://@/\",\\n    connect_args = dict(\\n        authenticator = \"externalbrowser\",\\n        warehouse = \"xxx\",\\n        role = \"xxx\",\\n    )\\n)\\n\\n...\\n\\n```\\n\\nYou can also provide both kwargs and secrets.toml values, and they will be merged (typically, kwargs take precedence).\\n\\nConnection considerations in frequently used or long-running apps\\n\\nBy default, connection objects are cached without expiration using st.cache_resource. In most cases this is desired. You can do st.experimental_connection(\\'myconn\\', type=MyConnection, ttl=<N>) if you want the connection object to expire after some time.\\n\\nMany connection types are expected to be long-running or completely stateless, so expiration is unnecessary. Suppose a connection becomes stale (such as a cached token expiring or a server-side connection being closed). In that case, every connection has a reset() method, which will invalidate the cached version and cause Streamlit to recreate the connection the next time it is retrieved\\n\\nst.cache_data without an expiration. When an app can run many different read operations with large results, it can cause high memory usage over time and results to become stale in a long-running app, the same as with any other usage of\\n\\nCaching for more information.\\n\\nFor apps that could get significant concurrent usage, ensure that you understand any thread safety implications of your connection, particularly when using a connection built by a third party. Connections built by Streamlit should provide thread-safe operations by default.\\n\\nBuild your own connection\\n\\nBuilding your own basic connection implementation using an existing driver or SDK is quite straightforward in most cases. However, you can add more complex functionality with further effort. This custom implementation can be a great way to extend support to a new data source and contribute to the Streamlit ecosystem.\\n\\nMaintaining a tailored internal Connection implementation across many apps can be a powerful practice for organizations with frequently used access patterns and data sources.\\n\\nCheck out the Build your own Connection page in the st.experimental connection demo app below for a quick tutorial and working implementation. This demo builds a minimal but very functional Connection on top of DuckDB.\\n\\nThe typical steps are:\\n\\nDeclare the Connection class, extending ExperimentalBaseConnection with the type parameter bound to the underlying connection object:\\n\\n```python\\n   from streamlit.connections import ExperimentalBaseConnection\\n   import duckdb\\n\\nclass DuckDBConnection(ExperimentalBaseConnection[duckdb.DuckDBPyConnection])\\n   ```\\n\\nImplement the _connect method that reads any kwargs, external config/credential locations, and Streamlit secrets to initialize the underlying connection:\\n\\npython\\n   def _connect(self, **kwargs) -> duckdb.DuckDBPyConnection:\\n       if \\'database\\' in kwargs:\\n           db = kwargs.pop(\\'database\\')\\n       else:\\n           db = self._secrets[\\'database\\']\\n       return duckdb.connect(database=db, **kwargs)\\n\\nAdd useful helper methods that make sense for your connection (wrapping them in st.cache_data where caching is desired)\\n\\nConnection-building best practices\\n\\nWe recommend applying the following best practices to make your Connection consistent with the Connections built into Streamlit and the wider Streamlit ecosystem. These practices are especially important for Connections that you intend to distribute publicly.\\n\\nExtend existing drivers or SDKs, and default to semantics that makes sense for their existing users.\\n\\nYou should rarely need to implement complex data access logic from scratch when building a Connection. Use existing popular Python drivers and clients whenever possible. Doing so makes your Connection easier to maintain, more secure, and enables users to get the latest features. E.g. SQLConnection extends SQLAlchemy, FileConnection extends fsspec, GsheetsConnection extends gspread, etc.\\n\\nConsider using access patterns, method/argument naming, and return values that are consistent with the underlying package and familiar to existing users of that package.\\n\\nIntuitive, easy to use read methods.\\n\\nMuch of the power of st.experimental_connection is providing intuitive, easy-to-use read methods that enable app developers to get started quickly. Most connections should expose at least one read method that is:\\n\\nNamed with a simple verb, like read(), query(), or get()\\n\\nWrapped by st.cache_data by default, with at least ttl= argument supported\\n\\nIf the result is in a tabular format, it returns a pandas DataFrame\\n\\nProvides commonly used keyword arguments (such as paging or formatting) with sensible defaults - ideally, the common case requires only 1-2 arguments.\\n\\nConfig, secrets, and precedence in _connect method.\\n\\nEvery Connection should support commonly used connection parameters provided via Streamlit secrets and keyword arguments. The names should match the ones used when initializing or configuring the underlying package.\\n\\nAdditionally, where relevant, Connections should support data source specific configuration through existing standard environment variables or config / credential files. In many cases, the underlying package provides constructors or factory functions that already handle this easily.\\n\\nWhen you can specify the same connection parameters in multiple places, we recommend using the following precedence order when possible (highest to lowest):\\n\\nKeyword arguments specified in the code\\n\\nStreamlit secrets\\n\\ndata source specific configuration (if relevant)\\n\\nHandling thread safety and stale connections.\\n\\nConnections should provide thread-safe operations when practical (which should be most of the time) and clearly document any considerations around this. Most underlying drivers or SDKs should provide thread-safe objects or methods - use these when possible.\\n\\nIf the underlying driver or SDK has a risk of stateful connection objects becoming stale or invalid, consider building a low impact health check or reset/retry pattern into the access methods. The SQLConnection built into Streamlit has a good example of this pattern using tenacity and the built-in Connection.reset() method. An alternate approach is to encourage developers to set an appropriate TTL on the st.experimental_connection() call to ensure it periodically reinitializes the connection object.', metadata={'source': 'docs/content/library/advanced-features/connecting-to-data.md'}),\n",
       " Document(page_content='title: Theming\\nslug: /library/advanced-features/theming\\n\\nTheming\\n\\nIn this guide, we provide examples of how Streamlit page elements are affected\\nby the various theme config options. For a more high-level overview of\\nStreamlit themes, see the Themes section of the\\nmain concepts documentation.\\n\\nStreamlit themes are defined using regular config options: a theme can be set\\nvia command line flag when starting your app using streamlit run or by\\ndefining it in the [theme] section of a .streamlit/config.toml file. For\\nmore information on setting config options, please refer to the\\nStreamlit configuration documentation.\\n\\nThe following config options show the default Streamlit Light theme recreated\\nin the [theme] section of a .streamlit/config.toml file.\\n\\ntoml\\n[theme]\\nprimaryColor=\"#F63366\"\\nbackgroundColor=\"#FFFFFF\"\\nsecondaryBackgroundColor=\"#F0F2F6\"\\ntextColor=\"#262730\"\\nfont=\"sans serif\"\\n\\nLet\\'s go through each of these options, providing screenshots to demonstrate\\nwhat parts of a Streamlit app they affect where needed.\\n\\nprimaryColor\\n\\nprimaryColor defines the accent color most often used throughout a Streamlit\\napp. A few examples of Streamlit widgets that use primaryColor include\\nst.checkbox, st.slider, and st.text_input (when focused).\\n\\nAny CSS color can be used as the value for primaryColor and the other color\\noptions below. This means that theme colors can be specified in hex or with\\nbrowser-supported color names like \"green\", \"yellow\", and\\n\"chartreuse\". They can even be defined in the RGB and HSL formats!\\n\\nbackgroundColor\\n\\nDefines the background color used in the main content area of your app.\\n\\nsecondaryBackgroundColor\\n\\nThis color is used where a second background color is needed for added\\ncontrast. Most notably, it is the sidebar\\'s background color. It is also used \\nas the background color for most interactive widgets.\\n\\ntextColor\\n\\nThis option controls the text color for most of your Streamlit app.\\n\\nfont\\n\\nSelects the font used in your Streamlit app. Valid values are \"sans serif\",\\n\"serif\", and \"monospace\". This option defaults to \"sans serif\" if unset\\nor invalid.\\n\\nNote that code blocks are always rendered using the monospace font regardless of\\nthe font selected here.\\n\\nbase\\n\\nAn easy way to define custom themes that make small changes to one of the\\npreset Streamlit themes is to use the base option. Using base, the\\nStreamlit Light theme can be recreated as a custom theme by writing the\\nfollowing:\\n\\ntoml\\n[theme]\\nbase=\"light\"\\n\\nThe base option allows you to specify a preset Streamlit theme that your\\ncustom theme inherits from. Any theme config options not defined in your theme\\nsettings have their values set to those of the base theme. Valid values for\\nbase are \"light\" and \"dark\".\\n\\nFor example, the following theme config defines a custom theme nearly identical\\nto the Streamlit Dark theme, but with a new primaryColor.\\n\\ntoml\\n[theme]\\nbase=\"dark\"\\nprimaryColor=\"purple\"\\n\\nIf base itself is omitted, it defaults to \"light\", so you can define a\\ncustom theme that changes the font of the Streamlit Light theme to serif with\\nthe following config\\n\\ntoml\\n[theme]\\nfont=\"serif\"', metadata={'source': 'docs/content/library/advanced-features/theming.md'}),\n",
       " Document(page_content='title: Configuration\\nslug: /library/advanced-features/configuration\\n\\nConfiguration\\n\\nStreamlit provides four different ways to set configuration options. This list is in reverse order of precedence, i.e. command line flags take precedence over environment variables when the same configuration option is provided multiple times.\\n\\nIf changes to .streamlit/config.toml are made while the app is running, the server needs to be restarted for changes to be reflected in the app.\\n\\nIn a global config file at ~/.streamlit/config.toml for macOS/Linux or %userprofile%/.streamlit/config.toml for Windows:\\n\\ntoml\\n   [server]\\n   port = 80\\n\\nIn a per-project config file at $CWD/.streamlit/config.toml, where\\n   $CWD is the folder you\\'re running Streamlit from.\\n\\nThrough STREAMLIT_* environment variables, such as:\\n\\nbash\\n   export STREAMLIT_SERVER_PORT=80\\n   export STREAMLIT_SERVER_COOKIE_SECRET=dontforgottochangeme\\n\\nAs flags on the command line when running streamlit run:\\n\\nbash\\n   streamlit run your_script.py --server.port 80\\n\\nTo set configuration options on Streamlit Community Cloud, read Optionally, add a configuration file in the Streamlit Community Cloud docs.\\n\\nTelemetry\\n\\nAs mentioned during the installation process, Streamlit collects usage statistics. You can find out\\nmore by reading our Privacy Notice, but the high-level\\nsummary is that although we collect telemetry data we cannot see and do not store information\\ncontained in Streamlit apps.\\n\\nIf you\\'d like to opt out of usage statistics, add the following to your config file:\\n\\ntoml\\n[browser]\\ngatherUsageStats = false\\n\\nTheming\\n\\nYou can change the base colors of your app using the [theme] section of the configuration system.\\nTo learn more, see Theming.\\n\\nView all configuration options\\n\\nAs described in Command-line options, you can\\nview all available configuration option using:\\n\\nbash\\nstreamlit config show\\n\\nBelow are all the sections and options you can have in your .streamlit/config.toml file:\\n\\nGlobal\\n\\n```toml\\n[global]\\n\\nBy default, Streamlit checks if the Python watchdog module is available\\n\\nand, if not, prints a warning asking for you to install it. The watchdog\\n\\nmodule is not required, but highly recommended. It improves Streamlit\\'s\\n\\nability to detect changes to files in your filesystem.\\n\\nIf you\\'d like to turn off this warning, set this to True.\\n\\nDefault: false\\n\\ndisableWatchdogWarning = false\\n\\nBy default, Streamlit displays a warning when a user sets both a widget\\n\\ndefault value in the function defining the widget and a widget value via\\n\\nthe widget\\'s key in st.session_state.\\n\\nIf you\\'d like to turn off this warning, set this to True.\\n\\nDefault: false\\n\\ndisableWidgetStateDuplicationWarning = false\\n\\nIf True, will show a warning when you run a Streamlit-enabled script\\n\\nvia \"python my_script.py\".\\n\\nDefault: true\\n\\nshowWarningOnDirectExecution = true\\n\\nDataFrame serialization.\\n\\nAcceptable values:\\n\\n\\'legacy\\' : Serialize DataFrames using Streamlit\\'s custom format. Slow\\n\\nbut battle-tested.\\n\\n\\'arrow\\'  : Serialize DataFrames using Apache Arrow. Much faster and\\n\\nversatile.\\n\\nDefault: \"arrow\"\\n\\ndataFrameSerialization = \"arrow\"\\n```\\n\\nLogger\\n\\n```toml\\n[logger]\\n\\nLevel of logging: \\'error\\', \\'warning\\', \\'info\\', or \\'debug\\'.\\n\\nDefault: \\'info\\'\\n\\nlevel = \"info\"\\n\\nString format for logging messages. If logger.datetimeFormat is set,\\n\\nlogger messages will default to %(asctime)s.%(msecs)03d %(message)s. See\\n\\nPython\\'s documentation for available attributes:\\n\\nhttps://docs.python.org/2.6/library/logging.html#formatter-objects\\n\\nDefault: \"%(asctime)s %(message)s\"\\n\\nmessageFormat = \"%(asctime)s %(message)s\"\\n```\\n\\nClient\\n\\n```toml\\n[client]\\n\\nWhether to enable st.cache. This does not affect st.cache_data or\\n\\nst.cache_resource.\\n\\nDefault: true\\n\\ncaching = true\\n\\nIf false, makes your Streamlit script not draw to a\\n\\nStreamlit app.\\n\\nDefault: true\\n\\ndisplayEnabled = true\\n\\nControls whether uncaught app exceptions and deprecation warnings\\n\\nare displayed in the browser. By default, this is set to True and\\n\\nStreamlit displays app exceptions and associated tracebacks, and\\n\\ndeprecation warnings, in the browser.\\n\\nIf set to False, deprecation warnings and full exception messages\\n\\nwill print to the console only. Exceptions will still display in the\\n\\nbrowser with a generic error message. For now, the exception type and\\n\\ntraceback show in the browser also, but they will be removed in the\\n\\nfuture.\\n\\nDefault: true\\n\\nshowErrorDetails = true\\n\\nChange the visibility of items in the toolbar, options menu,\\n\\nand settings dialog (top right of the app).\\n\\nAllowed values:\\n\\n\"auto\"      : Show the developer options if the app is accessed through\\n\\nlocalhost or through Streamlit Community Cloud as a developer.\\n\\nHide them otherwise.\\n\\n\"developer\" : Show the developer options.\\n\\n\"viewer\"    : Hide the developer options.\\n\\n\"minimal\"   : Show only options set externally (e.g. through\\n\\nStreamlit Community Cloud) or through st.set_page_config.\\n\\nIf there are no options left, hide the menu.\\n\\nDefault: \"auto\"\\n\\ntoolbarMode = \"auto\"\\n```\\n\\nRunner\\n\\n```toml\\n[runner]\\n\\nAllows you to type a variable or string by itself in a single line of\\n\\nPython code to write it to the app.\\n\\nDefault: true\\n\\nmagicEnabled = true\\n\\nInstall a Python tracer to allow you to stop or pause your script at\\n\\nany point and introspect it. As a side-effect, this slows down your\\n\\nscript\\'s execution.\\n\\nDefault: false\\n\\ninstallTracer = false\\n\\nSets the MPLBACKEND environment variable to Agg inside Streamlit to\\n\\nprevent Python crashing.\\n\\nDefault: true\\n\\nfixMatplotlib = true\\n\\nRun the Python Garbage Collector after each script execution. This\\n\\ncan help avoid excess memory use in Streamlit apps, but could\\n\\nintroduce delay in rerunning the app script for high-memory-use\\n\\napplications.\\n\\nDefault: true\\n\\npostScriptGC = true\\n\\nHandle script rerun requests immediately, rather than waiting for script\\n\\nexecution to reach a yield point. This makes Streamlit much more\\n\\nresponsive to user interaction, but it can lead to race conditions in\\n\\napps that mutate session_state data outside of explicit session_state\\n\\nassignment statements.\\n\\nDefault: true\\n\\nfastReruns = true\\n\\nRaise an exception after adding unserializable data to Session State.\\n\\nSome execution environments may require serializing all data in Session\\n\\nState, so it may be useful to detect incompatibility during development,\\n\\nor when the execution environment will stop supporting it in the future.\\n\\nDefault: false\\n\\nenforceSerializableSessionState = false\\n```\\n\\nServer\\n\\n```toml\\n[server]\\n\\nList of folders that should not be watched for changes. This\\n\\nimpacts both \"Run on Save\" and @st.cache.\\n\\nRelative paths will be taken as relative to the current working directory.\\n\\nExample: [\\'/home/user1/env\\', \\'relative/path/to/folder\\']\\n\\nDefault: []\\n\\nfolderWatchBlacklist = []\\n\\nChange the type of file watcher used by Streamlit, or turn it off\\n\\ncompletely.\\n\\nAllowed values:\\n\\n\"auto\"     : Streamlit will attempt to use the watchdog module, and\\n\\nfalls back to polling if watchdog is not available.\\n\\n\"watchdog\" : Force Streamlit to use the watchdog module.\\n\\n\"poll\"     : Force Streamlit to always use polling.\\n\\n\"none\"     : Streamlit will not watch files.\\n\\nDefault: \"auto\"\\n\\nfileWatcherType = \"auto\"\\n\\nSymmetric key used to produce signed cookies. If deploying on multiple\\n\\nreplicas, this should be set to the same value across all replicas to ensure\\n\\nthey all share the same secret.\\n\\nDefault: randomly generated secret key.\\n\\ncookieSecret = \"a-random-key-appears-here\"\\n\\nIf false, will attempt to open a browser window on start.\\n\\nDefault: false unless (1) we are on a Linux box where DISPLAY is unset, or\\n\\n(2) we are running in the Streamlit Atom plugin.\\n\\nheadless = false\\n\\nAutomatically rerun script when the file is modified on disk.\\n\\nDefault: false\\n\\nrunOnSave = false\\n\\nThe address where the server will listen for client and browser\\n\\nconnections. Use this if you want to bind the server to a specific address.\\n\\nIf set, the server will only be accessible from this address, and not from\\n\\nany aliases (like localhost).\\n\\nDefault: (unset)\\n\\naddress =\\n\\nThe port where the server will listen for browser connections.\\n\\nDefault: 8501\\n\\nport = 8501\\n\\nThe base path for the URL where Streamlit should be served from.\\n\\nDefault: \"\"\\n\\nbaseUrlPath = \"\"\\n\\nEnables support for Cross-Origin Resource Sharing (CORS) protection, for\\n\\nadded security.\\n\\nDue to conflicts between CORS and XSRF, if server.enableXsrfProtection is\\n\\non and server.enableCORS is off at the same time, we will prioritize\\n\\nserver.enableXsrfProtection.\\n\\nDefault: true\\n\\nenableCORS = true\\n\\nEnables support for Cross-Site Request Forgery (XSRF) protection, for added\\n\\nsecurity.\\n\\nDue to conflicts between CORS and XSRF, if server.enableXsrfProtection is\\n\\non and server.enableCORS is off at the same time, we will prioritize\\n\\nserver.enableXsrfProtection.\\n\\nDefault: true\\n\\nenableXsrfProtection = true\\n\\nMax size, in megabytes, for files uploaded with the file_uploader.\\n\\nDefault: 200\\n\\nmaxUploadSize = 200\\n\\nMax size, in megabytes, of messages that can be sent via the WebSocket\\n\\nconnection.\\n\\nDefault: 200\\n\\nmaxMessageSize = 200\\n\\nEnables support for websocket compression.\\n\\nDefault: false\\n\\nenableWebsocketCompression = false\\n\\nEnable serving files from a static directory in the running app\\'s\\n\\ndirectory.\\n\\nDefault: false\\n\\nenableStaticServing = false\\n\\nServer certificate file for connecting via HTTPS.\\n\\nMust be set at the same time as \"server.sslKeyFile\".\\n\\n[\\'DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through\\n\\nsecurity audits or performance tests. For the production environment, we\\n\\nrecommend performing SSL termination by the load balancer or the reverse\\n\\nproxy.\\']\\n\\nsslCertFile =\\n\\nCryptographic key file for connecting via HTTPS.\\n\\nMust be set at the same time as \"server.sslCertFile\".\\n\\n[\\'DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through\\n\\nsecurity audits or performance tests. For the production environment, we\\n\\nrecommend performing SSL termination by the load balancer or the reverse\\n\\nproxy.\\']\\n\\nsslKeyFile =\\n\\n```\\n\\nBrowser\\n\\n```toml\\n[browser]\\n\\nInternet address where users should point their browsers in order to\\n\\nconnect to the app. Can be IP address or DNS name and path.\\n\\nThis is used to:\\n\\nSet the correct URL for CORS and XSRF protection purposes.\\n\\nShow the URL on the terminal\\n\\nOpen the browser\\n\\nDefault: \"localhost\"\\n\\nserverAddress = \"localhost\"\\n\\nWhether to send usage statistics to Streamlit.\\n\\nDefault: true\\n\\ngatherUsageStats = true\\n\\nPort where users should point their browsers in order to connect to the\\n\\napp.\\n\\nThis is used to:\\n\\nSet the correct URL for CORS and XSRF protection purposes.\\n\\nShow the URL on the terminal\\n\\nOpen the browser\\n\\nDefault: whatever value is set in server.port.\\n\\nserverPort = 8501\\n```\\n\\nMapbox\\n\\n```toml\\n[mapbox]\\n\\nConfigure Streamlit to use a custom Mapbox\\n\\ntoken for elements like st.pydeck_chart and st.map.\\n\\nTo get a token for yourself, create an account at\\n\\nhttps://mapbox.com. It\\'s free (for moderate usage levels)!\\n\\nDefault: \"\"\\n\\ntoken = \"\"\\n```\\n\\nDeprecation\\n\\n```toml\\n[deprecation]\\n\\nSet to false to disable the deprecation warning for the file uploader\\n\\nencoding.\\n\\nDefault: true\\n\\nshowfileUploaderEncoding = true\\n\\nSet to false to disable the deprecation warning for using the global pyplot\\n\\ninstance.\\n\\nDefault: true\\n\\nshowPyplotGlobalUse = true\\n```\\n\\nTheme\\n\\n```toml\\n[theme]\\n\\nThe preset Streamlit theme that your custom theme inherits from.\\n\\nOne of \"light\" or \"dark\".\\n\\nbase =\\n\\nPrimary accent color for interactive elements.\\n\\nprimaryColor =\\n\\nBackground color for the main content area.\\n\\nbackgroundColor =\\n\\nBackground color used for the sidebar and most interactive widgets.\\n\\nsecondaryBackgroundColor =\\n\\nColor used for almost all text.\\n\\ntextColor =\\n\\nFont family for all text in the app, except code blocks. One of \"sans serif\",\\n\\n\"serif\", or \"monospace\".\\n\\nfont =\\n\\n```\\n\\n```toml\\n[global]\\n\\nBy default, Streamlit checks if the Python watchdog module is available\\n\\nand, if not, prints a warning asking for you to install it. The watchdog\\n\\nmodule is not required, but highly recommended. It improves Streamlit\\'s\\n\\nability to detect changes to files in your filesystem.\\n\\nIf you\\'d like to turn off this warning, set this to True.\\n\\nDefault: false\\n\\ndisableWatchdogWarning = false\\n\\nBy default, Streamlit displays a warning when a user sets both a widget\\n\\ndefault value in the function defining the widget and a widget value via\\n\\nthe widget\\'s key in st.session_state.\\n\\nIf you\\'d like to turn off this warning, set this to True.\\n\\nDefault: false\\n\\ndisableWidgetStateDuplicationWarning = false\\n\\nIf True, will show a warning when you run a Streamlit-enabled script\\n\\nvia \"python my_script.py\".\\n\\nDefault: true\\n\\nshowWarningOnDirectExecution = true\\n\\nDataFrame serialization.\\n\\nAcceptable values:\\n\\n\\'legacy\\' : Serialize DataFrames using Streamlit\\'s custom format. Slow\\n\\nbut battle-tested.\\n\\n\\'arrow\\'  : Serialize DataFrames using Apache Arrow. Much faster and\\n\\nversatile.\\n\\nDefault: \"arrow\"\\n\\ndataFrameSerialization = \"arrow\"\\n\\n[logger]\\n\\nLevel of logging: \\'error\\', \\'warning\\', \\'info\\', or \\'debug\\'.\\n\\nDefault: \\'info\\'\\n\\nlevel = \"info\"\\n\\nString format for logging messages. If logger.datetimeFormat is set,\\n\\nlogger messages will default to %(asctime)s.%(msecs)03d %(message)s. See\\n\\nPython\\'s documentation for available attributes:\\n\\nhttps://docs.python.org/2.6/library/logging.html#formatter-objects\\n\\nDefault: \"%(asctime)s %(message)s\"\\n\\nmessageFormat = \"%(asctime)s %(message)s\"\\n\\n[client]\\n\\nWhether to enable st.cache. This does not affect st.cache_data or\\n\\nst.cache_resource.\\n\\nDefault: true\\n\\ncaching = true\\n\\nIf false, makes your Streamlit script not draw to a\\n\\nStreamlit app.\\n\\nDefault: true\\n\\ndisplayEnabled = true\\n\\nControls whether uncaught app exceptions and deprecation warnings\\n\\nare displayed in the browser. By default, this is set to True and\\n\\nStreamlit displays app exceptions and associated tracebacks, and\\n\\ndeprecation warnings, in the browser.\\n\\nIf set to False, deprecation warnings and full exception messages\\n\\nwill print to the console only. Exceptions will still display in the\\n\\nbrowser with a generic error message. For now, the exception type and\\n\\ntraceback show in the browser also, but they will be removed in the\\n\\nfuture.\\n\\nDefault: true\\n\\nshowErrorDetails = true\\n\\nChange the visibility of items in the toolbar, options menu,\\n\\nand settings dialog (top right of the app).\\n\\nAllowed values:\\n\\n\"auto\"      : Show the developer options if the app is accessed through\\n\\nlocalhost or through Streamlit Community Cloud as a developer.\\n\\nHide them otherwise.\\n\\n\"developer\" : Show the developer options.\\n\\n\"viewer\"    : Hide the developer options.\\n\\n\"minimal\"   : Show only options set externally (e.g. through\\n\\nStreamlit Community Cloud) or through st.set_page_config.\\n\\nIf there are no options left, hide the menu.\\n\\nDefault: \"auto\"\\n\\ntoolbarMode = \"auto\"\\n\\n[runner]\\n\\nAllows you to type a variable or string by itself in a single line of\\n\\nPython code to write it to the app.\\n\\nDefault: true\\n\\nmagicEnabled = true\\n\\nInstall a Python tracer to allow you to stop or pause your script at\\n\\nany point and introspect it. As a side-effect, this slows down your\\n\\nscript\\'s execution.\\n\\nDefault: false\\n\\ninstallTracer = false\\n\\nSets the MPLBACKEND environment variable to Agg inside Streamlit to\\n\\nprevent Python crashing.\\n\\nDefault: true\\n\\nfixMatplotlib = true\\n\\nRun the Python Garbage Collector after each script execution. This\\n\\ncan help avoid excess memory use in Streamlit apps, but could\\n\\nintroduce delay in rerunning the app script for high-memory-use\\n\\napplications.\\n\\nDefault: true\\n\\npostScriptGC = true\\n\\nHandle script rerun requests immediately, rather than waiting for script\\n\\nexecution to reach a yield point. This makes Streamlit much more\\n\\nresponsive to user interaction, but it can lead to race conditions in\\n\\napps that mutate session_state data outside of explicit session_state\\n\\nassignment statements.\\n\\nDefault: true\\n\\nfastReruns = true\\n\\nRaise an exception after adding unserializable data to Session State.\\n\\nSome execution environments may require serializing all data in Session\\n\\nState, so it may be useful to detect incompatibility during development,\\n\\nor when the execution environment will stop supporting it in the future.\\n\\nDefault: false\\n\\nenforceSerializableSessionState = false\\n\\n[server]\\n\\nList of folders that should not be watched for changes. This\\n\\nimpacts both \"Run on Save\" and @st.cache.\\n\\nRelative paths will be taken as relative to the current working directory.\\n\\nExample: [\\'/home/user1/env\\', \\'relative/path/to/folder\\']\\n\\nDefault: []\\n\\nfolderWatchBlacklist = []\\n\\nChange the type of file watcher used by Streamlit, or turn it off\\n\\ncompletely.\\n\\nAllowed values:\\n\\n\"auto\"     : Streamlit will attempt to use the watchdog module, and\\n\\nfalls back to polling if watchdog is not available.\\n\\n\"watchdog\" : Force Streamlit to use the watchdog module.\\n\\n\"poll\"     : Force Streamlit to always use polling.\\n\\n\"none\"     : Streamlit will not watch files.\\n\\nDefault: \"auto\"\\n\\nfileWatcherType = \"auto\"\\n\\nSymmetric key used to produce signed cookies. If deploying on multiple\\n\\nreplicas, this should be set to the same value across all replicas to ensure\\n\\nthey all share the same secret.\\n\\nDefault: randomly generated secret key.\\n\\ncookieSecret = \"a-random-key-appears-here\"\\n\\nIf false, will attempt to open a browser window on start.\\n\\nDefault: false unless (1) we are on a Linux box where DISPLAY is unset, or\\n\\n(2) we are running in the Streamlit Atom plugin.\\n\\nheadless = false\\n\\nAutomatically rerun script when the file is modified on disk.\\n\\nDefault: false\\n\\nrunOnSave = false\\n\\nThe address where the server will listen for client and browser\\n\\nconnections. Use this if you want to bind the server to a specific address.\\n\\nIf set, the server will only be accessible from this address, and not from\\n\\nany aliases (like localhost).\\n\\nDefault: (unset)\\n\\naddress =\\n\\nThe port where the server will listen for browser connections.\\n\\nDefault: 8501\\n\\nport = 8501\\n\\nThe base path for the URL where Streamlit should be served from.\\n\\nDefault: \"\"\\n\\nbaseUrlPath = \"\"\\n\\nEnables support for Cross-Origin Resource Sharing (CORS) protection, for\\n\\nadded security.\\n\\nDue to conflicts between CORS and XSRF, if server.enableXsrfProtection is\\n\\non and server.enableCORS is off at the same time, we will prioritize\\n\\nserver.enableXsrfProtection.\\n\\nDefault: true\\n\\nenableCORS = true\\n\\nEnables support for Cross-Site Request Forgery (XSRF) protection, for added\\n\\nsecurity.\\n\\nDue to conflicts between CORS and XSRF, if server.enableXsrfProtection is\\n\\non and server.enableCORS is off at the same time, we will prioritize\\n\\nserver.enableXsrfProtection.\\n\\nDefault: true\\n\\nenableXsrfProtection = true\\n\\nMax size, in megabytes, for files uploaded with the file_uploader.\\n\\nDefault: 200\\n\\nmaxUploadSize = 200\\n\\nMax size, in megabytes, of messages that can be sent via the WebSocket\\n\\nconnection.\\n\\nDefault: 200\\n\\nmaxMessageSize = 200\\n\\nEnables support for websocket compression.\\n\\nDefault: false\\n\\nenableWebsocketCompression = false\\n\\nEnable serving files from a static directory in the running app\\'s\\n\\ndirectory.\\n\\nDefault: false\\n\\nenableStaticServing = false\\n\\nServer certificate file for connecting via HTTPS.\\n\\nMust be set at the same time as \"server.sslKeyFile\".\\n\\n[\\'DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through\\n\\nsecurity audits or performance tests. For the production environment, we\\n\\nrecommend performing SSL termination by the load balancer or the reverse\\n\\nproxy.\\']\\n\\nsslCertFile =\\n\\nCryptographic key file for connecting via HTTPS.\\n\\nMust be set at the same time as \"server.sslCertFile\".\\n\\n[\\'DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through\\n\\nsecurity audits or performance tests. For the production environment, we\\n\\nrecommend performing SSL termination by the load balancer or the reverse\\n\\nproxy.\\']\\n\\nsslKeyFile =\\n\\n[browser]\\n\\nInternet address where users should point their browsers in order to\\n\\nconnect to the app. Can be IP address or DNS name and path.\\n\\nThis is used to:\\n\\nSet the correct URL for CORS and XSRF protection purposes.\\n\\nShow the URL on the terminal\\n\\nOpen the browser\\n\\nDefault: \"localhost\"\\n\\nserverAddress = \"localhost\"\\n\\nWhether to send usage statistics to Streamlit.\\n\\nDefault: true\\n\\ngatherUsageStats = true\\n\\nPort where users should point their browsers in order to connect to the\\n\\napp.\\n\\nThis is used to:\\n\\nSet the correct URL for CORS and XSRF protection purposes.\\n\\nShow the URL on the terminal\\n\\nOpen the browser\\n\\nDefault: whatever value is set in server.port.\\n\\nserverPort = 8501\\n\\n[mapbox]\\n\\nConfigure Streamlit to use a custom Mapbox\\n\\ntoken for elements like st.pydeck_chart and st.map.\\n\\nTo get a token for yourself, create an account at\\n\\nhttps://mapbox.com. It\\'s free (for moderate usage levels)!\\n\\nDefault: \"\"\\n\\ntoken = \"\"\\n\\n[deprecation]\\n\\nSet to false to disable the deprecation warning for the file uploader\\n\\nencoding.\\n\\nDefault: true\\n\\nshowfileUploaderEncoding = true\\n\\nSet to false to disable the deprecation warning for using the global pyplot\\n\\ninstance.\\n\\nDefault: true\\n\\nshowPyplotGlobalUse = true\\n\\n[theme]\\n\\nThe preset Streamlit theme that your custom theme inherits from.\\n\\nOne of \"light\" or \"dark\".\\n\\nbase =\\n\\nPrimary accent color for interactive elements.\\n\\nprimaryColor =\\n\\nBackground color for the main content area.\\n\\nbackgroundColor =\\n\\nBackground color used for the sidebar and most interactive widgets.\\n\\nsecondaryBackgroundColor =\\n\\nColor used for almost all text.\\n\\ntextColor =\\n\\nFont family for all text in the app, except code blocks. One of \"sans serif\",\\n\\n\"serif\", or \"monospace\".\\n\\nfont =\\n\\n```', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content=\"title: Pre-release features\\nslug: /library/advanced-features/prerelease\\n\\nPre-release features\\n\\nAt Streamlit, we like to move quick while keeping things stable. In our latest effort to move even faster without sacrificing stability, we're offering our bold and fearless users two ways to try out Streamlit's bleeding-edge features:\\n\\nExperimental features\\n\\nNightly releases\\n\\nExperimental Features\\n\\nLess stable Streamlit features have one naming convention: st.experimental_. This distinction is a prefix we attach to our command names to make sure their status is clear to everyone.\\n\\nHere's a quick rundown of what you get from each naming convention:\\n\\nst: this is where our core features like st.write and st.dataframe live. If we ever make backward-incompatible changes to these, they will take place gradually and with months of announcements and warnings.\\n\\nexperimental: this is where we'll put all new features that may or may not ever make it into Streamlit core. This gives you a chance to try the next big thing we're cooking up weeks or months before we're ready to stabilize its API. We don't know whether these features have a future, but we want you to have access to everything we're trying, and work with us to figure them out.\\n\\nFeatures with the experimental_ naming convention are things that we're still working on or trying\\nto understand. If these features are successful, at some point they'll become part of Streamlit\\ncore. If unsuccessful, these features are removed without much notice. While in experimental, a feature's API and behaviors may not be stable, and it's possible they could change in ways that aren't backward-compatible.\\n\\nExperimental features and their APIs may change or be removed at any time.\\n\\nThe lifecycle of an experimental feature\\n\\nA feature is added with the experimental_ prefix.\\n\\nThe feature is potentially tweaked over time, with possible API/behavior breakages.\\n\\nIf successful, we promote the feature to Streamlit core and remove it from experimental_:\\n\\na. The feature's API stabilizes and the feature is cloned without the experimental_ prefix, so it exists as both st and experimental_. At this point, users will see a warning when using the version of the feature with the experimental_ prefix -- but the feature will still work.\\n\\nb. At some point, the code of the experimental_-prefixed feature is removed, but there will still be a stub of the function prefixed with experimental_ that shows an error with appropriate instructions.\\n\\nc. Finally, at a later date the experimental_ version is removed.\\n\\nIf unsuccessful, the feature is removed without much notice and we leave a stub in experimental_ that shows an error with instructions.\\n\\nNightly releases\\n\\nIn addition to experimental features, we offer another way to try out Streamlit's newest features: nightly releases.\\n\\nAt the end of each day (at night 🌛), our bots run automated tests against the latest Streamlit code and, if everything looks good, it publishes them as the streamlit-nightly package. This means the nightly build includes all our latest features, bug fixes, and other enhancements on the same day they land on our codebase.\\n\\nHow does this differ from official releases?\\n\\nOfficial Streamlit releases go not only through both automated tests but also rigorous manual testing, while nightly releases only have automated tests. It's important to keep in mind that new features introduced in nightly releases often lack polish. In our official releases, we always make double-sure all new features are ready for prime time.\\n\\nHow do I use the nightly release?\\n\\nAll you need to do is install the streamlit-nightly package:\\n\\nbash\\npip uninstall streamlit\\npip install streamlit-nightly --upgrade\\n\\nYou should never have both streamlit and streamlit-nightly installed in the same environment!\\n\\nWhy should I use the nightly release?\\n\\nBecause you can't wait for official releases, and you want to help us find bugs early!\\n\\nWhy shouldn't I use the nightly release?\\n\\nWhile our automated tests have high coverage, there's still a significant likelihood that there will be some bugs in the nightly code.\\n\\nCan I choose which nightly release I want to install?\\n\\nIf you'd like to use a specific version, you can find the version number in our Release history. Specify the desired version using pip as usual: pip install streamlit-nightly==x.yy.zz-123456.\\n\\nCan I compare changes between releases?\\n\\nIf you'd like to review the changes for a nightly release, you can use the comparison tool on GitHub.\", metadata={'source': 'docs/content/library/advanced-features/prerelease-features.md'}),\n",
       " Document(page_content=\"title: HTTPS support\\nslug: /library/advanced-features/https-support\\n\\nHTTPS support\\n\\nMany apps need to be accessed with SSL / TLS protocol or https://.\\n\\nWe recommend performing SSL termination in a reverse proxy or load balancer for self-hosted and production use cases, not directly in the app. Streamlit Community Cloud uses this approach, and every major cloud and app hosting platform should allow you to configure it and provide extensive documentation. You can find some of these platforms in our Deployment tutorials.\\n\\nTo terminate SSL in your Streamlit app, you must configure server.sslCertFile and server.sslKeyFile. Learn how to set config options in Configuration.\\n\\nDetails on usage\\n\\nThe configuration value should be a local file path to a cert file and key file. These must be available at the time the app starts.\\n\\nBoth server.sslCertFile and server.sslKeyFile must be specified. If only one is specified, your app will exit with an error.\\n\\nThis feature will not work in Community Cloud. Community Cloud already serves your app with TLS.\\n\\nIn a production environment, we recommend performing SSL termination by the load balancer or the reverse proxy, not using this option. The use of this option in Streamlit has not gone through extensive security audits or performance tests.\\n\\nExample usage\\n\\n```toml\\n\\n.streamlit/config.toml\\n\\n[server]\\nsslCertFile = '/path/to/certchain.pem'\\nsslKeyFile = '/path/to/private.key'\\n```\", metadata={'source': 'docs/content/library/advanced-features/https.md'}),\n",
       " Document(page_content='title: Button behavior and examples\\nslug: /library/advanced-features/button-behavior-and-examples\\n\\nButton behavior and examples\\n\\nSummary\\n\\nButtons created with st.button do not retain state. They return True on the script rerun resulting from their click and immediately return to False on the next script rerun. If a displayed element is nested inside if st.button(\\'Click me\\'):, the element will be visible when the button is clicked and disappear as soon as the user takes their next action. This is because the script reruns and the button return value becomes False.\\n\\nIn this guide, we will illustrate the use of buttons and explain common misconceptions. Read on to see a variety of examples that expand on st.button using st.session_state. Anti-patterns are included at the end. Go ahead and pull up your favorite code editor so you can streamlit run the examples as you read. Check out Streamlit\\'s Main concepts if you haven\\'t run your own Streamlit scripts yet.\\n\\nWhen to use if st.button()\\n\\nWhen code is conditioned on a button\\'s value, it will execute once in response to the button being clicked and not again (until the button is clicked again).\\n\\nGood to nest inside buttons:\\n\\nTransient messages that immediately disappear.\\n\\nOnce-per-click processes that saves data to session state, a file, or\\n  a database.\\n\\nBad to nest inside buttons:\\n\\nDisplayed items that should persist as the user continues.\\n\\nOther widgets which cause the script to rerun when used.\\n\\nProcesses that neither modify session state nor write to a file/database.*\\n\\nThis can be appropriate when disposable results are desired. If you\\nhave a \"Validate\" button, that could be a process conditioned directly on a\\nbutton. It could be used to create an alert to say \\'Valid\\' or \\'Invalid\\' with no\\nneed to keep that info.\\n\\nCommon logic with buttons\\n\\nShow a temporary message with a button\\n\\nIf you want to give the user a quick button to check if an entry is valid, but not keep that check displayed as the user continues.\\n\\nIn this example, a user can click a button to check if their animal string is in the animal_shelter list. When the user clicks \"Check availability\" they will see \"We have that animal!\" or \"We don\\'t have that animal.\" If they change the animal in st.text_input, the script reruns and the message disappears until they click \"Check availability\" again.\\n\\n```python\\nimport streamlit as st\\n\\nanimal_shelter = [\\'cat\\', \\'dog\\', \\'rabbit\\', \\'bird\\']\\n\\nanimal = st.text_input(\\'Type an animal\\')\\n\\nif st.button(\\'Check availability\\'):\\n    have_it = animal.lower() in animal_shelter\\n    \\'We have that animal!\\' if have_it else \\'We don\\\\\\'t have that animal.\\'\\n```\\n\\nNote: The above example uses magic to render the message on the frontend.\\n\\nStateful button\\n\\nIf you want a clicked button to continue to be True, create a value in st.session_state and use the button to set that value to True in a callback.\\n\\n```python\\nimport streamlit as st\\n\\nif \\'clicked\\' not in st.session_state:\\n    st.session_state.clicked = False\\n\\ndef click_button():\\n    st.session_state.clicked = True\\n\\nst.button(\\'Click me\\', on_click=click_button)\\n\\nif st.session_state.clicked:\\n    # The message and nested widget will remain on the page\\n    st.write(\\'Button clicked!\\')\\n    st.slider(\\'Select a value\\')\\n```\\n\\nToggle button\\n\\nIf you want a button to work like a toggle switch, consider using st.checkbox. Otherwise, you can use a button with a callback function to reverse a boolean value saved in st.session_state.\\n\\nIn this example, we use st.button to toggle another widget on and off. By displaying st.slider conditionally on a value in st.session_state, the user can interact with the slider without it disappearing.\\n\\n```python\\nimport streamlit as st\\n\\nif \\'button\\' not in st.session_state:\\n    st.session_state.button = False\\n\\ndef click_button():\\n    st.session_state.button = not st.session_state.button\\n\\nst.button(\\'Click me\\', on_click=click_button)\\n\\nif st.session_state.button:\\n    # The message and nested widget will remain on the page\\n    st.write(\\'Button is on!\\')\\n    st.slider(\\'Select a value\\')\\nelse:\\n    st.write(\\'Button is off!\\')\\n```\\n\\nAlternatively, you can use the value in st.session_state on the slider\\'s disabled parameter.\\n\\n```python\\nimport streamlit as st\\n\\nif \\'button\\' not in st.session_state:\\n    st.session_state.button = False\\n\\ndef click_button():\\n    st.session_state.button = not st.session_state.button\\n\\nst.button(\\'Click me\\', on_click=click_button)\\n\\nst.slider(\\'Select a value\\', disabled=st.session_state.button)\\n```\\n\\nButtons to continue or control stages of a process\\n\\nAnother alternative to nesting content inside a button is to use a value in st.session_state that designates the \"step\" or \"stage\" of a process. In this example, we have four stages in our script: 0. Before the user begins.\\n\\nUser enters their name.\\n\\nUser chooses a color.\\n\\nUser gets a thank-you message.\\n   A button at the beginning advances the stage from 0 to 1. A button at the end resets the stage from 3 to 0. The other widgets used in stage 1 and 2 have callbacks to set the stage. If you have a process with dependant steps and want to keep previous stages visible, such a callback forces a user to retrace subsequent stages if they change an earlier widget.\\n\\n```python\\nimport streamlit as st\\n\\nif \\'stage\\' not in st.session_state:\\n    st.session_state.stage = 0\\n\\ndef set_state(i):\\n    st.session_state.stage = i\\n\\nif st.session_state.stage == 0:\\n    st.button(\\'Begin\\', on_click=set_state, args=[1])\\n\\nif st.session_state.stage >= 1:\\n    name = st.text_input(\\'Name\\', on_change=set_state, args=[2])\\n\\nif st.session_state.stage >= 2:\\n    st.write(f\\'Hello {name}!\\')\\n    color = st.selectbox(\\n        \\'Pick a Color\\',\\n        [None, \\'red\\', \\'orange\\', \\'green\\', \\'blue\\', \\'violet\\'],\\n        on_change=set_state, args=[3]\\n    )\\n    if color is None:\\n        set_state(2)\\n\\nif st.session_state.stage >= 3:\\n    st.write(f\\':{color}[Thank you!]\\')\\n    st.button(\\'Start Over\\', on_click=set_state, args=[0])\\n```\\n\\nButtons to modify st.session_state\\n\\nIf you modify st.session_state inside of a button, you must consider where that button is within the script.\\n\\nA slight problem\\n\\nIn this example, we access st.session_state.name both before and after the buttons which modify it. When a button (\"Jane\" or \"John\") is clicked, the script reruns. The info displayed before the buttons lags behind the info written after the button. The data in st.session_state before the button is not updated. When the script executes the button function, that is when the conditional code to update st.session_state creates the change. Thus, this change is reflected after the button.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nif \\'name\\' not in st.session_state:\\n    st.session_state[\\'name\\'] = \\'John Doe\\'\\n\\nst.header(st.session_state[\\'name\\'])\\n\\nif st.button(\\'Jane\\'):\\n    st.session_state[\\'name\\'] = \\'Jane Doe\\'\\n\\nif st.button(\\'John\\'):\\n    st.session_state[\\'name\\'] = \\'John Doe\\'\\n\\nst.header(st.session_state[\\'name\\'])\\n```\\n\\nLogic used in a callback\\n\\nCallbacks are a clean way to modify st.session_state. Callbacks are executed as a prefix to the script rerunning, so the position of the button relative to accessing data is not important.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nif \\'name\\' not in st.session_state:\\n    st.session_state[\\'name\\'] = \\'John Doe\\'\\n\\ndef change_name(name):\\n    st.session_state[\\'name\\'] = name\\n\\nst.header(st.session_state[\\'name\\'])\\n\\nst.button(\\'Jane\\', on_click=change_name, args=[\\'Jane Doe\\'])\\nst.button(\\'John\\', on_click=change_name, args=[\\'John Doe\\'])\\n\\nst.header(st.session_state[\\'name\\'])\\n```\\n\\nLogic nested in a button with a rerun\\n\\nAlthough callbacks are often preferred to avoid extra reruns, our first \\'John Doe\\'/\\'Jane Doe\\' example can be modified by adding st.experimental_rerun instead. If you need to acces data in st.session_state before the button that modifies it, you can include st.experimental_rerun to rerun the script after the change has been committed. This means the script will rerun twice when a button is clicked.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nif \\'name\\' not in st.session_state:\\n    st.session_state[\\'name\\'] = \\'John Doe\\'\\n\\nst.header(st.session_state[\\'name\\'])\\n\\nif st.button(\\'Jane\\'):\\n    st.session_state[\\'name\\'] = \\'Jane Doe\\'\\n    st.experimental_rerun()\\n\\nif st.button(\\'John\\'):\\n    st.session_state[\\'name\\'] = \\'John Doe\\'\\n    st.experimental_rerun()\\n\\nst.header(st.session_state[\\'name\\'])\\n```\\n\\nButtons to modify or reset other widgets\\n\\nWhen a button is used to modify or reset another widget, it is the same as the above examples to modify st.session_state. However, an extra consideration exists: you cannot modify a key-value pair in st.session_state if the widget with that key has already been rendered on the page for the current script run.\\n\\nDon\\'t do this!\\n\\n```python\\nimport streamlit as st\\n\\nst.text_input(\\'Name\\', key=\\'name\\')\\n\\nThese buttons will error because their nested code changes\\n\\na widget\\'s state after that widget within the script.\\n\\nif st.button(\\'Clear name\\'):\\n    st.session_state.name = \\'\\'\\nif st.button(\\'Streamlit!\\'):\\n    set_name(\\'Streamlit\\')\\n```\\n\\nOption 1: Use a key for the button and put the logic before the widget\\n\\nIf you assign a key to a button, you can condition code on a button\\'s state by using its value in st.session_state. This means that logic depending on your button can be in your script before that button. In the following example, we use the .get() method on st.session_state because the keys for the buttons will not exist when the script runs for the first time. The .get() method will return False if it can\\'t find the key. Otherwise, it will return the value of the key.\\n\\n```python\\nimport streamlit as st\\n\\nUse the get method since the keys won\\'t be in session_state\\n\\non the first script run\\n\\nif st.session_state.get(\\'clear\\'):\\n    st.session_state[\\'name\\'] = \\'\\'\\nif st.session_state.get(\\'streamlit\\'):\\n    st.session_state[\\'name\\'] = \\'Streamlit\\'\\n\\nst.text_input(\\'Name\\', key=\\'name\\')\\n\\nst.button(\\'Clear name\\', key=\\'clear\\')\\nst.button(\\'Streamlit!\\', key=\\'streamlit\\')\\n```\\n\\nOption 2: Use a callback\\n\\n```python\\nimport streamlit as st\\n\\nst.text_input(\\'Name\\', key=\\'name\\')\\n\\ndef set_name(name):\\n    st.session_state.name = name\\n\\nst.button(\\'Clear name\\', on_click=set_name, args=[\\'\\'])\\nst.button(\\'Streamlit!\\', on_click=set_name, args=[\\'Streamlit\\'])\\n```\\n\\nOption 3: Use containers\\n\\nBy using st.container you can have widgets appear in different orders in your script and frontend view (webpage).\\n\\n```python\\nimport streamlit as st\\n\\nbegin = st.container()\\n\\nif st.button(\\'Clear name\\'):\\n    st.session_state.name = \\'\\'\\nif st.button(\\'Streamlit!\\'):\\n    st.session_state.name = (\\'Streamlit\\')\\n\\nThe widget is second in logic, but first in display\\n\\nbegin.text_input(\\'Name\\', key=\\'name\\')\\n```\\n\\nButtons to add other widgets dynamically\\n\\n```python\\nimport streamlit as st\\n\\ndef display_input_row(index):\\n    left, middle, right = st.columns(3)\\n    left.text_input(\\'First\\', key=f\\'first_{index}\\')\\n    middle.text_input(\\'Middle\\', key=f\\'middle_{index}\\')\\n    right.text_input(\\'Last\\', key=f\\'last_{index}\\')\\n\\nif \\'rows\\' not in st.session_state:\\n    st.session_state[\\'rows\\'] = 0\\n\\ndef increase_rows():\\n    st.session_state[\\'rows\\'] += 1\\n\\nst.button(\\'Add person\\', on_click=increase_rows)\\n\\nfor i in range(st.session_state[\\'rows\\']):\\n    display_input_row(i)\\n\\nShow the results\\n\\nst.subheader(\\'People\\')\\nfor i in range(st.session_state[\\'rows\\']):\\n    st.write(\\n        f\\'Person {i+1}:\\',\\n        st.session_state[f\\'first_{i}\\'],\\n        st.session_state[f\\'middle_{i}\\'],\\n        st.session_state[f\\'last_{i}\\']\\n    )\\n```\\n\\nButtons to handle expensive or file-writing processes\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport time\\n\\ndef expensive_process(option, add):\\n    with st.spinner(\\'Processing...\\'):\\n        time.sleep(5)\\n    df = pd.DataFrame({\\'A\\': [1, 2, 3], \\'B\\': [4, 5, 6], \\'C\\':[7, 8, 9]}) + add\\n    return (df, add)\\n\\ncols = st.columns(2)\\noption = cols[0].selectbox(\\'Select a number\\', options=[\\'1\\', \\'2\\', \\'3\\'])\\nadd = cols[1].number_input(\\'Add a number\\', min_value=0, max_value=10)\\n\\nif \\'processed\\' not in st.session_state:\\n    st.session_state.processed = {}\\n\\nProcess and save results\\n\\nif st.button(\\'Process\\'):\\n    result = expensive_process(option, add)\\n    st.session_state.processed[option] = result\\n\\nif option in st.session_state.processed:\\n    st.write(f\\'Option {option} processed with add {add}\\')\\n    st.write(st.session_state.processed[option][0])\\n```\\n\\nAstute observers may think, \"This feels a little like caching.\" We are only saving results relative to one parameter, but the pattern could easily be expanded to save results relative to both parameters. In that sense, yes, it has some similarities to caching, but also some important differences. When you save results in st.session_state, the results are only available to the current user in their current session. If you use st.cache_data instead, the results are available to all users across all sessions. Furthermore, if you want to update a saved result, you have to clear all saved results for that function to do so.\\n\\nAnti-patterns\\n\\nHere are some simplified examples of how buttons can go wrong. Be on the lookout for these common mistakes.\\n\\nButtons nested inside buttons\\n\\n```python\\nimport streamlit as st\\n\\nif st.button(\\'Button 1\\'):\\n    st.write(\\'Button 1 was clicked\\')\\n    if st.button(\\'Button 2\\'):\\n        # This will never be executed.\\n        st.write(\\'Button 2 was clicked\\')\\n```\\n\\nOther widgets nested inside buttons\\n\\n```python\\nimport streamlit as st\\n\\nif st.button(\\'Sign up\\'):\\n    name = st.text_input(\\'Name\\')\\n\\n```\\n\\nNesting a process inside a button without saving to session state\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nfile = st.file_uploader(\"Upload a file\", type=\"csv\")\\n\\nif st.button(\\'Get data\\'):\\n    df = pd.read_csv(file)\\n    # This display will go away with the user\\'s next action.\\n    st.write(df)\\n\\nif st.button(\\'Save\\'):\\n    # This will always error.\\n    df.to_csv(\\'data.csv\\')\\n```', metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md'}),\n",
       " Document(page_content='title: ☰ App menu\\nslug: /library/advanced-features/app-menu\\n\\n☰ App menu\\n\\nStreamlit provides a configurable menu within your app to access convenient tools for developers and viewers. By default, you can access developer options from the app menu when viewing an app locally or on Streamlit Community Cloud while logged into an account with administrative access. While viewing an app, click the icon in the upper-right corner to access the menu.\\n\\nMenu options\\n\\nThe menu is split into two sections. The upper section contains options available to all viewers and the lower section contains options for developers. Read more about customizing this menu at the end of this page.\\n\\nRerun\\n\\nYou can manually trigger a rerun of your app by clicking \"Rerun\" from the app menu. This rerun will not reset your session. Your widget states and values stored in st.session_state will be preserved. As a shortcut, without opening the app menu, you can rerun your app by pressing \"R\" on your keyboard (if you aren\\'t currently focused on an input element).\\n\\nSettings\\n\\nWith the \"Settings\" option, you can control the appearance of your app while it is running. If viewing the app locally, you can set how your app responds to changes in your source code. See more about development flow in Main concepts. You can also force your app to appear in wide mode, even if not set within the script using st.set_page_config.\\n\\nTheme settings\\n\\nAfter clicking \"Settings\" from the app menu, you can choose between \"Light\", \"Dark\", or \"Use system setting\" for the app\\'s base theme. Click on \"Edit active theme\" to modify the theme, color-by-color.\\n\\nPrint\\n\\nClick \"Print\" to open a print dialog. This option uses your browser\\'s built-in print-to-pdf function.\\n\\nRecord a screencast\\n\\nYou can easily make screen recordings right from your app! Screen recording is supported in the latest versions of Chrome, Edge, and Firefox. Ensure your browser is up-to-date for compatibility. Depending on your current settings, you may need to grant permission to your browser to record your screen or to use your microphone if recording a voiceover.\\n\\nWhile viewing your app, open the app menu from the upper-right corner.\\n\\nClick \"Record a screencast.\"\\n\\nIf you want to record audio through your microphone, check \"Also record audio.\"\\n\\nClick \"Start recording.\" (You may be prompted by your OS to permit your browser to record your screen or use your microphone.)\\n\\nSelect which tab, window, or monitor you want to record from the listed options. The interface will vary depending on your browser.\\n\\nClick \"Share.\"\\n\\nWhile recording, you will see a red circle on your app\\'s tab and on the app menu icon. If you want to cancel the recording, click \"Stop sharing\" at the bottom of your app.\\n\\nWhen you are done recording, press \"Esc\" on your keyboard or click \"Stop recording\" from your app\\'s menu.\\n\\nFollow your browser\\'s instructions to save your recording. Your saved recording will be available where your browser saves downloads.\\n\\nThe whole process looks like this:\\n\\nAbout\\n\\nYou can conveniently check what version of Streamlit is running from the \"About\" option. Developers also have the option to customize the message shown here using st.set_page_config.\\n\\nDeveloper options\\n\\nBy default, developer options only show when viewing an app locally or when viewing a Community Cloud app while logged in with administrative permission. You can customize the menu if you want to make these options available for all users.\\n\\nClear cache\\n\\nReset your app\\'s cache by clicking \"Clear cache\" from the app\\'s menu or by pressing \"C\" on your keyboard while not focused on an input element. This will remove all cached entries for @st.cache_data and @st.cache_resource.\\n\\nDeploy this app\\n\\nIf you are running an app locally from within a git repo, you can deploy your app to Streamlit Community Cloud in a few easy clicks! Make sure your work has been pushed to your online GitHub repository before beginning. For the greatest convenience, make sure you have already created your Community Cloud account and are signed in. Click \"Deploy this app\" to be taken directly to Community Cloud\\'s \"Deploy an app\" page. Your app\\'s repository, branch, and file name will be prefilled to match your current app! Learn more about deploying an app on Streamlit Community Cloud.\\n\\nCustomize the menu\\n\\nUsing client.toobarMode in your app\\'s configuration, you can make the app menu appear in the following ways:\\n\\n\"developer\" — Show the developer options to all viewers.\\n\\n\"viewer\" — Hide the developer options from all viewers.\\n\\n\"minimal\" — Show only those options set externally. These can be options declared through st.set_page_config or options populated through Streamlit Community Cloud.\\n\\n\"auto\" — This is the default and will show the developer options when accessed through localhost or through Streamlit Community Cloud when logged into an administrative account for the app. Otherwise, the developer options will not show.', metadata={'source': 'docs/content/library/advanced-features/app-menu.md'}),\n",
       " Document(page_content=\"title: Components\\nslug: /library/components\\n\\nCustom Components\\n\\nComponents are third-party Python modules that extend what's possible with Streamlit.\\n\\nHow to use a Component\\n\\nComponents are super easy to use:\\n\\nStart by finding the Component you'd like to use. Two great resources for this are:\\n\\nThe Component gallery\\n\\nThis thread,\\n     by Fanilo A. from our forums.\\n\\nInstall the Component using your favorite Python package manager. This step and all following\\n   steps are described in your component's instructions.\\n\\nFor example, to use the fantastic AgGrid\\n   Component, you first install it with:\\n\\npython\\n   pip install streamlit-aggrid\\n\\nIn your Python code, import the Component as described in its instructions. For AgGrid, this step\\n   is:\\n\\npython\\n   from st_aggrid import AgGrid\\n\\n...now you're ready to use it! For AgGrid, that's:\\n\\npython\\n   AgGrid(my_dataframe)\\n\\nMaking your own Component\\n\\nIf you're interested in making your own component, check out the following resources:\\n\\nCreate a Component\\n\\nPublish a Component\\n\\nComponents API\\n\\nBlog post for when we launched Components!\\n\\nAlternatively, if you prefer to learn using videos, our engineer Tim Conkling has put together some\\namazing tutorials:\\n\\nVideo tutorial, part 1\\n\\nVideo tutorial, part 2\", metadata={'source': 'docs/content/library/components/components.md'}),\n",
       " Document(page_content='title: Publish a Component\\nslug: /library/components/publish\\n\\nPublish a Component\\n\\nPublish to PyPI\\n\\nPublishing your Streamlit Component to PyPI makes it easily accessible to Python users around the world. This step is completely optional, so if you won’t be releasing your component publicly, you can skip this section!\\n\\nFor static Streamlit Components, publishing a Python package to PyPI follows the same steps as the\\ncore PyPI packaging instructions. A static Component likely contains only Python code, so once you have your\\nsetup.py file correct and\\ngenerate your distribution files, you\\'re ready to\\nupload to PyPI.\\n\\nBi-directional Streamlit Components at minimum include both Python and JavaScript code, and as such, need a bit more preparation before they can be published on PyPI. The remainder of this page focuses on the bi-directional Component preparation process.\\n\\nPrepare your Component\\n\\nA bi-directional Streamlit Component varies slightly from a pure Python library in that it must contain pre-compiled frontend code. This is how base Streamlit works as well; when you pip install streamlit, you are getting a Python library where the HTML and frontend code contained within it have been compiled into static assets.\\n\\nThe component-template GitHub repo provides the folder structure necessary for PyPI publishing. But before you can publish, you\\'ll need to do a bit of housekeeping:\\n\\nGive your Component a name, if you haven\\'t already\\n\\nRename the template/my_component/ folder to template/<component name>/\\n\\nPass your component\\'s name as the the first argument to declare_component()\\n\\nEdit MANIFEST.in, change the path for recursive-include from package/frontend/build * to <component name>/frontend/build *\\n\\nEdit setup.py, adding your component\\'s name and other relevant info\\n\\nCreate a release build of your frontend code. This will add a new directory, frontend/build/, with your compiled frontend in it:\\n\\nbash\\n   cd frontend\\n   npm run build\\n\\nPass the build folder\\'s path as the path parameter to declare_component. (If you\\'re using the template Python file, you can set _RELEASE = True at the top of the file):\\n\\n```python\\n      import streamlit.components.v1 as components\\n\\n```\\n\\nBuild a Python wheel\\n\\nOnce you\\'ve changed the default my_component references, compiled the HTML and JavaScript code and set your new component name in components.declare_component(), you\\'re ready to build a Python wheel:\\n\\nMake sure you have the latest versions of setuptools, wheel, and twine\\n\\nCreate a wheel from the source code:\\n\\nbash\\n    # Run this from your component\\'s top-level directory; that is,\\n    # the directory that contains `setup.py`\\n    python setup.py sdist bdist_wheel\\n\\nUpload your wheel to PyPI\\n\\nWith your wheel created, the final step is to upload to PyPI. The instructions here highlight how to upload to Test PyPI, so that you can learn the mechanics of the process without worrying about messing anything up. Uploading to PyPI follows the same basic procedure.\\n\\nCreate an account on Test PyPI if you don\\'t already have one\\n\\nVisit https://test.pypi.org/account/register/ and complete the steps\\n\\nVisit https://test.pypi.org/manage/account/#api-tokens and create a new API token. Don’t limit the token scope to a particular project, since you are creating a new project. Copy your token before closing the page, as you won’t be able to retrieve it again.\\n\\nUpload your wheel to Test PyPI. twine will prompt you for a username and password. For the username, use __token__. For the password, use your token value from the previous step, including the pypi- prefix:\\n\\nbash\\n   python3 -m twine upload --repository testpypi dist/*\\n\\nInstall your newly-uploaded package in a new Python project to make sure it works:\\n\\nbash\\n    python -m pip install --index-url https://test.pypi.org/simple/ --no-deps example-pkg-YOUR-USERNAME-HERE\\n\\nIf all goes well, you\\'re ready to upload your library to PyPI by following the instructions at https://packaging.python.org/tutorials/packaging-projects/#next-steps.\\n\\nCongratulations, you\\'ve created a publicly-available Streamlit Component!\\n\\nPromote your Component!\\n\\nWe\\'d love to help you share your Component with the Streamlit Community! To share it, please post on the Streamlit \\'Show the Community!\\' Forum category with the title similar to \"New Component: <your component name>, a new way to do X\".\\n\\nYou can also Tweet at us @streamlit so that we can retweet your announcement for you.\\n\\nIf you host your code on GitHub, add the tag streamlit-component, so that it\\'s listed in the GitHub streamlit-component topic:', metadata={'source': 'docs/content/library/components/publish-component.md'}),\n",
       " Document(page_content='title: Create a Component\\nslug: /library/components/create\\n\\nCreate a Component\\n\\nIf you are only interested in using Streamlit Components, then you can skip this section and\\nhead over to the Streamlit Components Gallery to find and install\\ncomponents created by the community!\\n\\nDevelopers can write JavaScript and HTML \"components\" that can be rendered in Streamlit apps. Streamlit Components can receive data from, and also send data to, Streamlit Python scripts.\\n\\nStreamlit Components let you expand the functionality provided in the base Streamlit package. Use Streamlit Components to create the needed functionality for your use-case, then wrap it up in a Python package and share with the broader Streamlit community!\\n\\nTypes of Streamlit Components you could create include:\\n\\nCustom versions of existing Streamlit elements and widgets, such as st.slider or st.file_uploader.\\n\\nCompletely new Streamlit elements and widgets by wrapping existing React.js, Vue.js, or other JavaScript widget toolkits.\\n\\nRendering Python objects having methods that output HTML, such as IPython __repr_html__.\\n\\nConvenience functions for commonly-used web features like GitHub gists and Pastebin.\\n\\nCheck out these Streamlit Components tutorial videos by Streamlit engineer Tim Conkling to get started:\\n\\nPart 1: Setup and Architecture\\n\\nPart 2: Make a Slider Widget', metadata={'source': 'docs/content/library/components/create-component.md'}),\n",
       " Document(page_content='title: Optimize performance with st.cache\\nslug: /library/advanced-features/st.cache\\n\\nst.cache was deprecated in version 1.18.0. Use st.cache_data or st.cache_resource instead. Learn more in Caching.\\n\\nOptimize performance with st.cache\\n\\nStreamlit provides a caching mechanism that allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations. This is done with the @st.cache decorator.\\n\\nWhen you mark a function with the @st.cache decorator, it tells Streamlit that whenever the function is called it needs to check a few things:\\n\\nThe input parameters that you called the function with\\n\\nThe value of any external variable used in the function\\n\\nThe body of the function\\n\\nThe body of any function used inside the cached function\\n\\nIf this is the first time Streamlit has seen these four components with these exact values and in this exact combination and order, it runs the function and stores the result in a local cache. Then, next time the cached function is called, if none of these components changed, Streamlit will just skip executing the function altogether and, instead, return the output previously stored in the cache.\\n\\nThe way Streamlit keeps track of changes in these components is through hashing. Think of the cache as an in-memory key-value store, where the key is a hash of all of the above and the value is the actual output object passed by reference.\\n\\nFinally, @st.cache supports arguments to configure the cache\\'s behavior. You can find more information on those in our API reference.\\n\\nLet\\'s take a look at a few examples that illustrate how caching works in a Streamlit app.\\n\\nExample 1: Basic usage\\n\\nFor starters, let\\'s take a look at a sample app that has a function that performs an expensive, long-running computation. Without caching, this function is rerun each time the app is refreshed, leading to a poor user experience. Copy this code into a new app and try it out yourself:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\ndef expensive_computation(a, b):\\n    time.sleep(2)  # 👈 This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nTry pressing R to rerun the app, and notice how long it takes for the result to show up. This is because expensive_computation(a, b) is being re-executed every time the app runs. This isn\\'t a great experience.\\n\\nLet\\'s add the @st.cache decorator:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache  # 👈 Added this\\ndef expensive_computation(a, b):\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nNow run the app again and you\\'ll notice that it is much faster every time you press R to rerun. To understand what is happening, let\\'s add an st.write inside the function:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)  # 👈 Changed this\\ndef expensive_computation(a, b):\\n    # 👇 Added this\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nNow when you rerun the app the text \"Cache miss\" appears on the first run, but not on any subsequent runs. That\\'s because the cached function is only being executed once, and every time after that you\\'re actually hitting the cache.\\n\\nYou may have noticed that we\\'ve added the suppress_st_warning keyword to the @st.cache decorators. That\\'s because the cached function above uses a Streamlit command itself (st.write in this case), and when Streamlit sees that, it shows a warning that your command will only execute when you get a cache miss. More often than not, when you see that warning it\\'s because there\\'s a bug in your code. However, in our case we\\'re using the st.write command to demonstrate when the cache is being missed, so the behavior Streamlit is warning us about is exactly what we want. As a result, we are passing in suppress_st_warning=True to turn that warning off.\\n\\nExample 2: When the function arguments change\\n\\nWithout stopping the previous app server, let\\'s change one of the arguments to our cached function:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = 210  # 👈 Changed this\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nNow the first time you rerun the app it\\'s a cache miss. This is evidenced by the \"Cache miss\" text showing up and the app taking 2s to finish running. After that, if you press R to rerun, it\\'s always a cache hit. That is, no such text shows up and the app is fast again.\\n\\nThis is because Streamlit notices whenever the arguments a and b change and determines whether the function should be re-executed and re-cached.\\n\\nExample 3: When the function body changes\\n\\nWithout stopping and restarting your Streamlit server, let\\'s remove the widget from our app and modify the function\\'s code by adding a + 1 to the return value.\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b + 1  # 👈 Added a +1 at the end here\\n\\na = 2\\nb = 210\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nThe first run is a \"Cache miss\", but when you press R each subsequent run is a cache hit. This is because on first run, Streamlit detected that the function body changed, reran the function, and put the result in the cache.\\n\\nIf you change the function back the result will already be in the Streamlit cache from a previous run. Try it out!\\n\\nExample 4: When an inner function changes\\n\\nLet\\'s make our cached function depend on another function internally:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\ndef inner_func(a, b):\\n    st.write(\"inner_func(\", a, \",\", b, \") ran\")\\n    return a * b\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return inner_func(a, b) + 1\\n\\na = 2\\nb = 210\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nWhat you see is the usual:\\n\\nThe first run results in a cache miss.\\n\\nEvery subsequent rerun results in a cache hit.\\n\\nBut now let\\'s try modifying the inner_func():\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\ndef inner_func(a, b):\\n    st.write(\"inner_func(\", a, \",\", b, \") ran\")\\n    return a ** b  # 👈 Changed the * to ** here\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return inner_func(a, b) + 1\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nEven though inner_func() is not annotated with @st.cache, when we edit its body we cause a \"Cache miss\" in the outer expensive_computation().\\n\\nThat\\'s because Streamlit always traverses your code and its dependencies to verify that the cached values are still valid. This means that while developing your app you can edit your code freely without worrying about the cache. Any change you make to your app, Streamlit should do the right thing!\\n\\nStreamlit is also smart enough to only traverse dependencies that belong to your app, and skip over any dependency that comes from an installed Python library.\\n\\nExample 5: Use caching to speed up your app across users\\n\\nGoing back to our original function, let\\'s add a widget to control the value of b:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = st.slider(\"Pick a number\", 0, 10)  # 👈 Changed this\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nWhat you\\'ll see:\\n\\nIf you move the slider to a number Streamlit hasn\\'t seen before, you\\'ll have a cache miss again. And every subsequent rerun with the same number will be a cache hit, of course.\\n\\nIf you move the slider back to a number Streamlit has seen before, the cache is hit and the app is fast as expected.\\n\\nIn computer science terms, what is happening here is that @st.cache is memoizing expensive_computation(a, b).\\n\\nBut now let\\'s go one step further! Try the following:\\n\\nMove the slider to a number you haven\\'t tried before, such as 9.\\n\\nPretend you\\'re another user by opening another browser tab pointing to your Streamlit app (usually at http://localhost:8501)\\n\\nIn the new tab, move the slider to 9.\\n\\nNotice how this is actually a cache hit! That is, you don\\'t actually see the \"Cache miss\" text on the second tab even though that second user never moved the slider to 9 at any point prior to this.\\n\\nThis happens because the Streamlit cache is global to all users. So everyone contributes to everyone else\\'s performance.\\n\\nExample 6: Mutating cached values\\n\\nAs mentioned in the overview section, the Streamlit cache stores items by reference. This allows the Streamlit cache to support structures that aren\\'t memory-managed by Python, such as TensorFlow objects. However, it can also lead to unexpected behavior — which is why Streamlit has a few checks to guide developers in the right direction. Let\\'s look into those checks now.\\n\\nLet\\'s write an app that has a cached function which returns a mutable object, and then let\\'s follow up by mutating that object:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return {\"output\": a * b}  # 👈 Mutable object\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n\\nres[\"output\"] = \"result was manually mutated\"  # 👈 Mutated cached value\\n\\nst.write(\"Mutated result:\", res)\\n```\\n\\nWhen you run this app for the first time, you should see three messages on the screen:\\n\\nCache miss (...)\\n\\nResult: {output: 42}\\n\\nMutated result: {output: \"result was manually mutated\"}\\n\\nNo surprises here. But now notice what happens when you rerun you app (i.e. press R):\\n\\nResult: {output: \"result was manually mutated\"}\\n\\nMutated result: {output: \"result was manually mutated\"}\\n\\nCached object mutated. (...)\\n\\nSo what\\'s up?\\n\\nWhat\\'s going on here is that Streamlit caches the output res by reference. When you mutated res[\"output\"] outside the cached function you ended up inadvertently modifying the cache. This means every subsequent call to expensive_computation(2, 21) will return the wrong value!\\n\\nSince this behavior is usually not what you\\'d expect, Streamlit tries to be helpful and show you a warning, along with some ideas about how to fix your code.\\n\\nIn this specific case, the fix is just to not mutate res[\"output\"] outside the cached function. There was no good reason for us to do that anyway! Another solution would be to clone the result value with res = deepcopy(expensive_computation(2, 21)). Check out the section entitled Fixing caching issues for more information on these approaches and more.\\n\\nAdvanced caching\\n\\nIn caching, you learned about the Streamlit cache, which is accessed with the @st.cache decorator. In this article you\\'ll see how Streamlit\\'s caching functionality is implemented, so that you can use it to improve the performance of your Streamlit apps.\\n\\nThe cache is a key-value store, where the key is a hash of:\\n\\nThe input parameters that you called the function with\\n\\nThe value of any external variable used in the function\\n\\nThe body of the function\\n\\nThe body of any function used inside the cached function\\n\\nAnd the value is a tuple of:\\n\\nThe cached output\\n\\nA hash of the cached output (you\\'ll see why soon)\\n\\nFor both the key and the output hash, Streamlit uses a specialized hash function that knows how to traverse code, hash special objects, and can have its behavior customized by the user.\\n\\nFor example, when the function expensive_computation(a, b), decorated with @st.cache, is executed with a=2 and b=21, Streamlit does the following:\\n\\nComputes the cache key\\n\\nIf the key is found in the cache, then:\\n\\nExtracts the previously-cached (output, output_hash) tuple.\\n\\nPerforms an Output Mutation Check, where a fresh hash of the output is computed and compared to the stored output_hash.\\nIf the two hashes are different, shows a Cached Object Mutated warning. (Note: Setting allow_output_mutation=True disables this step).\\n\\nIf the input key is not found in the cache, then:\\n\\nExecutes the cached function (i.e. output = expensive_computation(2, 21)).\\n\\nCalculates the output_hash from the function\\'s output.\\n\\nStores key → (output, output_hash) in the cache.\\n\\nReturns the output.\\n\\nIf an error is encountered an exception is raised. If the error occurs while hashing either the key or the output an UnhashableTypeError error is thrown. If you run into any issues, see fixing caching issues.\\n\\nThe hash_funcs parameter\\n\\nAs described above, Streamlit\\'s caching functionality relies on hashing to calculate the key for cached objects, and to detect unexpected mutations in the cached result.\\n\\nFor added expressive power, Streamlit lets you override this hashing process using the hash_funcs argument. Suppose you define a type called FileReference which points to a file in the filesystem:\\n\\n```python\\nclass FileReference:\\n    def init(self, filename):\\n        self.filename = filename\\n\\n@st.cache\\ndef func(file_reference):\\n    ...\\n```\\n\\nBy default, Streamlit hashes custom classes like FileReference by recursively navigating their structure. In this case, its hash is the hash of the filename property. As long as the file name doesn\\'t change, the hash will remain constant.\\n\\nHowever, what if you wanted to have the hasher check for changes to the file\\'s modification time, not just its name? This is possible with @st.cache\\'s hash_funcs parameter:\\n\\n```python\\nclass FileReference:\\n    def init(self, filename):\\n        self.filename = filename\\n\\ndef hash_file_reference(file_reference):\\n    filename = file_reference.filename\\n    return (filename, os.path.getmtime(filename))\\n\\n@st.cache(hash_funcs={FileReference: hash_file_reference})\\ndef func(file_reference):\\n    ...\\n```\\n\\nAdditionally, you can hash FileReference objects by the file\\'s contents:\\n\\n```python\\nclass FileReference:\\n    def init(self, filename):\\n        self.filename = filename\\n\\ndef hash_file_reference(file_reference):\\n    with open(file_reference.filename) as f:\\n      return f.read()\\n\\n@st.cache(hash_funcs={FileReference: hash_file_reference})\\ndef func(file_reference):\\n    ...\\n```\\n\\nBecause Streamlit\\'s hash function works recursively, you don\\'t have to hash the contents inside hash_file_reference Instead, you can return a primitive type, in this case the contents of the file, and Streamlit\\'s internal hasher will compute the actual hash from it.\\n\\nTypical hash functions\\n\\nWhile it\\'s possible to write custom hash functions, let\\'s take a look at some of the tools that Python provides out of the box. Here\\'s a list of some hash functions and when it makes sense to use them.\\n\\nPython\\'s id function | Example\\n\\nSpeed: Fast\\n\\nUse case: If you\\'re hashing a singleton object, like an open database connection or a TensorFlow session. These are objects that will only be instantiated once, no matter how many times your script reruns.\\n\\nlambda _: None | Example\\n\\nSpeed: Fast\\n\\nUse case: If you want to turn off hashing of this type. This is useful if you know the object is not going to change.\\n\\nPython\\'s hash() function | Example\\n\\nSpeed: Can be slow based the size of the object being cached\\n\\nUse case: If Python already knows how to hash this type correctly.\\n\\nCustom hash function | Example\\n\\nSpeed: N/a\\n\\nUse case: If you\\'d like to override how Streamlit hashes a particular type.\\n\\nExample 1: Pass a database connection around\\n\\nSuppose we want to open a database connection that can be reused across multiple runs of a Streamlit app. For this you can make use of the fact that cached objects are stored by reference to automatically initialize and reuse the connection:\\n\\npython\\n@st.cache(allow_output_mutation=True)\\ndef get_database_connection():\\n    return db.get_connection()\\n\\nWith just 3 lines of code, the database connection is created once and stored in the cache. Then, every subsequent time get_database_connection is called, the already-created connection object is reused automatically. In other words, it becomes a singleton.\\n\\nUse the allow_output_mutation=True flag to suppress the immutability check. This prevents Streamlit from trying to hash the output connection, and also turns off Streamlit\\'s mutation warning in the process.\\n\\nWhat if you want to write a function that receives a database connection as input? For that, you\\'ll use hash_funcs:\\n\\npython\\n@st.cache(hash_funcs={DBConnection: id})\\ndef get_users(connection):\\n    # Note: We assume that connection is of type DBConnection.\\n    return connection.execute_sql(\\'SELECT * from Users\\')\\n\\nHere, we use Python\\'s built-in id function, because the connection object is coming from the Streamlit cache via the get_database_connection function. This means that the same connection instance is passed around every time, and therefore it always has the same id. However, if you happened to have a second connection object around that pointed to an entirely different database, it would still be safe to pass it to get_users because its id is guaranteed to be different than the first id.\\n\\nThese design patterns apply any time you have an object that points to an external resource, such as a database connection or Tensorflow session.\\n\\nExample 2: Turn off hashing for a specific type\\n\\nYou can turn off hashing entirely for a particular type by giving it a custom hash function that returns a constant. One reason that you might do this is to avoid hashing large, slow-to-hash objects that you know are not going to change. For example:\\n\\npython\\n@st.cache(hash_funcs={pd.DataFrame: lambda _: None})\\ndef func(huge_constant_dataframe):\\n    ...\\n\\nWhen Streamlit encounters an object of this type, it always converts the object into None, no matter which instance of FooType its looking at. This means all instances are hash to the same value, which effectively cancels out the hashing mechanism.\\n\\nExample 3: Use Python\\'s hash() function\\n\\nSometimes, you might want to use Python’s default hashing instead of Streamlit\\'s. For example, maybe you\\'ve encountered a type that Streamlit is unable to hash, but it\\'s hashable with Python\\'s built-in hash() function:\\n\\npython\\n@st.cache(hash_funcs={FooType: hash})\\ndef func(...):\\n    ...', metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content='title: Components API\\nslug: /library/components/components-api\\n\\nComponents API Reference\\n\\nThe first step in developing a Streamlit Component is deciding whether to create a static component (i.e. rendered once, controlled by Python) or to create a bi-directional component that can communicate from Python to JavaScript and back.\\n\\nCreate a static component\\n\\nIf your goal in creating a Streamlit Component is solely to display HTML code or render a chart from a Python visualization library, Streamlit provides two methods that greatly simplify the process: components.html() and components.iframe().\\n\\nIf you are unsure whether you need bi-directional communication, start here first!\\n\\nRender an HTML string\\n\\nWhile st.text, st.markdown and st.write make it easy to write text to a Streamlit app, sometimes you\\'d rather implement a custom piece of HTML. Similarly, while Streamlit natively supports many charting libraries, you may want to implement a specific HTML/JavaScript template for a new charting library. components.html works by giving you the ability to embed an iframe inside of a Streamlit app that contains your desired output.\\n\\nExample\\n\\n```python\\nimport streamlit as st\\nimport streamlit.components.v1 as components\\n\\nbootstrap 4 collapse example\\n\\ncomponents.html(\\n    \"\"\"\\n\\nCollapsible Group Item #1\\n\\nCollapsible Group Item #1 content\\n\\nCollapsible Group Item #2\\n\\nCollapsible Group Item #2 content\\n\\nRender an iframe URL\\n\\ncomponents.iframe is similar in features to components.html, with the difference being that components.iframe takes a URL as its input. This is used for situations where you want to include an entire page within a Streamlit app.\\n\\nExample\\n\\n```python\\nimport streamlit as st\\nimport streamlit.components.v1 as components\\n\\nembed streamlit docs in a streamlit app\\n\\ncomponents.iframe(\"https://docs.streamlit.io/en/latest\")\\n```\\n\\nCreate a bi-directional component\\n\\nA bi-directional Streamlit Component has two parts:\\n\\nA frontend, which is built out of HTML and any other web tech you like (JavaScript, React, Vue, etc.), and gets rendered in Streamlit apps via an iframe tag.\\n\\nA Python API, which Streamlit apps use to instantiate and talk to that frontend\\n\\nTo make the process of creating bi-directional Streamlit Components easier, we\\'ve created a React template and a TypeScript-only template in the Streamlit Component-template GitHub repo. We also provide some example Components in the same repo.\\n\\nDevelopment Environment Setup\\n\\nTo build a Streamlit Component, you need the following installed in your development environment:\\n\\nPython 3.8 - Python 3.11\\n\\nStreamlit 1.11.1 or higher\\n\\nnodejs\\n\\nnpm or yarn\\n\\nClone the component-template GitHub repo, then decide whether you want to use the React.js (\"template\") or plain TypeScript (\"template-reactless\") template.\\n\\nInitialize and build the component template frontend from the terminal:\\n\\n```bash\\n   # React template\\n   template/my_component/frontend\\n   npm install    # Initialize the project and install npm dependencies\\n   npm run start  # Start the Webpack dev server\\n\\n# or\\n\\n# TypeScript-only template\\n   template-reactless/my_component/frontend\\n   npm install    # Initialize the project and install npm dependencies\\n   npm run start  # Start the Webpack dev server\\n   ```\\n\\nFrom a separate terminal, run the Streamlit app (Python) that declares and uses the component:\\n\\n```bash\\n   # React template\\n   cd template\\n   . venv/bin/activate # or similar to activate the venv/conda environment where Streamlit is installed\\n   streamlit run my_component/init.py # run the example\\n\\n# or\\n\\n# TypeScript-only template\\n   cd template-reactless\\n   . venv/bin/activate # or similar to activate the venv/conda environment where Streamlit is installed\\n   streamlit run my_component/init.py # run the example\\n   ```\\n\\nAfter running the steps above, you should see a Streamlit app in your browser that looks like this:\\n\\nThe example app from the template shows how bi-directional communication is implemented. The Streamlit Component displays a button (Python → JavaScript), and the end-user can click the button. Each time the button is clicked, the JavaScript front-end increments the counter value and passes it back to Python (JavaScript → Python), which is then displayed by Streamlit (Python → JavaScript).\\n\\nFrontend\\n\\nBecause each Streamlit Component is its own webpage that gets rendered into an iframe, you can use just about any web tech you\\'d like to create that web page. We provide two templates to get started with in the Streamlit Components-template GitHub repo; one of those templates uses React and the other does not.\\n\\nEven if you\\'re not already familiar with React, you may still want to check out the React-based\\ntemplate. It handles most of the boilerplate required to send and receive data from Streamlit, and\\nyou can learn the bits of React you need as you go.\\n\\nIf you\\'d rather not use React, please read this section anyway! It explains the fundamentals of\\nStreamlit ↔ Component communication.\\n\\nReact\\n\\nThe React-based template is in template/my_component/frontend/src/MyComponent.tsx.\\n\\nMyComponent.render() is called automatically when the component needs to be re-rendered (just like in any React app)\\n\\nArguments passed from the Python script are available via the this.props.args dictionary:\\n\\n```python\\n\\nSend arguments in Python:\\n\\nresult = my_component(greeting=\"Hello\", name=\"Streamlit\")\\n```\\n\\njavascript\\n// Receive arguments in frontend:\\nlet greeting = this.props.args[\"greeting\"]; // greeting = \"Hello\"\\nlet name = this.props.args[\"name\"]; // name = \"Streamlit\"\\n\\nUse Streamlit.setComponentValue() to return data from the component to the Python script:\\n\\njavascript\\n// Set value in frontend:\\nStreamlit.setComponentValue(3.14);\\n\\n```python\\n\\nAccess value in Python:\\n\\nresult = my_component(greeting=\"Hello\", name=\"Streamlit\")\\nst.write(\"result = \", result) # result = 3.14\\n```\\n\\nWhen you call Streamlit.setComponentValue(new_value), that new value is sent to Streamlit, which then re-executes the Python script from top to bottom. When the script is re-executed, the call to my_component(...) will return the new value.\\n\\nFrom a code flow perspective, it appears that you\\'re transmitting data synchronously with the frontend: Python sends the arguments to JavaScript, and JavaScript returns a value to Python, all in a single function call! But in reality this is all happening asynchronously, and it\\'s the re-execution of the Python script that achieves the sleight of hand.\\n\\nUse Streamlit.setFrameHeight() to control the height of your component. By default, the React template calls this automatically (see StreamlitComponentBase.componentDidUpdate()). You can override this behavior if you need more control.\\n\\nThere\\'s a tiny bit of magic in the last line of the file: export default withStreamlitConnection(MyComponent) - this does some handshaking with Streamlit, and sets up the mechanisms for bi-directional data communication.\\n\\nTypeScript-only\\n\\nThe TypeScript-only template is in template-reactless/my_component/frontend/src/MyComponent.tsx.\\n\\nThis template has much more code than its React sibling, in that all the mechanics of handshaking, setting up event listeners, and updating the component\\'s frame height are done manually. The React version of the template handles most of these details automatically.\\n\\nTowards the bottom of the source file, the template calls Streamlit.setComponentReady() to tell Streamlit it\\'s ready to start receiving data. (You\\'ll generally want to do this after creating and loading everything that the Component relies on.)\\n\\nIt subscribes to Streamlit.RENDER_EVENT to be notified of when to redraw. (This event won\\'t be fired until setComponentReady is called)\\n\\nWithin its onRender event handler, it accesses the arguments passed in the Python script via event.detail.args\\n\\nIt sends data back to the Python script in the same way that the React template does—clicking on the \"Click Me!\" button calls Streamlit.setComponentValue()\\n\\nIt informs Streamlit when its height may have changed via Streamlit.setFrameHeight()\\n\\nWorking with Themes\\n\\nCustom component theme support requires streamlit-component-lib version 1.2.0 or higher.\\n\\nAlong with sending an args object to your component, Streamlit also sends\\na theme object defining the active theme so that your component can adjust\\nits styling in a compatible way. This object is sent in the same message as\\nargs, so it can be accessed via this.props.theme (when using the React\\ntemplate) or event.detail.theme (when using the plain TypeScript template).\\n\\nThe theme object has the following shape:\\n\\njson\\n{\\n  \"base\": \"lightORdark\",\\n  \"primaryColor\": \"someColor1\",\\n  \"backgroundColor\": \"someColor2\",\\n  \"secondaryBackgroundColor\": \"someColor3\",\\n  \"textColor\": \"someColor4\",\\n  \"font\": \"someFont\"\\n}\\n\\nThe base option allows you to specify a preset Streamlit theme that your custom theme inherits from. Any theme config options not defined in your theme settings have their values set to those of the base theme. Valid values for base are \"light\" and \"dark\".\\n\\nNote that the theme object has fields with the same names and semantics as the\\noptions in the \"theme\" section of the config options printed with the command\\nstreamlit config show.\\n\\nWhen using the React template, the following CSS variables are also set\\nautomatically.\\n\\ncss\\n--base\\n--primary-color\\n--background-color\\n--secondary-background-color\\n--text-color\\n--font\\n\\nIf you\\'re not familiar with\\nCSS variables,\\nthe TLDR version is that you can use them like this:\\n\\ncss\\n.mySelector {\\n  color: var(--text-color);\\n}\\n\\nThese variables match the fields defined in the theme object above, and\\nwhether to use CSS variables or the theme object in your component is a matter\\nof personal preference.\\n\\nOther frontend details\\n\\nBecause you\\'re hosting your component from a dev server (via npm run start), any changes you make should be automatically reflected in the Streamlit app when you save.\\n\\nIf you want to add more packages to your component, run npm add to add them from within your component\\'s frontend/ directory.\\n\\nbash\\nnpm add baseui\\n\\nTo build a static version of your component, run npm run build. See Prepare your Component for more information\\n\\nPython API\\n\\ncomponents.declare_component() is all that\\'s required to create your Component\\'s Python API:\\n\\npython\\n  import streamlit.components.v1 as components\\n  my_component = components.declare_component(\\n    \"my_component\",\\n    url=\"http://localhost:3001\"\\n  )\\n\\nYou can then use the returned my_component function to send and receive data with your frontend code:\\n\\n```python\\n\\nSend data to the frontend using named arguments.\\n\\nreturn_value = my_component(name=\"Blackbeard\", ship=\"Queen Anne\\'s Revenge\")\\n\\nmy_component\\'s return value is the data returned from the frontend.\\n\\nst.write(\"Value = \", return_value)\\n```\\n\\nWhile the above is all you need to define from the Python side to have a working Component, we recommend creating a \"wrapper\" function with named arguments and default values, input validation and so on. This will make it easier for end-users to understand what data values your function accepts and allows for defining helpful docstrings.\\n\\nPlease see this example from the Components-template for an example of creating a wrapper function.\\n\\nData serialization\\n\\nPython → Frontend\\n\\nYou send data from Python to the frontend by passing keyword args to your Component\\'s invoke function (that is, the function returned from declare_component). You can send the following types of data from Python to the frontend:\\n\\nAny JSON-serializable data\\n\\nnumpy.array\\n\\npandas.DataFrame\\n\\nAny JSON-serializable data gets serialized to a JSON string, and deserialized to its JavaScript equivalent. numpy.array and pandas.DataFrame get serialized using Apache Arrow and are deserialized as instances of ArrowTable, which is a custom type that wraps Arrow structures and provides a convenient API on top of them.\\n\\nCheck out the CustomDataframe and SelectableDataTable Component example code for more context on how to use ArrowTable.\\n\\nFrontend → Python\\n\\nYou send data from the frontend to Python via the Streamlit.setComponentValue() API (which is part of the template code). Unlike arg-passing from Python → frontend, this API takes a single value. If you want to return multiple values, you\\'ll need to wrap them in an Array or Object.\\n\\nCustom Components can send JSON-serializable data from the frontend to Python, as well as Apache Arrow ArrowTables to represent dataframes.', metadata={'source': 'docs/content/library/components/components-api.md'}),\n",
       " Document(page_content='title: st.sidebar\\nslug: /library/api-reference/layout/st.sidebar\\ndescription: st.sidebar displays items in a sidebar.\\n\\nst.sidebar\\n\\nAdd widgets to sidebar\\n\\nNot only can you add interactivity to your app with widgets, you can organize them into a sidebar. Elements can be passed to st.sidebar using object notation and with notation.\\n\\nThe following two snippets are equivalent:\\n\\n```python\\n\\nObject notation\\n\\nst.sidebar.[element_name]\\n```\\n\\n```python\\n\\n\"with\" notation\\n\\nwith st.sidebar:\\n    st.[element_name]\\n```\\n\\nEach element that\\'s passed to st.sidebar is pinned to the left, allowing users to focus on the content in your app.\\n\\nThe sidebar is resizable! Drag and drop the right border of the sidebar to resize it! ↔️\\n\\nHere\\'s an example of how you\\'d add a selectbox and a radio button to your sidebar:\\n\\n```python\\nimport streamlit as st\\n\\nUsing object notation\\n\\nadd_selectbox = st.sidebar.selectbox(\\n    \"How would you like to be contacted?\",\\n    (\"Email\", \"Home phone\", \"Mobile phone\")\\n)\\n\\nUsing \"with\" notation\\n\\nwith st.sidebar:\\n    add_radio = st.radio(\\n        \"Choose a shipping method\",\\n        (\"Standard (5-15 days)\", \"Express (2-5 days)\")\\n    )\\n```\\n\\nThe only elements that aren\\'t supported using object notation are st.echo and st.spinner. To use these elements, you must use with notation.\\n\\nHere\\'s an example of how you\\'d add st.echo and st.spinner to your sidebar:\\n\\n```python\\nimport streamlit as st\\n\\nwith st.sidebar:\\n    with st.echo():\\n        st.write(\"This code will be printed to the sidebar.\")\\n\\n```', metadata={'source': 'docs/content/library/api/layout/sidebar.md'}),\n",
       " Document(page_content='title: st.container\\nslug: /library/api-reference/layout/st.container\\ndescription: st.container inserts a multi-element container.', metadata={'source': 'docs/content/library/api/layout/container.md'}),\n",
       " Document(page_content='title: st.empty\\nslug: /library/api-reference/layout/st.empty\\ndescription: st.empty inserts a single-element container.', metadata={'source': 'docs/content/library/api/layout/empty.md'}),\n",
       " Document(page_content='title: Layouts and Containers\\nslug: /library/api-reference/layout\\n\\nLayouts and Containers\\n\\nComplex layouts\\n\\nStreamlit provides several options for controlling how different elements are laid out on the screen.\\n\\nSidebar\\n\\nDisplay items in a sidebar.\\n\\npython\\nst.sidebar.write(\"This lives in the sidebar\")\\nst.sidebar.button(\"Click me!\")\\n\\nColumns\\n\\nInsert containers laid out as side-by-side columns.\\n\\npython\\ncol1, col2 = st.columns(2)\\ncol1.write(\"this is column 1\")\\ncol2.write(\"this is column 2\")\\n\\nTabs\\n\\nInsert containers separated into tabs.\\n\\npython\\ntab1, tab2 = st.tabs([\"Tab 1\", \"Tab2\"])\\ntab1.write(\"this is tab 1\")\\ntab2.write(\"this is tab 2\")\\n\\nExpander\\n\\nInsert a multi-element container that can be expanded/collapsed.\\n\\npython\\nwith st.expander(\"Open to see more\"):\\n  st.write(\"This is more content\")\\n\\nContainer\\n\\nInsert a multi-element container.\\n\\npython\\nc = st.container()\\nst.write(\"This will show last\")\\nc.write(\"This will show first\")\\nc.write(\"This will show second\")\\n\\nEmpty\\n\\nInsert a single-element container.\\n\\npython\\nc = st.empty()\\nst.write(\"This will show last\")\\nc.write(\"This will be replaced\")\\nc.write(\"This will show first\")\\n\\nStreamlit Elements\\n\\nCreate a draggable and resizable dashboard in Streamlit. Created by @okls.\\n\\n```python\\nfrom streamlit_elements import elements, mui, html\\n\\nwith elements(\"new_element\"):\\n  mui.Typography(\"Hello world\")\\n```\\n\\nPydantic\\n\\nAuto-generate Streamlit UI from Pydantic Models and Dataclasses. Created by @lukasmasuch.\\n\\n```python\\nimport streamlit_pydantic as sp\\n\\nsp.pydantic_form(key=\"my_form\",\\n  model=ExampleModel)\\n```\\n\\nStreamlit Pages\\n\\nAn experimental version of Streamlit Multi-Page Apps. Created by @blackary.\\n\\n```python\\nfrom st_pages import Page, show_pages, add_page_title\\n\\nshow_pages([ Page(\"streamlit_app.py\", \"Home\", \"🏠\"),\\n  Page(\"other_pages/page2.py\", \"Page 2\", \":books:\"), ])\\n```', metadata={'source': 'docs/content/library/api/layout/layout.md'}),\n",
       " Document(page_content='title: st.expander\\nslug: /library/api-reference/layout/st.expander\\ndescription: st.expander inserts a multi-element container that can be expanded/collapsed.', metadata={'source': 'docs/content/library/api/layout/expander.md'}),\n",
       " Document(page_content='title: st.columns\\nslug: /library/api-reference/layout/st.columns\\ndescription: st.columns inserts containers laid out as side-by-side columns.', metadata={'source': 'docs/content/library/api/layout/columns.md'}),\n",
       " Document(page_content='title: st.tabs\\nslug: /library/api-reference/layout/st.tabs\\ndescription: st.tabs inserts containers separated into tabs.', metadata={'source': 'docs/content/library/api/layout/tabs.md'}),\n",
       " Document(page_content='title: st.form\\nslug: /library/api-reference/control-flow/st.form\\ndescription: st.form creates a form that batches elements together with a “Submit\" button.', metadata={'source': 'docs/content/library/api/control-flow/form.md'}),\n",
       " Document(page_content='title: st.stop\\nslug: /library/api-reference/control-flow/st.stop\\ndescription: st.stop stops the execution immediately.', metadata={'source': 'docs/content/library/api/control-flow/stop.md'}),\n",
       " Document(page_content='title: st.experimental_rerun\\nslug: /library/api-reference/control-flow/st.experimental_rerun\\ndescription: st.experimental_rerun will rerun the script immediately.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.', metadata={'source': 'docs/content/library/api/control-flow/experimental_rerun.md'}),\n",
       " Document(page_content='title: st.form_submit_button\\nslug: /library/api-reference/control-flow/st.form_submit_button\\ndescription: st.form_submit_button displays a form submit button.', metadata={'source': 'docs/content/library/api/control-flow/form_submit_button.md'}),\n",
       " Document(page_content='title: Control flow\\nslug: /library/api-reference/control-flow\\n\\nControl flow\\n\\nChange execution\\n\\nBy default, Streamlit apps execute the script entirely, but we allow some functionality to handle control flow in your applications.\\n\\nStop execution\\n\\nStops execution immediately.\\n\\npython\\nst.stop()\\n\\nRerun script\\n\\nRerun the script immediately.\\n\\npython\\nst.experimental_rerun()\\n\\nGroup multiple widgets\\n\\nBy default, Streamlit reruns your script everytime a user interacts with your app.\\nHowever, sometimes it\\'s a better user experience to wait until a group of related\\nwidgets is filled before actually rerunning the script. That\\'s what st.form is for!\\n\\nForms\\n\\nCreate a form that batches elements together with a “Submit\" button.\\n\\npython\\nwith st.form(key=\"my_form\"):\\n    username = st.text_input(\"Username\")\\n    password = st.text_input(\"Password\")\\n    st.form_submit_button(\"Login\")\\n\\nForm submit button\\n\\nDisplay a form submit button.\\n\\npython\\nwith st.form(key=\"my_form\"):\\n    username = st.text_input(\"Username\")\\n    password = st.text_input(\"Password\")\\n    st.form_submit_button(\"Login\")\\n\\nAutorefresh\\n\\nForce a refresh without tying up a script. Created by @kmcgrady.\\n\\n```python\\nfrom streamlit_autorefresh import st_autorefresh\\n\\nst_autorefresh(interval=2000, limit=100,\\n  key=\"fizzbuzzcounter\")\\n```\\n\\nPydantic\\n\\nAuto-generate Streamlit UI from Pydantic Models and Dataclasses. Created by @lukasmasuch.\\n\\n```python\\nimport streamlit_pydantic as sp\\n\\nsp.pydantic_form(key=\"my_form\",\\n  model=ExampleModel)\\n```\\n\\nStreamlit Pages\\n\\nAn experimental version of Streamlit Multi-Page Apps. Created by @blackary.\\n\\n```python\\nfrom st_pages import Page, show_pages, add_page_title\\n\\nshow_pages([ Page(\"streamlit_app.py\", \"Home\", \"🏠\"),\\n  Page(\"other_pages/page2.py\", \"Page 2\", \":books:\"), ])\\n```', metadata={'source': 'docs/content/library/api/control-flow/control-flow.md'}),\n",
       " Document(page_content='title: Caching\\nslug: /library/advanced-features/caching\\n\\nDocumentation for the deprecated @st.cache decorator can be found in Optimize performance with st.cache.\\n\\nCaching\\n\\nStreamlit runs your script from top to bottom at every user interaction or code change. This execution model makes development super easy. But it comes with two major challenges:\\n\\nLong-running functions run again and again, which slows down your app.\\n\\nObjects get recreated again and again, which makes it hard to persist them across reruns or sessions.\\n\\nBut don\\'t worry! Streamlit lets you tackle both issues with its built-in caching mechanism. Caching stores the results of slow function calls, so they only need to run once. This makes your app much faster and helps with persisting objects across reruns.\\n\\nMinimal example\\n\\nBasic usage\\n\\nAdvanced usage\\n\\nMigrating from st.cache\\n\\nMinimal example\\n\\nTo cache a function in Streamlit, you must decorate it with one of two decorators (st.cache_data or st.cache_resource):\\n\\npython\\n@st.cache_data\\ndef long_running_function(param1, param2):\\n    return …\\n\\nIn this example, decorating long_running_function with @st.cache_data tells Streamlit that whenever the function is called, it checks two things:\\n\\nThe values of the input parameters (in this case, param1 and param2).\\n\\nThe code inside the function.\\n\\nIf this is the first time Streamlit sees these parameter values and function code, it runs the function and stores the return value in a cache. The next time the function is called with the same parameters and code (e.g., when a user interacts with the app), Streamlit will skip executing the function altogether and return the cached value instead. During development, the cache updates automatically as the function code changes, ensuring that the latest changes are reflected in the cache.\\n\\nAs mentioned, there are two caching decorators:\\n\\nst.cache_data\\xa0is the recommended way to cache computations that return data: loading a DataFrame from CSV, transforming a NumPy array, querying an API, or any other function that returns a serializable data object (str, int, float, DataFrame, array, list, …). It creates a new copy of the data at each function call, making it safe against mutations and race conditions. The behavior of st.cache_data is what you want in most cases – so if you\\'re unsure, start with\\xa0st.cache_data\\xa0and see if it works!\\n\\nst.cache_resource\\xa0is the recommended way to cache global resources like ML models or database connections – unserializable objects that you don\\'t want to load multiple times. Using it, you can share these resources across all reruns and sessions of an app without copying or duplication. Note that any mutations to the cached return value directly mutate the object in the cache (more details below).\\n\\nBasic usage\\n\\nst.cache_data\\n\\nst.cache_data is your go-to command for all functions that return data – whether DataFrames, NumPy arrays, str, int, float, or other serializable types. It\\'s the right command for almost all use cases!\\n\\nUsage\\n\\nLet\\'s look at an example of using\\xa0st.cache_data. Suppose your app loads the Uber ride-sharing dataset – a CSV file of 50 MB – from the internet into a DataFrame:\\n\\n```python\\ndef load_data(url):\\n    df = pd.read_csv(url)  # 👈 Download the data\\n    return df\\n\\ndf = load_data(\"https://github.com/plotly/datasets/raw/master/uber-rides-data1.csv\")\\nst.dataframe(df)\\n\\nst.button(\"Rerun\")\\n```\\n\\nRunning the load_data function takes 2 to 30 seconds, depending on your internet connection. (Tip: if you are on a slow connection, use this 5 MB dataset instead). Without caching, the download is rerun each time the app is loaded or with user interaction. Try it yourself by clicking the button we added! Not a great experience… 😕\\n\\nNow let\\'s add the\\xa0@st.cache_data\\xa0decorator on load_data:\\n\\n```python\\n@st.cache_data  # 👈 Add the caching decorator\\ndef load_data(url):\\n    df = pd.read_csv(url)\\n    return df\\n\\ndf = load_data(\"https://github.com/plotly/datasets/raw/master/uber-rides-data1.csv\")\\nst.dataframe(df)\\n\\nst.button(\"Rerun\")\\n```\\n\\nRun the app again. You\\'ll notice that the slow download only happens on the first run. Every subsequent rerun should be almost instant! 💨\\n\\nBehavior\\n\\nHow does this work? Let\\'s go through the behavior of st.cache_data step by step:\\n\\nOn the first run, Streamlit recognizes that it has never called the load_data function with the specified parameter value (the URL of the CSV file) So it runs the function and downloads the data.\\n\\nNow our caching mechanism becomes active: the returned DataFrame is serialized (converted to bytes) via\\xa0pickle\\xa0and stored in the cache (together with the value of the url parameter).\\n\\nOn the next run, Streamlit checks the cache for an entry of load_data with the specific url. There is one! So it retrieves the cached object, deserializes it to a DataFrame, and returns it instead of re-running the function and downloading the data again.\\n\\nThis process of serializing and deserializing the cached object creates a copy of our original DataFrame. While this copying behavior may seem unnecessary, it\\'s what we want when caching data objects since it effectively prevents mutation and concurrency issues. Read the section “Mutation and concurrency issues\" below to understand this in more detail.\\n\\nExamples\\n\\nDataFrame transformations\\n\\nIn the example above, we already showed how to cache loading a DataFrame. It can also be useful to cache DataFrame transformations such as df.filter, df.apply, or df.sort_values. Especially with large DataFrames, these operations can be slow.\\n\\npython\\n@st.cache_data\\ndef transform(df):\\n    df = df.filter(items=[\\'one\\', \\'three\\'])\\n    df = df.apply(np.sum, axis=0)\\n    return df\\n\\nArray computations\\n\\nSimilarly, it can make sense to cache computations on NumPy arrays:\\n\\npython\\n@st.cache_data\\ndef add(arr1, arr2):\\n    return arr1 + arr2\\n\\nDatabase queries\\n\\nYou usually make SQL queries to load data into your app when working with databases. Repeatedly running these queries can be slow, cost money, and degrade the performance of your database. We strongly recommend caching any database queries in your app. See also our guides on connecting Streamlit to different databases for in-depth examples.\\n\\n```python\\nconnection = database.connect()\\n\\n@st.cache_data\\ndef query():\\n    return pd.read_sql_query(\"SELECT * from table\", connection)\\n```\\n\\nYou should set a ttl (time to live) to get new results from your database. If you set st.cache_data(ttl=3600), Streamlit invalidates any cached values after 1 hour (3600 seconds) and runs the cached function again. See details in Controlling cache size and duration.\\n\\nAPI calls\\n\\nSimilarly, it makes sense to cache API calls. Doing so also avoids rate limits.\\n\\npython\\n@st.cache_data\\ndef api_call():\\n    response = requests.get(\\'https://jsonplaceholder.typicode.com/posts/1\\')\\n    return response.json()\\n\\nRunning ML models (inference)\\n\\nRunning complex machine learning models can use significant time and memory. To avoid rerunning the same computations over and over, use caching.\\n\\npython\\n@st.cache_data\\ndef run_model(inputs):\\n    return model(inputs)\\n\\nst.cache_resource\\n\\nst.cache_resource is the right command to cache “resources\" that should be available globally across all users, sessions, and reruns. It has more limited use cases than st.cache_data, especially for caching database connections and ML models.\\n\\nUsage\\n\\nAs an example for st.cache_resource, let\\'s look at a typical machine learning app. As a first step, we need to load an ML model. We do this with Hugging Face\\'s transformers library:\\n\\npython\\nfrom transformers import pipeline\\nmodel = pipeline(\"sentiment-analysis\")  # 👈 Load the model\\n\\nIf we put this code into a Streamlit app directly, the app will load the model at each rerun or user interaction. Repeatedly loading the model poses two problems:\\n\\nLoading the model takes time and slows down the app.\\n\\nEach session loads the model from scratch, which takes up a huge amount of memory.\\n\\nInstead, it would make much more sense to load the model once and use that same object across all users and sessions. That\\'s exactly the use case for st.cache_resource! Let\\'s add it to our app and process some text the user entered:\\n\\n```python\\nfrom transformers import pipeline\\n\\n@st.cache_resource  # 👈 Add the caching decorator\\ndef load_model():\\n    return pipeline(\"sentiment-analysis\")\\n\\nmodel = load_model()\\n\\nquery = st.text_input(\"Your query\", value=\"I love Streamlit! 🎈\")\\nif query:\\n    result = model(query)[0]  # 👈 Classify the query text\\n    st.write(result)\\n```\\n\\nIf you run this app, you\\'ll see that the app calls load_model only once – right when the app starts. Subsequent runs will reuse that same model stored in the cache, saving time and memory!\\n\\nBehavior\\n\\nUsing st.cache_resource is very similar to using st.cache_data. But there are a few important differences in behavior:\\n\\nst.cache_resource does not create a copy of the cached return value but instead stores the object itself in the cache. All mutations on the function\\'s return value directly affect the object in the cache, so you must ensure that mutations from multiple sessions do not cause problems. In short, the return value must be thread-safe.\\n\\nUsing st.cache_resource on objects that are not thread-safe might lead to crashes or corrupted data. Learn more below under Mutation and concurrency issues.\\n\\nNot creating a copy means there\\'s just one global instance of the cached return object, which saves memory, e.g. when using a large ML model. In computer science terms, we create a singleton.\\n\\nReturn values of functions do not need to be serializable. This behavior is great for types not serializable by nature, e.g., database connections, file handles, or threads. Caching these objects with st.cache_data is not possible.\\n\\nExamples\\n\\nDatabase connections\\n\\nst.cache_resource is useful for connecting to databases. Usually, you\\'re creating a connection object that you want to reuse globally for every query. Creating a new connection object at each run would be inefficient and might lead to connection errors. That\\'s exactly what st.cache_resource can do, e.g., for a Postgres database:\\n\\n```python\\n@st.cache_resource\\ndef init_connection():\\n    host = \"hh-pgsql-public.ebi.ac.uk\"\\n    database = \"pfmegrnargs\"\\n    user = \"reader\"\\n    password = \"NWDMCE5xdipIjRrp\"\\n    return psycopg2.connect(host=host, database=database, user=user, password=password)\\n\\nconn = init_connection()\\n```\\n\\nOf course, you can do the same for any other database. Have a look at our guides on how to connect Streamlit to databases for in-depth examples.\\n\\nLoading ML models\\n\\nYour app should always cache ML models, so they are not loaded into memory again for every new session. See the example above for how this works with 🤗\\xa0Hugging Face models. You can do the same thing for PyTorch, TensorFlow, etc. Here\\'s an example for PyTorch:\\n\\n```python\\n@st.cache_resource\\ndef load_model():\\n    model = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\\n    model.eval()\\n    return model\\n\\nmodel = load_model()\\n```\\n\\nDeciding which caching decorator to use\\n\\nThe sections above showed many common examples for each caching decorator. But there are edge cases for which it\\'s less trivial to decide which caching decorator to use. Eventually, it all comes down to the difference between “data\" and “resource\":\\n\\nData are serializable objects (objects that can be converted to bytes via\\xa0pickle) that you could easily save to disk. Imagine all the types you would usually store in a database or on a file system – basic types like str, int, and float, but also arrays, DataFrames, images, or combinations of these types (lists, tuples, dicts, and so on).\\n\\nResources are unserializable objects that you usually would not save to disk or a database. They are often more complex, non-permanent objects like database connections, ML models, file handles, threads, etc.\\n\\nFrom the types listed above, it should be obvious that most objects in Python are “data.\" That\\'s also why st.cache_data is the correct command for almost all use cases. st.cache_resource is a more exotic command that you should only use in specific situations.\\n\\nOr if you\\'re lazy and don\\'t want to think too much, look up your use case or return type in the table below 😉:\\n\\n| Use case                             |                                                                                                       Typical return types |                                                                                                                                            Caching decorator |\\n| :----------------------------------- | -------------------------------------------------------------------------------------------------------------------------: | -----------------------------------------------------------------------------------------------------------------------------------------------------------: |\\n| Reading a CSV file with pd.read_csv  |                                                                                                           pandas.DataFrame |                                                                                                                                                st.cache_data |\\n| Reading a text file                  |                                                                                                           str, list of str |                                                                                                                                                st.cache_data |\\n| Transforming pandas dataframes       |                                                                                            pandas.DataFrame, pandas.Series |                                                                                                                                                st.cache_data |\\n| Computing with numpy arrays          |                                                                                                              numpy.ndarray |                                                                                                                                                st.cache_data |\\n| Simple computations with basic types |                                                                                                         str, int, float, … |                                                                                                                                                st.cache_data |\\n| Querying a database                  |                                                                                                           pandas.DataFrame |                                                                                                                                                st.cache_data |\\n| Querying an API                      |                                                                                                pandas.DataFrame, str, dict |                                                                                                                                                st.cache_data |\\n| Running an ML model (inference)      |                                                                                     pandas.DataFrame, str, int, dict, list |                                                                                                                                                st.cache_data |\\n| Creating or processing images        |                                                                                             PIL.Image.Image, numpy.ndarray |                                                                                                                                                st.cache_data |\\n| Creating charts                      |                                                        matplotlib.figure.Figure, plotly.graph_objects.Figure, altair.Chart | st.cache_data (but some libraries require st.cache_resource, since the chart object is not serializable – make sure not to mutate the chart after creation!) |\\n| Loading ML models                    |                                                             transformers.Pipeline, torch.nn.Module, tensorflow.keras.Model |                                                                                                                                            st.cache_resource |\\n| Initializing database connections    | pyodbc.Connection, sqlalchemy.engine.base.Engine, psycopg2.connection, mysql.connector.MySQLConnection, sqlite3.Connection |                                                                                                                                            st.cache_resource |\\n| Opening persistent file handles      |                                                                                                         _io.TextIOWrapper |                                                                                                                                            st.cache_resource |\\n| Opening persistent threads           |                                                                                                           threading.thread |                                                                                                                                            st.cache_resource |\\n\\nAdvanced usage\\n\\nControlling cache size and duration\\n\\nIf your app runs for a long time and constantly caches functions, you might run into two problems:\\n\\nThe app runs out of memory because the cache is too large.\\n\\nObjects in the cache become stale, e.g. because you cached old data from a database.\\n\\nYou can combat these problems with the ttl and max_entries parameters, which are available for both caching decorators.\\n\\nThe ttl (time-to-live) parameter\\n\\nttl sets a time to live on a cached function. If that time is up and you call the function again, the app will discard any old, cached values, and the function will be rerun. The newly computed value will then be stored in the cache. This behavior is useful for preventing stale data (problem 2) and the cache from growing too large (problem 1). Especially when pulling data from a database or API, you should always set a ttl so you are not using old data. Here\\'s an example:\\n\\npython\\n@st.cache_data(ttl=3600)  # 👈 Cache data for 1 hour (=3600 seconds)\\ndef get_api_data():\\n    data = api.get(...)\\n    return data\\n\\nYou can also set ttl values using timedelta, e.g., ttl=datetime.timedelta(hours=1).\\n\\nThe max_entries parameter\\n\\nmax_entries sets the maximum number of entries in the cache. An upper bound on the number of cache entries is useful for limiting memory (problem 1), especially when caching large objects. The oldest entry will be removed when a new entry is added to a full cache. Here\\'s an example:\\n\\npython\\n@st.cache_data(max_entries=1000)  # 👈 Maximum 1000 entries in the cache\\ndef get_large_array(seed):\\n    np.random.seed(seed)\\n    arr = np.random.rand(100000)\\n    return arr\\n\\nCustomizing the spinner\\n\\nBy default, Streamlit shows a small loading spinner in the app when a cached function is running. You can modify it easily with the show_spinner parameter, which is available for both caching decorators:\\n\\n```python\\n@st.cache_data(show_spinner=False)  # 👈 Disable the spinner\\ndef get_api_data():\\n    data = api.get(...)\\n    return data\\n\\n@st.cache_data(show_spinner=\"Fetching data from API...\")  # 👈 Use custom text for spinner\\ndef get_api_data():\\n    data = api.get(...)\\n    return data\\n```\\n\\nExcluding input parameters\\n\\nIn a cached function, all input parameters must be hashable. Let\\'s quickly explain why and what it means. When the function is called, Streamlit looks at its parameter values to determine if it was cached before. Therefore, it needs a reliable way to compare the parameter values across function calls. Trivial for a string or int – but complex for arbitrary objects! Streamlit uses hashing to solve that. It converts the parameter to a stable key and stores that key. At the next function call, it hashes the parameter again and compares it with the stored hash key.\\n\\nUnfortunately, not all parameters are hashable! E.g., you might pass an unhashable database connection or ML model to your cached function. In this case, you can exclude input parameters from caching. Simply prepend the parameter name with an underscore (e.g., _param1), and it will not be used for caching. Even if it changes, Streamlit will return a cached result if all the other parameters match up.\\n\\nHere\\'s an example:\\n\\n```python\\n@st.cache_data\\ndef fetch_data(_db_connection, num_rows):  # 👈 Don\\'t hash _db_connection\\n    data = _db_connection.fetch(num_rows)\\n    return data\\n\\nconnection = init_connection()\\nfetch_data(connection, 10)\\n```\\n\\nBut what if you want to cache a function that takes an unhashable parameter? For example, you might want to cache a function that takes an ML model as input and returns the layer names of that model. Since the model is the only input parameter, you cannot exclude it from caching. In this case you can use the hash_funcs parameter to specify a custom hashing function for the model.\\n\\nThe hash_funcs parameter\\n\\nAs described above, Streamlit\\'s caching decorators hash the input parameters and cached function\\'s signature to determine whether the function has been run before and has a return value stored (\"cache hit\") or needs to be run (\"cache miss\"). Input parameters that are not hashable by Streamlit\\'s hashing implementation can be ignored by prepending an underscore to their name. But there two rare cases where this is undesirable. i.e. where you want to hash the parameter that Streamlit is unable to hash:\\n\\nWhen Streamlit\\'s hashing mechanism fails to hash a parameter, resulting in a UnhashableParamError being raised.\\n\\nWhen you want to override Streamlit\\'s default hashing mechanism for a parameter.\\n\\nLet\\'s discuss each of these cases in turn with examples.\\n\\nExample 1: Hashing a custom class\\n\\nStreamlit does not know how to hash custom classes. If you pass a custom class to a cached function, Streamlit will raise a UnhashableParamError. For example, let\\'s define a custom class MyCustomClass that accepts an initial integer score. Let\\'s also define a cached function multiply_score that multiplies the score by a multiplier:\\n\\n```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\n@st.cache_data\\ndef multiply_score(obj: MyCustomClass, multiplier: int) -> int:\\n    return obj.my_score * multiplier\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(multiply_score(score, multiplier))\\n```\\n\\nIf you run this app, you\\'ll see that Streamlit raises a UnhashableParamError since it does not know how to hash MyCustomClass:\\n\\npython\\nUnhashableParamError: Cannot hash argument \\'obj\\' (of type __main__.MyCustomClass) in \\'multiply_score\\'.\\n\\nTo fix this, we can use the hash_funcs parameter to tell Streamlit how to hash MyCustomClass. We do this by passing a dictionary to hash_funcs that maps the name of the parameter to a hash function. The choice of hash function is up to the developer. In this case, let\\'s define a custom hash function hash_func that takes the custom class as input and returns the score. We want the score to be the unique identifier of the object, so we can use it to deterministically hash the object:\\n\\n```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\ndef hash_func(obj: MyCustomClass) -> int:\\n    return obj.my_score  # or any other value that uniquely identifies the object\\n\\n@st.cache_data(hash_funcs={MyCustomClass: hash_func})\\ndef multiply_score(obj: MyCustomClass, multiplier: int) -> int:\\n    return obj.my_score * multiplier\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(multiply_score(score, multiplier))\\n```\\n\\nNow if you run the app, you\\'ll see that Streamlit no longer raises a UnhashableParamError and the app runs as expected.\\n\\nLet\\'s now consider the case where multiply_score is an attribute of MyCustomClass and we want to hash the entire object:\\n\\n```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(score.multiply_score(multiplier))\\n```\\n\\nIf you run this app, you\\'ll see that Streamlit raises a UnhashableParamError since it cannot hash the argument \\'self\\' (of type __main__.MyCustomClass) in \\'multiply_score\\'. A simple fix here could be to use Python\\'s hash() function to hash the object:\\n\\n```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(score.multiply_score(multiplier))\\n```\\n\\nAbove, the hash function is defined as lambda x: hash(x.my_score). This creates a hash based on the my_score attribute of the MyCustomClass instance. As long as my_score remains the same, the hash remains the same. Thus, the result of multiply_score can be retrieved from the cache without recomputation.\\n\\nAs an astute Pythonista, you may have been tempted to use Python\\'s id() function to hash the object like so:\\n\\n```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(score.multiply_score(multiplier))\\n```\\n\\nThis is why we discourage using it as hash func, and instead encourage functions that return deterministic, true hash values. That said, if you know what you\\'re doing, you can use id() as a hash function. Just be aware of the consequences. For example, id is often the correct hash func when you\\'re passing the result of an @st.cache_resource function as the input param to another cached function. There\\'s a whole class of object types that aren’t otherwise hashable.\\n\\nExample 2: Hashing a Pydantic model\\n\\nLet\\'s consider another example where we want to hash a Pydantic model:\\n\\n```python\\nimport streamlit as st\\nfrom pydantic import BaseModel\\n\\nclass Person(BaseModel):\\n    name: str\\n\\n@st.cache_data\\ndef identity(person: Person):\\n    return person\\n\\nperson = identity(Person(name=\"Lee\"))\\nst.write(f\"The person is {person.name}\")\\n```\\n\\nAbove, we define a custom class Person using Pydantic\\'s BaseModel with a single attribute name. We also define an identity function which accepts an instance of Person as an arg and returns it without modification. This function is intended to cache the result, therefore, if called multiple times with the same Person instance, it won\\'t recompute but return the cached instance.\\n\\nIf you run the app, however, you\\'ll run into a UnhashableParamError: Cannot hash argument \\'person\\' (of type __main__.Person) in \\'identity\\'. error. This is because Streamlit does not know how to hash the Person class. To fix this, we can use the hash_funcs kwarg to tell Streamlit how to hash Person.\\n\\nIn the version below, we define a custom hash function hash_func that takes the Person instance as input and returns the name attribute. We want the name to be the unique identifier of the object, so we can use it to deterministically hash the object:\\n\\n```python\\nimport streamlit as st\\nfrom pydantic import BaseModel\\n\\nclass Person(BaseModel):\\n    name: str\\n\\n@st.cache_data(hash_funcs={Person: lambda p: p.name})\\ndef identity(person: Person):\\n    return person\\n\\nperson = identity(Person(name=\"Lee\"))\\nst.write(f\"The person is {person.name}\")\\n```\\n\\nExample 3: Hashing a ML model\\n\\nThere may be cases where you want to pass your favorite machine learning model to a cached function. For example, let\\'s say you want to pass a TensorFlow model to a cached function, based on what model the user selects in the app. You might try something like this:\\n\\n```python\\nimport streamlit as st\\nimport tensorflow as tf\\n\\n@st.cache_resource\\ndef load_base_model(option):\\n    if option == 1:\\n        return tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\")\\n    else:\\n        return tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\")\\n\\n@st.cache_resource\\ndef load_layers(base_model):\\n    return [layer.name for layer in base_model.layers]\\n\\noption = st.radio(\"Model 1 or 2\", [1, 2])\\n\\nbase_model = load_base_model(option)\\n\\nlayers = load_layers(base_model)\\n\\nst.write(layers)\\n```\\n\\nIn the above app, the user can select one of two models. Based on the selection, the app loads the corresponding model and passes it to load_layers. This function then returns the names of the layers in the model. If you run the app, you\\'ll see that Streamlit raises a UnhashableParamError since it cannot hash the argument \\'base_model\\' (of type keras.engine.functional.Functional) in \\'load_layers\\'.\\n\\nIf you disable hashing for base_model by prepending an underscore to its name, you\\'ll observe that regardless of which base model is chosen, the layers displayed are same. This subtle bug is due to the fact that the load_layers function is not re-run when the base model changes. This is because Streamlit does not hash the base_model argument, so it does not know that the function needs to be re-run when the base model changes.\\n\\n```python\\nimport streamlit as st\\nimport tensorflow as tf\\nfrom keras.engine.functional import Functional\\n\\n@st.cache_resource\\ndef load_base_model(option):\\n    if option == 1:\\n        return tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\")\\n    else:\\n        return tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\")\\n\\n@st.cache_resource(hash_funcs={Functional: lambda x: x.name})\\ndef load_layers(base_model):\\n    return [layer.name for layer in base_model.layers]\\n\\noption = st.radio(\"Model 1 or 2\", [1, 2])\\n\\nbase_model = load_base_model(option)\\n\\nlayers = load_layers(base_model)\\n\\nst.write(layers)\\n```\\n\\nIn the above case, we could also have used hash_funcs={Functional: id} as the hash function. This is because id is often the correct hash func when you\\'re passing the result of an @st.cache_resource function as the input param to another cached function.\\n\\nExample 4: Overriding Streamlit\\'s default hashing mechanism\\n\\nLet\\'s consider another example where we want to override Streamlit\\'s default hashing mechanism for a pytz-localized datetime object:\\n\\n```python\\nfrom datetime import datetime\\nimport pytz\\nimport streamlit as st\\n\\ntz = pytz.timezone(\"Europe/Berlin\")\\n\\n@st.cache_data\\ndef load_data(dt):\\n    return dt\\n\\nnow = datetime.now()\\nst.text(load_data(dt=now))\\n\\nnow_tz = tz.localize(datetime.now())\\nst.text(load_data(dt=now_tz))\\n```\\n\\n```python\\nfrom datetime import datetime\\n\\nimport pytz\\nimport streamlit as st\\n\\ntz = pytz.timezone(\"Europe/Berlin\")\\n\\n@st.cache_data(hash_funcs={datetime: lambda x: x.strftime(\"%a %d %b %Y, %I:%M%p\")})\\ndef load_data(dt):\\n    return dt\\n\\nnow = datetime.now()\\nst.text(load_data(dt=now))\\n\\nnow_tz = tz.localize(datetime.now())\\nst.text(load_data(dt=now_tz))\\n```\\n\\nLet\\'s now consider a case where we want to override Streamlit\\'s default hashing mechanism for NumPy arrays. While Streamlit natively hashes Pandas and NumPy objects, there may be cases where you want to override Streamlit\\'s default hashing mechanism for these objects.\\n\\nFor example, let\\'s say we create a cache-decorated show_data function that accepts a NumPy array and returns it without modification. In the bellow app, data = df[\"str\"].unique() (which is a NumPy array) is passed to the show_data function.\\n\\n```python\\nimport time\\nimport numpy as np\\nimport pandas as pd\\nimport streamlit as st\\n\\n@st.cache_data\\ndef get_data():\\n    df = pd.DataFrame({\"num\": [112, 112, 2, 3], \"str\": [\"be\", \"a\", \"be\", \"c\"]})\\n    return df\\n\\n@st.cache_data\\ndef show_data(data):\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return data\\n\\ndf = get_data()\\ndata = df[\"str\"].unique()\\n\\nst.dataframe(show_data(data))\\nst.button(\"Re-run\")\\n```\\n\\nSince data is always the same, we expect the show_data function to return the cached value. However, if you run the app, and click the Re-run button, you\\'ll notice that the show_data function is re-run each time. We can assume this behavior is a consequence of Streamlit\\'s default hashing mechanism for NumPy arrays.\\n\\nTo work around this, let\\'s define a custom hash function hash_func that takes a NumPy array as input and returns a string representation of the array:\\n\\n```python\\nimport time\\nimport numpy as np\\nimport pandas as pd\\nimport streamlit as st\\n\\n@st.cache_data\\ndef get_data():\\n    df = pd.DataFrame({\"num\": [112, 112, 2, 3], \"str\": [\"be\", \"a\", \"be\", \"c\"]})\\n    return df\\n\\n@st.cache_data(hash_funcs={np.ndarray: str})\\ndef show_data(data):\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return data\\n\\ndf = get_data()\\ndata = df[\"str\"].unique()\\n\\nst.dataframe(show_data(data))\\nst.button(\"Re-run\")\\n```\\n\\nNow if you run the app, and click the Re-run button, you\\'ll notice that the show_data function is no longer re-run each time. It\\'s important to note here that our choice of hash function was very naive and not necessarily the best choice. For example, if the NumPy array is large, converting it to a string representation may be expensive. In such cases, it is up to you as the developer to define what a good hash function is for your use case.\\n\\nStatic elements\\n\\nSince version 1.16.0, cached functions can contain Streamlit commands! For example, you can do this:\\n\\npython\\n@st.cache_data\\ndef get_api_data():\\n    data = api.get(...)\\n    st.success(\"Fetched data from API!\")  # 👈 Show a success message\\n    return data\\n\\nAs we know, Streamlit only runs this function if it hasn\\'t been cached before. On this first run, the st.success message will appear in the app. But what happens on subsequent runs? It still shows up! Streamlit realizes that there is an st. command inside the cached function, saves it during the first run, and replays it on subsequent runs. Replaying static elements works for both caching decorators.\\n\\nYou can also use this functionality to cache entire parts of your UI:\\n\\npython\\n@st.cache_data\\ndef show_data():\\n    st.header(\"Data analysis\")\\n    data = api.get(...)\\n    st.success(\"Fetched data from API!\")\\n    st.write(\"Here is a plot of the data:\")\\n    st.line_chart(data)\\n    st.write(\"And here is the raw data:\")\\n    st.dataframe(data)\\n\\nInput widgets\\n\\nYou can also use interactive input widgets like st.slider or st.text_input in cached functions. Widget replay is an experimental feature at the moment. To enable it, you need to set the experimental_allow_widgets parameter:\\n\\npython\\n@st.cache_data(experimental_allow_widgets=True)  # 👈 Set the parameter\\ndef get_data():\\n    num_rows = st.slider(\"Number of rows to get\")  # 👈 Add a slider\\n    data = api.get(..., num_rows)\\n    return data\\n\\nStreamlit treats the slider like an additional input parameter to the cached function. If you change the slider position, Streamlit will see if it has already cached the function for this slider value. If yes, it will return the cached value. If not, it will rerun the function using the new slider value.\\n\\nUsing widgets in cached functions is extremely powerful because it lets you cache entire parts of your app. But it can be dangerous! Since Streamlit treats the widget value as an additional input parameter, it can easily lead to excessive memory usage. Imagine your cached function has five sliders and returns a 100 MB DataFrame. Then we\\'ll add 100 MB to the cache for every permutation of these five slider values – even if the sliders do not influence the returned data! These additions can make your cache explode very quickly. Please be aware of this limitation if you use widgets in cached functions. We recommend using this feature only for isolated parts of your UI where the widgets directly influence the cached return value.\\n\\nSupport for widgets in cached functions is experimental. We may change or remove it anytime without warning. Please use it with care!\\n\\nTwo widgets are currently not supported in cached functions: st.file_uploader and st.camera_input. We may support them in the future. Feel free to open a GitHub issue if you need them!\\n\\nDealing with large data\\n\\nAs we explained, you should cache data objects with st.cache_data. But this can be slow for extremely large data, e.g., DataFrames or arrays with >100 million rows. That\\'s because of the copying behavior of st.cache_data: on the first run, it serializes the return value to bytes and deserializes it on subsequent runs. Both operations take time.\\n\\nIf you\\'re dealing with extremely large data, it can make sense to use st.cache_resource instead. It does not create a copy of the return value via serialization/deserialization and is almost instant. But watch out: any mutation to the function\\'s return value (such as dropping a column from a DataFrame or setting a value in an array) directly manipulates the object in the cache. You must ensure this doesn\\'t corrupt your data or lead to crashes. See the section on Mutation and concurrency issues below.\\n\\nWhen benchmarking st.cache_data on pandas DataFrames with four columns, we found that it becomes slow when going beyond 100 million rows. The table shows runtimes for both caching decorators at different numbers of rows (all with four columns):\\n\\n|                   |                 | 10M rows | 50M rows | 100M rows | 200M rows |\\n| ----------------- | --------------- | :------: | :------: | :-------: | :-------: |\\n| st.cache_data     | First run*     |  0.4 s   |   3 s    |   14 s    |   28 s    |\\n|                   | Subsequent runs |  0.2 s   |   1 s    |    2 s    |    7 s    |\\n| st.cache_resource | First run*     |  0.01 s  |  0.1 s   |   0.2 s   |    1 s    |\\n|                   | Subsequent runs |   0 s    |   0 s    |    0 s    |    0 s    |\\n\\n|                                                                                                                                                              |\\n| :----------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| *For the first run, the table only shows the overhead time of using the caching decorator. It does not include the runtime of the cached function itself. |\\n\\nMutation and concurrency issues\\n\\nIn the sections above, we talked a lot about issues when mutating return objects of cached functions. This topic is complicated! But it\\'s central to understanding the behavior differences between st.cache_data and st.cache_resource. So let\\'s dive in a bit deeper.\\n\\nFirst, we should clearly define what we mean by mutations and concurrency:\\n\\nBy mutations, we mean any changes made to a cached function\\'s return value after that function has been called. I.e. something like this:\\n\\n```python\\n  @st.cache_data\\n  def create_list():\\n      l = [1, 2, 3]\\n\\nl = create_list()  # 👈 Call the function\\n  l[0] = 2  # 👈 Mutate its return value\\n  ```\\n\\nBy concurrency, we mean that multiple sessions can cause these mutations at the same time. Streamlit is a web framework that needs to handle many users and sessions connecting to an app. If two people view an app at the same time, they will both cause the Python script to rerun, which may manipulate cached return objects at the same time – concurrently.\\n\\nMutating cached return objects can be dangerous. It can lead to exceptions in your app and even corrupt your data (which can be worse than a crashed app!). Below, we\\'ll first explain the copying behavior of st.cache_data and show how it can avoid mutation issues. Then, we\\'ll show how concurrent mutations can lead to data corruption and how to prevent it.\\n\\nCopying behavior\\n\\nst.cache_data creates a copy of the cached return value each time the function is called. This avoids most mutations and concurrency issues. To understand it in detail, let\\'s go back to the Uber ridesharing example from the section on st.cache_data above. We are making two modifications to it:\\n\\nWe are using st.cache_resource instead of st.cache_data. st.cache_resource does not create a copy of the cached object, so we can see what happens without the copying behavior.\\n\\nAfter loading the data, we manipulate the returned DataFrame (in place!) by dropping the column \"Lat\".\\n\\nHere\\'s the code:\\n\\n```python\\n@st.cache_resource   # 👈 Turn off copying behavior\\ndef load_data(url):\\n    df = pd.read_csv(url)\\n    return df\\n\\ndf = load_data(\"https://raw.githubusercontent.com/plotly/datasets/master/uber-rides-data1.csv\")\\nst.dataframe(df)\\n\\ndf.drop(columns=[\\'Lat\\'], inplace=True)  # 👈 Mutate the dataframe inplace\\n\\nst.button(\"Rerun\")\\n```\\n\\nLet\\'s run it and see what happens! The first run should work fine. But in the second run, you see an exception: KeyError: \"[\\'Lat\\'] not found in axis\". Why is that happening? Let\\'s go step by step:\\n\\nOn the first run, Streamlit runs load_data and stores the resulting DataFrame in the cache. Since we\\'re using st.cache_resource, it does not create a copy but stores the original DataFrame.\\n\\nThen we drop the column \"Lat\" from the DataFrame. Note that this is dropping the column from the original DataFrame stored in the cache. We are manipulating it!\\n\\nOn the second run, Streamlit returns that exact same manipulated DataFrame from the cache. It does not have the column \"Lat\" anymore! So our call to df.drop results in an exception. Pandas cannot drop a column that doesn\\'t exist.\\n\\nThe copying behavior of st.cache_data prevents this kind of mutation error. Mutations can only affect a specific copy and not the underlying object in the cache. The next rerun will get its own, unmutated copy of the DataFrame. You can try it yourself, just replace st.cache_resource with st.cache_data above, and you\\'ll see that everything works.\\n\\nBecause of this copying behavior,\\xa0st.cache_data\\xa0is the recommended way to cache data transforms and computations – anything that returns a serializable object.\\n\\nConcurrency issues\\n\\nNow let\\'s look at what can happen when multiple users concurrently mutate an object in the cache. Let\\'s say you have a function that returns a list. Again, we are using st.cache_resource to cache it so that we are not creating a copy:\\n\\n```python\\n@st.cache_resource\\ndef create_list():\\n    l = [1, 2, 3]\\n    return l\\n\\nl = create_list()\\nfirst_list_value = l[0]\\nl[0] = first_list_value + 1\\n\\nst.write(\"l[0] is:\", l[0])\\n```\\n\\nLet\\'s say user A runs the app. They will see the following output:\\n\\npython\\nl[0] is: 2\\n\\nLet\\'s say another user, B, visits the app right after. In contrast to user A, they will see the following output:\\n\\npython\\nl[0] is: 3\\n\\nNow, user A reruns the app immediately after user B. They will see the following output:\\n\\npython\\nl[0] is: 4\\n\\nWhat is happening here? Why are all outputs different?\\n\\nWhen user A visits the app,\\xa0create_list()\\xa0is called, and the list\\xa0[1, 2, 3]\\xa0is stored in the cache. This list is then returned to user A. The first value of the list, 1, is assigned to first_list_value , and l[0]\\xa0is changed to 2.\\n\\nWhen user B visits the app,\\xa0create_list()\\xa0returns the mutated list from the cache:\\xa0[2, 2, 3]. The first value of the list, 2, is assigned to first_list_value and l[0]\\xa0is changed to 3.\\n\\nWhen user A reruns the app,\\xa0create_list()\\xa0returns the mutated list again:\\xa0[3, 2, 3]. The first value of the list, 3, is assigned to first_list_value, and l[0]\\xa0is changed to 4.\\n\\nIf you think about it, this makes sense. Users A and B use the same list object (the one stored in the cache). And since the list object is mutated, user A\\'s change to the list object is also reflected in user B\\'s app.\\n\\nThis is why you must be careful about mutating objects cached with st.cache_resource, especially when multiple users access the app concurrently. If we had used\\xa0st.cache_data\\xa0instead of\\xa0st.cache_resource, the app would have copied the list object for each user, and the above example would have worked as expected – users A and B would have both seen:\\n\\npython\\nl[0] is: 2\\n\\nThis toy example might seem benign. But data corruption can be extremely dangerous! Imagine we had worked with the financial records of a large bank here. You surely don\\'t want to wake up with less money on your account just because someone used the wrong caching decorator 😉\\n\\nMigrating from st.cache\\n\\nWe introduced the caching commands described above in Streamlit 1.18.0. Before that, we had one catch-all command st.cache. Using it was often confusing, resulted in weird exceptions, and was slow. That\\'s why we replaced st.cache with the new commands in 1.18.0 (read more in this blog post). The new commands provide a more intuitive and efficient way to cache your data and resources and are intended to replace st.cache in all new development.\\n\\nIf your app is still using st.cache, don\\'t despair! Here are a few notes on migrating:\\n\\nst.cache is deprecated. • New versions of Streamlit will show a deprecation warning if your app uses it.\\n\\nWe will not remove st.cache soon, so you don\\'t need to worry about your 2-year-old app breaking. But we encourage you to try the new commands going forward – they will be way less annoying!\\n\\nSwitching code to the new commands should be easy in most cases. To decide whether to use st.cache_data or st.cache_resource, read Deciding which caching decorator to use. Streamlit will also recognize common use cases and show hints right in the deprecation warnings.\\n\\nMost parameters from st.cache are also present in the new commands, with a few exceptions:\\n\\nallow_output_mutation does not exist anymore. You can safely delete it. Just make sure you use the right caching command for your use case.\\n\\nsuppress_st_warning does not exist anymore. You can safely delete it. Cached functions can now contain Streamlit commands and will replay them. If you want to use widgets inside cached functions, set experimental_allow_widgets=True. See here.\\n\\nhash_funcs does not exist anymore. You can exclude parameters from caching (and being hashed) by prepending them with an underscore: _excluded_param. See here.\\n\\nIf you have any questions or issues during the migration process, please contact us on the forum, and we will be happy to assist you. 🎈', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='title: st.write\\nslug: /library/api-reference/write-magic/st.write\\ndescription: st.write writes arguments to the app.\\n\\nFeatured video\\n\\nLearn what the st.write and magic commands are and how to use them.', metadata={'source': 'docs/content/library/api/write-magic/write.md'}),\n",
       " Document(page_content='title: st.multiselect\\nslug: /library/api-reference/widgets/st.multiselect\\ndescription: st.multiselect displays a multiselect widget. The multiselect widget starts as empty.', metadata={'source': 'docs/content/library/api/widgets/multiselect.md'}),\n",
       " Document(page_content=\"title: Magic\\nslug: /library/api-reference/write-magic/magic\\n\\nMagic\\n\\nMagic commands are a feature in Streamlit that allows you to write almost anything (markdown, data,\\ncharts) without having to type an explicit command at all. Just put the thing you want to show on\\nits own line of code, and it will appear in your app. Here's an example:\\n\\n```python\\n\\nDraw a title and some text to the app:\\n\\n'''\\n\\nThis is the document title\\n\\nThis is some markdown.\\n'''\\n\\nimport pandas as pd\\ndf = pd.DataFrame({'col1': [1,2,3]})\\ndf  # 👈 Draw the dataframe\\n\\nx = 10\\n'x', x  # 👈 Draw the string 'x' and then the value of x\\n\\nAlso works with most supported chart types\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\narr = np.random.normal(1, 1, size=100)\\nfig, ax = plt.subplots()\\nax.hist(arr, bins=20)\\n\\nfig  # 👈 Draw a Matplotlib chart\\n```\\n\\nHow Magic works\\n\\nAny time Streamlit sees either a variable or literal\\nvalue on its own line, it automatically writes that to your app using\\nst.write (which you'll learn about later).\\n\\nAlso, magic is smart enough to ignore docstrings. That is, it ignores the\\nstrings at the top of files and functions.\\n\\nIf you prefer to call Streamlit commands more explicitly, you can always turn\\nmagic off in your ~/.streamlit/config.toml with the following setting:\\n\\ntoml\\n[runner]\\nmagicEnabled = false\\n\\nRight now, Magic only works in the main Python app file, not in imported files. See GitHub issue #288 for a discussion of the issues.\\n\\nFeatured video\\n\\nLearn what the st.write and magic commands are and how to use them.\", metadata={'source': 'docs/content/library/api/write-magic/magic.md'}),\n",
       " Document(page_content='title: st.write and magic commands\\nslug: /library/api-reference/write-magic\\n\\nst.write and magic commands\\n\\nStreamlit has two easy ways to display information into your app, which should typically be the\\nfirst thing you try: st.write and magic.\\n\\nst.write\\n\\nWrite arguments to the app.\\n\\npython\\nst.write(\"Hello **world**!\")\\nst.write(my_data_frame)\\nst.write(my_mpl_figure)\\n\\nMagic\\n\\nAny time Streamlit sees either a variable or literal value on its own line, it automatically writes that to your app using st.write\\n\\npython\\n\"Hello **world**!\"\\nmy_data_frame\\nmy_mpl_figure', metadata={'source': 'docs/content/library/api/write-magic/write-magic.md'}),\n",
       " Document(page_content='title: st.radio\\nslug: /library/api-reference/widgets/st.radio\\ndescription: st.radio displays a radio button widget.\\n\\nWidgets can customize how to hide their labels with the label_visibility parameter. If \"hidden\", the label doesn’t show but there is still empty space for it above the widget (equivalent to label=\"\"). If \"collapsed\", both the label and the space are removed. Default is \"visible\". Radio buttons can also be disabled with the disabled parameter, and oriented horizontally with the horizontal parameter:\\n\\n```python\\nimport streamlit as st\\n\\nStore the initial value of widgets in session state\\n\\nif \"visibility\" not in st.session_state:\\n    st.session_state.visibility = \"visible\"\\n    st.session_state.disabled = False\\n    st.session_state.horizontal = False\\n\\ncol1, col2 = st.columns(2)\\n\\nwith col1:\\n    st.checkbox(\"Disable radio widget\", key=\"disabled\")\\n    st.checkbox(\"Orient radio options horizontally\", key=\"horizontal\")\\n\\nwith col2:\\n    st.radio(\\n        \"Set label visibility 👇\",\\n        [\"visible\", \"hidden\", \"collapsed\"],\\n        key=\"visibility\",\\n        label_visibility=st.session_state.visibility,\\n        disabled=st.session_state.disabled,\\n        horizontal=st.session_state.horizontal,\\n    )\\n```\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit\\'s core functions, the radio button! 🔘\\n\\nIn the video below, we\\'ll take it a step further and learn how to combine a button, checkbox and radio button!', metadata={'source': 'docs/content/library/api/widgets/radio.md'}),\n",
       " Document(page_content='title: st.date_input\\nslug: /library/api-reference/widgets/st.date_input\\ndescription: st.date_input displays a date input widget.', metadata={'source': 'docs/content/library/api/widgets/date_input.md'}),\n",
       " Document(page_content='title: st.text_input\\nslug: /library/api-reference/widgets/st.text_input\\ndescription: st.text_input displays a single-line text input widget.\\n\\nText input widgets can customize how to hide their labels with the label_visibility parameter. If \"hidden\", the label doesn’t show but there is still empty space for it above the widget (equivalent to label=\"\"). If \"collapsed\", both the label and the space are removed. Default is \"visible\". Text input widgets can also be disabled with the disabled parameter, and can display an optional placeholder text when the text input is empty using the placeholder parameter:\\n\\n```python\\nimport streamlit as st\\n\\nStore the initial value of widgets in session state\\n\\nif \"visibility\" not in st.session_state:\\n    st.session_state.visibility = \"visible\"\\n    st.session_state.disabled = False\\n\\ncol1, col2 = st.columns(2)\\n\\nwith col1:\\n    st.checkbox(\"Disable text input widget\", key=\"disabled\")\\n    st.radio(\\n        \"Set text input label visibility 👉\",\\n        key=\"visibility\",\\n        options=[\"visible\", \"hidden\", \"collapsed\"],\\n    )\\n    st.text_input(\\n        \"Placeholder for the other text input widget\",\\n        \"This is a placeholder\",\\n        key=\"placeholder\",\\n    )\\n\\nwith col2:\\n    text_input = st.text_input(\\n        \"Enter some text 👇\",\\n        label_visibility=st.session_state.visibility,\\n        disabled=st.session_state.disabled,\\n        placeholder=st.session_state.placeholder,\\n    )\\n\\n```', metadata={'source': 'docs/content/library/api/widgets/text_input.md'}),\n",
       " Document(page_content='title: st.color_picker\\nslug: /library/api-reference/widgets/st.color_picker\\ndescription: st.color_picker displays a color picker widget.', metadata={'source': 'docs/content/library/api/widgets/color_picker.md'}),\n",
       " Document(page_content='title: st.file_uploader\\nslug: /library/api-reference/widgets/st.file_uploader\\ndescription: st.file_uploader displays a file uploader widget.', metadata={'source': 'docs/content/library/api/widgets/file_uploader.md'}),\n",
       " Document(page_content=\"title: st.select_slider\\nslug: /library/api-reference/widgets/st.select_slider\\ndescription: st.select_slider displays a slider widget to select items from a list.\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit's core functions, the select slider! 🎈\\n\\nIn the video below, we'll take it a step further and make a double-ended slider.\", metadata={'source': 'docs/content/library/api/widgets/select_slider.md'}),\n",
       " Document(page_content=\"title: st.button\\nslug: /library/api-reference/widgets/st.button\\ndescription: st.button displays a button widget.\\nkeywords: button\\n\\nAdvanced functionality\\n\\nAlthough a button is the simplest of input widgets, it's very common for buttons to be deeply tied to the use of st.session_state. Check out our advanced guide on Button behavior and examples.\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit's core functions, the button!\\n\\nIn the video below, we'll take it a step further and learn how to combine a button, checkbox and radio button!\", metadata={'source': 'docs/content/library/api/widgets/button.md'}),\n",
       " Document(page_content='title: st.camera_input\\nslug: /library/api-reference/widgets/st.camera_input\\ndescription: st.camera_input displays a widget to upload images from a camera\\n\\nTo read the image file buffer as bytes, you can use getvalue() on the UploadedFile object.\\n\\n```python\\nimport streamlit as st\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as bytes:\\n    bytes_data = img_file_buffer.getvalue()\\n    # Check the type of bytes_data:\\n    # Should output: \\n    st.write(type(bytes_data))\\n```\\n\\nst.camera_input returns an object of the UploadedFile class, which a subclass of BytesIO. Therefore it is a \"file-like\" object. This means you can pass it anywhere where a file is expected, similar to st.file_uploader.\\n\\nImage processing examples\\n\\nPillow,\\n\\nNumPy,\\n\\nOpenCV,\\n\\nTensorFlow,\\n\\ntorchvision, and\\n\\nPyTorch.\\n\\nWhile we provide examples for the most popular use-cases and libraries, you are welcome to adapt these examples to your own needs and favorite libraries.\\n\\nPillow (PIL) and NumPy\\n\\nEnsure you have installed Pillow and NumPy.\\n\\nTo read the image file buffer as a PIL Image and convert it to a NumPy array:\\n\\n```python\\nimport streamlit as st\\nfrom PIL import Image\\nimport numpy as np\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a PIL Image:\\n    img = Image.open(img_file_buffer)\\n\\n```\\n\\nOpenCV (cv2)\\n\\nEnsure you have installed OpenCV and NumPy.\\n\\nTo read the image file buffer with OpenCV:\\n\\n```python\\nimport streamlit as st\\nimport cv2\\nimport numpy as np\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer with OpenCV:\\n    bytes_data = img_file_buffer.getvalue()\\n    cv2_img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)\\n\\n```\\n\\nTensorFlow\\n\\nEnsure you have installed TensorFlow.\\n\\nTo read the image file buffer as a 3 dimensional uint8 tensor with TensorFlow:\\n\\n```python\\nimport streamlit as st\\nimport tensorflow as tf\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a 3D uint8 tensor with TensorFlow:\\n    bytes_data = img_file_buffer.getvalue()\\n    img_tensor = tf.io.decode_image(bytes_data, channels=3)\\n\\n```\\n\\nTorchvision\\n\\nEnsure you have installed Torchvision (it is not bundled with PyTorch) and PyTorch.\\n\\nTo read the image file buffer as a 3 dimensional uint8 tensor with torchvision.io:\\n\\n```python\\nimport streamlit as st\\nimport torch\\nimport torchvision\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a 3D uint8 tensor with torchvision.io:\\n    bytes_data = img_file_buffer.getvalue()\\n    torch_img = torchvision.io.decode_image(\\n        torch.frombuffer(bytes_data, dtype=torch.uint8)\\n    )\\n\\n```\\n\\nPyTorch\\n\\nEnsure you have installed PyTorch and NumPy.\\n\\nTo read the image file buffer as a 3 dimensional uint8 tensor with PyTorch:\\n\\n```python\\nimport streamlit as st\\nimport torch\\nimport numpy as np\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a 3D uint8 tensor with PyTorch:\\n    bytes_data = img_file_buffer.getvalue()\\n    torch_img = torch.ops.image.decode_image(\\n        torch.from_numpy(np.frombuffer(bytes_data, np.uint8)), 3\\n    )\\n\\n```', metadata={'source': 'docs/content/library/api/widgets/camera_input.md'}),\n",
       " Document(page_content='title: st.download_button\\nslug: /library/api-reference/widgets/st.download_button\\ndescription: st.download_button displays a download button widget.', metadata={'source': 'docs/content/library/api/widgets/download_button.md'}),\n",
       " Document(page_content='title: st.number_input\\nslug: /library/api-reference/widgets/st.number_input\\ndescription: st.number_input displays a numeric input widget.', metadata={'source': 'docs/content/library/api/widgets/number_input.md'}),\n",
       " Document(page_content=\"title: st.slider\\nslug: /library/api-reference/widgets/st.slider\\ndescription: st.slider displays a slider widget.\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit's core functions, the slider!\\n\\nIn the video below, we'll take it a step further and make a double-ended slider.\", metadata={'source': 'docs/content/library/api/widgets/slider.md'}),\n",
       " Document(page_content='title: st.text_area\\nslug: /library/api-reference/widgets/st.text_area\\ndescription: st.text_area displays a multi-line text input widget.', metadata={'source': 'docs/content/library/api/widgets/text_area.md'}),\n",
       " Document(page_content='title: st.time_input\\nslug: /library/api-reference/widgets/st.time_input\\ndescription: st.time_input displays a time input widget.', metadata={'source': 'docs/content/library/api/widgets/time_input.md'}),\n",
       " Document(page_content=\"title: st.checkbox\\nslug: /library/api-reference/widgets/st.checkbox\\ndescription: st.checkbox displays a checkbox widget.\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit's core functions, the checkbox! ☑\\n\\nIn the video below, we'll take it a step further and learn how to combine a button, checkbox and radio button!\", metadata={'source': 'docs/content/library/api/widgets/checkbox.md'}),\n",
       " Document(page_content='title: st.selectbox\\nslug: /library/api-reference/widgets/st.selectbox\\ndescription: st.selectbox displays a select widget.\\n\\nSelect widgets can customize how to hide their labels with the label_visibility parameter. If \"hidden\", the label doesn’t show but there is still empty space for it above the widget (equivalent to label=\"\"). If \"collapsed\", both the label and the space are removed. Default is \"visible\". Select widgets can also be disabled with the disabled parameter:\\n\\n```python\\nimport streamlit as st\\n\\nStore the initial value of widgets in session state\\n\\nif \"visibility\" not in st.session_state:\\n    st.session_state.visibility = \"visible\"\\n    st.session_state.disabled = False\\n\\ncol1, col2 = st.columns(2)\\n\\nwith col1:\\n    st.checkbox(\"Disable selectbox widget\", key=\"disabled\")\\n    st.radio(\\n        \"Set selectbox label visibility 👉\",\\n        key=\"visibility\",\\n        options=[\"visible\", \"hidden\", \"collapsed\"],\\n    )\\n\\nwith col2:\\n    option = st.selectbox(\\n        \"How would you like to be contacted?\",\\n        (\"Email\", \"Home phone\", \"Mobile phone\"),\\n        label_visibility=st.session_state.visibility,\\n        disabled=st.session_state.disabled,\\n    )\\n```', metadata={'source': 'docs/content/library/api/widgets/selectbox.md'}),\n",
       " Document(page_content=\"title: Connections and databases\\nslug: /library/api-reference/connections\\n\\nConnections and databases\\n\\nSetup your connection\\n\\nCreate a connection\\n\\nConnect to a data source or API\\n\\npython\\nconn = st.experimental_connection('pets_db', type='sql')\\npet_owners = conn.query('select * from pet_owners')\\nst.dataframe(pet_owners)\\n\\nBuilt-in connections\\n\\nSQLConnection\\n\\nA connection to a SQL database using SQLAlchemy.\\n\\npython\\nconn = st.experimental_connection('sql')\\n\\nSnowparkConnection\\n\\nA connection to Snowflake Snowpark.\\n\\npython\\nconn = st.experimental_connection('snowpark')\\n\\nThird-party connections\\n\\nConnection base class\\n\\nBuild your own connection with ExperimentalBaseConnection.\\n\\npython\\nclass MyConnection(ExperimentalBaseConnection[myconn.MyConnection]):\\n    def _connect(self, **kwargs) -> MyConnection:\\n        return myconn.connect(**self._secrets, **kwargs)\\n    def query(self, query):\\n        return self._instance.query(query)\", metadata={'source': 'docs/content/library/api/connections/connections.md'}),\n",
       " Document(page_content='title: st.connections.SnowparkConnection\\nslug: /library/api-reference/connections/st.connections.snowparkconnection\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nThis page only contains on the st.connections.SnowparkConnection API. For a deeper dive into creating and managing data connections within Streamlit apps, read Connecting to data.', metadata={'source': 'docs/content/library/api/connections/connections-snowpark.md'}),\n",
       " Document(page_content='title: Session State\\nslug: /library/api-reference/session-state\\ndescription: st.session_state is a way to share variables between reruns, for each user session.\\n\\nSession State\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks. Session state also persists across apps inside a multipage app.\\n\\nCheck out this Session State basics tutorial video by Streamlit Developer Advocate Dr. Marisa Smith to get started:\\n\\nInitialize values in Session State\\n\\nThe Session State API follows a field-based API, which is very similar to Python dictionaries:\\n\\n```python\\n\\nInitialization\\n\\nif \\'key\\' not in st.session_state:\\n    st.session_state[\\'key\\'] = \\'value\\'\\n\\nSession State also supports attribute based syntax\\n\\nif \\'key\\' not in st.session_state:\\n    st.session_state.key = \\'value\\'\\n```\\n\\nReads and updates\\n\\nRead the value of an item in Session State and display it by passing to st.write :\\n\\n```python\\n\\nRead\\n\\nst.write(st.session_state.key)\\n\\nOutputs: value\\n\\n```\\n\\nUpdate an item in Session State by assigning it a value:\\n\\npython\\nst.session_state.key = \\'value2\\'     # Attribute API\\nst.session_state[\\'key\\'] = \\'value2\\'  # Dictionary like API\\n\\nCurious about what is in Session State? Use st.write or magic:\\n\\n```python\\nst.write(st.session_state)\\n\\nWith magic:\\n\\nst.session_state\\n```\\n\\nStreamlit throws a handy exception if an uninitialized variable is accessed:\\n\\n```python\\nst.write(st.session_state[\\'value\\'])\\n\\nThrows an exception!\\n\\n```\\n\\nDelete items\\n\\nDelete items in Session State using the syntax to delete items in any Python dictionary:\\n\\n```python\\n\\nDelete a single key-value pair\\n\\ndel st.session_state[key]\\n\\nDelete all the items in Session state\\n\\nfor key in st.session_state.keys():\\n    del st.session_state[key]\\n```\\n\\nSession State can also be cleared by going to Settings → Clear Cache, followed by Rerunning the app.\\n\\nSession State and Widget State association\\n\\nEvery widget with a key is automatically added to Session State:\\n\\n```python\\nst.text_input(\"Your name\", key=\"name\")\\n\\nThis exists now:\\n\\nst.session_state.name\\n```\\n\\nUse Callbacks to update Session State\\n\\nA callback is a python function which gets called when an input widget changes.\\n\\nOrder of execution: When updating Session state in response to events, a callback function gets executed first, and then the app is executed from top to bottom.\\n\\nCallbacks can be used with widgets using the parameters on_change (or on_click), args, and kwargs:\\n\\nParameters\\n\\non_change or on_click - The function name to be used as a callback\\n\\nargs (tuple) - List of arguments to be passed to the callback function\\n\\nkwargs (dict) - Named arguments to be passed to the callback function\\n\\nWidgets which support the on_change event:\\n\\nst.checkbox\\n\\nst.color_picker\\n\\nst.date_input\\n\\nst.multiselect\\n\\nst.number_input\\n\\nst.radio\\n\\nst.select_slider\\n\\nst.selectbox\\n\\nst.slider\\n\\nst.text_area\\n\\nst.text_input\\n\\nst.time_input\\n\\nst.file_uploader\\n\\nWidgets which support the on_click event:\\n\\nst.button\\n\\nst.download_button\\n\\nst.form_submit_button\\n\\nTo add a callback, define a callback function above the widget declaration and pass it to the widget via the on_change (or on_click ) parameter.\\n\\nForms and Callbacks\\n\\nWidgets inside a form can have their values be accessed and set via the Session State API. st.form_submit_button can have a callback associated with it. The callback gets executed upon clicking on the submit button. For example:\\n\\n```python\\ndef form_callback():\\n    st.write(st.session_state.my_slider)\\n    st.write(st.session_state.my_checkbox)\\n\\nwith st.form(key=\\'my_form\\'):\\n    slider_input = st.slider(\\'My slider\\', 0, 10, 5, key=\\'my_slider\\')\\n    checkbox_input = st.checkbox(\\'Yes or No\\', key=\\'my_checkbox\\')\\n    submit_button = st.form_submit_button(label=\\'Submit\\', on_click=form_callback)\\n```\\n\\nCaveats and limitations\\n\\nOnly the st.form_submit_button has a callback in forms. Other widgets inside a form are not allowed to have callbacks.\\n\\non_change and on_click events are only supported on input type widgets.\\n\\nModifying the value of a widget via the Session state API, after instantiating it, is not allowed and will raise a StreamlitAPIException. For example:\\n\\n```python\\n  slider = st.slider(\\n      label=\\'My Slider\\', min_value=1,\\n      max_value=10, value=5, key=\\'my_slider\\')\\n\\nst.session_state.my_slider = 7\\n\\n# Throws an exception!\\n  ```\\n\\nSetting the widget state via the Session State API and using the value parameter in the widget declaration is not recommended, and will throw a warning on the first run. For example:\\n\\n```python\\n  st.session_state.my_slider = 7\\n\\nslider = st.slider(\\n      label=\\'Choose a Value\\', min_value=1,\\n      max_value=10, value=5, key=\\'my_slider\\')\\n  ```\\n\\nSetting the state of button-like widgets: st.button, st.download_button, and st.file_uploader via the Session State API is not allowed. Such type of widgets are by default False and have ephemeral True states which are only valid for a single run. For example:\\n\\n```python\\n  if \\'my_button\\' not in st.session_state:\\n      st.session_state.my_button = True\\n\\nst.button(\\'My button\\', key=\\'my_button\\')\\n\\n# Throws an exception!\\n  ```', metadata={'source': 'docs/content/library/api/state/state.md'}),\n",
       " Document(page_content=\"title: st.experimental_connection\\nslug: /library/api-reference/connections/st.experimental_connection\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nThis page only contains on the st.experimental_connection API. For a deeper dive into creating and managing data connections within Streamlit apps, read Connecting to data.\\n\\nFor a comprehensive overview of this feature, check out this video tutorial by Joshua Carroll, Streamlit's Product Manager for Developer Experience. You'll learn about the feature's utility in creating and managing data connections within your apps by using real-world examples.\", metadata={'source': 'docs/content/library/api/connections/experimental-connection.md'}),\n",
       " Document(page_content='title: st.connections.SQLConnection\\nslug: /library/api-reference/connections/st.connections.sqlconnection\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nThis page only contains on the st.connections.SQLConnection API. For a deeper dive into creating and managing data connections within Streamlit apps, read Connecting to data.\\n\\nBasic usage:\\n\\n```python\\nimport streamlit as st\\n\\nconn = st.experimental_connection(\"sql\")\\ndf = conn.query(\"select * from pet_owners\")\\nst.dataframe(df)\\n```\\n\\nIn case you want to pass a connection URL (or other parameters) directly, it also works:\\n\\npython\\nconn = st.experimental_connection(\\n    \"local_db\",\\n    type=\"sql\",\\n    url=\"mysql://user:pass@localhost:3306/mydb\"\\n)\\n\\nOr specify parameters in secrets:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.mydb]\\ndialect = \"mysql\"\\nusername = \"myuser\"\\npassword = \"password\"\\nhost = \"localhost\"\\ndatabase = \"mydb\"\\n```\\n\\n```python\\n\\nstreamlit_app.py\\n\\nconn = st.experimental_connection(\"mydb\", type=\"sql\", autocommit=True)\\n```\\n\\nAs described above, some cloud databases use extra **kwargs to specify credentials. These can be passed via secrets using the create_engine_kwargs section:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowflake]\\nurl = \"snowflake://@/\"\\n\\n[connections.snowflake.create_engine_kwargs.connect_args]\\nauthenticator = \"externalbrowser\"\\nrole = \"...\"\\n\\n...\\n\\n```', metadata={'source': 'docs/content/library/api/connections/connections-sql.md'}),\n",
       " Document(page_content='title: Input widgets\\nslug: /library/api-reference/widgets\\n\\nInput widgets\\n\\nWith widgets, Streamlit allows you to bake interactivity directly into your apps with buttons, sliders, text inputs, and more.\\n\\nButton\\n\\nDisplay a button widget.\\n\\npython\\nclicked = st.button(\"Click me\")\\n\\nDownload button\\n\\nDisplay a download button widget.\\n\\npython\\nst.download_button(\"Download file\", file)\\n\\nCheckbox\\n\\nDisplay a checkbox widget.\\n\\npython\\nselected = st.checkbox(\"I agree\")\\n\\nRadio\\n\\nDisplay a radio button widget.\\n\\npython\\nchoice = st.radio(\"Pick one\", [\"cats\", \"dogs\"])\\n\\nSelectbox\\n\\nDisplay a select widget.\\n\\npython\\nchoice = st.selectbox(\"Pick one\", [\"cats\", \"dogs\"])\\n\\nMultiselect\\n\\nDisplay a multiselect widget. The multiselect widget starts as empty.\\n\\npython\\nchoices = st.multiselect(\"Buy\", [\"milk\", \"apples\", \"potatoes\"])\\n\\nSlider\\n\\nDisplay a slider widget.\\n\\npython\\nnumber = st.slider(\"Pick a number\", 0, 100)\\n\\nSelect slider\\n\\nDisplay a slider widget to select items from a list.\\n\\npython\\nsize = st.select_slider(\"Pick a size\", [\"S\", \"M\", \"L\"])\\n\\nText input\\n\\nDisplay a single-line text input widget.\\n\\npython\\nname = st.text_input(\"First name\")\\n\\nNumber input\\n\\nDisplay a numeric input widget.\\n\\npython\\nchoice = st.number_input(\"Pick a number\", 0, 10)\\n\\nText area\\n\\nDisplay a multi-line text input widget.\\n\\npython\\ntext = st.text_area(\"Text to translate\")\\n\\nDate input\\n\\nDisplay a date input widget.\\n\\npython\\ndate = st.date_input(\"Your birthday\")\\n\\nTime input\\n\\nDisplay a time input widget.\\n\\npython\\ntime = st.time_input(\"Meeting time\")\\n\\nFile uploader\\n\\nDisplay a file uploader widget.\\n\\npython\\ndata = st.file_uploader(\"Upload a CSV\")\\n\\nCamera input\\n\\nDisplay a widget that allows users to upload images directly from a camera.\\n\\npython\\nimage = st.camera_input(\"Take a picture\")\\n\\nColor picker\\n\\nDisplay a color picker widget.\\n\\npython\\ncolor = st.color_picker(\"Pick a color\")\\n\\nStreamlit Elements\\n\\nCreate a draggable and resizable dashboard in Streamlit. Created by @okls.\\n\\n```python\\nfrom streamlit_elements import elements, mui, html\\n\\nwith elements(\"new_element\"):\\n  mui.Typography(\"Hello world\")\\n```\\n\\nTags\\n\\nAdd tags to your Streamlit apps. Created by @gagan3012.\\n\\n```python\\nfrom streamlit_tags import st_tags\\n\\nst_tags(label=\\'# Enter Keywords:\\', text=\\'Press enter to add more\\', value=[\\'Zero\\', \\'One\\', \\'Two\\'],\\nsuggestions=[\\'five\\', \\'six\\', \\'seven\\', \\'eight\\', \\'nine\\', \\'three\\', \\'eleven\\', \\'ten\\', \\'four\\'], maxtags = 4, key=\\'1\\')\\n```\\n\\nStqdm\\n\\nThe simplest way to handle a progress bar in streamlit app. Created by @Wirg.\\n\\n```python\\nfrom stqdm import stqdm\\n\\nfor _ in stqdm(range(50)):\\n    sleep(0.5)\\n```\\n\\nTimeline\\n\\nDisplay a Timeline in Streamlit apps using TimelineJS. Created by @innerdoc.\\n\\n```python\\nfrom streamlit_timeline import timeline\\n\\nwith open(\\'example.json\\', \"r\") as f:\\n  timeline(f.read(), height=800)\\n```\\n\\nCamera input live\\n\\nAlternative for st.camera_input which returns the webcam images live. Created by @blackary.\\n\\n```python\\nfrom camera_input_live import camera_input_live\\n\\nimage = camera_input_live()\\nst.image(value)\\n```\\n\\nStreamlit Ace\\n\\nAce editor component for Streamlit. Created by @okld.\\n\\n```python\\nfrom streamlit_ace import st_ace\\n\\ncontent = st_ace()\\ncontent\\n```\\n\\nStreamlit Chat\\n\\nStreamlit Component for a Chatbot UI. Created by @AI-Yash.\\n\\n```python\\nfrom streamlit_chat import message\\n\\nmessage(\"My message\")\\nmessage(\"Hello bot!\", is_user=True)  # align\\'s the message to the right\\n```\\n\\nStreamlit Option Menu\\n\\nSelect a single item from a list of options in a menu. Created by @victoryhb.\\n\\n```python\\nfrom streamlit_option_menu import option_menu\\n\\noption_menu(\"Main Menu\", [\"Home\", \\'Settings\\'],\\n  icons=[\\'house\\', \\'gear\\'], menu_icon=\"cast\", default_index=1)\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.stoggle import stoggle\\n\\nstoggle(\\n    \"Click me!\", \"\"\"🥷 Surprise! Here\\'s some additional content\"\"\",)\\n```', metadata={'source': 'docs/content/library/api/widgets/widgets.md'}),\n",
       " Document(page_content='title: st.connections.ExperimentalBaseConnection\\nslug: /library/api-reference/connections/st.connections.experimentalbaseconnection\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nThis page only contains on the st.connections.ExperimentalBaseConnection API. For a deeper dive into creating and managing data connections within Streamlit apps, read Connecting to data.', metadata={'source': 'docs/content/library/api/connections/connections-experimentalbaseconnection.md'}),\n",
       " Document(page_content=\"title: st.experimental_get_query_params\\nslug: /library/api-reference/utilities/st.experimental_get_query_params\\ndescription: st.experimental_get_query_params returns query parameters currently showing in the browser's URL bar.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\", metadata={'source': 'docs/content/library/api/utilities/experimental_get_query_params.md'}),\n",
       " Document(page_content=\"title: st.experimental_set_query_params\\nslug: /library/api-reference/utilities/st.experimental_set_query_params\\ndescription: st.experimental_set_query_params sets query parameters shown in the browser's URL bar.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\", metadata={'source': 'docs/content/library/api/utilities/experimental_set_query_params.md'}),\n",
       " Document(page_content='title: st.set_page_config\\nslug: /library/api-reference/utilities/st.set_page_config\\ndescription: st.set_page_config configures the default settings of the page.', metadata={'source': 'docs/content/library/api/utilities/set_page_config.md'}),\n",
       " Document(page_content=\"title: st.help\\nslug: /library/api-reference/utilities/st.help\\ndescription: st.help displays object's doc string, nicely formatted.\", metadata={'source': 'docs/content/library/api/utilities/help.md'}),\n",
       " Document(page_content='title: st.echo\\nslug: /library/api-reference/utilities/st.echo\\ndescription: st.echo displays some code on the app, then execute it. Useful for tutorials.\\n\\nDisplay code\\n\\nSometimes you want your Streamlit app to contain both your usual\\nStreamlit graphic elements and the code that generated those elements.\\nThat\\'s where st.echo() comes in.\\n\\nOk so let\\'s say you have the following file, and you want to make its\\napp a little bit more self-explanatory by making that middle section\\nvisible in the Streamlit app:\\n\\n```python\\nimport streamlit as st\\n\\ndef get_user_name():\\n    return \\'John\\'\\n\\n------------------------------------------------\\n\\nWant people to see this part of the code...\\n\\ndef get_punctuation():\\n    return \\'!!!\\'\\n\\ngreeting = \"Hi there, \"\\nuser_name = get_user_name()\\npunctuation = get_punctuation()\\n\\nst.write(greeting, user_name, punctuation)\\n\\n...up to here\\n\\n------------------------------------------------\\n\\nfoo = \\'bar\\'\\nst.write(\\'Done!\\')\\n```\\n\\nThe file above creates a Streamlit app containing the words \"Hi there,\\nJohn\", and then \"Done!\".\\n\\nNow let\\'s use st.echo() to make that middle section of the code visible\\nin the app:\\n\\n```python\\nimport streamlit as st\\n\\ndef get_user_name():\\n    return \\'John\\'\\n\\nwith st.echo():\\n    # Everything inside this block will be both printed to the screen\\n    # and executed.\\n\\nAnd now we\\'re back to not printing to the screen\\n\\nfoo = \\'bar\\'\\nst.write(\\'Done!\\')\\n```\\n\\nIt\\'s that simple!\\n\\nYou can have multiple st.echo() blocks in the same file.\\nUse it as often as you wish!', metadata={'source': 'docs/content/library/api/utilities/echo.md'}),\n",
       " Document(page_content='title: Placeholders, help, and options\\nslug: /library/api-reference/utilities\\n\\nPlaceholders, help, and options\\n\\nThere are a handful of methods that allow you to create placeholders in your\\napp, provide help using doc strings, get and modify configuration options and query parameters.\\n\\nSet page title, favicon, and more\\n\\nConfigures the default settings of the page.\\n\\npython\\nst.set_page_config(\\n  title=\"My app\",\\n  favicon=\":shark:\",\\n)\\n\\nEcho\\n\\nDisplay some code on the app, then execute it. Useful for tutorials.\\n\\npython\\nwith st.echo():\\n  st.write(\\'This code will be printed\\')\\n\\nGet help\\n\\nDisplay object’s doc string, nicely formatted.\\n\\npython\\nst.help(st.write)\\nst.help(pd.DataFrame)\\n\\nGet query parameters\\n\\nReturn the query parameters that are currently showing in the browser\\'s URL bar.\\n\\npython\\nst.experimental_get_query_params()\\n\\nSet query parameters\\n\\nSet the query parameters that are shown in the browser\\'s URL bar.\\n\\npython\\nst.experimental_set_query_params(\\n  show_map=True,\\n  selected=[\"asia\"]\\n)', metadata={'source': 'docs/content/library/api/utilities/utilities.md'}),\n",
       " Document(page_content='title: st.chat_message\\nslug: /library/api-reference/chat/st.chat_message\\ndescription: st.chat_message inserts a chat message container into the app.\\n\\nRead the Build conversational apps tutorial to learn how to use st.chat_message and st.chat_input to build chat-based apps.', metadata={'source': 'docs/content/library/api/chat/chat-message.md'}),\n",
       " Document(page_content='title: st.cache_resource.clear\\nslug: /library/api-reference/performance/st.cache_resource.clear\\ndescription: st.cache_resource.clear clears all cache_resource caches.\\n\\nExample\\n\\nIn the example below, pressing the \"Clear All\" button will clear all cache_resource caches. i.e. Clears cached global resources from all functions decorated with @st.cache_resource.\\n\\n```python\\nimport streamlit as st\\nfrom transformers import BertModel\\n\\n@st.cache_resource\\n def get_database_session(url):\\n     # Create a database session object that points to the URL.\\n     return session\\n\\n@st.cache_resource\\ndef get_model(model_type):\\n    # Create a model of the specified type.\\n    return BertModel.from_pretrained(model_type)\\n\\nif st.button(\"Clear All\"):\\n    # Clears all st.cache_resource caches:\\n    st.cache_resource.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/cache-resource-clear.md'}),\n",
       " Document(page_content='title: st.chat_input\\nslug: /library/api-reference/chat/st.chat_input\\ndescription: st.chat_input displays a chat input widget.\\n\\nRead the Build conversational apps tutorial to learn how to use st.chat_message and st.chat_input to build chat-based apps.', metadata={'source': 'docs/content/library/api/chat/chat-input.md'}),\n",
       " Document(page_content='title: Chat elements\\nslug: /library/api-reference/chat\\n\\nChat elements\\n\\nStreamlit provides a few commands to help you build conversational apps. These chat elements are designed to be used in conjunction with each other, but you can also use them separately.\\n\\nst.chat_message lets you insert a chat message container into the app so you can display messages from the user or the app. Chat containers can contain other Streamlit elements, including charts, tables, text, and more. st.chat_input lets you display a chat input widget so the user can type in a message.\\n\\nChat message\\n\\nInsert a chat message container.\\n\\npython\\nimport numpy as np\\nwith st.chat_message(\"user\"):\\n    st.write(\"Hello 👋\")\\n    st.line_chart(np.random.randn(30, 3))\\n\\nChat input\\n\\nDisplay a chat input widget.\\n\\npython\\nprompt = st.chat_input(\"Say something\")\\nif prompt:\\n    st.write(f\"The user has sent: {prompt}\")', metadata={'source': 'docs/content/library/api/chat/chat.md'}),\n",
       " Document(page_content='title: st.experimental_singleton.clear\\nslug: /library/api-reference/performance/st.experimental_singleton.clear\\ndescription: st.experimental_singleton.clear clears all singleton caches.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nExample\\n\\nIn the example below, pressing the \"Clear All\" button will clear all singleton caches. i.e. Clears cached singleton objects from all functions decorated with @st.experimental_singleton.\\n\\n```python\\nimport streamlit as st\\nfrom transformers import BertModel\\n\\n@st.experimental_singleton\\n def get_database_session(url):\\n     # Create a database session object that points to the URL.\\n     return session\\n\\n@st.experimental_singleton\\ndef get_model(model_type):\\n    # Create a model of the specified type.\\n    return BertModel.from_pretrained(model_type)\\n\\nif st.button(\"Clear All\"):\\n    # Clears all singleton caches:\\n    st.experimental_singleton.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/experimental-singleton-clear.md'}),\n",
       " Document(page_content='title: st.experimental_memo.clear\\nslug: /library/api-reference/performance/st.experimental_memo.clear\\ndescription: st.experimental_memo.clear clears all in-memory and on-disk memo caches.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nExample\\n\\nIn the example below, pressing the \"Clear All\" button will clear memoized values from all functions decorated with @st.experimental_memo.\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_memo\\ndef square(x):\\n    return x**2\\n\\n@st.experimental_memo\\ndef cube(x):\\n    return x**3\\n\\nif st.button(\"Clear All\"):\\n    # Clear values from all memoized functions:\\n    # i.e. clear values from both square and cube\\n    st.experimental_memo.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/experimental-memo-clear.md'}),\n",
       " Document(page_content='title: st.cache_data\\nslug: /library/api-reference/performance/st.cache_data\\ndescription: st.cache_data is used to cache functions that return data (e.g. dataframe transforms, database queries, ML inference).\\n\\nThis page only contains information on the st.cache_data API. For a deeper dive into caching and how to use it, check out Caching.\\n\\nUsing Streamlit commands in cached functions\\n\\nStatic elements\\n\\nSince version 1.16.0, cached functions can contain Streamlit commands! For example, you can do this:\\n\\npython\\n@st.cache_data\\ndef get_api_data():\\n    data = api.get(...)\\n    st.success(\"Fetched data from API!\")  # 👈 Show a success message\\n    return data\\n\\nAs we know, Streamlit only runs this function if it hasn’t been cached before. On this first run, the st.success message will appear in the app. But what happens on subsequent runs? It still shows up! Streamlit realizes that there is an st. command inside the cached function, saves it during the first run, and replays it on subsequent runs. Replaying static elements works for both caching decorators.\\n\\nYou can also use this functionality to cache entire parts of your UI:\\n\\npython\\n@st.cache_data\\ndef show_data():\\n    st.header(\"Data analysis\")\\n    data = api.get(...)\\n    st.success(\"Fetched data from API!\")\\n    st.write(\"Here is a plot of the data:\")\\n    st.line_chart(data)\\n    st.write(\"And here is the raw data:\")\\n    st.dataframe(data)\\n\\nInput widgets\\n\\nYou can also use interactive input widgets like st.slider or st.text_input in cached functions. Widget replay is an experimental feature at the moment. To enable it, you need to set the experimental_allow_widgets parameter:\\n\\npython\\n@st.cache_data(experimental_allow_widgets=True)  # 👈 Set the parameter\\ndef get_data():\\n    num_rows = st.slider(\"Number of rows to get\")  # 👈 Add a slider\\n    data = api.get(..., num_rows)\\n    return data\\n\\nStreamlit treats the slider like an additional input parameter to the cached function. If you change the slider position, Streamlit will see if it has already cached the function for this slider value. If yes, it will return the cached value. If not, it will rerun the function using the new slider value.\\n\\nUsing widgets in cached functions is extremely powerful because it lets you cache entire parts of your app. But it can be dangerous! Since Streamlit treats the widget value as an additional input parameter, it can easily lead to excessive memory usage. Imagine your cached function has five sliders and returns a 100 MB DataFrame. Then we’ll add 100 MB to the cache for every permutation of these five slider values – even if the sliders do not influence the returned data! These additions can make your cache explode very quickly. Please be aware of this limitation if you use widgets in cached functions. We recommend using this feature only for isolated parts of your UI where the widgets directly influence the cached return value.\\n\\nSupport for widgets in cached functions is currently experimental. We may change or remove it anytime without warning. Please use it with care!\\n\\nTwo widgets are currently not supported in cached functions: st.file_uploader and st.camera_input. We may support them in the future. Feel free to open a GitHub issue if you need them!', metadata={'source': 'docs/content/library/api/performance/cache-data.md'}),\n",
       " Document(page_content='title: st.cache_resource\\nslug: /library/api-reference/performance/st.cache_resource\\ndescription: st.cache_resource is used to cache functions that return shared global resources (e.g. database connections, ML models).\\n\\nThis page only contains information on the st.cache_resource API. For a deeper dive into caching and how to use it, check out Caching.\\n\\nUsing Streamlit commands in cached functions\\n\\nStatic elements\\n\\nSince version 1.16.0, cached functions can contain Streamlit commands! For example, you can do this:\\n\\n```python\\nfrom transformers import pipeline\\n\\n@st.cache_resource\\ndef load_model():\\n    model = pipeline(\"sentiment-analysis\")\\n    st.success(\"Loaded NLP model from Hugging Face!\")  # 👈 Show a success message\\n    return model\\n```\\n\\nAs we know, Streamlit only runs this function if it hasn’t been cached before. On this first run, the st.success message will appear in the app. But what happens on subsequent runs? It still shows up! Streamlit realizes that there is an st. command inside the cached function, saves it during the first run, and replays it on subsequent runs. Replaying static elements works for both caching decorators.\\n\\nYou can also use this functionality to cache entire parts of your UI:\\n\\npython\\n@st.cache_resource\\ndef load_model():\\n    st.header(\"Data analysis\")\\n    model = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\\n    st.success(\"Loaded model!\")\\n    st.write(\"Turning on evaluation mode...\")\\n    model.eval()\\n    st.write(\"Here\\'s the model:\")\\n    return model\\n\\nInput widgets\\n\\nYou can also use interactive input widgets like st.slider or st.text_input in cached functions. Widget replay is an experimental feature at the moment. To enable it, you need to set the experimental_allow_widgets parameter:\\n\\npython\\n@st.cache_data(experimental_allow_widgets=True)  # 👈 Set the parameter\\ndef load_model():\\n    pretrained = st.checkbox(\"Use pre-trained model:\")  # 👈 Add a checkbox\\n    model = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT, pretrained=pretrained)\\n    return model\\n\\nStreamlit treats the checkbox like an additional input parameter to the cached function. If you uncheck it, Streamlit will see if it has already cached the function for this checkbox state. If yes, it will return the cached value. If not, it will rerun the function using the new slider value.\\n\\nUsing widgets in cached functions is extremely powerful because it lets you cache entire parts of your app. But it can be dangerous! Since Streamlit treats the widget value as an additional input parameter, it can easily lead to excessive memory usage. Imagine your cached function has five sliders and returns a 100 MB DataFrame. Then we’ll add 100 MB to the cache for every permutation of these five slider values – even if the sliders do not influence the returned data! These additions can make your cache explode very quickly. Please be aware of this limitation if you use widgets in cached functions. We recommend using this feature only for isolated parts of your UI where the widgets directly influence the cached return value.\\n\\nSupport for widgets in cached functions is currently experimental. We may change or remove it anytime without warning. Please use it with care!\\n\\nTwo widgets are currently not supported in cached functions: st.file_uploader and st.camera_input. We may support them in the future. Feel free to open a GitHub issue if you need them!', metadata={'source': 'docs/content/library/api/performance/cache-resource.md'}),\n",
       " Document(page_content='title: st.cache_data.clear\\nslug: /library/api-reference/performance/st.cache_data.clear\\ndescription: st.cache_data.clear clears all in-memory and on-disk data caches.\\n\\nExample\\n\\nIn the example below, pressing the \"Clear All\" button will clear memoized values from all functions decorated with @st.cache_data.\\n\\n```python\\nimport streamlit as st\\n\\n@st.cache_data\\ndef square(x):\\n    return x**2\\n\\n@st.cache_data\\ndef cube(x):\\n    return x**3\\n\\nif st.button(\"Clear All\"):\\n    # Clear values from all all in-memory and on-disk data caches:\\n    # i.e. clear values from both square and cube\\n    st.cache_data.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/cache-data-clear.md'}),\n",
       " Document(page_content='title: st.cache\\nslug: /library/api-reference/performance/st.cache\\ndescription: st.cache is used to memoize function executions.\\n\\nst.cache\\n\\nWhen you mark a function with Streamlit’s cache annotation, it tells Streamlit\\nthat whenever the function is called it should check three things:\\n\\nThe name of the function\\n\\nThe actual code that makes up the body of the function\\n\\nThe input parameters that you called the function with\\n\\nIf this is the first time Streamlit has seen those three items, with those exact\\nvalues, and in that exact combination, it runs the function and stores the\\nresult in a local cache.\\n\\nThen, next time the function is called, if those three values have not changed\\nStreamlit knows it can skip executing the function altogether. Instead, it just\\nreads the output from the local cache and passes it on to the caller.\\n\\nThe main limitation is that Streamlit’s cache feature doesn’t know about\\nchanges that take place outside the body of the annotated function.\\n\\nFor more information about the Streamlit cache, its configuration parameters,\\nand its limitations, see Caching.', metadata={'source': 'docs/content/library/api/performance/cache.md'}),\n",
       " Document(page_content='title: st.experimental_memo\\nslug: /library/api-reference/performance/st.experimental_memo\\ndescription: st.experimental_memo is used to memoize function executions.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nPersistent memo caches currently don\\'t support TTL. ttl will be ignored if persist is specified:\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_memo(ttl=60, persist=\"disk\")\\ndef load_data():\\n    return 42\\n\\nst.write(load_data())\\n```\\n\\nAnd a warning will be logged to your terminal:\\n\\n```bash\\nstreamlit run app.py\\n\\nYou can now view your Streamlit app in your browser.\\n  Local URL: http://localhost:8501\\n  Network URL: http://192.168.1.1:8501\\n\\n2022-09-22 13:35:41.587 The memoized function \\'load_data\\' has a TTL that will be ignored. Persistent memo caches currently don\\'t support TTL.\\n```\\n\\nReplay static st elements in cache-decorated functions\\n\\nFunctions decorated with @st.experimental_memo can contain static st elements. When a cache-decorated function is executed, we record the element and block messages produced, so the elements will appear in the app even when execution of the function is skipped because the result was cached.\\n\\nIn the example below, the @st.experimental_memo decorator is used to cache the execution of the load_data function, that returns a pandas DataFrame. Notice the cached function also contains a st.area_chart command, which will be replayed when the function is skipped because the result was cached.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport streamlit as st\\n\\n@st.experimental_memo\\ndef load_data(rows):\\n    chart_data = pd.DataFrame(\\n        np.random.randn(rows, 10),\\n        columns=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"],\\n    )\\n    # Contains a static element st.area_chart\\n    st.area_chart(chart_data) # This will be recorded and displayed even when the function is skipped\\n    return chart_data\\n\\ndf = load_data(20)\\nst.dataframe(df)\\n```\\n\\nSupported static st elements in cache-decorated functions include:\\n\\nst.alert\\n\\nst.altair_chart\\n\\nst.area_chart\\n\\nst.audio\\n\\nst.bar_chart\\n\\nst.ballons\\n\\nst.bokeh_chart\\n\\nst.caption\\n\\nst.code\\n\\nst.components.v1.html\\n\\nst.components.v1.iframe\\n\\nst.container\\n\\nst.dataframe\\n\\nst.echo\\n\\nst.empty\\n\\nst.error\\n\\nst.exception\\n\\nst.expander\\n\\nst.experimental_get_query_params\\n\\nst.experimental_set_query_params\\n\\nst.form\\n\\nst.form_submit_button\\n\\nst.graphviz_chart\\n\\nst.help\\n\\nst.image\\n\\nst.info\\n\\nst.json\\n\\nst.latex\\n\\nst.line_chart\\n\\nst.markdown\\n\\nst.metric\\n\\nst.plotly_chart\\n\\nst.progress\\n\\nst.pydeck_chart\\n\\nst.snow\\n\\nst.spinner\\n\\nst.success\\n\\nst.table\\n\\nst.text\\n\\nst.vega_lite_chart\\n\\nst.video\\n\\nst.warning\\n\\nReplay input widgets in cache-decorated functions\\n\\nIn addition to static elements, functions decorated with @st.experimental_memo can also contain input widgets! Replaying input widgets is disabled by default. To enable it, you can set the experimental_allow_widgets parameter for @st.experimental_memo to True. The example below enables widget replaying, and shows the use of a checkbox widget within a cache-decorated function.\\n\\n```python\\nimport streamlit as st\\n\\nEnable widget replay\\n\\n@st.experimental_memo(experimental_allow_widgets=True)\\ndef func():\\n    # Contains an input widget\\n    st.checkbox(\"Works!\")\\n\\nfunc()\\n```\\n\\nIf the cache decorated function contains input widgets, but experimental_allow_widgets is set to False or unset, Streamlit will throw a CachedStFunctionWarning, like the one below:\\n\\n```python\\nimport streamlit as st\\n\\nWidget replay is disabled by default\\n\\n@st.experimental_memo\\ndef func():\\n    # Streamlit will throw a CachedStFunctionWarning\\n    st.checkbox(\"Doesn\\'t work\")\\n\\nfunc()\\n```\\n\\nHow widget replay works\\n\\nLet\\'s demystify how widget replay in cache-decorated functions works and gain a conceptual understanding. Widget values are treated as additional inputs to the function, and are used to determine whether the function should be executed or not. Consider the following example:\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_memo(experimental_allow_widgets=True)\\ndef plus_one(x):\\n    y = x + 1\\n    if st.checkbox(\"Nuke the value 💥\"):\\n        st.write(\"Value was nuked, returning 0\")\\n        y = 0\\n    return y\\n\\nst.write(plus_one(2))\\n```\\n\\nIn order to know which value the cache should return (in case of a cache hit), Streamlit treats the checkbox state (checked / unchecked) as an additional input to the function plus_one (just like x). If the user checks the checkbox (thereby changing its state), we look up the cache for the same value of x (2) and the same checkbox state (checked). If the cache contains a value for this combination of inputs, we return it. Otherwise, we execute the function and store the result in the cache.\\n\\nLet\\'s now understand how enabling and disabling widget replay changes the behavior of the function.\\n\\nWidget replay disabled\\n\\nWidgets in cached functions throw a CachedStFunctionWarning and are ignored.\\n\\nOther static elements in cached functions replay as expected.\\n\\nWidget replay enabled\\n\\nWidgets in cached functions don\\'t lead to a warning, and are replayed as expected.\\n\\nInteracting with a widget in a cached function will cause the function to be executed again, and the cache to be updated.\\n\\nWidgets in cached functions retain their state across reruns.\\n\\nEach unique combination of widget values is treated as a separate input to the function, and is used to determine whether the function should be executed or not. i.e. Each unique combination of widget values has its own cache entry; the cached function runs the first time and the saved value is used afterwards.\\n\\nCalling a cached function multiple times in one script run with the same arguments triggers a DuplicateWidgetID error.\\n\\nIf the arguments to a cached function change, widgets from that function that render again retain their state.\\n\\nChanging the source code of a cached function invalidates the cache.\\n\\nBoth st.experimental_memo and st.experimental_singleton support widget replay.\\n\\nFundamentally, the behavior of a function with (supported) widgets in it doesn\\'t change when it is decorated with @st.experimental_memo or @st.experimental_singleton. The only difference is that the function is only executed when we detect a cache \"miss\".\\n\\nSupported widgets\\n\\nAll input widgets are supported in cache-decorated functions. The following is an exhaustive list of supported widgets:\\n\\nst.button\\n\\nst.camera_input\\n\\nst.checkbox\\n\\nst.color_picker\\n\\nst.date_input\\n\\nst.download_button\\n\\nst.file_uploader\\n\\nst.multiselect\\n\\nst.number_input\\n\\nst.radio\\n\\nst.selectbox\\n\\nst.select_slider\\n\\nst.slider\\n\\nst.text_area\\n\\nst.text_input\\n\\nst.time_input', metadata={'source': 'docs/content/library/api/performance/experimental-memo.md'}),\n",
       " Document(page_content=\"title: Mutate charts\\nslug: /library/api-reference/mutate\\ndescription: st.add_rows appends a dataframe to the bottom of the current one in certain elements, for optimized data updates.\\n\\nMutate charts\\n\\nSometimes you display a chart or dataframe and want to modify live as the app\\nruns (for example, in a loop). Depending on what you're looking for, there are 3 different ways to\\ndo this:\\n\\nUsing st.empty to replace a single element.\\n\\nUsing st.container or\\n   st.columns to replace multiple elements.\\n\\nUsing add_rows to append data to specific types of elements.\\n\\nHere we discuss that last case.\", metadata={'source': 'docs/content/library/api/mutate/mutate.md'}),\n",
       " Document(page_content='title: st.audio\\nslug: /library/api-reference/media/st.audio\\ndescription: st.audio displays an audio player.', metadata={'source': 'docs/content/library/api/media/audio.md'}),\n",
       " Document(page_content='title: Optimize performance\\nslug: /library/api-reference/performance\\n\\nOptimize performance\\n\\nStreamlit provides powerful cache primitives for data and global resources. They allow your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations.\\n\\nCache data\\n\\nFunction decorator to cache functions that return data (e.g. dataframe transforms, database queries, ML inference).\\n\\npython\\n@st.cache_data\\ndef long_function(param1, param2):\\n  # Perform expensive computation here or\\n  # fetch data from the web here\\n  return data\\n\\nCache resource\\n\\nFunction decorator to cache functions that return global resources (e.g. database connections, ML models).\\n\\npython\\n@st.cache_resource\\ndef init_model():\\n  # Return a global resource here\\n  return pipeline(\\n    \"sentiment-analysis\",\\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\\n  )\\n\\nClear cached data\\n\\nClear all in-memory and on-disk data caches.\\n\\n```python\\n@st.cache_data\\ndef long_function(param1, param2):\\n  # Perform expensive computation here or\\n  # fetch data from the web here\\n  return data\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all cache_data functions\\n  st.cache_data.clear()\\n```\\n\\nClear cached resources\\n\\nClear all st.cache_resource caches.\\n\\n```python\\n@st.cache_resource\\ndef init_model():\\n  # Return a global resource here\\n  return pipeline(\\n    \"sentiment-analysis\",\\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\\n  )\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all cache_resource functions\\n  st.cache_data.clear()\\n```\\n\\nAll the below commands were deprecated in version 1.18.0. Use the new commands above instead. Learn more in Caching.\\n\\nDeprecated commands\\n\\nThis command was deprecated in version 1.18.0. Use st.cache_data or st.cache_resource instead.\\n\\nCaching\\n\\nFunction decorator to memoize function executions.\\n\\npython\\n@st.cache(ttl=3600)\\ndef run_long_computation(arg1, arg2):\\n  # Do stuff here\\n  return computation_output\\n\\nThis command was deprecated in version 1.18.0. Use st.cache_data instead.\\n\\nMemo\\n\\nExperimental function decorator to memoize function executions.\\n\\npython\\n@st.experimental_memo\\ndef fetch_and_clean_data(url):\\n  # Fetch data from URL here, and then clean it up.\\n  return data\\n\\nThis command was deprecated in version 1.18.0. Use st.cache_resource instead.\\n\\nSingleton\\n\\nExperimental function decorator to store singleton objects.\\n\\npython\\n@st.experimental_singleton\\ndef get_database_session(url):\\n  # Create a database session object that points to the URL.\\n  return session\\n\\nThis command was deprecated in version 1.18.0. Use st.cache_data.clear instead.\\n\\nClear memo\\n\\nClear all in-memory and on-disk memo caches.\\n\\n```python\\n@st.experimental_memo\\ndef fetch_and_clean_data(url):\\n  # Fetch data from URL here, and then clean it up.\\n  return data\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all memoized functions\\n  st.experimental_memo.clear()\\n```\\n\\nThis command was deprecated in version 1.18.0. Use st.cache_resource.clearinstead.\\n\\nClear singleton\\n\\nClear all singleton caches.\\n\\n```python\\n@st.experimental_singleton\\ndef get_database_session(url):\\n  # Create a database session object that points to the URL.\\n  return session\\n\\nif st.button(\"Clear All\"):\\n  # Clears all singleton caches:\\n  st.experimental_singleton.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/performance.md'}),\n",
       " Document(page_content='title: st.image\\nslug: /library/api-reference/media/st.image\\ndescription: st.image displays an image or list of images.', metadata={'source': 'docs/content/library/api/media/image.md'}),\n",
       " Document(page_content='title: st.video\\nslug: /library/api-reference/media/st.video\\ndescription: st.video displays a video player.', metadata={'source': 'docs/content/library/api/media/video.md'}),\n",
       " Document(page_content='title: st.experimental_singleton\\nslug: /library/api-reference/performance/st.experimental_singleton\\ndescription: st.experimental_singleton is a function decorator used to store singleton objects.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nValidating the cache\\n\\nThe @st.experimental_singleton decorator is used to cache the output of a function, so that it only needs to be executed once. This can improve performance in certain situations, such as when a function takes a long time to execute or makes a network request.\\n\\nHowever, in some cases, the cached output may become invalid over time, such as when a database connection times out. To handle this, the @st.experimental_singleton decorator supports an optional validate parameter, which accepts a validation function that is called each time the cached output is accessed. If the validation function returns False, the cached output is discarded and the decorated function is executed again.\\n\\nBest Practices\\n\\nUse the validate parameter when the cached output may become invalid over time, such as when a database connection or an API key expires.\\n\\nUse the validate parameter judiciously, as it will add an additional overhead of calling the validation function each time the cached output is accessed.\\n\\nMake sure that the validation function is as fast as possible, as it will be called each time the cached output is accessed.\\n\\nConsider to validate cached data periodically, instead of each time it is accessed, to mitigate the performance impact.\\n\\nHandle errors that may occur during validation and provide a fallback mechanism if the validation fails.\\n\\nReplay static st elements in cache-decorated functions\\n\\nFunctions decorated with @st.experimental_singleton can contain static st elements. When a cache-decorated function is executed, we record the element and block messages produced, so the elements will appear in the app even when execution of the function is skipped because the result was cached.\\n\\nIn the example below, the @st.experimental_singleton decorator is used to cache the execution of the get_model function, that returns a 🤗 Hugging Face Transformers model. Notice the cached function also contains a st.bar_chart command, which will be replayed when the function is skipped because the result was cached.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport streamlit as st\\nfrom transformers import AutoModel\\n\\n@st.experimental_singleton\\ndef get_model(model_type):\\n    # Contains a static element st.bar_chart\\n    st.bar_chart(\\n        np.random.rand(10, 1)\\n    )  # This will be recorded and displayed even when the function is skipped\\n\\nbert_model = get_model(\"distilbert-base-uncased\")\\nst.help(bert_model) # Display the model\\'s docstring\\n```\\n\\nSupported static st elements in cache-decorated functions include:\\n\\nst.alert\\n\\nst.altair_chart\\n\\nst.area_chart\\n\\nst.audio\\n\\nst.bar_chart\\n\\nst.ballons\\n\\nst.bokeh_chart\\n\\nst.caption\\n\\nst.code\\n\\nst.components.v1.html\\n\\nst.components.v1.iframe\\n\\nst.container\\n\\nst.dataframe\\n\\nst.echo\\n\\nst.empty\\n\\nst.error\\n\\nst.exception\\n\\nst.expander\\n\\nst.experimental_get_query_params\\n\\nst.experimental_set_query_params\\n\\nst.form\\n\\nst.form_submit_button\\n\\nst.graphviz_chart\\n\\nst.help\\n\\nst.image\\n\\nst.info\\n\\nst.json\\n\\nst.latex\\n\\nst.line_chart\\n\\nst.markdown\\n\\nst.metric\\n\\nst.plotly_chart\\n\\nst.progress\\n\\nst.pydeck_chart\\n\\nst.snow\\n\\nst.spinner\\n\\nst.success\\n\\nst.table\\n\\nst.text\\n\\nst.vega_lite_chart\\n\\nst.video\\n\\nst.warning\\n\\nReplay input widgets in cache-decorated functions\\n\\nIn addition to static elements, functions decorated with @st.experimental_singleton can also contain input widgets! Replaying input widgets is disabled by default. To enable it, you can set the experimental_allow_widgets parameter for @st.experimental_singleton to True. The example below enables widget replaying, and shows the use of a checkbox widget within a cache-decorated function.\\n\\n```python\\nimport streamlit as st\\n\\nEnable widget replay\\n\\n@st.experimental_singleton(experimental_allow_widgets=True)\\ndef func():\\n    # Contains an input widget\\n    st.checkbox(\"Works!\")\\n\\nfunc()\\n```\\n\\nIf the cache decorated function contains input widgets, but experimental_allow_widgets is set to False or unset, Streamlit will throw a CachedStFunctionWarning, like the one below:\\n\\n```python\\nimport streamlit as st\\n\\nWidget replay is disabled by default\\n\\n@st.experimental_singleton\\ndef func():\\n    # Streamlit will throw a CachedStFunctionWarning\\n    st.checkbox(\"Doesn\\'t work\")\\n\\nfunc()\\n```\\n\\nHow widget replay works\\n\\nLet\\'s demystify how widget replay in cache-decorated functions works and gain a conceptual understanding. Widget values are treated as additional inputs to the function, and are used to determine whether the function should be executed or not. Consider the following example:\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_singleton(experimental_allow_widgets=True)\\ndef plus_one(x):\\n    y = x + 1\\n    if st.checkbox(\"Nuke the value 💥\"):\\n        st.write(\"Value was nuked, returning 0\")\\n        y = 0\\n    return y\\n\\nst.write(plus_one(2))\\n```\\n\\nIn order to know which value the cache should return (in case of a cache hit), Streamlit treats the checkbox state (checked / unchecked) as an additional input to the function plus_one (just like x). If the user checks the checkbox (thereby changing its state), we look up the cache for the same value of x (2) and the same checkbox state (checked). If the cache contains a value for this combination of inputs, we return it. Otherwise, we execute the function and store the result in the cache.\\n\\nLet\\'s now understand how enabling and disabling widget replay changes the behavior of the function.\\n\\nWidget replay disabled\\n\\nWidgets in cached functions throw a CachedStFunctionWarning and are ignored.\\n\\nOther static elements in cached functions replay as expected.\\n\\nWidget replay enabled\\n\\nWidgets in cached functions don\\'t lead to a warning, and are replayed as expected.\\n\\nInteracting with a widget in a cached function will cause the function to be executed again, and the cache to be updated.\\n\\nWidgets in cached functions retain their state across reruns.\\n\\nEach unique combination of widget values is treated as a separate input to the function, and is used to determine whether the function should be executed or not. i.e. Each unique combination of widget values has its own cache entry; the cached function runs the first time and the saved value is used afterwards.\\n\\nCalling a cached function multiple times in one script run with the same arguments triggers a DuplicateWidgetID error.\\n\\nIf the arguments to a cached function change, widgets from that function that render again retain their state.\\n\\nChanging the source code of a cached function invalidates the cache.\\n\\nBoth st.experimental_singleton and st.experimental_memo support widget replay.\\n\\nFundamentally, the behavior of a function with (supported) widgets in it doesn\\'t change when it is decorated with @st.experimental_singleton or @st.experimental_memo. The only difference is that the function is only executed when we detect a cache \"miss\".\\n\\nSupported widgets\\n\\nAll input widgets are supported in cache-decorated functions. The following is an exhaustive list of supported widgets:\\n\\nst.button\\n\\nst.camera_input\\n\\nst.checkbox\\n\\nst.color_picker\\n\\nst.date_input\\n\\nst.download_button\\n\\nst.file_uploader\\n\\nst.multiselect\\n\\nst.number_input\\n\\nst.radio\\n\\nst.selectbox\\n\\nst.select_slider\\n\\nst.slider\\n\\nst.text_area\\n\\nst.text_input\\n\\nst.time_input', metadata={'source': 'docs/content/library/api/performance/experimental-singleton.md'}),\n",
       " Document(page_content='title: st.graphviz_chart\\nslug: /library/api-reference/charts/st.graphviz_chart\\ndescription: st.graphviz_chart displays a graph using the dagre-d3 library.', metadata={'source': 'docs/content/library/api/charts/graphviz_chart.md'}),\n",
       " Document(page_content='title: st.bar_chart\\nslug: /library/api-reference/charts/st.bar_chart\\ndescription: st.bar_chart displays a bar chart.', metadata={'source': 'docs/content/library/api/charts/bar_chart.md'}),\n",
       " Document(page_content='title: st.plotly_chart\\nslug: /library/api-reference/charts/st.plotly_chart\\ndescription: st.plotly_chart displays an interactive Plotly chart.\\n\\nTheming\\n\\nPlotly charts are displayed using the Streamlit theme by default. This theme is sleek, user-friendly, and incorporates Streamlit\\'s color palette. The added benefit is that your charts better integrate with the rest of your app\\'s design.\\n\\nThe Streamlit theme is available from Streamlit 1.16.0 through the theme=\"streamlit\" keyword argument. To disable it, and use Plotly\\'s native theme, use theme=None instead.\\n\\nLet\\'s look at an example of charts with the Streamlit theme and the native Plotly theme:\\n\\n```python\\nimport plotly.express as px\\nimport streamlit as st\\n\\ndf = px.data.gapminder()\\n\\nfig = px.scatter(\\n    df.query(\"year==2007\"),\\n    x=\"gdpPercap\",\\n    y=\"lifeExp\",\\n    size=\"pop\",\\n    color=\"continent\",\\n    hover_name=\"country\",\\n    log_x=True,\\n    size_max=60,\\n)\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Plotly native theme\"])\\nwith tab1:\\n    # Use the Streamlit theme.\\n    # This is the default. So you can also omit the theme argument.\\n    st.plotly_chart(fig, theme=\"streamlit\", use_container_width=True)\\nwith tab2:\\n    # Use the native Plotly theme.\\n    st.plotly_chart(fig, theme=None, use_container_width=True)\\n```\\n\\nClick the tabs in the interactive app below to see the charts with the Streamlit theme enabled and disabled.\\n\\nIf you\\'re wondering if your own customizations will still be taken into account, don\\'t worry! You can still make changes to your chart configurations. In other words, although we now enable the Streamlit theme by default, you can overwrite it with custom colors or fonts. For example, if you want a chart line to be green instead of the default red, you can do it!\\n\\nHere\\'s an example of an Plotly chart where a custom color scale is defined and reflected:\\n\\n```python\\nimport plotly.express as px\\nimport streamlit as st\\n\\nst.subheader(\"Define a custom colorscale\")\\ndf = px.data.iris()\\nfig = px.scatter(\\n    df,\\n    x=\"sepal_width\",\\n    y=\"sepal_length\",\\n    color=\"sepal_length\",\\n    color_continuous_scale=\"reds\",\\n)\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Plotly native theme\"])\\nwith tab1:\\n    st.plotly_chart(fig, theme=\"streamlit\", use_container_width=True)\\nwith tab2:\\n    st.plotly_chart(fig, theme=None, use_container_width=True)\\n```\\n\\nNotice how the custom color scale is still reflected in the chart, even when the Streamlit theme is enabled 👇\\n\\nFor many more examples of Plotly charts with and without the Streamlit theme, check out the plotly.streamlit.app.', metadata={'source': 'docs/content/library/api/charts/plotly_chart.md'}),\n",
       " Document(page_content='title: st.bokeh_chart\\nslug: /library/api-reference/charts/st.bokeh_chart\\ndescription: st.bokeh_chart displays an interactive Bokeh chart.', metadata={'source': 'docs/content/library/api/charts/bokeh_chart.md'}),\n",
       " Document(page_content='title: st.map\\nslug: /library/api-reference/charts/st.map\\ndescription: st.map displays a map with points on it.', metadata={'source': 'docs/content/library/api/charts/map.md'}),\n",
       " Document(page_content='title: st.pyplot\\nslug: /library/api-reference/charts/st.pyplot\\ndescription: st.pyplot displays a matplotlib.pyplot figure.', metadata={'source': 'docs/content/library/api/charts/pyplot.md'}),\n",
       " Document(page_content='title: Media elements\\nslug: /library/api-reference/media\\n\\nMedia elements\\n\\nIt\\'s easy to embed images, videos, and audio files directly into your Streamlit apps.\\n\\nImage\\n\\nDisplay an image or list of images.\\n\\npython\\nst.image(numpy_array)\\nst.image(image_bytes)\\nst.image(file)\\nst.image(\"https://example.com/myimage.jpg\")\\n\\nAudio\\n\\nDisplay an audio player.\\n\\npython\\nst.audio(numpy_array)\\nst.audio(audio_bytes)\\nst.audio(file)\\nst.audio(\"https://example.com/myaudio.mp3\", format=\"audio/mp3\")\\n\\nVideo\\n\\nDisplay a video player.\\n\\npython\\nst.video(numpy_array)\\nst.video(video_bytes)\\nst.video(file)\\nst.video(\"https://example.com/myvideo.mp4\", format=\"video/mp4\")\\n\\nStreamlit Webrtc\\n\\nHandling and transmitting real-time video/audio streams with Streamlit. Created by @whitphx.\\n\\n```python\\nfrom streamlit_webrtc import webrtc_streamer\\n\\nwebrtc_streamer(key=\"sample\")\\n```\\n\\nDrawable Canvas\\n\\nProvides a sketching canvas using Fabric.js. Created by @andfanilo.\\n\\n```python\\nfrom streamlit_drawable_canvas import st_canvas\\n\\nst_canvas(fill_color=\"rgba(255, 165, 0, 0.3)\", stroke_width=stroke_width, stroke_color=stroke_color, background_color=bg_color, background_image=Image.open(bg_image) if bg_image else None, update_streamlit=realtime_update, height=150, drawing_mode=drawing_mode, point_display_radius=point_display_radius if drawing_mode == \\'point\\' else 0, key=\"canvas\",)\\n```\\n\\nImage Comparison\\n\\nCompare images with a slider using JuxtaposeJS. Created by @fcakyon.\\n\\n```python\\nfrom streamlit_image_comparison import image_comparison\\n\\nimage_comparison(img1=\"image1.jpg\", img2=\"image2.jpg\",)\\n```\\n\\nStreamlit Cropper\\n\\nA simple image cropper for Streamlit. Created by @turner-anderson.\\n\\n```python\\nfrom streamlit_cropper import st_cropper\\n\\nst_cropper(img, realtime_update=realtime_update, box_color=box_color, aspect_ratio=aspect_ratio)\\n```\\n\\nImage Coordinates\\n\\nGet the coordinates of clicks on an image. Created by @blackary.\\n\\n```python\\nfrom streamlit_image_coordinates import streamlit_image_coordinates\\n\\nstreamlit_image_coordinates(\"https://placekitten.com/200/300\")\\n```\\n\\nStreamlit Lottie\\n\\nIntegrate Lottie animations inside your Streamlit app. Created by @andfanilo.\\n\\n```python\\nlottie_hello = load_lottieurl(\"https://assets5.lottiefiles.com/packages/lf20_V9t630.json\")\\n\\nst_lottie(lottie_hello, key=\"hello\")\\n```', metadata={'source': 'docs/content/library/api/media/media.md'}),\n",
       " Document(page_content='title: st.vega_lite_chart\\nslug: /library/api-reference/charts/st.vega_lite_chart\\ndescription: st.vega_lite_chart displays a chart using the Vega-Lite library.\\n\\nTheming\\n\\nVega-Lite charts are displayed using the Streamlit theme by default. This theme is sleek, user-friendly, and incorporates Streamlit\\'s color palette. The added benefit is that your charts better integrate with the rest of your app\\'s design.\\n\\nThe Streamlit theme is available from Streamlit 1.16.0 through the theme=\"streamlit\" keyword argument. To disable it, and use Vega-Lite\\'s native theme, use theme=None instead.\\n\\nLet\\'s look at an example of charts with the Streamlit theme and the native Vega-Lite theme:\\n\\n```python\\nimport streamlit as st\\nfrom vega_datasets import data\\n\\nsource = data.cars()\\n\\nchart = {\\n    \"mark\": \"point\",\\n    \"encoding\": {\\n        \"x\": {\\n            \"field\": \"Horsepower\",\\n            \"type\": \"quantitative\",\\n        },\\n        \"y\": {\\n            \"field\": \"Miles_per_Gallon\",\\n            \"type\": \"quantitative\",\\n        },\\n        \"color\": {\"field\": \"Origin\", \"type\": \"nominal\"},\\n        \"shape\": {\"field\": \"Origin\", \"type\": \"nominal\"},\\n    },\\n}\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Vega-Lite native theme\"])\\n\\nwith tab1:\\n    # Use the Streamlit theme.\\n    # This is the default. So you can also omit the theme argument.\\n    st.vega_lite_chart(\\n        source, chart, theme=\"streamlit\", use_container_width=True\\n    )\\nwith tab2:\\n    st.vega_lite_chart(\\n        source, chart, theme=None, use_container_width=True\\n    )\\n```\\n\\nClick the tabs in the interactive app below to see the charts with the Streamlit theme enabled and disabled.\\n\\nIf you\\'re wondering if your own customizations will still be taken into account, don\\'t worry! You can still make changes to your chart configurations. In other words, although we now enable the Streamlit theme by default, you can overwrite it with custom colors or fonts. For example, if you want a chart line to be green instead of the default red, you can do it!', metadata={'source': 'docs/content/library/api/charts/vega_lite_chart.md'}),\n",
       " Document(page_content='title: st.pydeck_chart\\nslug: /library/api-reference/charts/st.pydeck_chart\\ndescription: st.pydeck_chart displays a chart using the PyDeck library.', metadata={'source': 'docs/content/library/api/charts/pydeck_chart.md'}),\n",
       " Document(page_content='title: st.line_chart\\nslug: /library/api-reference/charts/st.line_chart\\ndescription: st.line_chart displays a line chart.', metadata={'source': 'docs/content/library/api/charts/line_chart.md'}),\n",
       " Document(page_content='title: st.area_chart\\nslug: /library/api-reference/charts/st.area_chart\\ndescription: st.area_chart displays an area chart.', metadata={'source': 'docs/content/library/api/charts/area_chart.md'}),\n",
       " Document(page_content='title: st.altair_chart\\nslug: /library/api-reference/charts/st.altair_chart\\ndescription: st.altair_chart displays a chart using the Altair library.\\n\\nTheming\\n\\nAltair charts are displayed using the Streamlit theme by default. This theme is sleek, user-friendly, and incorporates Streamlit\\'s color palette. The added benefit is that your charts better integrate with the rest of your app\\'s design.\\n\\nThe Streamlit theme is available from Streamlit 1.16.0 through the theme=\"streamlit\" keyword argument. To disable it, and use Altair\\'s native theme, use theme=None instead.\\n\\nLet\\'s look at an example of charts with the Streamlit theme and the native Altair theme:\\n\\n```python\\nimport altair as alt\\nfrom vega_datasets import data\\n\\nsource = data.cars()\\n\\nchart = alt.Chart(source).mark_circle().encode(\\n    x=\\'Horsepower\\',\\n    y=\\'Miles_per_Gallon\\',\\n    color=\\'Origin\\',\\n).interactive()\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Altair native theme\"])\\n\\nwith tab1:\\n    # Use the Streamlit theme.\\n    # This is the default. So you can also omit the theme argument.\\n    st.altair_chart(chart, theme=\"streamlit\", use_container_width=True)\\nwith tab2:\\n    # Use the native Altair theme.\\n    st.altair_chart(chart, theme=None, use_container_width=True)\\n```\\n\\nClick the tabs in the interactive app below to see the charts with the Streamlit theme enabled and disabled.\\n\\nIf you\\'re wondering if your own customizations will still be taken into account, don\\'t worry! You can still make changes to your chart configurations. In other words, although we now enable the Streamlit theme by default, you can overwrite it with custom colors or fonts. For example, if you want a chart line to be green instead of the default red, you can do it!\\n\\nHere\\'s an example of an Altair chart where manual color passing is done and reflected:\\n\\n```python\\nimport altair as alt\\nimport streamlit as st\\nfrom vega_datasets import data\\n\\nsource = data.seattle_weather()\\n\\nscale = alt.Scale(\\n    domain=[\"sun\", \"fog\", \"drizzle\", \"rain\", \"snow\"],\\n    range=[\"#e7ba52\", \"#a7a7a7\", \"#aec7e8\", \"#1f77b4\", \"#9467bd\"],\\n)\\ncolor = alt.Color(\"weather:N\", scale=scale)\\n\\nWe create two selections:\\n\\na brush that is active on the top panel\\n\\na multi-click that is active on the bottom panel\\n\\nbrush = alt.selection_interval(encodings=[\"x\"])\\nclick = alt.selection_multi(encodings=[\"color\"])\\n\\nTop panel is scatter plot of temperature vs time\\n\\npoints = (\\n    alt.Chart()\\n    .mark_point()\\n    .encode(\\n        alt.X(\"monthdate(date):T\", title=\"Date\"),\\n        alt.Y(\\n            \"temp_max:Q\",\\n            title=\"Maximum Daily Temperature (C)\",\\n            scale=alt.Scale(domain=[-5, 40]),\\n        ),\\n        color=alt.condition(brush, color, alt.value(\"lightgray\")),\\n        size=alt.Size(\"precipitation:Q\", scale=alt.Scale(range=[5, 200])),\\n    )\\n    .properties(width=550, height=300)\\n    .add_selection(brush)\\n    .transform_filter(click)\\n)\\n\\nBottom panel is a bar chart of weather type\\n\\nbars = (\\n    alt.Chart()\\n    .mark_bar()\\n    .encode(\\n        x=\"count()\",\\n        y=\"weather:N\",\\n        color=alt.condition(click, color, alt.value(\"lightgray\")),\\n    )\\n    .transform_filter(brush)\\n    .properties(\\n        width=550,\\n    )\\n    .add_selection(click)\\n)\\n\\nchart = alt.vconcat(points, bars, data=source, title=\"Seattle Weather: 2012-2015\")\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Altair native theme\"])\\n\\nwith tab1:\\n    st.altair_chart(chart, theme=\"streamlit\", use_container_width=True)\\nwith tab2:\\n    st.altair_chart(chart, theme=None, use_container_width=True)\\n```\\n\\nNotice how the custom colors are still reflected in the chart, even when the Streamlit theme is enabled 👇\\n\\nFor many more examples of Altair charts with and without the Streamlit theme, check out the altair.streamlit.app.\\n\\nAnnotating charts\\n\\nAltair also allows you to annotate your charts with text, images, and emojis. You can do this by creating layered charts, which let you overlay two different charts on top of each other. The idea is to use the first chart to show the data, and the second chart to show the annotations. The second chart can then be overlaid on top of the first chart using the + operator to create a layered chart.\\n\\nLet\\'s walk through an example of annotating a time-series chart with text and an emoji.\\n\\nStep 1: Create the base chart\\n\\nIn this example, we create a time-series chart to track the evolution of stock prices. The chart is interactive and contains a multi-line tooltip. Click here to learn more about multi-line tooltips in Altair.\\n\\nFirst, we import the required libraries and load the example stocks dataset using the vega_datasets package:\\n\\n```python\\nimport altair as alt\\nimport pandas as pd\\nimport streamlit as st\\nfrom vega_datasets import data\\n\\nWe use @st.cache_data to keep the dataset in cache\\n\\n@st.cache_data\\ndef get_data():\\n    source = data.stocks()\\n    source = source[source.date.gt(\"2004-01-01\")]\\n    return source\\n\\nsource = get_data()\\n```\\n\\nNext, we define a function get_chart() to create the interactive time-series chart of the stock prices with a multi-line tooltip. The x-axis represents the date, and the y-axis represents the stock price.\\n\\nWe then invoke get_chart() that takes the stock prices dataframe as an input and returns a chart object. This is going to be our base chart on which we will overlay the annotations in Step 2.\\n\\n```python\\n\\nDefine the base time-series chart.\\n\\ndef get_chart(data):\\n    hover = alt.selection_single(\\n        fields=[\"date\"],\\n        nearest=True,\\n        on=\"mouseover\",\\n        empty=\"none\",\\n    )\\n\\nchart = get_chart(source)\\n```\\n\\nStep 2: Annotate the chart\\n\\nNow that we have our first chart that shows the data, we can annotate it with text and an emoji. Let\\'s overlay the ⬇ emoji on top of the time-series chart at specifc points of interest. We want users to hover over the ⬇ emoji to see the associated annotation text.\\n\\nFor simplicity, let\\'s annotate four specific dates and set the height of the annotations at constant value of 10.\\n\\nYou can vary the horizontal and vertical postions of the annotations by replacing the hard-coded values with the output of Streamlit widgets! Click here to jump to a live example below, and develop an intuition for the ideal horizontal and vertical positions of the annotations by playing with Streamlit widgets.\\n\\nTo do so, we create a dataframe annotations_df containing the dates, annotation text, and the height of the annotations:\\n\\n```python\\n\\nAdd annotations\\n\\nANNOTATIONS = [\\n    (\"Mar 01, 2008\", \"Pretty good day for GOOG\"),\\n    (\"Dec 01, 2007\", \"Something\\'s going wrong for GOOG & AAPL\"),\\n    (\"Nov 01, 2008\", \"Market starts again thanks to...\"),\\n    (\"Dec 01, 2009\", \"Small crash for GOOG after...\"),\\n]\\nannotations_df = pd.DataFrame(ANNOTATIONS, columns=[\"date\", \"event\"])\\nannotations_df.date = pd.to_datetime(annotations_df.date)\\nannotations_df[\"y\"] = 10\\n```\\n\\nUsing this dataframe, we create a scatter plot with the x-axis representing the date, and the y-axis representing the height of the annotation. The data point at the specific date and height is represented by the ⬇ emoji, using Altair\\'s mark_text() mark.\\n\\nThe annotation text is displayed as a tooltip when users hover over the ⬇ emoji. This is achieved using Altair\\'s encode() method to map the event column containing the annotation text to the visual attribute ⬇ of the plot.\\n\\npython\\nannotation_layer = (\\n    alt.Chart(annotations_df)\\n    .mark_text(size=20, text=\"⬇\", dx=-8, dy=-10, align=\"left\")\\n    .encode(\\n        x=\"date:T\",\\n        y=alt.Y(\"y:Q\"),\\n        tooltip=[\"event\"],\\n    )\\n    .interactive()\\n)\\n\\nFinally, we overlay the annotations on top of the base chart using the + operator to create the final layered chart! 🎈\\n\\npython\\nst.altair_chart(\\n    (chart + annotation_layer).interactive(),\\n    use_container_width=True\\n)\\n\\nTo use images instead of emojis, replace the line containing .mark_text() with .mark_image(), and replace image_url below with the URL of the image:\\n\\npython\\n.mark_image(\\n    width=12,\\n    height=12,\\n    url=\"image_url\",\\n)\\n\\nInteractive example\\n\\nNow that you\\'ve learned how to annotate charts, the sky\\'s the limit! We\\'ve extended the above example to let you interactively paste your favorite emoji and set its position on the chart with Streamlit widgets. 👇', metadata={'source': 'docs/content/library/api/charts/altair_chart.md'}),\n",
       " Document(page_content='title: st.error\\nslug: /library/api-reference/status/st.error\\ndescription: st.error displays error message.', metadata={'source': 'docs/content/library/api/status/error.md'}),\n",
       " Document(page_content='title: Chart elements\\nslug: /library/api-reference/charts\\n\\nChart elements\\n\\nStreamlit supports several different charting libraries, and our goal is to\\ncontinually add support for more. Right now, the most basic library in our\\narsenal is Matplotlib. Then there are also\\ninteractive charting libraries like Vega\\nLite (2D charts) and\\ndeck.gl (maps and 3D charts). And\\nfinally we also provide a few chart types that are \"native\" to Streamlit,\\nlike st.line_chart and st.area_chart.\\n\\nSimple line charts\\n\\nDisplay a line chart.\\n\\npython\\nst.line_chart(my_data_frame)\\n\\nSimple area charts\\n\\nDisplay an area chart.\\n\\npython\\nst.area_chart(my_data_frame)\\n\\nSimple bar charts\\n\\nDisplay a bar chart.\\n\\npython\\nst.bar_chart(my_data_frame)\\n\\nScatterplots on maps\\n\\nDisplay a map with points on it.\\n\\npython\\nst.map(my_data_frame)\\n\\nMatplotlib\\n\\nDisplay a matplotlib.pyplot figure.\\n\\npython\\nst.pyplot(my_mpl_figure)\\n\\nAltair\\n\\nDisplay a chart using the Altair library.\\n\\npython\\nst.altair_chart(my_altair_chart)\\n\\nVega-Lite\\n\\nDisplay a chart using the Vega-Lite library.\\n\\npython\\nst.vega_lite_chart(my_vega_lite_chart)\\n\\nPlotly\\n\\nDisplay an interactive Plotly chart.\\n\\npython\\nst.plotly_chart(my_plotly_chart)\\n\\nBokeh\\n\\nDisplay an interactive Bokeh chart.\\n\\npython\\nst.bokeh_chart(my_bokeh_chart)\\n\\nPyDeck\\n\\nDisplay a chart using the PyDeck library.\\n\\npython\\nst.pydeck_chart(my_pydeck_chart)\\n\\nGraphViz\\n\\nDisplay a graph using the dagre-d3 library.\\n\\npython\\nst.graphviz_chart(my_graphviz_spec)\\n\\nPlost\\n\\nA deceptively simple plotting library for Streamlit. Created by @tvst.\\n\\npython\\nimport plost\\nplost.line_chart(my_dataframe, x=\\'time\\', y=\\'stock_value\\', color=\\'stock_name\\',)\\n\\nHiPlot\\n\\nHigh dimensional Interactive Plotting. Created by @facebookresearch.\\n\\npython\\ndata = [{\\'dropout\\':0.1, \\'lr\\': 0.001, \\'loss\\': 10.0, \\'optimizer\\': \\'SGD\\'}, {\\'dropout\\':0.15, \\'lr\\': 0.01, \\'loss\\': 3.5, \\'optimizer\\': \\'Adam\\'}, {\\'dropout\\':0.3, \\'lr\\': 0.1, \\'loss\\': 4.5, \\'optimizer\\': \\'Adam\\'}]\\nhip.Experiment.from_iterable(data).display()\\n\\nECharts\\n\\nHigh dimensional Interactive Plotting. Created by @andfanilo.\\n\\npython\\nfrom streamlit_echarts import st_echarts\\nst_echarts(options=options)\\n\\nStreamlit Folium\\n\\nStreamlit Component for rendering Folium maps. Created by @randyzwitch.\\n\\npython\\nm = folium.Map(location=[39.949610, -75.150282], zoom_start=16)\\nst_data = st_folium(m, width=725)\\n\\nSpacy-Streamlit\\n\\nspaCy building blocks and visualizers for Streamlit apps. Created by @explosion.\\n\\npython\\nmodels = [\"en_core_web_sm\", \"en_core_web_md\"]\\nspacy_streamlit.visualize(models, \"Sundar Pichai is the CEO of Google.\")\\n\\nStreamlit Agraph\\n\\nA Streamlit Graph Vis, based on react-grah-vis. Created by @ChrisDelClea.\\n\\npython\\nfrom streamlit_agraph import agraph, Node, Edge, Config\\nagraph(nodes=nodes, edges=edges, config=config)\\n\\nStreamlit Lottie\\n\\nIntegrate Lottie animations inside your Streamlit app. Created by @andfanilo.\\n\\npython\\nlottie_hello = load_lottieurl(\"https://assets5.lottiefiles.com/packages/lf20_V9t630.json\")\\nst_lottie(lottie_hello, key=\"hello\")\\n\\nPlotly Events\\n\\nMake Plotly charts interactive!. Created by @null-jones.\\n\\npython\\nfig = px.line(x=[1], y=[1])\\nselected_points = plotly_events(fig)\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\npython\\nchart += get_annotations_chart(annotations=[(\"Mar 01, 2008\", \"Pretty good day for GOOG\"), (\"Dec 01, 2007\", \"Something\\'s going wrong for GOOG & AAPL\"), (\"Nov 01, 2008\", \"Market starts again thanks to...\"), (\"Dec 01, 2009\", \"Small crash for GOOG after...\"),],)\\nst.altair_chart(chart, use_container_width=True)', metadata={'source': 'docs/content/library/api/charts/charts.md'}),\n",
       " Document(page_content='title: st.progress\\nslug: /library/api-reference/status/st.progress\\ndescription: st.progress displays a progress bar.', metadata={'source': 'docs/content/library/api/status/progress.md'}),\n",
       " Document(page_content='title: st.spinner\\nslug: /library/api-reference/status/st.spinner\\ndescription: st.spinner temporarily displays a message while executing a block of code.', metadata={'source': 'docs/content/library/api/status/spinner.md'}),\n",
       " Document(page_content='title: st.success\\nslug: /library/api-reference/status/st.success\\ndescription: st.success displays a success message.', metadata={'source': 'docs/content/library/api/status/success.md'}),\n",
       " Document(page_content='title: st.snow\\nslug: /library/api-reference/status/st.snow\\ndescription: st.snow displays celebratory snowflakes!', metadata={'source': 'docs/content/library/api/status/snow.md'}),\n",
       " Document(page_content='title: st.info\\nslug: /library/api-reference/status/st.info\\ndescription: st.info displays an informational message.', metadata={'source': 'docs/content/library/api/status/info.md'}),\n",
       " Document(page_content='title: st.balloons\\nslug: /library/api-reference/status/st.balloons\\ndescription: st.balloons displays celebratory balloons!', metadata={'source': 'docs/content/library/api/status/balloons.md'}),\n",
       " Document(page_content='title: st.exception\\nslug: /library/api-reference/status/st.exception\\ndescription: st.exception displays an exception.', metadata={'source': 'docs/content/library/api/status/exception.md'}),\n",
       " Document(page_content='title: Display progress and status\\nslug: /library/api-reference/status\\n\\nDisplay progress and status\\n\\nStreamlit provides a few methods that allow you to add animation to your\\napps. These animations include progress bars, status messages (like\\nwarnings), and celebratory balloons.\\n\\nProgress bar\\n\\nDisplay a progress bar.\\n\\npython\\nfor i in range(101):\\n  st.progress(i)\\n  do_something_slow()\\n\\nSpinner\\n\\nTemporarily displays a message while executing a block of code.\\n\\npython\\nwith st.spinner(\"Please wait...\"):\\n  do_something_slow()\\n\\nBalloons\\n\\nDisplay celebratory balloons!\\n\\n```python\\ndo_something()\\n\\nCelebrate when all done!\\n\\nst.balloons()\\n```\\n\\nSnowflakes\\n\\nDisplay celebratory snowflakes!\\n\\n```python\\ndo_something()\\n\\nCelebrate when all done!\\n\\nst.snow()\\n```\\n\\nError box\\n\\nDisplay error message.\\n\\npython\\nst.error(\"We encountered an error\")\\n\\nWarning box\\n\\nDisplay warning message.\\n\\npython\\nst.warning(\"Unable to fetch image. Skipping...\")\\n\\nInfo box\\n\\nDisplay an informational message.\\n\\npython\\nst.info(\"Dataset is updated every day at midnight.\")\\n\\nSuccess box\\n\\nDisplay a success message.\\n\\npython\\nst.success(\"Match found!\")\\n\\nException output\\n\\nDisplay an exception.\\n\\npython\\ne = RuntimeError(\"This is an exception of type RuntimeError\")\\nst.exception(e)\\n\\nStqdm\\n\\nThe simplest way to handle a progress bar in streamlit app. Created by @Wirg.\\n\\n```python\\nfrom stqdm import stqdm\\n\\nfor _ in stqdm(range(50)):\\n    sleep(0.5)\\n```\\n\\nCustom notification box\\n\\nA custom notification box with the ability to close it out. Created by @Socvest.\\n\\n```python\\nfrom streamlit_custom_notification_box import custom_notification_box\\n\\nstyles = {\\'material-icons\\':{\\'color\\': \\'red\\'}, \\'text-icon-link-close-container\\': {\\'box-shadow\\': \\'#3896de 0px 4px\\'}, \\'notification-text\\': {\\'\\':\\'\\'}, \\'close-button\\':{\\'\\':\\'\\'}, \\'link\\':{\\'\\':\\'\\'}}\\ncustom_notification_box(icon=\\'info\\', textDisplay=\\'We are almost done with your registration...\\', externalLink=\\'more info\\', url=\\'#\\', styles=styles, key=\"foo\")\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.let_it_rain import rain\\n\\nrain(emoji=\"🎈\", font_size=54,\\n  falling_speed=5, animation_length=\"infinite\",)\\n```', metadata={'source': 'docs/content/library/api/status/status.md'}),\n",
       " Document(page_content='title: st.warning\\nslug: /library/api-reference/status/st.warning\\ndescription: st.warning displays warning message.', metadata={'source': 'docs/content/library/api/status/warning.md'}),\n",
       " Document(page_content='title: st.metric\\nslug: /library/api-reference/data/st.metric\\ndescription: st.metric displays a metric in big bold font, with an optional indicator of how the metric changed.', metadata={'source': 'docs/content/library/api/data/metric.md'}),\n",
       " Document(page_content='title: API Reference\\nslug: /library/api-reference\\nnext: caching\\nprevious: index.md\\n\\nAPI reference\\n\\nStreamlit makes it easy for you to visualize, mutate, and share data. The API\\nreference is organized by activity type, like displaying data or optimizing\\nperformance. Each section includes methods associated with the activity type,\\nincluding examples.\\n\\nBrowse our API below and click to learn more about any of our available commands! 🎈\\n\\nDisplay almost anything\\n\\nst.write\\n\\nWrite arguments to the app.\\n\\npython\\nst.write(\"Hello **world**!\")\\nst.write(my_data_frame)\\nst.write(my_mpl_figure)\\n\\nMagic\\n\\nAny time Streamlit sees either a variable or literal value on its own line, it automatically writes that to your app using st.write\\n\\npython\\n\"Hello **world**!\"\\nmy_data_frame\\nmy_mpl_figure\\n\\nText elements\\n\\nMarkdown\\n\\nDisplay string formatted as Markdown.\\n\\npython\\nst.markdown(\"Hello **world**!\")\\n\\nTitle\\n\\nDisplay text in title formatting.\\n\\npython\\nst.title(\"The app title\")\\n\\nHeader\\n\\nDisplay text in header formatting.\\n\\npython\\nst.header(\"This is a header\")\\n\\nSubheader\\n\\nDisplay text in subheader formatting.\\n\\npython\\nst.subheader(\"This is a subheader\")\\n\\nCaption\\n\\nDisplay text in small font.\\n\\npython\\nst.caption(\"This is written small caption text\")\\n\\nCode block\\n\\nDisplay a code block with optional syntax highlighting.\\n\\npython\\nst.code(\"a = 1234\")\\n\\nPreformatted text\\n\\nWrite fixed-width and preformatted text.\\n\\npython\\nst.text(\"Hello world\")\\n\\nLaTeX\\n\\nDisplay mathematical expressions formatted as LaTeX.\\n\\npython\\nst.latex(\"\\\\int a x^2 \\\\,dx\")\\n\\nDivider\\n\\nDisplay a horizontal rule.\\n\\npython\\nst.divider()\\n\\nAnnotated text\\n\\nDisplay annotated text in Streamlit apps. Created by @tvst.\\n\\npython\\nannotated_text(\"This \", (\"is\", \"verb\"), \" some \", (\"annotated\", \"adj\"), (\"text\", \"noun\"), \" for those of \", (\"you\", \"pronoun\"), \" who \", (\"like\", \"verb\"), \" this sort of \", (\"thing\", \"noun\"), \".\")\\n\\nDrawable Canvas\\n\\nProvides a sketching canvas using Fabric.js. Created by @andfanilo.\\n\\npython\\nst_canvas(fill_color=\"rgba(255, 165, 0, 0.3)\", stroke_width=stroke_width, stroke_color=stroke_color, background_color=bg_color, background_image=Image.open(bg_image) if bg_image else None, update_streamlit=realtime_update, height=150, drawing_mode=drawing_mode, point_display_radius=point_display_radius if drawing_mode == \\'point\\' else 0, key=\"canvas\",)\\n\\nTags\\n\\nAdd tags to your Streamlit apps. Created by @gagan3012.\\n\\npython\\nst_tags(label=\\'# Enter Keywords:\\', text=\\'Press enter to add more\\', value=[\\'Zero\\', \\'One\\', \\'Two\\'], suggestions=[\\'five\\', \\'six\\', \\'seven\\', \\'eight\\', \\'nine\\', \\'three\\', \\'eleven\\', \\'ten\\', \\'four\\'], maxtags = 4, key=\\'1\\')\\n\\nNLU\\n\\nApply text mining on a dataframe. Created by @JohnSnowLabs.\\n\\npython\\nnlu.load(\\'sentiment\\').predict(\\'I love NLU! <3\\')\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\npython\\nmention(label=\"An awesome Streamlit App\", icon=\"streamlit\",  url=\"https://extras.streamlit.app\",)\\n\\nData elements\\n\\nDataframes\\n\\nDisplay a dataframe as an interactive table.\\n\\npython\\nst.dataframe(my_data_frame)\\n\\nData editor\\n\\nDisplay a data editor widget.\\n\\npython\\nedited = st.data_editor(df, num_rows=\"dynamic\")\\n\\nColumn configuration\\n\\nConfigure the display and editing behavior of dataframes and data editors.\\n\\npython\\nst.column_config.NumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nStatic tables\\n\\nDisplay a static table.\\n\\npython\\nst.table(my_data_frame)\\n\\nMetrics\\n\\nDisplay a metric in big bold font, with an optional indicator of how the metric changed.\\n\\npython\\nst.metric(\"My metric\", 42, 2)\\n\\nDicts and JSON\\n\\nDisplay object or string as a pretty-printed JSON string.\\n\\npython\\nst.json(my_dict)\\n\\nStreamlit Aggrid\\n\\nImplementation of Ag-Grid component for Streamlit. Created by @PablocFonseca.\\n\\n```python\\ndf = pd.DataFrame({\\'col1\\': [1, 2, 3], \\'col2\\': [4, 5, 6]})\\ngrid_return = AgGrid(df, editable=True)\\n\\nnew_df = grid_return[\\'data\\']\\n```\\n\\nStreamlit Folium\\n\\nStreamlit Component for rendering Folium maps. Created by @randyzwitch.\\n\\n```python\\nm = folium.Map(location=[39.949610, -75.150282], zoom_start=16)\\nfolium.Marker([39.949610, -75.150282], popup=\"Liberty Bell\", tooltip=\"Liberty Bell\").add_to(m)\\n\\nst_data = st_folium(m, width=725)\\n```\\n\\nPandas Profiling\\n\\nPandas profiling component for Streamlit. Created by @okld.\\n\\n```python\\ndf = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\\npr = df.profile_report()\\n\\nst_profile_report(pr)\\n```\\n\\nImage Coordinates\\n\\nGet the coordinates of clicks on an image. Created by @blackary.\\n\\n```python\\nfrom streamlit_image_coordinates import streamlit_image_coordinates\\nvalue = streamlit_image_coordinates(\"https://placekitten.com/200/300\")\\n\\nst.write(value)\\n```\\n\\nPlotly Events\\n\\nMake Plotly charts interactive!. Created by @null-jones.\\n\\n```python\\nfrom streamlit_plotly_events import plotly_events\\nfig = px.line(x=[1], y=[1])\\n\\nselected_points = plotly_events(fig)\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.metric_cards import style_metric_cards\\ncol3.metric(label=\"No Change\", value=5000, delta=0)\\n\\nstyle_metric_cards()\\n```\\n\\nChart elements\\n\\nSimple line charts\\n\\nDisplay a line chart.\\n\\npython\\nst.line_chart(my_data_frame)\\n\\nSimple area charts\\n\\nDisplay an area chart.\\n\\npython\\nst.area_chart(my_data_frame)\\n\\nSimple bar charts\\n\\nDisplay a bar chart.\\n\\npython\\nst.bar_chart(my_data_frame)\\n\\nScatterplots on maps\\n\\nDisplay a map with points on it.\\n\\npython\\nst.map(my_data_frame)\\n\\nMatplotlib\\n\\nDisplay a matplotlib.pyplot figure.\\n\\npython\\nst.pyplot(my_mpl_figure)\\n\\nAltair\\n\\nDisplay a chart using the Altair library.\\n\\npython\\nst.altair_chart(my_altair_chart)\\n\\nVega-Lite\\n\\nDisplay a chart using the Vega-Lite library.\\n\\npython\\nst.vega_lite_chart(my_vega_lite_chart)\\n\\nPlotly\\n\\nDisplay an interactive Plotly chart.\\n\\npython\\nst.plotly_chart(my_plotly_chart)\\n\\nBokeh\\n\\nDisplay an interactive Bokeh chart.\\n\\npython\\nst.bokeh_chart(my_bokeh_chart)\\n\\nPyDeck\\n\\nDisplay a chart using the PyDeck library.\\n\\npython\\nst.pydeck_chart(my_pydeck_chart)\\n\\nGraphViz\\n\\nDisplay a graph using the dagre-d3 library.\\n\\npython\\nst.graphviz_chart(my_graphviz_spec)\\n\\nPlost\\n\\nA deceptively simple plotting library for Streamlit. Created by @tvst.\\n\\npython\\nimport plost\\nplost.line_chart(my_dataframe, x=\\'time\\', y=\\'stock_value\\', color=\\'stock_name\\',)\\n\\nHiPlot\\n\\nHigh dimensional Interactive Plotting. Created by @facebookresearch.\\n\\npython\\ndata = [{\\'dropout\\':0.1, \\'lr\\': 0.001, \\'loss\\': 10.0, \\'optimizer\\': \\'SGD\\'}, {\\'dropout\\':0.15, \\'lr\\': 0.01, \\'loss\\': 3.5, \\'optimizer\\': \\'Adam\\'}, {\\'dropout\\':0.3, \\'lr\\': 0.1, \\'loss\\': 4.5, \\'optimizer\\': \\'Adam\\'}]\\nhip.Experiment.from_iterable(data).display()\\n\\nECharts\\n\\nHigh dimensional Interactive Plotting. Created by @andfanilo.\\n\\npython\\nfrom streamlit_echarts import st_echarts\\nst_echarts(options=options)\\n\\nStreamlit Folium\\n\\nStreamlit Component for rendering Folium maps. Created by @randyzwitch.\\n\\npython\\nm = folium.Map(location=[39.949610, -75.150282], zoom_start=16)\\nst_data = st_folium(m, width=725)\\n\\nSpacy-Streamlit\\n\\nspaCy building blocks and visualizers for Streamlit apps. Created by @explosion.\\n\\npython\\nmodels = [\"en_core_web_sm\", \"en_core_web_md\"]\\nspacy_streamlit.visualize(models, \"Sundar Pichai is the CEO of Google.\")\\n\\nStreamlit Agraph\\n\\nA Streamlit Graph Vis, based on react-grah-vis. Created by @ChrisDelClea.\\n\\npython\\nfrom streamlit_agraph import agraph, Node, Edge, Config\\nagraph(nodes=nodes, edges=edges, config=config)\\n\\nStreamlit Lottie\\n\\nIntegrate Lottie animations inside your Streamlit app. Created by @andfanilo.\\n\\npython\\nlottie_hello = load_lottieurl(\"https://assets5.lottiefiles.com/packages/lf20_V9t630.json\")\\nst_lottie(lottie_hello, key=\"hello\")\\n\\nPlotly Events\\n\\nMake Plotly charts interactive!. Created by @null-jones.\\n\\npython\\nfig = px.line(x=[1], y=[1])\\nselected_points = plotly_events(fig)\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\npython\\nchart += get_annotations_chart(annotations=[(\"Mar 01, 2008\", \"Pretty good day for GOOG\"), (\"Dec 01, 2007\", \"Something\\'s going wrong for GOOG & AAPL\"), (\"Nov 01, 2008\", \"Market starts again thanks to...\"), (\"Dec 01, 2009\", \"Small crash for GOOG after...\"),],)\\nst.altair_chart(chart, use_container_width=True)\\n\\nInput widgets\\n\\nButton\\n\\nDisplay a button widget.\\n\\npython\\nclicked = st.button(\"Click me\")\\n\\nData editor\\n\\nDisplay a data editor widget.\\n\\npython\\nedited = st.experimental_data_editor(df, num_rows=\"dynamic\")\\n\\nDownload button\\n\\nDisplay a download button widget.\\n\\npython\\nst.download_button(\"Download file\", file)\\n\\nCheckbox\\n\\nDisplay a checkbox widget.\\n\\npython\\nselected = st.checkbox(\"I agree\")\\n\\nRadio\\n\\nDisplay a radio button widget.\\n\\npython\\nchoice = st.radio(\"Pick one\", [\"cats\", \"dogs\"])\\n\\nSelectbox\\n\\nDisplay a select widget.\\n\\npython\\nchoice = st.selectbox(\"Pick one\", [\"cats\", \"dogs\"])\\n\\nMultiselect\\n\\nDisplay a multiselect widget. The multiselect widget starts as empty.\\n\\npython\\nchoices = st.multiselect(\"Buy\", [\"milk\", \"apples\", \"potatoes\"])\\n\\nSlider\\n\\nDisplay a slider widget.\\n\\npython\\nnumber = st.slider(\"Pick a number\", 0, 100)\\n\\nSelect-slider\\n\\nDisplay a slider widget to select items from a list.\\n\\npython\\nsize = st.select_slider(\"Pick a size\", [\"S\", \"M\", \"L\"])\\n\\nText input\\n\\nDisplay a single-line text input widget.\\n\\npython\\nname = st.text_input(\"First name\")\\n\\nNumber input\\n\\nDisplay a numeric input widget.\\n\\npython\\nchoice = st.number_input(\"Pick a number\", 0, 10)\\n\\nText-area\\n\\nDisplay a multi-line text input widget.\\n\\npython\\ntext = st.text_area(\"Text to translate\")\\n\\nDate input\\n\\nDisplay a date input widget.\\n\\npython\\ndate = st.date_input(\"Your birthday\")\\n\\nTime input\\n\\nDisplay a time input widget.\\n\\npython\\ntime = st.time_input(\"Meeting time\")\\n\\nFile Uploader\\n\\nDisplay a file uploader widget.\\n\\npython\\ndata = st.file_uploader(\"Upload a CSV\")\\n\\nCamera input\\n\\nDisplay a widget that allows users to upload images directly from a camera.\\n\\npython\\nimage = st.camera_input(\"Take a picture\")\\n\\nColor picker\\n\\nDisplay a color picker widget.\\n\\npython\\ncolor = st.color_picker(\"Pick a color\")\\n\\nStreamlit Elements\\n\\nCreate a draggable and resizable dashboard in Streamlit. Created by @okls.\\n\\n```python\\nfrom streamlit_elements import elements, mui, html\\n\\nwith elements(\"new_element\"):\\n  mui.Typography(\"Hello world\")\\n```\\n\\nTags\\n\\nAdd tags to your Streamlit apps. Created by @gagan3012.\\n\\n```python\\nfrom streamlit_tags import st_tags\\n\\nst_tags(label=\\'# Enter Keywords:\\', text=\\'Press enter to add more\\', value=[\\'Zero\\', \\'One\\', \\'Two\\'],\\nsuggestions=[\\'five\\', \\'six\\', \\'seven\\', \\'eight\\', \\'nine\\', \\'three\\', \\'eleven\\', \\'ten\\', \\'four\\'], maxtags = 4, key=\\'1\\')\\n```\\n\\nStqdm\\n\\nThe simplest way to handle a progress bar in streamlit app. Created by @Wirg.\\n\\n```python\\nfrom stqdm import stqdm\\n\\nfor _ in stqdm(range(50)):\\n    sleep(0.5)\\n```\\n\\nTimeline\\n\\nDisplay a Timeline in Streamlit apps using TimelineJS. Created by @innerdoc.\\n\\n```python\\nfrom streamlit_timeline import timeline\\n\\nwith open(\\'example.json\\', \"r\") as f:\\n  timeline(f.read(), height=800)\\n```\\n\\nCamera input live\\n\\nAlternative for st.camera_input which returns the webcam images live. Created by @blackary.\\n\\n```python\\nfrom camera_input_live import camera_input_live\\n\\nimage = camera_input_live()\\nst.image(value)\\n```\\n\\nStreamlit Ace\\n\\nAce editor component for Streamlit. Created by @okld.\\n\\n```python\\nfrom streamlit_ace import st_ace\\n\\ncontent = st_ace()\\ncontent\\n```\\n\\nStreamlit Chat\\n\\nStreamlit Component for a Chatbot UI. Created by @AI-Yash.\\n\\n```python\\nfrom streamlit_chat import message\\n\\nmessage(\"My message\")\\nmessage(\"Hello bot!\", is_user=True)  # align\\'s the message to the right\\n```\\n\\nStreamlit Option Menu\\n\\nSelect a single item from a list of options in a menu. Created by @victoryhb.\\n\\n```python\\nfrom streamlit_option_menu import option_menu\\n\\noption_menu(\"Main Menu\", [\"Home\", \\'Settings\\'],\\n  icons=[\\'house\\', \\'gear\\'], menu_icon=\"cast\", default_index=1)\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.stoggle import stoggle\\n\\nstoggle(\\n    \"Click me!\", \"\"\"🥷 Surprise! Here\\'s some additional content\"\"\",)\\n```\\n\\nMedia elements\\n\\nImage\\n\\nDisplay an image or list of images.\\n\\npython\\nst.image(numpy_array)\\nst.image(image_bytes)\\nst.image(file)\\nst.image(\"https://example.com/myimage.jpg\")\\n\\nAudio\\n\\nDisplay an audio player.\\n\\npython\\nst.audio(numpy_array)\\nst.audio(audio_bytes)\\nst.audio(file)\\nst.audio(\"https://example.com/myaudio.mp3\", format=\"audio/mp3\")\\n\\nVideo\\n\\nDisplay a video player.\\n\\npython\\nst.video(numpy_array)\\nst.video(video_bytes)\\nst.video(file)\\nst.video(\"https://example.com/myvideo.mp4\", format=\"video/mp4\")\\n\\nStreamlit Webrtc\\n\\nHandling and transmitting real-time video/audio streams with Streamlit. Created by @whitphx.\\n\\n```python\\nfrom streamlit_webrtc import webrtc_streamer\\n\\nwebrtc_streamer(key=\"sample\")\\n```\\n\\nDrawable Canvas\\n\\nProvides a sketching canvas using Fabric.js. Created by @andfanilo.\\n\\n```python\\nfrom streamlit_drawable_canvas import st_canvas\\n\\nst_canvas(fill_color=\"rgba(255, 165, 0, 0.3)\", stroke_width=stroke_width, stroke_color=stroke_color, background_color=bg_color, background_image=Image.open(bg_image) if bg_image else None, update_streamlit=realtime_update, height=150, drawing_mode=drawing_mode, point_display_radius=point_display_radius if drawing_mode == \\'point\\' else 0, key=\"canvas\",)\\n```\\n\\nImage Comparison\\n\\nCompare images with a slider using JuxtaposeJS. Created by @fcakyon.\\n\\n```python\\nfrom streamlit_image_comparison import image_comparison\\n\\nimage_comparison(img1=\"image1.jpg\", img2=\"image2.jpg\",)\\n```\\n\\nStreamlit Cropper\\n\\nA simple image cropper for Streamlit. Created by @turner-anderson.\\n\\n```python\\nfrom streamlit_cropper import st_cropper\\n\\nst_cropper(img, realtime_update=realtime_update, box_color=box_color, aspect_ratio=aspect_ratio)\\n```\\n\\nImage Coordinates\\n\\nGet the coordinates of clicks on an image. Created by @blackary.\\n\\n```python\\nfrom streamlit_image_coordinates import streamlit_image_coordinates\\n\\nstreamlit_image_coordinates(\"https://placekitten.com/200/300\")\\n```\\n\\nStreamlit Lottie\\n\\nIntegrate Lottie animations inside your Streamlit app. Created by @andfanilo.\\n\\n```python\\nlottie_hello = load_lottieurl(\"https://assets5.lottiefiles.com/packages/lf20_V9t630.json\")\\n\\nst_lottie(lottie_hello, key=\"hello\")\\n```\\n\\nLayouts and containers\\n\\nSidebar\\n\\nDisplay items in a sidebar.\\n\\npython\\nst.sidebar.write(\"This lives in the sidebar\")\\nst.sidebar.button(\"Click me!\")\\n\\nColumns\\n\\nInsert containers laid out as side-by-side columns.\\n\\npython\\ncol1, col2 = st.columns(2)\\ncol1.write(\"this is column 1\")\\ncol2.write(\"this is column 2\")\\n\\nTabs\\n\\nInsert containers separated into tabs.\\n\\npython\\ntab1, tab2 = st.tabs([\"Tab 1\", \"Tab2\"])\\ntab1.write(\"this is tab 1\")\\ntab2.write(\"this is tab 2\")\\n\\nExpander\\n\\nInsert a multi-element container that can be expanded/collapsed.\\n\\npython\\nwith st.expander(\"Open to see more\"):\\n  st.write(\"This is more content\")\\n\\nContainer\\n\\nInsert a multi-element container.\\n\\npython\\nc = st.container()\\nst.write(\"This will show last\")\\nc.write(\"This will show first\")\\nc.write(\"This will show second\")\\n\\nEmpty\\n\\nInsert a single-element container.\\n\\npython\\nc = st.empty()\\nst.write(\"This will show last\")\\nc.write(\"This will be replaced\")\\nc.write(\"This will show first\")\\n\\nStreamlit Elements\\n\\nCreate a draggable and resizable dashboard in Streamlit. Created by @okls.\\n\\n```python\\nfrom streamlit_elements import elements, mui, html\\n\\nwith elements(\"new_element\"):\\n  mui.Typography(\"Hello world\")\\n```\\n\\nPydantic\\n\\nAuto-generate Streamlit UI from Pydantic Models and Dataclasses. Created by @lukasmasuch.\\n\\n```python\\nimport streamlit_pydantic as sp\\n\\nsp.pydantic_form(key=\"my_form\",\\n  model=ExampleModel)\\n```\\n\\nStreamlit Pages\\n\\nAn experimental version of Streamlit Multi-Page Apps. Created by @blackary.\\n\\n```python\\nfrom st_pages import Page, show_pages, add_page_title\\n\\nshow_pages([ Page(\"streamlit_app.py\", \"Home\", \"🏠\"),\\n  Page(\"other_pages/page2.py\", \"Page 2\", \":books:\"), ])\\n```\\n\\nChat elements\\n\\nStreamlit provides a few commands to help you build conversational apps. These chat elements are designed to be used in conjunction with each other, but you can also use them separately.\\n\\nst.chat_message lets you insert a chat message container into the app so you can display messages from the user or the app. Chat containers can contain other Streamlit elements, including charts, tables, text, and more. st.chat_input lets you display a chat input widget so the user can type in a message.\\n\\nChat message\\n\\nInsert a chat message container.\\n\\npython\\nimport numpy as np\\nwith st.chat_message(\"user\"):\\n    st.write(\"Hello 👋\")\\n    st.line_chart(np.random.randn(30, 3))\\n\\nChat input\\n\\nDisplay a chat input widget.\\n\\npython\\nprompt = st.chat_input(\"Say something\")\\nif prompt:\\n    st.write(f\"The user has sent: {prompt}\")\\n\\nDisplay progress and status\\n\\nProgress bar\\n\\nDisplay a progress bar.\\n\\npython\\nfor i in range(101):\\n  st.progress(i)\\n  do_something_slow()\\n\\nSpinner\\n\\nTemporarily displays a message while executing a block of code.\\n\\npython\\nwith st.spinner(\"Please wait...\"):\\n  do_something_slow()\\n\\nBalloons\\n\\nDisplay celebratory balloons!\\n\\n```python\\ndo_something()\\n\\nCelebrate when all done!\\n\\nst.balloons()\\n```\\n\\nSnowflakes\\n\\nDisplay celebratory snowflakes!\\n\\n```python\\ndo_something()\\n\\nCelebrate when all done!\\n\\nst.snow()\\n```\\n\\nError box\\n\\nDisplay error message.\\n\\npython\\nst.error(\"We encountered an error\")\\n\\nWarning box\\n\\nDisplay warning message.\\n\\npython\\nst.warning(\"Unable to fetch image. Skipping...\")\\n\\nInfo box\\n\\nDisplay an informational message.\\n\\npython\\nst.info(\"Dataset is updated every day at midnight.\")\\n\\nSuccess box\\n\\nDisplay a success message.\\n\\npython\\nst.success(\"Match found!\")\\n\\nException output\\n\\nDisplay an exception.\\n\\npython\\ne = RuntimeError(\"This is an exception of type RuntimeError\")\\nst.exception(e)\\n\\nStqdm\\n\\nThe simplest way to handle a progress bar in streamlit app. Created by @Wirg.\\n\\n```python\\nfrom stqdm import stqdm\\n\\nfor _ in stqdm(range(50)):\\n    sleep(0.5)\\n```\\n\\nCustom notification box\\n\\nA custom notification box with the ability to close it out. Created by @Socvest.\\n\\n```python\\nfrom streamlit_custom_notification_box import custom_notification_box\\n\\nstyles = {\\'material-icons\\':{\\'color\\': \\'red\\'}, \\'text-icon-link-close-container\\': {\\'box-shadow\\': \\'#3896de 0px 4px\\'}, \\'notification-text\\': {\\'\\':\\'\\'}, \\'close-button\\':{\\'\\':\\'\\'}, \\'link\\':{\\'\\':\\'\\'}}\\ncustom_notification_box(icon=\\'info\\', textDisplay=\\'We are almost done with your registration...\\', externalLink=\\'more info\\', url=\\'#\\', styles=styles, key=\"foo\")\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.let_it_rain import rain\\n\\nrain(emoji=\"🎈\", font_size=54,\\n  falling_speed=5, animation_length=\"infinite\",)\\n```\\n\\nControl flow\\n\\nForms\\n\\nCreate a form that batches elements together with a “Submit\" button.\\n\\npython\\nwith st.form(key=\\'my_form\\'):\\n    username = st.text_input(\"Username\")\\n    password = st.text_input(\"Password\")\\n    st.form_submit_button(\"Login\")\\n\\nStop execution\\n\\nStops execution immediately.\\n\\npython\\nst.stop()\\n\\nRerun script\\n\\nRerun the script immediately.\\n\\npython\\nst.experimental_rerun()\\n\\nAutorefresh\\n\\nForce a refresh without tying up a script. Created by @kmcgrady.\\n\\n```python\\nfrom streamlit_autorefresh import st_autorefresh\\n\\nst_autorefresh(interval=2000, limit=100,\\n  key=\"fizzbuzzcounter\")\\n```\\n\\nPydantic\\n\\nAuto-generate Streamlit UI from Pydantic Models and Dataclasses. Created by @lukasmasuch.\\n\\n```python\\nimport streamlit_pydantic as sp\\n\\nsp.pydantic_form(key=\"my_form\",\\n  model=ExampleModel)\\n```\\n\\nStreamlit Pages\\n\\nAn experimental version of Streamlit Multi-Page Apps. Created by @blackary.\\n\\n```python\\nfrom st_pages import Page, show_pages, add_page_title\\n\\nshow_pages([ Page(\"streamlit_app.py\", \"Home\", \"🏠\"),\\n  Page(\"other_pages/page2.py\", \"Page 2\", \":books:\"), ])\\n```\\n\\nDeveloper tools\\n\\nPandas Profiling\\n\\nPandas profiling component for Streamlit. Created by @okld.\\n\\n```python\\ndf = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\\npr = df.profile_report()\\n\\nst_profile_report(pr)\\n```\\n\\nStreamlit Ace\\n\\nAce editor component for Streamlit. Created by @okld.\\n\\n```python\\nfrom streamlit_ace import st_ace\\n\\ncontent = st_ace()\\ncontent\\n```\\n\\nStreamlit Anaylitics\\n\\nTrack & visualize user interactions with your streamlit app. Created by @jrieke.\\n\\n```python\\nimport streamlit_analytics\\n\\nwith streamlit_analytics.track():\\n    st.text_input(\"Write something\")\\n```\\n\\nUtilities\\n\\nSet page title, favicon, and more\\n\\nConfigures the default settings of the page.\\n\\npython\\nst.set_page_config(\\n  page_title=\"My app\",\\n  page_icon=\":shark:\",\\n)\\n\\nEcho\\n\\nDisplay some code on the app, then execute it. Useful for tutorials.\\n\\npython\\nwith st.echo():\\n  st.write(\\'This code will be printed\\')\\n\\nGet help\\n\\nDisplay object’s doc string, nicely formatted.\\n\\npython\\nst.help(st.write)\\nst.help(pd.DataFrame)\\n\\nGet query parameters\\n\\nReturn the query parameters that are currently showing in the browser\\'s URL bar.\\n\\npython\\nst.experimental_get_query_params()\\n\\nSet query parameters\\n\\nSet the query parameters that are shown in the browser\\'s URL bar.\\n\\npython\\nst.experimental_set_query_params(\\n  show_map=True,\\n  selected=[\"asia\"]\\n)\\n\\nMutate charts\\n\\nAdd rows\\n\\nAppend a dataframe to the bottom of the current one in certain elements, for optimized data updates.\\n\\npython\\nelement = st.line_chart(df)\\nelement.add_rows(df_with_extra_rows)\\n\\nState management\\n\\nSession state\\n\\nSession state is a way to share variables between reruns, for each user session.\\n\\npython\\nst.session_state[\\'key\\'] = value\\n\\nConnections and databases\\n\\nAuthenticator\\n\\nA secure authentication module to validate user credentials. Created by @mkhorasani.\\n\\n```python\\nimport streamlit_authenticator as stauth\\n\\nauthenticator = stauth.Authenticate( config[\\'credentials\\'], config[\\'cookie\\'][\\'name\\'],\\nconfig[\\'cookie\\'][\\'key\\'], config[\\'cookie\\'][\\'expiry_days\\'], config[\\'preauthorized\\'])\\n```\\n\\nWS localStorage\\n\\nA simple synchronous way of accessing localStorage from your app. Created by @gagangoku.\\n\\n```python\\nfrom streamlit_ws_localstorage import injectWebsocketCode\\n\\nret = conn.setLocalStorageVal(key=\\'k1\\', val=\\'v1\\')\\nst.write(\\'ret: \\' + ret)\\n```\\n\\nStreamlit Auth0\\n\\nThe fastest way to provide comprehensive login inside Streamlit. Created by @conradbez.\\n\\n```python\\nfrom auth0_component import login_button\\n\\nuser_info = login_button(clientId, domain = domain)\\nst.write(user_info)\\n```\\n\\nPerformance\\n\\nCache data\\n\\nFunction decorator to cache functions that return data (e.g. dataframe transforms, database queries, ML inference).\\n\\npython\\n@st.cache_data\\ndef long_function(param1, param2):\\n  # Perform expensive computation here or\\n  # fetch data from the web here\\n  return data\\n\\nCache resource\\n\\nFunction decorator to cache functions that return global resources (e.g. database connections, ML models).\\n\\npython\\n@st.cache_resource\\ndef init_model():\\n  # Return a global resource here\\n  return pipeline(\\n    \"sentiment-analysis\",\\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\\n  )\\n\\nClear cached data\\n\\nClear all in-memory and on-disk data caches.\\n\\n```python\\n@st.cache_data\\ndef long_function(param1, param2):\\n  # Perform expensive computation here or\\n  # fetch data from the web here\\n  return data\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all cache_data functions\\n  st.cache_data.clear()\\n```\\n\\nClear cached resources\\n\\nClear all st.cache_resource caches.\\n\\n```python\\n@st.cache_resource\\ndef init_model():\\n  # Return a global resource here\\n  return pipeline(\\n    \"sentiment-analysis\",\\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\\n  )\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all cache_resource functions\\n  st.cache_data.clear()\\n```\\n\\nConnections and databases\\n\\nSetup your connection\\n\\nCreate a connection\\n\\nConnect to a data source or API\\n\\npython\\nconn = st.experimental_connection(\\'pets_db\\', type=\\'sql\\')\\npet_owners = conn.query(\\'select * from pet_owners\\')\\nst.dataframe(pet_owners)\\n\\nBuilt-in connections\\n\\nSQLConnection\\n\\nA connection to a SQL database using SQLAlchemy.\\n\\npython\\nconn = st.experimental_connection(\\'sql\\')\\n\\nSnowparkConnection\\n\\nA connection to Snowflake Snowpark.\\n\\npython\\nconn = st.experimental_connection(\\'snowpark\\')\\n\\nThird-party connections\\n\\nConnection base class\\n\\nBuild your own connection with ExperimentalBaseConnection.\\n\\npython\\nclass MyConnection(ExperimentalBaseConnection[myconn.MyConnection]):\\n    def _connect(self, **kwargs) -> MyConnection:\\n        return myconn.connect(**self._secrets, **kwargs)\\n    def query(self, query):\\n        return self._instance.query(query)\\n\\nPersonalization\\n\\nUser info\\n\\nst.experimental_user returns information about the logged-in user of private apps on Streamlit Community Cloud.\\n\\npython\\nif st.experimental_user.email == \"foo@corp.com\":\\n  st.write(\"Welcome back, \", st.experimental_user.email)\\nelse:\\n  st.write(\"You are not authorized to view this page.\")', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='title: st.table\\nslug: /library/api-reference/data/st.table\\ndescription: st.table displays a static table.\\n\\nStatic tables with st.table are the most basic way to display dataframes. For the majority of cases, we recommend using st.dataframe to display interactive dataframes, and st.data_editor to let users edit dataframes.', metadata={'source': 'docs/content/library/api/data/table.md'}),\n",
       " Document(page_content='title: st.json\\nslug: /library/api-reference/data/st.json\\ndescription: st.json displays object or string as a pretty-printed JSON string.', metadata={'source': 'docs/content/library/api/data/json.md'}),\n",
       " Document(page_content='title: Data elements\\nslug: /library/api-reference/data\\n\\nData elements\\n\\nWhen you\\'re working with data, it is extremely valuable to visualize that\\ndata quickly, interactively, and from multiple different angles. That\\'s what\\nStreamlit is actually built and optimized for.\\n\\nYou can display data via charts, and you can display it in\\nraw form. These are the Streamlit commands you can use to display and interact with raw data.\\n\\nDataframes\\n\\nDisplay a dataframe as an interactive table.\\n\\npython\\nst.dataframe(my_data_frame)\\n\\nData editor\\n\\nDisplay a data editor widget.\\n\\npython\\nedited = st.data_editor(df, num_rows=\"dynamic\")\\n\\nColumn configuration\\n\\nConfigure the display and editing behavior of dataframes and data editors.\\n\\npython\\nst.column_config.NumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nStatic tables\\n\\nDisplay a static table.\\n\\npython\\nst.table(my_data_frame)\\n\\nMetrics\\n\\nDisplay a metric in big bold font, with an optional indicator of how the metric changed.\\n\\npython\\nst.metric(\"My metric\", 42, 2)\\n\\nDicts and JSON\\n\\nDisplay object or string as a pretty-printed JSON string.\\n\\npython\\nst.json(my_dict)\\n\\nStreamlit Aggrid\\n\\nImplementation of Ag-Grid component for Streamlit. Created by @PablocFonseca.\\n\\n```python\\ndf = pd.DataFrame({\\'col1\\': [1, 2, 3], \\'col2\\': [4, 5, 6]})\\ngrid_return = AgGrid(df, editable=True)\\n\\nnew_df = grid_return[\\'data\\']\\n```\\n\\nStreamlit Folium\\n\\nStreamlit Component for rendering Folium maps. Created by @randyzwitch.\\n\\n```python\\nm = folium.Map(location=[39.949610, -75.150282], zoom_start=16)\\nfolium.Marker([39.949610, -75.150282], popup=\"Liberty Bell\", tooltip=\"Liberty Bell\").add_to(m)\\n\\nst_data = st_folium(m, width=725)\\n```\\n\\nPandas Profiling\\n\\nPandas profiling component for Streamlit. Created by @okld.\\n\\n```python\\ndf = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\\npr = df.profile_report()\\n\\nst_profile_report(pr)\\n```\\n\\nImage Coordinates\\n\\nGet the coordinates of clicks on an image. Created by @blackary.\\n\\n```python\\nfrom streamlit_image_coordinates import streamlit_image_coordinates\\nvalue = streamlit_image_coordinates(\"https://placekitten.com/200/300\")\\n\\nst.write(value)\\n```\\n\\nPlotly Events\\n\\nMake Plotly charts interactive!. Created by @null-jones.\\n\\n```python\\nfrom streamlit_plotly_events import plotly_events\\nfig = px.line(x=[1], y=[1])\\n\\nselected_points = plotly_events(fig)\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.metric_cards import style_metric_cards\\ncol3.metric(label=\"No Change\", value=5000, delta=0)\\n\\nstyle_metric_cards()\\n```', metadata={'source': 'docs/content/library/api/data/data.md'}),\n",
       " Document(page_content='title: st.column_config.LineChartColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.linechartcolumn', metadata={'source': 'docs/content/library/api/data/column_config/linechartcolumn.md'}),\n",
       " Document(page_content='title: st.dataframe\\nslug: /library/api-reference/data/st.dataframe\\ndescription: st.dataframe displays a dataframe as an interactive table.\\n\\nThis page only contains information on the st.dataframe API. For a deeper dive into working with dataframes read Dataframes. If you want to let users interactively edit dataframes, check out st.data_editor.\\n\\nst.dataframe supports the use_container_width parameter to stretch across the full container width:\\n\\n```python\\nimport pandas as pd\\nimport streamlit as st\\n\\nCache the dataframe so it\\'s only loaded once\\n\\n@st.cache_data\\ndef load_data():\\n    return pd.DataFrame(\\n        {\\n            \"first column\": [1, 2, 3, 4],\\n            \"second column\": [10, 20, 30, 40],\\n        }\\n    )\\n\\nBoolean to resize the dataframe, stored as a session state variable\\n\\nst.checkbox(\"Use container width\", value=False, key=\"use_container_width\")\\n\\ndf = load_data()\\n\\nDisplay the dataframe and allow the user to stretch the dataframe\\n\\nacross the full width of the container, based on the checkbox value\\n\\nst.dataframe(df, use_container_width=st.session_state.use_container_width)\\n```\\n\\nColumn configuration\\n\\nWhen working with data in Streamlit, the st.column_config class is a powerful tool for configuring data display and interaction. Specifically designed for the column_config parameter in st.dataframe and st.data_editor, it provides a suite of methods to tailor your columns to various data types - from simple text and numbers to lists, URLs, images, and more.\\n\\nWhether it\\'s translating temporal data into user-friendly formats or utilizing charts and progress bars for clearer data visualization, column configuration not only provides the user with an enriched data viewing experience but also ensures that you\\'re equipped with the tools to present and interact with your data, just the way you want it.\\n\\nColumn\\n\\nConfigure a generic column.\\n\\npython\\nColumn(\"Streamlit Widgets\", width=\"medium\", help=\"Streamlit **widget** commands 🎈\")\\n\\nText column\\n\\nConfigure a text column.\\n\\npython\\nTextColumn(\"Widgets\", max_chars=50, validate=\"^st\\\\.[a-z_]+$\")\\n\\nNumber column\\n\\nConfigure a number column.\\n\\npython\\nNumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nCheckbox column\\n\\nConfigure a checkbox column.\\n\\npython\\nCheckboxColumn(\"Your favorite?\", help=\"Select your **favorite** widgets\")\\n\\nSelectbox column\\n\\nConfigure a selectbox column.\\n\\npython\\nSelectboxColumn(\"App Category\", options=[\"🤖 LLM\", \"📈 Data Viz\"])\\n\\nDatetime column\\n\\nConfigure a datetime column.\\n\\npython\\nDatetimeColumn(\"Appointment\", min_value=datetime(2023, 6, 1), format=\"D MMM YYYY, h:mm a\")\\n\\nDate column\\n\\nConfigure a date column.\\n\\npython\\nDateColumn(\"Birthday\", max_value=date(2005, 1, 1), format=\"DD.MM.YYYY\")\\n\\nTime column\\n\\nConfigure a time column.\\n\\npython\\nTimeColumn(\"Appointment\", min_value=time(8, 0, 0), format=\"hh:mm a\")\\n\\nList column\\n\\nConfigure a list column.\\n\\npython\\nListColumn(\"Sales (last 6 months)\", width=\"medium\")\\n\\nLink column\\n\\nConfigure a link column.\\n\\npython\\nLinkColumn(\"Trending apps\", max_chars=100, validate=\"^https://.*$\")\\n\\nImage column\\n\\nConfigure an image column.\\n\\npython\\nImageColumn(\"Preview Image\", help=\"The preview screenshots\")\\n\\nLine chart column\\n\\nConfigure a line chart column.\\n\\npython\\nLineChartColumn(\"Sales (last 6 months)\" y_min=0, y_max=100)\\n\\nBar chart column\\n\\nConfigure a bar chart column.\\n\\npython\\nBarChartColumn(\"Marketing spend\" y_min=0, y_max=100)\\n\\nProgress column\\n\\nConfigure a progress column.\\n\\npython\\nProgressColumn(\"Sales volume\", min_value=0, max_value=1000, format=\"$%f\")\\n\\nInteractivity\\n\\nDataframes displayed as interactive tables with st.dataframe have the following interactive features:\\n\\nColumn sorting: sort columns by clicking on their headers.\\n\\nColumn resizing: resize columns by dragging and dropping column header borders.\\n\\nTable (height, width) resizing: resize tables by dragging and dropping the bottom right corner of tables.\\n\\nSearch: search through data by clicking a table, using hotkeys (⌘ Cmd + F or Ctrl + F) to bring up the search bar, and using the search bar to filter data.\\n\\nCopy to clipboard: select one or multiple cells, copy them to clipboard, and paste them into your favorite spreadsheet software.', metadata={'source': 'docs/content/library/api/data/dataframe.md'}),\n",
       " Document(page_content='title: st.column_config.DateColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.datecolumn', metadata={'source': 'docs/content/library/api/data/column_config/datecolumn.md'}),\n",
       " Document(page_content='title: st.data_editor\\nslug: /library/api-reference/data/st.data_editor\\ndescription: st.data_editor display a data editor widget that allows you to edit dataframes and many other data structures in a table-like UI.\\n\\nThis page only contains information on the st.data_editor API. For a deeper dive into working with dataframes and the data editor\\'s capabilities and limitations, read Dataframes.\\n\\nColumn configuration\\n\\nWhen working with data in Streamlit, the st.column_config class is a powerful tool for configuring data display and interaction. Specifically designed for the column_config parameter in st.dataframe and st.data_editor, it provides a suite of methods to tailor your columns to various data types - from simple text and numbers to lists, URLs, images, and more.\\n\\nWhether it\\'s translating temporal data into user-friendly formats or utilizing charts and progress bars for clearer data visualization, column configuration not only provides the user with an enriched data viewing experience but also ensures that you\\'re equipped with the tools to present and interact with your data, just the way you want it.\\n\\nColumn\\n\\nConfigure a generic column.\\n\\npython\\nColumn(\"Streamlit Widgets\", width=\"medium\", help=\"Streamlit **widget** commands 🎈\")\\n\\nText column\\n\\nConfigure a text column.\\n\\npython\\nTextColumn(\"Widgets\", max_chars=50, validate=\"^st\\\\.[a-z_]+$\")\\n\\nNumber column\\n\\nConfigure a number column.\\n\\npython\\nNumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nCheckbox column\\n\\nConfigure a checkbox column.\\n\\npython\\nCheckboxColumn(\"Your favorite?\", help=\"Select your **favorite** widgets\")\\n\\nSelectbox column\\n\\nConfigure a selectbox column.\\n\\npython\\nSelectboxColumn(\"App Category\", options=[\"🤖 LLM\", \"📈 Data Viz\"])\\n\\nDatetime column\\n\\nConfigure a datetime column.\\n\\npython\\nDatetimeColumn(\"Appointment\", min_value=datetime(2023, 6, 1), format=\"D MMM YYYY, h:mm a\")\\n\\nDate column\\n\\nConfigure a date column.\\n\\npython\\nDateColumn(\"Birthday\", max_value=date(2005, 1, 1), format=\"DD.MM.YYYY\")\\n\\nTime column\\n\\nConfigure a time column.\\n\\npython\\nTimeColumn(\"Appointment\", min_value=time(8, 0, 0), format=\"hh:mm a\")\\n\\nList column\\n\\nConfigure a list column.\\n\\npython\\nListColumn(\"Sales (last 6 months)\", width=\"medium\")\\n\\nLink column\\n\\nConfigure a link column.\\n\\npython\\nLinkColumn(\"Trending apps\", max_chars=100, validate=\"^https://.*$\")\\n\\nImage column\\n\\nConfigure an image column.\\n\\npython\\nImageColumn(\"Preview Image\", help=\"The preview screenshots\")\\n\\nLine chart column\\n\\nConfigure a line chart column.\\n\\npython\\nLineChartColumn(\"Sales (last 6 months)\" y_min=0, y_max=100)\\n\\nBar chart column\\n\\nConfigure a bar chart column.\\n\\npython\\nBarChartColumn(\"Marketing spend\" y_min=0, y_max=100)\\n\\nProgress column\\n\\nConfigure a progress column.\\n\\npython\\nProgressColumn(\"Sales volume\", min_value=0, max_value=1000, format=\"$%f\")', metadata={'source': 'docs/content/library/api/data/data_editor.md'}),\n",
       " Document(page_content='title: st.column_config.BarChartColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.barchartcolumn', metadata={'source': 'docs/content/library/api/data/column_config/barchartcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.TimeColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.timecolumn', metadata={'source': 'docs/content/library/api/data/column_config/timecolumn.md'}),\n",
       " Document(page_content='title: st.column_config.NumberColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.numbercolumn', metadata={'source': 'docs/content/library/api/data/column_config/numbercolumn.md'}),\n",
       " Document(page_content='title: st.column_config.DatetimeColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.datetimecolumn', metadata={'source': 'docs/content/library/api/data/column_config/datetimecolumn.md'}),\n",
       " Document(page_content='title: st.column_config.CheckboxColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.checkboxcolumn', metadata={'source': 'docs/content/library/api/data/column_config/checkboxcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.TextColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.textcolumn', metadata={'source': 'docs/content/library/api/data/column_config/textcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.ImageColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.imagecolumn', metadata={'source': 'docs/content/library/api/data/column_config/imagecolumn.md'}),\n",
       " Document(page_content='title: st.column_config.SelectboxColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.selectboxcolumn', metadata={'source': 'docs/content/library/api/data/column_config/selectboxcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.Column\\nslug: /library/api-reference/data/st.column_config/st.column_config.column', metadata={'source': 'docs/content/library/api/data/column_config/column.md'}),\n",
       " Document(page_content='title: st.column_config.ListColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.listcolumn', metadata={'source': 'docs/content/library/api/data/column_config/listcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.ProgressColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.progresscolumn', metadata={'source': 'docs/content/library/api/data/column_config/progresscolumn.md'}),\n",
       " Document(page_content='title: st.column_config.LinkColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.linkcolumn', metadata={'source': 'docs/content/library/api/data/column_config/linkcolumn.md'}),\n",
       " Document(page_content='title: st.markdown\\nslug: /library/api-reference/text/st.markdown\\ndescription: st.markdown displays string formatted as Markdown.', metadata={'source': 'docs/content/library/api/text/markdown.md'}),\n",
       " Document(page_content='title: Personalize apps for the user\\nslug: /library/api-reference/personalization\\n\\nPersonalize apps for the user\\n\\nUser info\\n\\nst.experimental_user returns information about the logged-in user of private apps on Streamlit Community Cloud.\\n\\npython\\nif st.experimental_user.email == \"foo@corp.com\":\\n  st.write(\"Welcome back, \", st.experimental_user.email)\\nelse:\\n  st.write(\"You are not authorized to view this page.\")', metadata={'source': 'docs/content/library/api/personalization/personalization.md'}),\n",
       " Document(page_content='title: st.caption\\nslug: /library/api-reference/text/st.caption\\ndescription: st.caption displays text in small font.', metadata={'source': 'docs/content/library/api/text/caption.md'}),\n",
       " Document(page_content='title: st.code\\nslug: /library/api-reference/text/st.code\\ndescription: st.code displays a code block with optional syntax highlighting.', metadata={'source': 'docs/content/library/api/text/code.md'}),\n",
       " Document(page_content='title: st.column_config\\nslug: /library/api-reference/data/st.column_config\\n\\nColumn configuration\\n\\nWhen working with data in Streamlit, the st.column_config class is a powerful tool for configuring data display and interaction. Specifically designed for the column_config parameter in st.dataframe and st.data_editor, it provides a suite of methods to tailor your columns to various data types - from simple text and numbers to lists, URLs, images, and more.\\n\\nWhether it\\'s translating temporal data into user-friendly formats or utilizing charts and progress bars for clearer data visualization, column configuration not only provides the user with an enriched data viewing experience but also ensures that you\\'re equipped with the tools to present and interact with your data, just the way you want it.\\n\\nColumn\\n\\nConfigure a generic column.\\n\\npython\\nColumn(\"Streamlit Widgets\", width=\"medium\", help=\"Streamlit **widget** commands 🎈\")\\n\\nText column\\n\\nConfigure a text column.\\n\\npython\\nTextColumn(\"Widgets\", max_chars=50, validate=\"^st\\\\.[a-z_]+$\")\\n\\nNumber column\\n\\nConfigure a number column.\\n\\npython\\nNumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nCheckbox column\\n\\nConfigure a checkbox column.\\n\\npython\\nCheckboxColumn(\"Your favorite?\", help=\"Select your **favorite** widgets\")\\n\\nSelectbox column\\n\\nConfigure a selectbox column.\\n\\npython\\nSelectboxColumn(\"App Category\", options=[\"🤖 LLM\", \"📈 Data Viz\"])\\n\\nDatetime column\\n\\nConfigure a datetime column.\\n\\npython\\nDatetimeColumn(\"Appointment\", min_value=datetime(2023, 6, 1), format=\"D MMM YYYY, h:mm a\")\\n\\nDate column\\n\\nConfigure a date column.\\n\\npython\\nDateColumn(\"Birthday\", max_value=date(2005, 1, 1), format=\"DD.MM.YYYY\")\\n\\nTime column\\n\\nConfigure a time column.\\n\\npython\\nTimeColumn(\"Appointment\", min_value=time(8, 0, 0), format=\"hh:mm a\")\\n\\nList column\\n\\nConfigure a list column.\\n\\npython\\nListColumn(\"Sales (last 6 months)\", width=\"medium\")\\n\\nLink column\\n\\nConfigure a link column.\\n\\npython\\nLinkColumn(\"Trending apps\", max_chars=100, validate=\"^https://.*$\")\\n\\nImage column\\n\\nConfigure an image column.\\n\\npython\\nImageColumn(\"Preview Image\", help=\"The preview screenshots\")\\n\\nLine chart column\\n\\nConfigure a line chart column.\\n\\npython\\nLineChartColumn(\"Sales (last 6 months)\" y_min=0, y_max=100)\\n\\nBar chart column\\n\\nConfigure a bar chart column.\\n\\npython\\nBarChartColumn(\"Marketing spend\" y_min=0, y_max=100)\\n\\nProgress column\\n\\nConfigure a progress column.\\n\\npython\\nProgressColumn(\"Sales volume\", min_value=0, max_value=1000, format=\"$%f\")', metadata={'source': 'docs/content/library/api/data/column_config/index.md'}),\n",
       " Document(page_content='title: st.subheader\\nslug: /library/api-reference/text/st.subheader\\ndescription: st.subheader displays text in subheader formatting.', metadata={'source': 'docs/content/library/api/text/subheader.md'}),\n",
       " Document(page_content='title: st.header\\nslug: /library/api-reference/text/st.header\\ndescription: st.header displays text in header formatting.', metadata={'source': 'docs/content/library/api/text/header.md'}),\n",
       " Document(page_content='title: st.title\\nslug: /library/api-reference/text/st.title\\ndescription: st.title displays text in title formatting.', metadata={'source': 'docs/content/library/api/text/title.md'}),\n",
       " Document(page_content='title: st.latex\\nslug: /library/api-reference/text/st.latex\\ndescription: st.latex displays mathematical expressions formatted as LaTeX.', metadata={'source': 'docs/content/library/api/text/latex.md'}),\n",
       " Document(page_content='title: st.text\\nslug: /library/api-reference/text/st.text\\ndescription: st.text writes fixed-width and preformatted text.', metadata={'source': 'docs/content/library/api/text/text.md'}),\n",
       " Document(page_content='title: st.divider\\nslug: /library/api-reference/text/st.divider\\ndescription: st.divider displays a horizontal rule in your app.\\n\\nHere\\'s what it looks like in action when you have multiple elements in the app:\\n\\n```python\\nimport streamlit as st\\n\\nst.write(\"This is some text.\")\\n\\nst.slider(\"This is a slider\", 0, 100, (25, 75))\\n\\nst.divider()  # 👈 Draws a horizontal rule\\n\\nst.write(\"This text is between the horizontal rules.\")\\n\\nst.divider()  # 👈 Another horizontal rule\\n```', metadata={'source': 'docs/content/library/api/text/divider.md'}),\n",
       " Document(page_content='title: Text elements\\nslug: /library/api-reference/text\\n\\nText elements\\n\\nStreamlit apps usually start with a call to st.title to set the\\napp\\'s title. After that, there are 2 heading levels you can use:\\nst.header and st.subheader.\\n\\nPure text is entered with st.text, and Markdown with\\nst.markdown.\\n\\nWe also offer a \"swiss-army knife\" command called st.write, which accepts\\nmultiple arguments, and multiple data types. And as described above, you can\\nalso use magic commands in place of st.write.\\n\\nMarkdown\\n\\nDisplay string formatted as Markdown.\\n\\npython\\nst.markdown(\"Hello **world**!\")\\n\\nTitle\\n\\nDisplay text in title formatting.\\n\\npython\\nst.title(\"The app title\")\\n\\nHeader\\n\\nDisplay text in header formatting.\\n\\npython\\nst.header(\"This is a header\")\\n\\nSubheader\\n\\nDisplay text in subheader formatting.\\n\\npython\\nst.subheader(\"This is a subheader\")\\n\\nCaption\\n\\nDisplay text in small font.\\n\\npython\\nst.caption(\"This is written small caption text\")\\n\\nCode block\\n\\nDisplay a code block with optional syntax highlighting.\\n\\npython\\nst.code(\"a = 1234\")\\n\\nPreformatted text\\n\\nWrite fixed-width and preformatted text.\\n\\npython\\nst.text(\"Hello world\")\\n\\nLaTeX\\n\\nDisplay mathematical expressions formatted as LaTeX.\\n\\npython\\nst.latex(\"\\\\int a x^2 \\\\,dx\")\\n\\nDivider\\n\\nDisplay a horizontal rule.\\n\\npython\\nst.divider()\\n\\nAnnotated text\\n\\nDisplay annotated text in Streamlit apps. Created by @tvst.\\n\\npython\\nannotated_text(\"This \", (\"is\", \"verb\"), \" some \", (\"annotated\", \"adj\"), (\"text\", \"noun\"), \" for those of \", (\"you\", \"pronoun\"), \" who \", (\"like\", \"verb\"), \" this sort of \", (\"thing\", \"noun\"), \".\")\\n\\nDrawable Canvas\\n\\nProvides a sketching canvas using Fabric.js. Created by @andfanilo.\\n\\npython\\nst_canvas(fill_color=\"rgba(255, 165, 0, 0.3)\", stroke_width=stroke_width, stroke_color=stroke_color, background_color=bg_color, background_image=Image.open(bg_image) if bg_image else None, update_streamlit=realtime_update, height=150, drawing_mode=drawing_mode, point_display_radius=point_display_radius if drawing_mode == \\'point\\' else 0, key=\"canvas\",)\\n\\nTags\\n\\nAdd tags to your Streamlit apps. Created by @gagan3012.\\n\\npython\\nst_tags(label=\\'# Enter Keywords:\\', text=\\'Press enter to add more\\', value=[\\'Zero\\', \\'One\\', \\'Two\\'], suggestions=[\\'five\\', \\'six\\', \\'seven\\', \\'eight\\', \\'nine\\', \\'three\\', \\'eleven\\', \\'ten\\', \\'four\\'], maxtags = 4, key=\\'1\\')\\n\\nNLU\\n\\nApply text mining on a dataframe. Created by @JohnSnowLabs.\\n\\npython\\nnlu.load(\\'sentiment\\').predict(\\'I love NLU! <3\\')\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\npython\\nmention(label=\"An awesome Streamlit App\", icon=\"streamlit\",  url=\"https://extras.streamlit.app\",)', metadata={'source': 'docs/content/library/api/text/text-elements.md'}),\n",
       " Document(page_content='title: st.experimental_user\\nslug: /library/api-reference/personalization/st.experimental_user\\ndescription: st.experimental_user returns information about the logged-in user of private apps on Streamlit Community Cloud.\\n\\nst.experimental_user\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nst.experimental_user is a Streamlit command that returns information about the logged-in user on Streamlit Community Cloud. It allows developers to personalize apps for the user viewing the app. In a private Streamlit Community Cloud app, it returns a dictionary with the viewer\\'s email. This value of this field is empty in a public Streamlit Community Cloud app to prevent leaking user emails to developers.\\n\\nThe API closely resembles that of st.session_state and st.secrets. It follows a field-based API, which is very similar to Python dictionaries.\\n\\nAllowed fields\\n\\nThe st.experimental_user command returns a dictionary with only one field: email.\\n\\nDisplay the allowed field by passing the command to st.write:\\n\\n```python\\n\\nDisplay the contents of the dictionary\\n\\nst.write(st.experimental_user)\\n```\\n\\nThe above displays a dict with one field and value. The field is always email:\\n\\njson\\n{\\n  \"email\": \"value\"\\n}\\n\\nYou can check if a field exists in st.experimental_user:\\n\\n```python\\n\\nReturns True if the field exists\\n\\n\"email\" in st.experimental_user\\n\\nReturns False if the field does not exist\\n\\n\"name\" in st.experimental_user\\n```\\n\\nRead values\\n\\nRead the value of the email field and display it by passing to st.write:\\n\\n```python\\n\\nDictionary like API\\n\\nst.write(st.experimental_user[\\'email\\'])\\n\\nAttribute API\\n\\nst.write(st.experimental_user.email)\\n```\\n\\nThe above outputs either None or the logged-in user\\'s email or test@localhost.com, depending on where the app is running. Read further to learn about st.experimental_user\\'s context-dependent behavior.\\n\\nUpdates and modifications\\n\\nKeys and values for st.experimental_user cannot be updated or modified. Streamlit throws a handy StreamlitAPIException exception if you try to update them:\\n\\n```python\\nst.experimental_user.name = None\\n\\nThrows an exception!\\n\\nst.experimental_user.email = \"hello\"\\n\\nThrows an exception!\\n\\n```\\n\\nContext-dependent behavior\\n\\nThe value of st.experimental_user.email is context-dependent. It returns a value depending on where the app is running. The private or public app can be running on Streamlit Community Cloud, locally, or on a 3rd party cloud provider. Let\\'s look at the different scenarios.\\n\\nPrivate app on Streamlit Community Cloud\\n\\nUsers need to be logged in to Streamlit Community Cloud to view private apps. If a user is not logged, they see:\\n\\nIf a user is logged in, st.experimental_user.email returns the user\\'s email. Suppose a user logged in to Streamlit Community Cloud using jane@email.com:\\n\\n```python\\nst.experimental_user.email\\n\\nReturns: jane@email.com\\n\\n```\\n\\nPublic app on Streamlit Community Cloud\\n\\nCurrently, st.experimental_user.email returns information about the logged-in user of private apps on Streamlit Community Cloud. If used in a public app, it returns None. For example:\\n\\n```python\\nst.experimental_user.email\\n\\nReturns: None\\n\\n```\\n\\nThis value of this field is empty in a public Streamlit Community Cloud app to prevent leaking user emails to developers.\\n\\nLocal development\\n\\nWhen developing locally, st.experimental_user.email returns test@localhost.com. We don\\'t return None to make it easier to locally test this functionality. For example:\\n\\n```python\\nst.experimental_user.email\\n\\nReturns: test@localhost.com\\n\\n```\\n\\nApp deployed on a 3rd party cloud provider\\n\\nWhen deploying an app on a 3rd party cloud provider (e.g. Amazon EC2, Heroku, etc), st.experimental_user.email behaves the same as during local development. For example:\\n\\n```python\\nst.experimental_user.email # On a 3rd party cloud provider\\n\\nReturns: test@localhost.com\\n\\n```\\n\\nExamples\\n\\nThe ability to personalize apps for the user viewing the app is a great way to make your app more engaging.\\n\\nIt unlocks a plethora of use-cases for developers, some of which could include: showing additional controls for admins, visualizing a user\\'s Streamlit history, a personalized stock ticker, a chatbot app, and much more. We\\'re excited to see what you build with this feature!\\n\\nHere\\'s a code snippet that shows extra buttons for admins:\\n\\n```python\\n\\nShow extra buttons for admin users.\\n\\nADMIN_USERS = {\\n    \\'person1@email.com\\',\\n    \\'person2@email.com\\',\\n    \\'person3@email.com\\'\\n}\\nif st.experimental_user.email in ADMIN_USERS:\\n    display_the_extra_admin_buttons()\\ndisplay_the_interface_everyone_sees()\\n```\\n\\nShow different content to users based on their email address:\\n\\n```python\\n\\nShow different content based on the user\\'s email address.\\n\\nif st.experimental_user.email == \\'jane@email.com\\':\\n    display_jane_content()\\nelif st.experimental_user.email == \\'adam@foocorp.io\\':\\n    display_adam_content()\\nelse:\\n    st.write(\"Please contact us to get access!\")\\n```\\n\\nGreet users with their name that\\'s stored in a database:\\n\\n```python\\n\\nGreet the user by their name.\\n\\nif st.experimental_user.email:\\n    # Get the user\\'s name from the database.\\n    name = get_name_from_db(st.experimental_user.email)\\n    st.write(\\'Hello, %s!\\' % name)\\n```\\n\\nCaveats and limitations\\n\\nst.experimental_user is read-only. You cannot update or modify its value. Doing so will throw a StreamlitAPIException.\\n\\nA valid email is returned only if the user is logged in to Streamlit Community Cloud and the app is private. Else, None or test@localhost.com is returned.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.', metadata={'source': 'docs/content/library/api/personalization/experimental-user.md'}),\n",
       " Document(page_content=\"title: Get started\\nslug: /library/get-started\\n\\nGet started\\n\\nThis Get Started guide explains how Streamlit works, how to install Streamlit on your preferred\\noperating system, and how to create your first Streamlit app!\\n\\nhelps you set up your virtual environment and walks you through installing Streamlit on Windows, macOS, and Linux. Regardless of which package management tool and OS you're using, we recommend running the commands on this page in a virtual environment.\\n  \\n  \\n    introduces you to Streamlit's data model and development flow. You'll learn what makes Streamlit the most powerful way to build data apps, including the ability to display and style data, draw charts and maps, add interactive widgets, customize app layouts, cache computation, and define themes.\\n  \\n  \\n    using Streamlit's core features to fetch and cache data, draw charts, plot information on a map, and use interactive widgets, like a slider, to filter results.\\n  \\n  \\n    teaches you how to add pages to your app, including how to define pages, structure and run multipage apps, and navigate between pages. Once you understand the basics, create your first multipage app based on the familiar streamlit hello command!\\n  \\n  {/\\n    to Streamlit Community Cloud. With Streamlit Community Cloud your data team can directly serve the needs of the rest of the company. Quickly go from data to app, from prototype to production. Share apps in one click and collaborate instantly with live code updates.\\n/}\", metadata={'source': 'docs/content/library/get-started/index.md'}),\n",
       " Document(page_content='title: Multipage apps\\nslug: /library/get-started/multipage-apps\\n\\nMultipage apps\\n\\nAs apps grow large, it becomes useful to organize them into multiple pages. This makes the app easier to manage as a developer and easier to navigate as a user. Streamlit provides a frictionless way to create multipage apps. Pages are automatically shown in a nice navigation widget inside the app sidebar, and clicking on a page will navigate to the page without reloading the frontend — making app browsing incredibly fast!\\n\\nWe created a \"single-page app\" to explore a public Uber dataset for pickups and drop-offs in New York City on the previous page. In this guide, let’s learn how to create multipage apps. Once we have a solid foundation on what it takes to create multipage apps, let’s build one for ourselves in the next section!\\n\\nStructuring multipage apps\\n\\nLet\\'s understand what it takes to create multipage apps — including how to define pages, structure and run multipage apps, and navigate between pages in the user interface. Once you\\'ve understood the basics, you can jump right into the next section to convert the familiar streamlit hello command into a multipage app!\\n\\nRun a multipage app\\n\\nRunning a multipage app is identical to running a single-page app. The command to run a multipage app is:\\n\\npython\\nstreamlit run [entrypoint file]\\n\\nThe \"entrypoint file\" is the first page the app will show to the user. Once you have added pages to your app, the entrypoint file appears as the top-most page in the sidebar. You can think of the entrypoint file as your app\\'s \"main page\". For example, say your entrypoint file is Home.py. Then, to run your app, you can run streamlit run Home.py. This will start your app and execute the code in Home.py.\\n\\nAdding pages\\n\\nOnce you\\'ve created your entrypoint file, you can add pages by creating .py files in a pages/ directory relative to your entrypoint file. For example, if your entrypoint file is Home.py, then you can create a pages/About.py file to define the \"About\" page. Here\\'s a valid directory structure for a multipage app:\\n\\nHome.py # This is the file you run with \"streamlit run\"\\n└─── pages/\\n  └─── About.py # This is a page\\n  └─── 2_Page_two.py # This is another page\\n  └─── 3_😎_three.py # So is this\\n\\nWhen adding emojis to filenames, it’s best practice to include a numbered-prefix to make autocompletion in your terminal easier. Terminal-autocomplete can get confused by unicode (which is how emojis are represented).\\n\\nsection below. For example, the\\n\\nOnly .py files in the pages/ directory will be loaded as pages. Streamlit ignores all other files in the pages/ directory and subdirectories.\\n\\nHow pages are labeled and sorted in the UI\\n\\nPage labels in the sidebar UI are generated from filenames. They may differ from the page title set in st.set_page_config. Let\\'s learn what constitutes a valid filename for a page, how pages are displayed in the sidebar, and how pages are sorted.\\n\\nValid filenames for pages\\n\\nFilenames are composed of four different parts:\\n\\nA number — if the file is prefixed with a number.\\n\\nA separator — could be _, -, space, or any combination thereof.\\n\\nA label — which is everything up to, but not including, .py.\\n\\nThe extension — which is always .py.\\n\\nHow pages are displayed in the sidebar\\n\\nWhat is displayed in the sidebar is the label part of the filename:\\n\\nIf there\\'s no label, Streamlit uses the number as the label.\\n\\nIn the UI, Streamlit beautifies the label by replacing _ with space.\\n\\nHow pages are sorted in the sidebar\\n\\nSorting considers numbers in the filename to be actual numbers (integers):\\n\\nFiles that have a number appear before files without a number.\\n\\nFiles are sorted based on the number (if any), followed by the title (if any).\\n\\nWhen files are sorted, Streamlit treats the number as an actual number rather than a string. So 03 is the same as 3.\\n\\nThis table shows examples of filenames and their corresponding labels, sorted by the order in which they appear in the sidebar.\\n\\nExamples:\\n\\nEmojis can be used to make your page names more fun! For example, a file named 🏠_Home.py will create a page titled \"🏠 Home\" in the sidebar.\\n\\nNavigating between pages\\n\\nPages are automatically shown in a nice navigation UI inside the app\\'s sidebar. When you click on a page in the sidebar UI, Streamlit navigates to that page without reloading the entire frontend — making app browsing incredibly fast!\\n\\nYou can also navigate between pages using URLs. Pages have their own URLs, defined by the file\\'s label. When multiple files have the same label, Streamlit picks the first one (based on the ordering described above). Users can view a specific page by visiting the page\\'s URL.\\n\\nIf a user tries to access a URL for a page that does not exist, they will see a modal like the one below, saying the user has requested a page that was not found in the app’s pages/ directory.\\n\\nNotes\\n\\nPages support magic commands.\\n\\nPages support run-on-save. Additionally, when you save a page, this causes a rerun for users currently viewing that exact page.\\n\\nAdding or deleting a page causes the UI to update immediately.\\n\\nUpdating pages in the sidebar does not rerun the script.\\n\\nst.set_page_config works at the page level. When you set a title or favicon using st.set_page_config, this applies to the current page only.\\n\\nPages share the same Python modules globally:\\n\\n```python\\n  # page1.py\\n  import foo\\n  foo.hello = 123\\n\\n# page2.py\\n  import foo\\n  st.write(foo.hello)  # If page1 already executed, this should write 123\\n  ```\\n\\nPages share the same st.session_state:\\n\\n```python\\n  # page1.py\\n  import streamlit as st\\n  if \"shared\" not in st.session_state:\\n     st.session_state[\"shared\"] = True\\n\\n# page2.py\\n  import streamlit as st\\n  st.write(st.session_state[\"shared\"])\\n  # If page1 already executed, this should write True\\n  ```\\n\\nYou now have a solid understanding of multipage apps. You\\'ve learned how to structure apps, define pages, and navigate between pages in the user interface. It\\'s time to create your first multipage app! 🥳', metadata={'source': 'docs/content/library/get-started/multipage-apps/index.md'}),\n",
       " Document(page_content='title: Installation\\nslug: /library/get-started/installation\\n\\nInstall Streamlit\\n\\nTable of contents\\n\\nPrerequisites\\n\\nInstall Streamlit on Windows\\n\\nInstall Streamlit on macOS/Linux\\n\\nPrerequisites\\n\\nBefore you get started, you\\'re going to need a few things:\\n\\nYour favorite IDE or text editor\\n\\nPython 3.8 - Python 3.11\\n\\nPIP\\n\\nSet up your virtual environment\\n\\nRegardless of which package management tool you\\'re using, we recommend running\\nthe commands on this page in a virtual environment. This ensures that the dependencies\\npulled in for Streamlit don\\'t impact any other Python projects\\nyou\\'re working on.\\n\\nBelow are a few tools you can use for environment management:\\n\\nvenv\\n\\npipenv\\n\\npoetry\\n\\nvirtualenv\\n\\nconda\\n\\nInstall Streamlit on Windows\\n\\nStreamlit\\'s officially-supported environment manager on Windows is Anaconda Navigator.\\n\\nInstall Anaconda\\n\\nIf you don\\'t have Anaconda install yet, follow the steps provided on the Anaconda installation page.\\n\\nCreate a new environment with Streamlit\\n\\nNext you\\'ll need to set up your environment.\\n\\nFollow the steps provided by Anaconda to set up and manage your environment using the Anaconda Navigator.\\n\\nSelect the \"▶\" icon next to your new environment. Then select \"Open terminal\":\\n\\nIn the terminal that appears, type:\\n\\nbash\\n   pip install streamlit\\n\\nTest that the installation worked:\\n\\nbash\\n   streamlit hello\\n\\nStreamlit\\'s Hello app should appear in a new tab in your web browser!\\n\\nUse your new environment\\n\\nIn Anaconda Navigator, open a terminal in your environment (see step 2 above).\\n\\nIn the terminal that appears, use Streamlit as usual:\\n\\nbash\\n   streamlit run myfile.py\\n\\nInstall Streamlit on macOS/Linux\\n\\nStreamlit\\'s officially-supported package manager and environment manager for macOS and Linux are pip and venv, respectively. venv is a part of The Python Standard Library and comes bundled with your installation of Python. See instructions on how to install and use pip below.\\n\\nInstall pip\\n\\nInstall pip. More details about installing pip can be found in pip\\'s documentation.\\n\\nOn a macOS:\\n\\nbash\\npython -m ensurepip --upgrade\\n\\nOn Ubuntu with Python 3:\\n\\nbash\\nsudo apt-get install python3-pip\\n\\nFor other Linux distributions, see How to install PIP for Python.\\n\\nInstall Xcode command line tools on macOS\\n\\nOn macOS, you\\'ll need to install Xcode command line tools. They are required to compile some of Streamlit\\'s Python dependencies during installation. To install Xcode command line tools, run:\\n\\nbash\\nxcode-select --install\\n\\nCreate a new environment with Streamlit\\n\\nNavigate to your project folder:\\n\\nbash\\n   cd myproject\\n\\nCreate a new virtual environment in that folder and activate that environment:\\n\\nbash\\n   python -m venv .venv\\n\\nWhen you run the command above, a directory called .venv will appear in myproject/. This directory is where your virtual environment and its dependencies are installed.\\n\\nInstall Streamlit in your environment:\\n\\nbash\\n   pip install streamlit\\n\\nTest that the installation worked:\\n\\nbash\\n   streamlit hello\\n\\nStreamlit\\'s Hello app should appear in a new tab in your web browser!\\n\\nUse your new environment\\n\\nAny time you want to use the new environment, you first need to go to your project folder (where the .venv directory lives) and run:\\n\\nbash\\n   source .venv/bin/activate\\n\\nNow you can use Python and Streamlit as usual:\\n\\nbash\\n   streamlit run myfile.py\\n\\nTo stop the Streamlit server, press ctrl-C.\\n\\nWhen you\\'re done using this environment, type deactivate to return to your normal shell.\\n\\nNow that you\\'ve installed Streamlit, take a few minutes to read through Main concepts to understand Streamlit\\'s data flow model.', metadata={'source': 'docs/content/library/get-started/installation.md'}),\n",
       " Document(page_content='title: Create a multipage app\\nslug: /library/get-started/multipage-apps/create-a-multipage-app\\n\\nCreate a multipage app\\n\\nIn the last section, we learned what it takes to create multipage apps, including how to define pages, structure and run multipage apps, and navigate between pages in the user interface. If you need a refresher, now is a good time to take a look.\\n\\nIn this guide, let’s put our understanding of multipage apps to use by converting the familiar streamlit hello command to a multipage app!\\n\\nMotivation\\n\\nBefore Streamlit 1.10.0, the streamlit hello command was a large single-page app. As there was no support for multiple pages, we resorted to splitting the app\\'s content using st.selectbox in the sidebar to choose what content to run. The content is comprised of three demos for plotting, mapping, and dataframes.\\n\\nHere\\'s what the code and single-page app looked like:\\n\\nNotice how large the file is! Each app “page\" is written as a function, and the selectbox is used to pick which page to display. As our app grows, maintaining the code requires a lot of additional overhead. Moreover, we’re limited by the st.selectbox UI to choose which “page\" to run, we cannot customize individual page titles with st.set_page_config, and we’re unable to navigate between pages using URLs.\\n\\nConvert an existing app into a multipage app\\n\\nNow that we\\'ve identified the limitations of a single-page app, what can we do about it? Armed with our knowledge from the previous section, we can convert the existing app to be a multipage app, of course! At a high level, we need to perform the following steps:\\n\\nCreate a new pages folder in the same folder where the “entrypoint file\" (hello.py) lives\\n\\nRename our entrypoint file to Hello.py , so that the title in the sidebar is capitalized\\n\\nCreate three new files inside of pages:\\n\\npages/1_📈_Plotting_Demo.py\\n\\npages/2_🌍_Mapping_Demo.py\\n\\npages/3_📊_DataFrame_Demo.py\\n\\nMove the contents of the plotting_demo, mapping_demo, and data_frame_demo functions into their corresponding new files from Step 3\\n\\nRun streamlit run Hello.py to view your newly converted multipage app!\\n\\nNow, let’s walk through each step of the process and view the corresponding changes in code.\\n\\nCreate the entrypoint file\\n\\nWe rename our entrypoint file to Hello.py , so that the title in the sidebar is capitalized and only the code for the intro page is included. Additionally, we’re able to customize the page title and favicon — as it appears in the browser tab with st.set_page_config. We can do so for each of our pages too!\\n\\nNotice how the sidebar does not contain page labels as we haven’t created any pages yet.\\n\\nCreate multiple pages\\n\\nA few things to remember here:\\n\\nWe can change the ordering of pages in our MPA by adding numbers to the beginning of each Python file. If we add a 1 to the front of our file name, Streamlit will put that file first in the list.\\n\\nThe name of each Streamlit app is determined by the file name, so to change the app name you need to change the file name!\\n\\nWe can add some fun to our app by adding emojis to our file names that will render in our Streamlit app.\\n\\nEach page will have its own URL, defined by the name of the file.\\n\\nCheck out how we do all this below! For each new page, we create a new file inside the pages folder, and add the appropriate demo code into it.\\n\\nWith our additional pages created, we can now put it all together in the final step below.\\n\\nRun the multipage app\\n\\nTo run your newly converted multipage app, run:\\n\\nbash\\nstreamlit run Hello.py\\n\\nThat’s it! The Hello.py script now corresponds to the main page of your app, and other scripts that Streamlit finds in the pages folder will also be present in the new page selector that appears in the sidebar.\\n\\nNext steps\\n\\nCongratulations! 🎉 If you\\'ve read this far, chances are you\\'ve learned to create both single-page and multipage apps. Where you go from here is entirely up to your creativity! We’re excited to see what you’ll build now that adding additional pages to your apps is easier than ever. Try adding more pages to the app we\\'ve just built as an exercise. Also, stop by the forum to show off your multipage apps with the Streamlit community! 🎈\\n\\nHere are a few resources to help you get started:\\n\\nDeploy your app for free on Streamlit\\'s Community Cloud.\\n\\nPost a question or share your multipage app on our community forum.\\n\\nCheck out our documentation on multipage apps.\\n\\nRead through Advanced features for things like caching, theming, and adding statefulness to apps.\\n\\nBrowse our API reference for examples of every Streamlit command.', metadata={'source': 'docs/content/library/get-started/multipage-apps/create-a-multi-page-app.md'}),\n",
       " Document(page_content='title: Create an app\\nslug: /library/get-started/create-an-app\\n\\nCreate an app\\n\\nIf you\\'ve made it this far, chances are you\\'ve\\ninstalled Streamlit and\\nrun through the basics in our Main concepts guide. If\\nnot, now is a good time to take a look.\\n\\nThe easiest way to learn how to use Streamlit is to try things out yourself. As you read through this guide, test each method. As long as your app is running, every time you add a new element to your script and save, Streamlit\\'s UI will ask if you\\'d like to rerun the app and view the changes. This allows you to work in a fast interactive loop: you write some code, save it, review the output, write some more, and so on, until you\\'re happy with the results. The goal is to use Streamlit to create an interactive app for your data or model and along the way to use Streamlit to review, debug, perfect, and share your code.\\n\\nIn this guide, you\\'re going to use Streamlit\\'s core features to\\ncreate an interactive app; exploring a public Uber dataset for pickups and\\ndrop-offs in New York City. When you\\'re finished, you\\'ll know how to fetch\\nand cache data, draw charts, plot information on a map, and use interactive\\nwidgets, like a slider, to filter results.\\n\\nIf you\\'d like to skip ahead and see everything at once, the complete script\\nis available below.\\n\\nCreate your first app\\n\\nStreamlit is more than just a way to make data apps, it’s also a community of creators that share their apps and ideas and help each other make their work better. Please come join us on the community forum. We love to hear your questions, ideas, and help you work through your bugs — stop by today!\\n\\nThe first step is to create a new Python script. Let\\'s call it\\n   uber_pickups.py.\\n\\nOpen uber_pickups.py in your favorite IDE or text editor, then add these\\n   lines:\\n\\npython\\n   import streamlit as st\\n   import pandas as pd\\n   import numpy as np\\n\\nEvery good app has a title, so let\\'s add one:\\n\\npython\\n   st.title(\\'Uber pickups in NYC\\')\\n\\nNow it\\'s time to run Streamlit from the command line:\\n\\nbash\\n   streamlit run uber_pickups.py\\n\\nRunning a Streamlit app is no different than any other Python script. Whenever you need to view the app, you can use this command.\\n\\nDid you know you can also pass a URL to streamlit run? This is great when combined with GitHub Gists. For example:\\n\\nbash\\n   streamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n\\nAs usual, the app should automatically open in a new tab in your\\n   browser.\\n\\nFetch some data\\n\\nNow that you have an app, the next thing you\\'ll need to do is fetch the Uber\\ndataset for pickups and drop-offs in New York City.\\n\\nLet\\'s start by writing a function to load the data. Add this code to your\\n   script:\\n\\n```python\\n   DATE_COLUMN = \\'date/time\\'\\n   DATA_URL = (\\'https://s3-us-west-2.amazonaws.com/\\'\\n            \\'streamlit-demo-data/uber-raw-data-sep14.csv.gz\\')\\n\\ndef load_data(nrows):\\n       data = pd.read_csv(DATA_URL, nrows=nrows)\\n       lowercase = lambda x: str(x).lower()\\n       data.rename(lowercase, axis=\\'columns\\', inplace=True)\\n       data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])\\n       return data\\n   ```\\n\\nYou\\'ll notice that load_data is a plain old function that downloads some\\n   data, puts it in a Pandas dataframe, and converts the date column from text\\n   to datetime. The function accepts a single parameter (nrows), which\\n   specifies the number of rows that you want to load into the dataframe.\\n\\nNow let\\'s test the function and review the output. Below your function, add\\n   these lines:\\n\\npython\\n   # Create a text element and let the reader know the data is loading.\\n   data_load_state = st.text(\\'Loading data...\\')\\n   # Load 10,000 rows of data into the dataframe.\\n   data = load_data(10000)\\n   # Notify the reader that the data was successfully loaded.\\n   data_load_state.text(\\'Loading data...done!\\')\\n\\nYou\\'ll see a few buttons in the upper-right corner of your app asking if\\n   you\\'d like to rerun the app. Choose Always rerun, and you\\'ll see your\\n   changes automatically each time you save.\\n\\nOk, that\\'s underwhelming...\\n\\nIt turns out that it takes a long time to download data, and load 10,000 lines\\ninto a dataframe. Converting the date column into datetime isn’t a quick job\\neither. You don’t want to reload the data each time the app is updated –\\nluckily Streamlit allows you to cache the data.\\n\\nEffortless caching\\n\\nTry adding @st.cache_data before the load_data declaration:\\n\\npython\\n   @st.cache_data\\n   def load_data(nrows):\\n\\nThen save the script, and Streamlit will automatically rerun your app. Since\\n   this is the first time you’re running the script with @st.cache_data, you won\\'t\\n   see anything change. Let’s tweak your file a little bit more so that you can\\n   see the power of caching.\\n\\nReplace the line data_load_state.text(\\'Loading data...done!\\') with this:\\n\\npython\\n   data_load_state.text(\"Done! (using st.cache_data)\")\\n\\nNow save. See how the line you added appeared immediately? If you take a\\n   step back for a second, this is actually quite amazing. Something magical is\\n   happening behind the scenes, and it only takes one line of code to activate\\n   it.\\n\\nHow\\'s it work?\\n\\nLet\\'s take a few minutes to discuss how @st.cache_data actually works.\\n\\nWhen you mark a function with Streamlit’s cache annotation, it tells Streamlit\\nthat whenever the function is called that it should check two things:\\n\\nThe input parameters you used for the function call.\\n\\nThe code inside the function.\\n\\nIf this is the first time Streamlit has seen both these items, with these exact\\nvalues, and in this exact combination, it runs the function and stores the\\nresult in a local cache. The next time the function is called, if the two\\nvalues haven\\'t changed, then Streamlit knows it can skip executing the function\\naltogether. Instead, it reads the output from the local cache and passes it on\\nto the caller -- like magic.\\n\\n\"But, wait a second,\" you’re saying to yourself, \"this sounds too good to be\\ntrue. What are the limitations of all this awesomesauce?\"\\n\\nWell, there are a few:\\n\\nStreamlit will only check for changes within the current working directory.\\n   If you upgrade a Python library, Streamlit\\'s cache will only notice this if\\n   that library is installed inside your working directory.\\n\\nIf your function is not deterministic (that is, its output depends on random\\n   numbers), or if it pulls data from an external time-varying source (for\\n   example, a live stock market ticker service) the cached value will be\\n   none-the-wiser.\\n\\nLastly, you should avoid mutating the output of a function cached with st.cache_data since cached\\n   values are stored by reference.\\n\\nWhile these limitations are important to keep in mind, they tend not to be an\\nissue a surprising amount of the time. Those times, this cache is really\\ntransformational.\\n\\nWhenever you have a long-running computation in your code, consider\\nrefactoring it so you can use @st.cache_data, if possible. Please read Caching for more details.\\n\\nNow that you know how caching with Streamlit works, let’s get back to the Uber\\npickup data.\\n\\nInspect the raw data\\n\\nIt\\'s always a good idea to take a look at the raw data you\\'re working with\\nbefore you start working with it. Let\\'s add a subheader and a printout of the\\nraw data to the app:\\n\\npython\\nst.subheader(\\'Raw data\\')\\nst.write(data)\\n\\nIn the Main concepts guide you learned that\\nst.write will render almost anything you pass\\nto it. In this case, you\\'re passing in a dataframe and it\\'s rendering as an\\ninteractive table.\\n\\nst.write tries to do the right thing based on\\nthe data type of the input. If it isn\\'t doing what you expect you can use a\\nspecialized command like st.dataframe\\ninstead. For a full list, see API reference.\\n\\nDraw a histogram\\n\\nNow that you\\'ve had a chance to take a look at the dataset and observe what\\'s\\navailable, let\\'s take things a step further and draw a histogram to see what\\nUber\\'s busiest hours are in New York City.\\n\\nTo start, let\\'s add a subheader just below the raw data section:\\n\\npython\\n   st.subheader(\\'Number of pickups by hour\\')\\n\\nUse NumPy to generate a histogram that breaks down pickup times binned by\\n   hour:\\n\\npython\\n   hist_values = np.histogram(\\n       data[DATE_COLUMN].dt.hour, bins=24, range=(0,24))[0]\\n\\nNow, let\\'s use Streamlit\\'s\\n   st.bar_chart() method to draw this\\n   histogram.\\n\\npython\\n   st.bar_chart(hist_values)\\n\\nSave your script. This histogram should show up in your app right away.\\n   After a quick review, it looks like the busiest time is 17:00 (5 P.M.).\\n\\nTo draw this diagram we used Streamlit\\'s native bar_chart() method, but it\\'s\\nimportant to know that Streamlit supports more complex charting libraries like\\nAltair, Bokeh, Plotly, Matplotlib and more. For a full list, see\\nsupported charting libraries.\\n\\nPlot data on a map\\n\\nUsing a histogram with Uber\\'s dataset helped us determine what the busiest\\ntimes are for pickups, but what if we wanted to figure out where pickups were\\nconcentrated throughout the city. While you could use a bar chart to show this\\ndata, it wouldn\\'t be easy to interpret unless you were intimately familiar with\\nlatitudinal and longitudinal coordinates in the city. To show pickup\\nconcentration, let\\'s use Streamlit st.map()\\nfunction to overlay the data on a map of New York City.\\n\\nAdd a subheader for the section:\\n\\npython\\n   st.subheader(\\'Map of all pickups\\')\\n\\nUse the st.map() function to plot the data:\\n\\npython\\n   st.map(data)\\n\\nSave your script. The map is fully interactive. Give it a try by panning or\\n   zooming in a bit.\\n\\nAfter drawing your histogram, you determined that the busiest hour for Uber\\npickups was 17:00. Let\\'s redraw the map to show the concentration of pickups\\nat 17:00.\\n\\nLocate the following code snippet:\\n\\npython\\n   st.subheader(\\'Map of all pickups\\')\\n   st.map(data)\\n\\nReplace it with:\\n\\npython\\n   hour_to_filter = 17\\n   filtered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]\\n   st.subheader(f\\'Map of all pickups at {hour_to_filter}:00\\')\\n   st.map(filtered_data)\\n\\nYou should see the data update instantly.\\n\\nTo draw this map we used the st.map function that\\'s built into Streamlit, but\\nif you\\'d like to visualize complex map data, we encourage you to take a look at\\nthe st.pydeck_chart.\\n\\nFilter results with a slider\\n\\nIn the last section, when you drew the map, the time used to filter results was\\nhardcoded into the script, but what if we wanted to let a reader dynamically\\nfilter the data in real time? Using Streamlit\\'s widgets you can. Let\\'s add a\\nslider to the app with the st.slider() method.\\n\\nLocate hour_to_filter and replace it with this code snippet:\\n\\npython\\n   hour_to_filter = st.slider(\\'hour\\', 0, 23, 17)  # min: 0h, max: 23h, default: 17h\\n\\nUse the slider and watch the map update in real time.\\n\\nUse a button to toggle data\\n\\nSliders are just one way to dynamically change the composition of your app.\\nLet\\'s use the st.checkbox function to add a\\ncheckbox to your app. We\\'ll use this checkbox to show/hide the raw data\\ntable at the top of your app.\\n\\nLocate these lines:\\n\\npython\\n   st.subheader(\\'Raw data\\')\\n   st.write(data)\\n\\nReplace these lines with the following code:\\n\\npython\\n   if st.checkbox(\\'Show raw data\\'):\\n       st.subheader(\\'Raw data\\')\\n       st.write(data)\\n\\nWe\\'re sure you\\'ve got your own ideas. When you\\'re done with this tutorial, check out all the widgets that Streamlit exposes in our API Reference.\\n\\nLet\\'s put it all together\\n\\nThat\\'s it, you\\'ve made it to the end. Here\\'s the complete script for our interactive app.\\n\\nIf you\\'ve skipped ahead, after you\\'ve created your script, the command to run\\nStreamlit is streamlit run [app name].\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport numpy as np\\n\\nst.title(\\'Uber pickups in NYC\\')\\n\\nDATE_COLUMN = \\'date/time\\'\\nDATA_URL = (\\'https://s3-us-west-2.amazonaws.com/\\'\\n            \\'streamlit-demo-data/uber-raw-data-sep14.csv.gz\\')\\n\\n@st.cache_data\\ndef load_data(nrows):\\n    data = pd.read_csv(DATA_URL, nrows=nrows)\\n    lowercase = lambda x: str(x).lower()\\n    data.rename(lowercase, axis=\\'columns\\', inplace=True)\\n    data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])\\n    return data\\n\\ndata_load_state = st.text(\\'Loading data...\\')\\ndata = load_data(10000)\\ndata_load_state.text(\"Done! (using st.cache_data)\")\\n\\nif st.checkbox(\\'Show raw data\\'):\\n    st.subheader(\\'Raw data\\')\\n    st.write(data)\\n\\nst.subheader(\\'Number of pickups by hour\\')\\nhist_values = np.histogram(data[DATE_COLUMN].dt.hour, bins=24, range=(0,24))[0]\\nst.bar_chart(hist_values)\\n\\nSome number in the range 0-23\\n\\nhour_to_filter = st.slider(\\'hour\\', 0, 23, 17)\\nfiltered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]\\n\\nst.subheader(\\'Map of all pickups at %s:00\\' % hour_to_filter)\\nst.map(filtered_data)\\n```\\n\\nShare your app\\n\\nAfter you’ve built a Streamlit app, it\\'s time to share it! To show it off to the world you can use Streamlit Community Cloud to deploy, manage, and share your app for free.\\n\\nIt works in 3 simple steps:\\n\\nPut your app in a public GitHub repo (and make sure it has a requirements.txt!)\\n\\nSign into share.streamlit.io\\n\\nClick \\'Deploy an app\\' and then paste in your GitHub URL\\n\\nThat\\'s it! 🎈 You now have a publicly deployed app that you can share with the world. Click to learn more about how to use Streamlit Community Cloud.\\n\\nGet help\\n\\nThat\\'s it for getting started, now you can go and build your own apps! If you\\nrun into difficulties here are a few things you can do.\\n\\nCheck out our community forum and post a question\\n\\nQuick help from command line with streamlit help\\n\\nGo through our Knowledge Base for tips, step-by-step tutorials, and articles that answer your questions about creating and deploying Streamlit apps.\\n\\nRead more documentation! Check out:\\n\\nAdvanced features for things like caching, theming, and adding statefulness to apps.\\n\\nAPI reference for examples of every Streamlit command.', metadata={'source': 'docs/content/library/get-started/create-an-app.md'}),\n",
       " Document(page_content='title: Main concepts\\nslug: /library/get-started/main-concepts\\n\\nMain concepts\\n\\nWorking with Streamlit is simple. First you sprinkle a few Streamlit commands\\ninto a normal Python script, then you run it with streamlit run:\\n\\nbash\\nstreamlit run your_script.py [-- script args]\\n\\nAs soon as you run the script as shown above, a local Streamlit server will\\nspin up and your app will open in a new tab in your default web browser. The app\\nis your canvas, where you\\'ll draw charts, text, widgets, tables, and more.\\n\\nWhat gets drawn in the app is up to you. For example\\nst.text writes raw text to your app, and\\nst.line_chart draws — you guessed it — a\\nline chart. Refer to our API documentation to see all commands that\\nare available to you.\\n\\nWhen passing your script some custom arguments, they must be passed after two dashes. Otherwise the\\narguments get interpreted as arguments to Streamlit itself.\\n\\nAnother way of running Streamlit is to run it as a Python module. This can be\\nuseful when configuring an IDE like PyCharm to work with Streamlit:\\n\\n```bash\\n\\nRunning\\n\\npython -m streamlit run your_script.py\\n\\nis equivalent to:\\n\\nstreamlit run your_script.py\\n```\\n\\nYou can also pass a URL to streamlit run! This is great when combined with\\nGitHub Gists. For example:\\n\\nbash\\nstreamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n\\nDevelopment flow\\n\\nEvery time you want to update your app, save the source file. When you do\\nthat, Streamlit detects if there is a change and asks you whether you want to\\nrerun your app. Choose \"Always rerun\" at the top-right of your screen to\\nautomatically update your app every time you change its source code.\\n\\nThis allows you to work in a fast interactive loop: you type some code, save\\nit, try it out live, then type some more code, save it, try it out, and so on\\nuntil you\\'re happy with the results. This tight loop between coding and viewing\\nresults live is one of the ways Streamlit makes your life easier.\\n\\nWhile developing a Streamlit app, it\\'s recommended to lay out your editor and\\nbrowser windows side by side, so the code and the app can be seen at the same\\ntime. Give it a try!\\n\\nAs of Streamlit version 1.10.0 and higher, Streamlit apps cannot be run from the root directory of Linux distributions. If you try to run a Streamlit app from the root directory, Streamlit will throw a FileNotFoundError: [Errno 2] No such file or directory error. For more information, see GitHub issue #5239.\\n\\nIf you are using Streamlit version 1.10.0 or higher, your main script should live in a directory other than the root directory. When using Docker, you can use the WORKDIR command to specify the directory where your main script lives. For an example of how to do this, read Create a Dockerfile.\\n\\nData flow\\n\\nStreamlit\\'s architecture allows you to write apps the same way you write plain\\nPython scripts. To unlock this, Streamlit apps have a unique data flow: any\\ntime something must be updated on the screen, Streamlit reruns your entire\\nPython script from top to bottom.\\n\\nThis can happen in two situations:\\n\\nWhenever you modify your app\\'s source code.\\n\\nWhenever a user interacts with widgets in the app. For example, when dragging\\n  a slider, entering text in an input box, or clicking a button.\\n\\nWhenever a callback is passed to a widget via the on_change (or on_click) parameter, the callback will always run before the rest of your script. For details on the Callbacks API, please refer to our Session State API Reference Guide.\\n\\nAnd to make all of this fast and seamless, Streamlit does some heavy lifting\\nfor you behind the scenes. A big player in this story is the\\n@st.cache_data decorator, which allows developers to skip certain\\ncostly computations when their apps rerun. We\\'ll cover caching later in this\\npage.\\n\\nDisplay and style data\\n\\nThere are a few ways to display data (tables, arrays, data frames) in Streamlit\\napps. Below, you will be introduced to magic\\nand st.write(), which can be used to write\\nanything from text to tables. After that, let\\'s take a look at methods designed\\nspecifically for visualizing data.\\n\\nUse magic\\n\\nYou can also write to your app without calling any Streamlit methods.\\nStreamlit supports \"magic commands,\" which means you don\\'t have to use\\nst.write() at all! To see this in action try this snippet:\\n\\n```python\\n\"\"\"\\n\\nMy first app\\n\\nHere\\'s our first attempt at using data to create a table:\\n\"\"\"\\n\\nimport streamlit as st\\nimport pandas as pd\\ndf = pd.DataFrame({\\n  \\'first column\\': [1, 2, 3, 4],\\n  \\'second column\\': [10, 20, 30, 40]\\n})\\n\\ndf\\n```\\n\\nAny time that Streamlit sees a variable or a literal\\nvalue on its own line, it automatically writes that to your app using\\nst.write(). For more information, refer to the\\ndocumentation on magic commands.\\n\\nWrite a data frame\\n\\nAlong with magic commands,\\nst.write() is Streamlit\\'s \"Swiss Army knife\". You\\ncan pass almost anything to st.write():\\ntext, data, Matplotlib figures, Altair charts, and more. Don\\'t worry, Streamlit\\nwill figure it out and render things the right way.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nst.write(\"Here\\'s our first attempt at using data to create a table:\")\\nst.write(pd.DataFrame({\\n    \\'first column\\': [1, 2, 3, 4],\\n    \\'second column\\': [10, 20, 30, 40]\\n}))\\n```\\n\\nThere are other data specific functions like\\nst.dataframe() and\\nst.table() that you can also use for displaying\\ndata. Let\\'s understand when to use these features and how to add colors and styling to your data frames.\\n\\nYou might be asking yourself, \"why wouldn\\'t I always use st.write()?\" There are\\na few reasons:\\n\\nMagic and st.write() inspect the type of\\n   data that you\\'ve passed in, and then decide how to best render it in the\\n   app. Sometimes you want to draw it another way. For example, instead of\\n   drawing a dataframe as an interactive table, you may want to draw it as a\\n   static table by using st.table(df).\\n\\nThe second reason is that other methods return an object that can be used\\n   and modified, either by adding data to it or replacing it.\\n\\nFinally, if you use a more specific Streamlit method you can pass additional\\n   arguments to customize its behavior.\\n\\nFor example, let\\'s create a data frame and change its formatting with a Pandas\\nStyler object. In this example, you\\'ll use Numpy to generate a random sample,\\nand the st.dataframe() method to draw an\\ninteractive table.\\n\\nThis example uses Numpy to generate a random sample, but you can use Pandas\\nDataFrames, Numpy arrays, or plain Python arrays.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\n\\ndataframe = np.random.randn(10, 20)\\nst.dataframe(dataframe)\\n```\\n\\nLet\\'s expand on the first example using the Pandas Styler object to highlight\\nsome elements in the interactive table.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\ndataframe = pd.DataFrame(\\n    np.random.randn(10, 20),\\n    columns=(\\'col %d\\' % i for i in range(20)))\\n\\nst.dataframe(dataframe.style.highlight_max(axis=0))\\n```\\n\\nStreamlit also has a method for static table generation:\\nst.table().\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\ndataframe = pd.DataFrame(\\n    np.random.randn(10, 20),\\n    columns=(\\'col %d\\' % i for i in range(20)))\\nst.table(dataframe)\\n```\\n\\nDraw charts and maps\\n\\nStreamlit supports several popular data charting libraries like Matplotlib,\\nAltair, deck.gl, and more. In this section, you\\'ll\\nadd a bar chart, line chart, and a map to your app.\\n\\nDraw a line chart\\n\\nYou can easily add a line chart to your app with\\nst.line_chart(). We\\'ll generate a random\\nsample using Numpy and then chart it.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\nchart_data = pd.DataFrame(\\n     np.random.randn(20, 3),\\n     columns=[\\'a\\', \\'b\\', \\'c\\'])\\n\\nst.line_chart(chart_data)\\n```\\n\\nPlot a map\\n\\nWith st.map() you can display data points on a map.\\nLet\\'s use Numpy to generate some sample data and plot it on a map of\\nSan Francisco.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\nmap_data = pd.DataFrame(\\n    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],\\n    columns=[\\'lat\\', \\'lon\\'])\\n\\nst.map(map_data)\\n```\\n\\nWidgets\\n\\nWhen you\\'ve got the data or model into the state that you want to explore, you\\ncan add in widgets like st.slider(),\\nst.button() or\\nst.selectbox(). It\\'s really straightforward\\n— treat widgets as variables:\\n\\npython\\nimport streamlit as st\\nx = st.slider(\\'x\\')  # 👈 this is a widget\\nst.write(x, \\'squared is\\', x * x)\\n\\nOn first run, the app above should output the text \"0 squared is 0\". Then\\nevery time a user interacts with a widget, Streamlit simply reruns your script\\nfrom top to bottom, assigning the current state of the widget to your variable\\nin the process.\\n\\nFor example, if the user moves the slider to position 10, Streamlit will\\nrerun the code above and set x to 10 accordingly. So now you should see the\\ntext \"10 squared is 100\".\\n\\nWidgets can also be accessed by key, if you choose to specify a string to use as the unique key for the widget:\\n\\n```python\\nimport streamlit as st\\nst.text_input(\"Your name\", key=\"name\")\\n\\nYou can access the value at any point with:\\n\\nst.session_state.name\\n```\\n\\nEvery widget with a key is automatically added to Session State. For more information about Session State, its association with widget state, and its limitations, see Session State API Reference Guide.\\n\\nUse checkboxes to show/hide data\\n\\nOne use case for checkboxes is to hide or show a specific chart or section in\\nan app. st.checkbox() takes a single argument,\\nwhich is the widget label. In this sample, the checkbox is used to toggle a\\nconditional statement.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\nif st.checkbox(\\'Show dataframe\\'):\\n    chart_data = pd.DataFrame(\\n       np.random.randn(20, 3),\\n       columns=[\\'a\\', \\'b\\', \\'c\\'])\\n\\n```\\n\\nUse a selectbox for options\\n\\nUse st.selectbox to choose from a series. You\\ncan write in the options you want, or pass through an array or data frame\\ncolumn.\\n\\nLet\\'s use the df data frame we created earlier.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\ndf = pd.DataFrame({\\n    \\'first column\\': [1, 2, 3, 4],\\n    \\'second column\\': [10, 20, 30, 40]\\n    })\\n\\noption = st.selectbox(\\n    \\'Which number do you like best?\\',\\n     df[\\'first column\\'])\\n\\n\\'You selected: \\', option\\n```\\n\\nLayout\\n\\nStreamlit makes it easy to organize your widgets in a left panel sidebar with\\nst.sidebar. Each element that\\'s passed to\\nst.sidebar is pinned to the left, allowing\\nusers to focus on the content in your app while still having access to UI\\ncontrols.\\n\\nFor example, if you want to add a selectbox and a slider to a sidebar,\\nuse st.sidebar.slider and st.sidebar.selectbox instead of st.slider and\\nst.selectbox:\\n\\n```python\\nimport streamlit as st\\n\\nAdd a selectbox to the sidebar:\\n\\nadd_selectbox = st.sidebar.selectbox(\\n    \\'How would you like to be contacted?\\',\\n    (\\'Email\\', \\'Home phone\\', \\'Mobile phone\\')\\n)\\n\\nAdd a slider to the sidebar:\\n\\nadd_slider = st.sidebar.slider(\\n    \\'Select a range of values\\',\\n    0.0, 100.0, (25.0, 75.0)\\n)\\n```\\n\\nBeyond the sidebar, Streamlit offers several other ways to control the layout\\nof your app. st.columns lets you place widgets side-by-side, and\\nst.expander lets you conserve space by hiding away large content.\\n\\n```python\\nimport streamlit as st\\n\\nleft_column, right_column = st.columns(2)\\n\\nYou can use a column just like st.sidebar:\\n\\nleft_column.button(\\'Press me!\\')\\n\\nOr even better, call Streamlit functions inside a \"with\" block:\\n\\nwith right_column:\\n    chosen = st.radio(\\n        \\'Sorting hat\\',\\n        (\"Gryffindor\", \"Ravenclaw\", \"Hufflepuff\", \"Slytherin\"))\\n    st.write(f\"You are in {chosen} house!\")\\n```\\n\\nst.echo and st.spinner are not currently supported inside the sidebar\\nor layout options. Rest assured, though, we\\'re currently working on adding support for those too!\\n\\nShow progress\\n\\nWhen adding long running computations to an app, you can use\\nst.progress() to display status in real time.\\n\\nFirst, let\\'s import time. We\\'re going to use the time.sleep() method to\\nsimulate a long running computation:\\n\\npython\\nimport time\\n\\nNow, let\\'s create a progress bar:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n\\'Starting a long computation...\\'\\n\\nAdd a placeholder\\n\\nlatest_iteration = st.empty()\\nbar = st.progress(0)\\n\\nfor i in range(100):\\n  # Update the progress bar with each iteration.\\n  latest_iteration.text(f\\'Iteration {i+1}\\')\\n  bar.progress(i + 1)\\n  time.sleep(0.1)\\n\\n\\'...and now we\\\\\\'re done!\\'\\n```\\n\\nThemes\\n\\nStreamlit supports Light and Dark themes out of the box. Streamlit will first\\ncheck if the user viewing an app has a Light or Dark mode preference set by\\ntheir operating system and browser. If so, then that preference will be used.\\nOtherwise, the Light theme is applied by default.\\n\\nYou can also change the active theme from \"☰\" → \"Settings\".\\n\\nWant to add your own theme to an app? The \"Settings\" menu has a theme editor\\naccessible by clicking on \"Edit active theme\". You can use this editor to try\\nout different colors and see your app update live.\\n\\nWhen you\\'re happy with your work, themes can be saved by\\nsetting config options\\nin the [theme] config section. After you\\'ve defined a theme for your app, it\\nwill appear as \"Custom Theme\" in the theme selector and will be applied by\\ndefault instead of the included Light and Dark themes.\\n\\nMore information about the options available when defining a theme can be found\\nin the theme option documentation.\\n\\nThe theme editor menu is available only in local development. If you\\'ve deployed your app using\\nStreamlit Community Cloud, the \"Edit active theme\" button will no longer be displayed in the \"Settings\"\\nmenu.\\n\\nAnother way to experiment with different theme colors is to turn on the \"Run on save\" option, edit\\nyour config.toml file, and watch as your app reruns with the new theme colors applied.\\n\\nCaching\\n\\nThe Streamlit cache allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations.\\n\\nThe basic idea behind caching is to store the results of expensive function calls and return the cached result when the same inputs occur again rather than calling the function on subsequent runs.\\n\\nTo cache a function in Streamlit, you need to decorate it with one of two decorators (st.cache_data and st.cache_resource):\\n\\npython\\n@st.cache_data\\ndef long_running_function(param1, param2):\\n    return …\\n\\nIn this example, decorating long_running_function with @st.cache_data tells Streamlit that whenever the function is called, it checks two things:\\n\\nThe values of the input parameters (in this case, param1 and param2).\\n\\nThe code inside the function.\\n\\nIf this is the first time Streamlit sees these parameter values and function code, it runs the function and stores the return value in a cache. The next time the function is called with the same parameters and code (e.g., when a user interacts with the app), Streamlit will skip executing the function altogether and return the cached value instead. During development, the cache updates automatically as the function code changes, ensuring that the latest changes are reflected in the cache.\\n\\nAs mentioned, there are two caching decorators:\\n\\nst.cache_data\\xa0is the recommended way to cache computations that return data: loading a DataFrame from CSV, transforming a NumPy array, querying an API, or any other function that returns a serializable data object (str, int, float, DataFrame, array, list, …). It creates a new copy of the data at each function call, making it safe against mutations and race conditions. The behavior of st.cache_data is what you want in most cases – so if you\\'re unsure, start with\\xa0st.cache_data\\xa0and see if it works!\\n\\nst.cache_resource\\xa0is the recommended way to cache global resources like ML models or database connections – unserializable objects that you don’t want to load multiple times. Using it, you can share these resources across all reruns and sessions of an app without copying or duplication. Note that any mutations to the cached return value directly mutate the object in the cache (more details below).\\n\\nFor more information about the Streamlit caching decorators, their configuration parameters, and their limitations, see Caching.\\n\\nPages\\n\\nAs apps grow large, it becomes useful to organize them into multiple pages. This makes the app easier to manage as a developer and easier to navigate as a user. Streamlit provides a frictionless way to create multipage apps.\\n\\nWe designed this feature so that building a multipage app is as easy as building a single-page app! Just add more pages to an existing app as follows:\\n\\nIn the folder containing your main script, create a new pages folder. Let’s say your main script is named main_page.py.\\n\\nAdd new .py files in the pages folder to add more pages to your app.\\n\\nRun streamlit run main_page.py as usual.\\n\\nThat’s it! The main_page.py script will now correspond to the main page of your app. And you’ll see the other scripts from the pages folder in the sidebar page selector. For example:\\n\\nNow run streamlit run main_page.py and view your shiny new multipage app!\\n\\nOur documentation on Multipage apps teaches you how to add pages to your app, including how to define pages, structure and run multipage apps, and navigate between pages. Once you understand the basics, create your first multipage app!\\n\\nApp model\\n\\nNow that you know a little more about all the individual pieces, let\\'s close\\nthe loop and review how it works together:\\n\\nStreamlit apps are Python scripts that run from top to bottom\\n\\nEvery time a user opens a browser tab pointing to your app, the script is\\n   re-executed\\n\\nAs the script executes, Streamlit draws its output live in a browser\\n\\nScripts use the Streamlit cache to avoid recomputing expensive functions, so\\n   updates happen very fast\\n\\nEvery time a user interacts with a widget, your script is re-executed and\\n   the output value of that widget is set to the new value during that run.\\n\\nStreamlit apps can contain multiple pages, which are defined in separate\\n   .py files in a pages folder.', metadata={'source': 'docs/content/library/get-started/main-concepts.md'})]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "steamlit_doc_loader = DirectoryLoader('docs', glob=\"content/**/*.md\",\n",
    "                                      loader_cls=UnstructuredMarkdownLoader,\n",
    "                                      show_progress=True, use_multithreading=True)\n",
    "steamlit_doc = steamlit_doc_loader.load()\n",
    "steamlit_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='title: Welcome to Streamlit\\nfeatures:\\n  - title: Get started\\n    body: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\\n    color: violet-70\\n    icon: arrow_forward\\n    url: /library/get-started\\n    image: \"\"\\n  - title: API reference\\n    body: Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut\\n      fugit, sed quia.\\n    color: orange-70\\n    icon: dvr\\n    url: /library/api-reference\\n    image: \"\"\\n  - title: Topic guides\\n    body: Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut\\n      fugit, sed quia.\\n    color: l-blue-70\\n    icon: description\\n    url:\\n      Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit,\\n      sed quia.\\n    image: \"\"\\nwhats_new:\\n  - title: Feature title\\n    body: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Eleifend a\\n      facilisis sagittis, vitae nibh massa in facilisis et. Pretium eget non\\n      cursus purus tempus porta sodales.\\n    icon: visibility\\n    url: /library/get-started\\n    image: /img/logo.svg\\n  - title: Feature title\\n    body: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Eleifend a\\n      facilisis sagittis, vitae nibh massa in facilisis et. Pretium eget non\\n      cursus purus tempus porta sodales.\\n    url: /library/get-started\\n    icon: visibility\\n    image: \"\"\\n  - body: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Eleifend a\\n      facilisis sagittis, vitae nibh massa in facilisis et. Pretium eget non\\n      cursus purus tempus porta sodales.\\n    title: Feature title\\n    url: /library/get-started\\n    image: /img/logo.svg\\n    icon: edit\\n  - title: Feature title\\n    body: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Eleifend a\\n      facilisis sagittis, vitae nibh massa in facilisis et. Pretium eget non\\n      cursus purus tempus porta sodales.\\n    url: /library/get-started\\n    icon: edit\\n    image: \"\"\\nnews:\\n  - publish_date: April 7, 2021\\n    title: New Story 1\\n    body:', metadata={'source': 'docs/content/index.md'}),\n",
       " Document(page_content='title: New Story 1\\n    body:\\n      Lorem ipsum dolor sit amet, consectetur adipiscing elit. In venenatis leo\\n      felis, quis egestas est dignissim vel.\\n    url: https://www.streamlit.io\\n  - publish_date: April 7, 2021\\n    title: New Story 2\\n    body:\\n      Lorem ipsum dolor sit amet, consectetur adipiscing elit. In venenatis leo\\n      felis, quis egestas est dignissim vel. Pellentesque interdum massa metus,\\n      vitae tincidunt nibh molestie consequat. Sed congue commodo bibendum.\\n      Donec iaculis velit ante, a venenatis elit dictum ut. Etiam pellentesque\\n      libero ex, vitae eleifend quam egestas et. Vivamus pulvinar libero et\\n      iaculis placerat. Suspendisse bibendum orci a turpis suscipit dictum sed\\n      convallis risus. Aliquam eros erat, hendrerit vel viverra pretium,\\n      vulputate ac dui. Nullam vitae arcu ut enim tincidunt blandit. Donec\\n      sapien mi, vulputate eu sagittis pellentesque, imperdiet elementum nisl.\\n      In dapibus quam id magna pretium finibus. Curabitur id nunc dolor.\\n      Pellentesque eu tellus vitae neque sollicitudin sodales. Integer arcu\\n      urna, rutrum vel arcu sit amet, aliquam lobortis est.\\n    url: https://www.streamlit.io\\nnext: get-started', metadata={'source': 'docs/content/index.md'}),\n",
       " Document(page_content='Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps - so let’s get started!', metadata={'source': 'docs/content/index.md'}),\n",
       " Document(page_content='enabled: false\\ntitle: We value your privacy.\\n\\nWe would like to use cookies to help us understand how users interact with this website. This is used, for example, to find out which parts of this site should be further improved.\\n\\nMore information can be found in our Privacy Notice.', metadata={'source': 'docs/content/gdpr-banner.md'}),\n",
       " Document(page_content='title: Installing dependencies\\nslug: /knowledge-base/dependencies\\n\\nInstalling dependencies\\n\\nModuleNotFoundError: No module named\\n\\nImportError: libGL.so.1: cannot open shared object file: No such file or directory\\n\\nERROR: No matching distribution found for\\n\\nHow to install a package not on PyPI/Conda but available on GitHub\\n\\nInstall the Snowflake Connector for Python on Streamlit Community Cloud', metadata={'source': 'docs/content/kb/dependencies/index.md'}),\n",
       " Document(page_content='title: Knowledge Base\\nslug: /knowledge-base\\n\\nKnowledge base\\n\\nThe knowledge base is a self-serve library of tips, step-by-step tutorials, and articles that answer your questions about creating and deploying Streamlit apps.\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"local_library\"\\n    bold=\"Tutorials.\"\\n    href=\"/knowledge-base/tutorials\"\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"auto_awesome\"\\n    bold=\"Using Streamlit.\"\\n    href=\"/knowledge-base/using-streamlit\"\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"build\"\\n    bold=\"Streamlit Components.\"\\n    href=\"/knowledge-base/components\"\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"downloading\"\\n    bold=\"Installing dependencies.\"\\n    href=\"/knowledge-base/dependencies\"\\n\\n<InlineCallout\\n    color=\"orange-80\"\\n    icon=\"report\"\\n    bold=\"Deployment issues.\"\\n    href=\"/knowledge-base/deploy\"', metadata={'source': 'docs/content/kb/index.md'}),\n",
       " Document(page_content=\"title: ModuleNotFoundError No module named\\nslug: /knowledge-base/dependencies/module-not-found-error\\n\\nModuleNotFoundError: No module named\\n\\nProblem\\n\\nYou receive the error ModuleNotFoundError: No module named when you deploy an app on Streamlit Community Cloud.\\n\\nSolution\\n\\nThis error occurs when you import a module on Streamlit Community Cloud that isn’t included in your requirements file. Any external Python dependencies that are not distributed with a standard Python installation should be included in your requirements file.\\n\\nE.g. You will see ModuleNotFoundError: No module named 'sklearn' if you don’t include scikit-learn in your requirements file and import sklearn in your app.\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/getting-error-modulenotfounderror-no-module-named-beautifulsoup/9126\\n\\nhttps://discuss.streamlit.io/t/modulenotfounderror-no-module-named-vega-datasets/16354\", metadata={'source': 'docs/content/kb/dependencies/module-not-found-error.md'}),\n",
       " Document(page_content='title: Install the Snowflake Connector for Python on Streamlit Community Cloud\\nslug: /knowledge-base/dependencies/snowflake-connector-python-streamlit-cloud\\n\\nInstall the Snowflake Connector for Python on Streamlit Community Cloud\\n\\nThe Snowflake Connector for Python is available on PyPI and the installation instructions are found in the Snowflake documentation. When installing the connector, Snowflake recommends installing specific versions of its dependent libraries. The steps below will help you install the connector and its dependencies on Streamlit Community Cloud:\\n\\nDetermine the version of the Snowflake Connector for Python you want to install.\\n\\nDetermine the version of Python you want to use on Streamlit Community Cloud.\\n\\nTo install the connector and the dependent libraries, select the requirements file for that version of the connector and Python.\\n\\nAdd the raw GitHub URL of the requirements file to your requirements.txt file and prepend -r to the line.\\n   For example, if you want to install version 2.7.9 of the connector on Python 3.9, add the following line to your requirements.txt file:\\n\\nbash\\n   -r https://raw.githubusercontent.com/snowflakedb/snowflake-connector-python/v2.7.9/tested_requirements/requirements_39.reqs\\n\\nOn Streamlit Community Cloud, select the appropriate version of Python for your app by clicking \"Advanced settings\" before you deploy the app:\\n\\nThat\\'s it! You\\'re ready to use the Snowflake Connector for Python on Streamlit Community Cloud. ❄️🎈\\n\\nAs the Snowflake dependencies requirements files (.reqs) contain the pinned version of the connector, there is no need add a separate entry for the connector to requirements.txt.\\n\\nAdditional resources:\\n\\nInstalling the Python Connector\\n\\nUnable to Deploy streamlit app with snowflake-connector-python\\n\\nPrerequisite Python packages for the Snowflake Connector', metadata={'source': 'docs/content/kb/dependencies/snowflake-connector-python.md'}),\n",
       " Document(page_content='title: How to install a package not on PyPI/Conda but available on GitHub\\nslug: /knowledge-base/dependencies/install-package-not-pypi-conda-available-github\\n\\nHow to install a package not on PyPI/Conda but available on GitHub\\n\\nOverview\\n\\nAre you trying to deploy your app to Streamlit Community Cloud, but don\\'t know how to specify a Python dependency in your requirements file that is available on a public GitHub repo but not any package index like PyPI or Conda? If so, continue reading to find out how!\\n\\nLet\\'s suppose you want to install SomePackage and its Python dependencies from GitHub, a hosting service for the popular version control system (VCS) Git. And suppose SomePackage is found at the the following URL: https://github.com/SomePackage.git.\\n\\npip (via requirements.txt) supports installing from GitHub. This support requires a working executable to be available (for Git). It is used through a URL prefix: git+.\\n\\nSpecify the GitHub web URL\\n\\nTo install SomePackage, innclude the following in your requirements.txt file:\\n\\nbash\\ngit+https://github.com/SomePackage#egg=SomePackage\\n\\nYou can even specify a \"git ref\" such as branch name, a commit hash or a tag name, as shown in the examples below.\\n\\nSpecify a Git branch name\\n\\nInstall SomePackage by specifying a branch name such as main, master, develop, etc, in requirements.txt:\\n\\nbash\\ngit+https://github.com/SomePackage.git@main#egg=SomePackage\\n\\nSpecify a commit hash\\n\\nInstall SomePackage by specifying a commit hash in requirements.txt:\\n\\nbash\\ngit+https://github.com/SomePackage.git@eb40b4ff6f7c5c1e4366cgfg0671291bge918#egg=SomePackage\\n\\nSpecify a tag\\n\\nInstall SomePackage by specifying a tag in requirements.txt:\\n\\nbash\\ngit+https://github.com/SomePackage.git@v1.1.0#egg=SomePackage\\n\\nLimitations\\n\\nIt is currently not possible to install private packages from private GitHub repos using the URI form:\\n\\nbash\\ngit+https://{token}@github.com/user/project.git@{version}', metadata={'source': 'docs/content/kb/dependencies/install-package-pypi-github.md'}),\n",
       " Document(page_content='where version is a tag, a branch, or a commit. And token is a personal access token with read only permissions. Streamlit Community Cloud only supports installing public packages from public GitHub repos.', metadata={'source': 'docs/content/kb/dependencies/install-package-pypi-github.md'}),\n",
       " Document(page_content='title: ERROR No matching distribution found for\\nslug: /knowledge-base/dependencies/no-matching-distribution\\n\\nERROR: No matching distribution found for\\n\\nProblem\\n\\nYou receive the error ERROR: No matching distribution found for when you deploy an app on Streamlit Community Cloud.\\n\\nSolution\\n\\nThis error occurs when you deploy an app on Streamlit Community Cloud and have one or more of the following issues with your Python dependencies in your requirements file:\\n\\nThe package is part of the Python Standard Library. E.g. You will see ERROR: No matching distribution found for base64 if you include base64 in your requirements file, as it is part of the Python Standard Library. The solution is to not include the package in your requirements file. Only include packages in your requirements file that are not distributed with a standard Python installation.\\n\\nThe package name in your requirements file is misspelled. Double-check the package name before including it in your requirements file.\\n\\nThe package does not support the operating system on which your Streamlit app is running. E.g. You see ERROR: No matching distribution found for pywin32 while deploying to Streamlit Community Cloud. The pywin32 module provides access to many of the Windows APIs from Python. Apps deployed to Streamlit Community Cloud are executed in a Linux environment. As such, pywin32 fails to install on non-Windows systems, including on Streamlit Community Cloud. The solution is to either exclude pywin32 from your requirements file, or deploy your app on a cloud service offering Windows machines.\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/error-no-matching-distribution-found-for-base64/15758\\n\\nhttps://discuss.streamlit.io/t/error-could-not-find-a-version-that-satisfies-the-requirement-pywin32-301-from-versions-none/15343/2', metadata={'source': 'docs/content/kb/dependencies/no-matching-distribution.md'}),\n",
       " Document(page_content='title: ImportError libGL.so.1 cannot open shared object file No such file or directory\\nslug: /knowledge-base/dependencies/libgl\\n\\nImportError libGL.so.1 cannot open shared object file No such file or directory\\n\\nProblem\\n\\nYou receive the error ImportError libGL.so.1 cannot open shared object file No such file or directory when using OpenCV in your app deployed on Streamlit Community Cloud.\\n\\nSolution\\n\\nIf you use OpenCV in your app, include opencv-python-headless in your requirements file on Streamlit Community Cloud in place of opencv_contrib_python and opencv-python.\\n\\nIf opencv-python is a required (non-optional) dependency of your app or a dependency of a library used in your app, the above solution is not applicable. Instead, you can use the following solution:\\n\\nCreate a packages.txt file in your repo with the following line to install the apt-get dependency libgl:\\n\\nlibgl1', metadata={'source': 'docs/content/kb/dependencies/libgl.md'}),\n",
       " Document(page_content='title: Using Streamlit\\nslug: /knowledge-base/using-streamlit\\n\\nUsing Streamlit\\n\\nHere are some frequently asked questions about using Streamlit. If you feel something important is missing that everyone needs to know, please open an issue or submit a pull request and we\\'ll be happy to review it!\\n\\nSanity checks\\n\\nBatch elements and input widgets with st.form\\n\\nHow do I run my Streamlit script?\\n\\nHow can I make Streamlit watch for changes in other modules I\\'m importing in my app?\\n\\nWhat browsers does Streamlit support?\\n\\nWhat is the path of Streamlit’s config.toml file?\\n\\nWhere does st.file_uploader store uploaded files and when do they get deleted?\\n\\nHow do you retrieve the filename of a file uploaded with st.file_uploader?\\n\\nHow to remove \"· Streamlit\" from the app title?\\n\\nHow to download a file in Streamlit?\\n\\nHow to download a Pandas DataFrame as a CSV?\\n\\nHow can I make st.pydeck_chart use custom Mapbox styles?\\n\\nHow to insert elements out of order?\\n\\nHow to animate elements?\\n\\nAppend data to a table or chart\\n\\nHide row indices when displaying a dataframe\\n\\nHow do I upgrade to the latest version of Streamlit?\\n\\nWidget updating for every second input when using session state\\n\\nHow do I create an anchor link?\\n\\nHow do I enable camera access?\\n\\nWhy does Streamlit restrict nested st.columns?\\n\\nHow to host static files in Streamlit?\\n\\nWhat is serializable session state?', metadata={'source': 'docs/content/kb/using-streamlit/index.md'}),\n",
       " Document(page_content=\"title: Enabling camera access in your browser\\nslug: /knowledge-base/using-streamlit/enable-camera\\n\\nEnabling camera access in your browser\\n\\nStreamlit apps may include a widget to upload images directly from your computer's camera. To\\nsafeguard the users' privacy and security, browsers require users to explicitly allow access to the\\ncamera before this widget can work.\\n\\nTo learn how to enable camera access, please check the documentation for your browser:\\n\\nChrome\\n\\nSafari\\n\\nFirefox\", metadata={'source': 'docs/content/kb/using-streamlit/enable-camera.md'}),\n",
       " Document(page_content='title: Why does Streamlit restrict nested st.columns?\\nslug: /knowledge-base/using-streamlit/why-streamlit-restrict-nested-columns\\n\\nWhy does Streamlit restrict nested st.columns?\\n\\nStarting in version 1.18.0, Streamlit allows nesting st.columns inside other\\nst.columns with the following restrictions:\\n\\nIn the main area of the app, columns can be nested up to one level of nesting.\\n\\nIn the sidebar, columns cannot be nested.\\n\\nThese restrictions are in place to make Streamlit apps look good on all device sizes. Nesting columns multiple times often leads to a bad UI.\\nYou might be able to make it look good on one screen size but as soon as a user on a different screen views the app,\\nthey will have a bad experience. Some columns will be tiny, others will be way too long, and complex layouts will look out of place.\\nStreamlit tries its best to automatically resize elements to look good across devices, without any help from the developer.\\nBut for complex layouts with multiple levels of nesting, this is not possible.\\n\\nWe are always working on improving layout options though! So if you have a use case that requires a more complex layout,\\nplease open a GitHub issue, ideally with a sketch of what you want to do.', metadata={'source': 'docs/content/kb/using-streamlit/why-streamlit-restrict-nested-columns.md'}),\n",
       " Document(page_content='title: How to insert elements out of order?\\nslug: /knowledge-base/using-streamlit/insert-elements-out-of-order\\n\\nHow to insert elements out of order?\\n\\nYou can use the st.empty method as a placeholder,\\nto \"save\" a slot in your app that you can use later.\\n\\n```python\\nst.text(\\'This will appear first\\')\\n\\nAppends some text to the app.\\n\\nmy_slot1 = st.empty()\\n\\nAppends an empty slot to the app. We\\'ll use this later.\\n\\nmy_slot2 = st.empty()\\n\\nAppends another empty slot.\\n\\nst.text(\\'This will appear last\\')\\n\\nAppends some more text to the app.\\n\\nmy_slot1.text(\\'This will appear second\\')\\n\\nReplaces the first empty slot with a text string.\\n\\nmy_slot2.line_chart(np.random.randn(20, 2))\\n\\nReplaces the second empty slot with a chart.\\n\\n```', metadata={'source': 'docs/content/kb/using-streamlit/insert-elements-out-of-order.md'}),\n",
       " Document(page_content=\"title: How to animate elements?\\nslug: /knowledge-base/using-streamlit/animate-elements\\n\\nHow to animate elements?\\n\\nLet's combine some of the things you've learned to create compelling\\nanimations in your app.\\n\\n```python\\nprogress_bar = st.progress(0)\\nstatus_text = st.empty()\\nchart = st.line_chart(np.random.randn(10, 2))\\n\\nfor i in range(100):\\n    # Update progress bar.\\n    progress_bar.progress(i + 1)\\n\\nstatus_text.text('Done!')\\nst.balloons()\\n```\", metadata={'source': 'docs/content/kb/using-streamlit/animate-elements.md'}),\n",
       " Document(page_content=\"title: How can I make st.pydeck_chart use custom Mapbox styles?\\nslug: /knowledge-base/using-streamlit/pydeck-chart-custom-mapbox-styles\\n\\nHow can I make st.pydeck_chart use custom Mapbox styles?\\n\\nIf you are supplying a Mapbox token, but the resulting pydeck_chart doesn't show your custom Mapbox styles, please check that you are adding the Mapbox token to the Streamlit config.toml configuration file. Streamlit DOES NOT read Mapbox tokens from inside of a PyDeck specification (i.e. from inside of the Streamlit app). Please see this forum thread for more information.\", metadata={'source': 'docs/content/kb/using-streamlit/pydeck-chart-custom-mapbox-styles.md'}),\n",
       " Document(page_content='title: Widget updating for every second input when using session state\\nslug: /knowledge-base/using-streamlit/widget-updating-session-state\\n\\nWidget updating for every second input when using session state\\n\\nOverview\\n\\nYou are using session state to store page interactions in your app. When users interact with a widget in your app (e.g., click a button), you expect your app to update its widget states and reflect the new values. However, you notice that it doesn\\'t. Instead, users have to interact with the widget twice (e.g., click a button twice) for the app to show the correct values. What do you do now? 🤔 Let\\'s walk through the solution in the section below.\\n\\nSolution\\n\\nWhen using session state to update widgets or values in your script, you need to use the unique key you assigned to the widget, not the variable that you assigned your widget to. In the example code block below, the unique key assigned to the slider widget is slider, and the variable the widget is assigned to is slide_val.\\n\\nLet\\'s see this in an example. Say you want a user to click a button that resets a slider.\\n\\nTo have the slider\\'s value update on the button click, you need to use a callback function with the on_click parameter of st.button:\\n\\n```python\\n\\nthe callback function for the button will add 1 to the\\n\\nslider value up to 10\\n\\ndef plus_one():\\n    if st.session_state[\"slider\"] < 10:\\n        st.session_state.slider += 1\\n    else:\\n        pass\\n    return\\n\\nwhen creating the button, assign the name of your callback\\n\\nfunction to the on_click parameter\\n\\nadd_one = st.button(\"Add one to the slider\", on_click=plus_one, key=\"add_one\")\\n\\ncreate the slider\\n\\nslide_val = st.slider(\"Pick a number\", 0, 10, key=\"slider\")\\n```\\n\\nRelevant resources\\n\\nCaching Sqlite DB connection resulting in glitchy rendering of the page\\n\\nSelect all checkbox that is linked to selectbox of options', metadata={'source': 'docs/content/kb/using-streamlit/widget-updating-session-state.md'}),\n",
       " Document(page_content='title: How to download a Pandas DataFrame as a CSV?\\nslug: /knowledge-base/using-streamlit/how-download-pandas-dataframe-csv\\n\\nHow to download a Pandas DataFrame as a CSV?\\n\\nUse the st.download_button widget that is natively built into Streamlit. Check out a sample app demonstrating how you can use st.download_button to download common file formats.\\n\\nExample usage\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\ndf = pd.read_csv(\"dir/file.csv\")\\n\\n@st.experimental_memo\\ndef convert_df(df):\\n   return df.to_csv(index=False).encode(\\'utf-8\\')\\n\\ncsv = convert_df(df)\\n\\nst.download_button(\\n   \"Press to Download\",\\n   csv,\\n   \"file.csv\",\\n   \"text/csv\",\\n   key=\\'download-csv\\'\\n)\\n```\\n\\nAdditional resources:\\n\\nhttps://blog.streamlit.io/0-88-0-release-notes/\\n\\nhttps://streamlit-release-demos-0-88streamlit-app-0-88-v8ram3.streamlit.app/\\n\\nhttps://docs.streamlit.io/library/api-reference/widgets/st.download_button', metadata={'source': 'docs/content/kb/using-streamlit/how-download-pandas-dataframe-csv.md'}),\n",
       " Document(page_content='title: What is the path of Streamlit’s config.toml file?\\nslug: /knowledge-base/using-streamlit/path-streamlit-config-toml\\n\\nWhat is the path of Streamlit’s config.toml file?\\n\\nA global config file is found at ~/.streamlit/config.toml for macOS/Linux or %userprofile%/.streamlit/config.toml for Windows.\\n\\nA per-project config file can be created at $CWD/.streamlit/config.toml, where $CWD is the folder you’re running Streamlit from.\\n\\nClick here to learn more about Streamlit configuration options.', metadata={'source': 'docs/content/kb/using-streamlit/path-streamlit-config-toml.md'}),\n",
       " Document(page_content='title: How do I create an anchor link?\\nslug: /knowledge-base/using-streamlit/create-anchor-link\\n\\nHow do I create an anchor link?\\n\\nOverview\\n\\nHave you wanted to create anchors so that users of your app can directly navigate to specific sections by specifying #anchor in the URL? If so, let\\'s find out how.\\n\\nSolution\\n\\nAnchors are automatically added to header text.\\n\\nFor example, if you define a header text via the st.header() command as follows:\\n\\npython\\nst.header(\"Section 1\")\\n\\nThen you can create a link to this header using:\\n\\npython\\nst.markdown(\"[Section 1](#section-1)\")\\n\\nExamples\\n\\nDemo app: https://dataprofessor-streamlit-anchor-app-80kk8w.streamlit.app/\\n\\nGitHub repo: https://github.com/dataprofessor/streamlit/blob/main/anchor_app.py', metadata={'source': 'docs/content/kb/using-streamlit/create-anchor-link.md'}),\n",
       " Document(page_content='title: Hide row indices when displaying a dataframe\\nslug: /knowledge-base/using-streamlit/hide-row-indices-displaying-dataframe\\n\\nHide row indices when displaying a dataframe\\n\\nOverview\\n\\nStreamlit offers two ways to display a dataframe: as a static table using st.table(), and as an interactive table using st.dataframe().\\n\\nBoth options display row indices in the left-most column. To see this in action, let\\'s display a dataframe with random entries using both st.table() and st.dataframe():\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame(\\n    np.random.randn(10, 5),\\n    columns=(\"col %d\" % i for i in range(5)))\\n\\nDisplay a static table\\n\\nst.table(df)\\n\\nDisplay an interactive table\\n\\nst.dataframe(df)\\n```\\n\\nNotice how row indices are displayed to the left of the col0 column: 👇\\n\\nTo hide the column containing row indices, you can use CSS selectors to modify the visibility of the column. Before you display your dataframe, you must inject the appropriate CSS with st.markdown(), and set unsafe_allow_html=True.\\n\\nNow that you have a conceptual understanding of how to hide row indices, let\\'s implement it in code!\\n\\nHide row indices with st.table\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame(\\n    np.random.randn(10, 5),\\n    columns=(\"col %d\" % i for i in range(5)))\\n\\nCSS to inject contained in a string\\n\\nhide_table_row_index = \"\"\"\\n            \\n            thead tr th:first-child {display:none}\\n            tbody th {display:none}\\n            \\n            \"\"\"\\n\\nInject CSS with Markdown\\n\\nst.markdown(hide_table_row_index, unsafe_allow_html=True)\\n\\nDisplay a static table\\n\\nst.table(df)\\n```\\n\\nHide row indices with st.dataframe\\n\\nThe below workaround for st.dataframe does not work for streamlit>=1.10.0.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport numpy as np\\n\\ndf = pd.DataFrame(\\n    np.random.randn(10, 5),\\n    columns=(\"col %d\" % i for i in range(5)))\\n\\nCSS to inject contained in a string', metadata={'source': 'docs/content/kb/using-streamlit/hide-row-indices-display-dataframe.md'}),\n",
       " Document(page_content='CSS to inject contained in a string\\n\\nhide_dataframe_row_index = \"\"\"\\n            \\n            .row_heading.level0 {display:none}\\n            .blank {display:none}\\n            \\n            \"\"\"\\n\\nInject CSS with Markdown\\n\\nst.markdown(hide_dataframe_row_index, unsafe_allow_html=True)\\n\\nDisplay an interactive table\\n\\nst.dataframe(df)\\n```', metadata={'source': 'docs/content/kb/using-streamlit/hide-row-indices-display-dataframe.md'}),\n",
       " Document(page_content=\"title: Sanity checks\\nslug: /knowledge-base/using-streamlit/sanity-checks\\n\\nSanity checks\\n\\nIf you're having problems running your Streamlit app, here are a few things to try out.\\n\\nCheck #0: Are you using a Streamlit-supported version of Python?\\n\\nStreamlit will maintain backwards-compatibility with earlier Python versions as practical,\\nguaranteeing compatibility with at least the last three minor versions of Python 3.\\n\\nAs new versions of Python are released, we will try to be compatible with the new version as soon\\nas possible, though frequently we are at the mercy of other Python packages to support these new versions as well.\\n\\nStreamlit currently supports versions 3.8, 3.9, 3.10, and 3.11 of Python.\\n\\nCheck #1: Is Streamlit running?\\n\\nOn a Mac or Linux machine, type this on the terminal:\\n\\nbash\\nps -Al | grep streamlit\\n\\nIf you don't see streamlit run in the output (or streamlit hello, if that's\\nthe command you ran) then the Streamlit server is not running. So re-run your command and see if the bug goes away.\\n\\nCheck #2: Is this an already-fixed Streamlit bug?\\n\\nWe try to fix bugs quickly, so many times a problem will go away when you\\nupgrade Streamlit. So the first thing to try when having an issue is upgrading\\nto the latest version of Streamlit:\\n\\nbash\\npip install --upgrade streamlit\\nstreamlit version\\n\\n...and then verify that the version number printed corresponds to the version number displayed on PyPI.\\n\\nTry reproducing the issue now. If not fixed, keep reading on.\\n\\nCheck #3: Are you running the correct Streamlit binary?\\n\\nLet's check whether your Python environment is set up correctly. Edit the\\nStreamlit script where you're experiencing your issue, comment everything\\nout, and add these lines instead:\\n\\npython\\nimport streamlit as st\\nst.write(st.__version__)\\n\\n...then call streamlit run on your script and make sure it says the same\\nversion as above. If not the same version, check out these\\ninstructions for some sure-fire ways to set up your\\nenvironment.\", metadata={'source': 'docs/content/kb/using-streamlit/sanity-checks.md'}),\n",
       " Document(page_content=\"Check #4: Is your browser caching your app too aggressively?\\n\\nThere are two easy ways to check this:\\n\\nLoad your app in a browser then press Ctrl-Shift-R or ⌘-Shift-R to do a\\n   hard refresh (Chrome/Firefox).\\n\\nAs a test, run Streamlit on another port. This way the browser starts the\\n   page with a brand new cache. For that, pass the --server.port\\n   argument to Streamlit on the command line:\\n\\nbash\\n   streamlit run my_app.py --server.port=9876\\n\\nCheck #5: Is this a Streamlit regression?\\n\\nIf you've upgraded to the latest version of Streamlit and things aren't\\nworking, you can downgrade at any time using this command:\\n\\nbash\\npip install --upgrade streamlit==1.0.0\\n\\n...where 1.0.0 is the version you'd like to downgrade to. See\\nChangelog for a complete list of Streamlit versions.\\n\\nCheck #6 [Windows]: Is Python added to your PATH?\\n\\nWhen installed by downloading from python.org, Python is\\nnot automatically added to the Windows system PATH. Because of this, you may get error messages\\nlike the following:\\n\\nCommand Prompt:\\n\\nbash\\nC:\\\\Users\\\\streamlit> streamlit hello\\n'streamlit' is not recognized as an internal or external command,\\noperable program or batch file.\\n\\nPowerShell:\\n\\nbash\\nPS C:\\\\Users\\\\streamlit> streamlit hello\\nstreamlit : The term 'streamlit' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that\\nthe path is correct and try again.\\nAt line:1 char:1\\n+ streamlit hello\\n+ ~~~~~~~~~\\n    + CategoryInfo          : ObjectNotFound: (streamlit:String) [], CommandNotFoundException\\n    + FullyQualifiedErrorId : CommandNotFoundException\\n\\nTo resolve this issue, add Python to the Windows system PATH.\\n\\nAfter adding Python to your Windows PATH, you should then be able to follow the instructions in our Get Started section.\\n\\nCheck #7 [Windows]: Do you need Build Tools for Visual Studio installed?\", metadata={'source': 'docs/content/kb/using-streamlit/sanity-checks.md'}),\n",
       " Document(page_content='Streamlit includes pyarrow as an install dependency. Occasionally, when trying to install Streamlit from PyPI, you may see errors such as the following:\\n\\n```bash\\nUsing cached pyarrow-1.0.1.tar.gz (1.3 MB)\\n  Installing build dependencies ... error\\n  ERROR: Command errored out with exit status 1:\\n   command: \\'c:\\\\users\\\\streamlit\\\\appdata\\\\local\\\\programs\\\\python\\\\python38-32\\\\python.exe\\' \\'c:\\\\users\\\\streamlit\\\\appdata\\\\local\\\\programs\\\\python\\\\python38-32\\\\lib\\\\site-packages\\\\pip\\' install --ignore-installed --no-user --prefix \\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-s7owjrle\\\\overlay\\' --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- \\'cython >= 0.29\\' \\'numpy==1.14.5; python_version<\\'\"\\'\"\\'3.8\\'\"\\'\"\\'\\' \\'numpy==1.16.0; python_version>=\\'\"\\'\"\\'3.8\\'\"\\'\"\\'\\' setuptools setuptools_scm wheel\\n       cwd: None\\n\\nComplete output (319 lines):\\n\\nERROR: Command errored out with exit status 1: \\'c:\\\\users\\\\streamlit\\\\appdata\\\\local\\\\programs\\\\python\\\\python38-32\\\\python.exe\\' -u -c \\'import sys, setuptools, tokenize; sys.argv[0] = \\'\"\\'\"\\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-install-0jwfwx_u\\\\numpy\\\\setup.py\\'\"\\'\"\\'; file=\\'\"\\'\"\\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-install-0jwfwx_u\\\\numpy\\\\setup.py\\'\"\\'\"\\';f=getattr(tokenize, \\'\"\\'\"\\'open\\'\"\\'\"\\', open)(file);code=f.read().replace(\\'\"\\'\"\\'\\\\r\\\\n\\'\"\\'\"\\', \\'\"\\'\"\\'\\\\n\\'\"\\'\"\\');f.close();exec(compile(code, file, \\'\"\\'\"\\'exec\\'\"\\'\"\\'))\\' install --record \\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-record-eys4l2gc\\\\install-record.txt\\' --single-version-externally-managed --prefix \\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-s7owjrle\\\\overlay\\' --compile --install-headers \\'C:\\\\Users\\\\streamlit\\\\AppData\\\\Local\\\\Temp\\\\pip-build-env-s7owjrle\\\\overlay\\\\Include\\\\numpy\\' Check the logs for full command output.\\n\\n```', metadata={'source': 'docs/content/kb/using-streamlit/sanity-checks.md'}),\n",
       " Document(page_content='```\\n\\nThis error indicates that Python is trying to compile certain libraries during install, but it cannot find the proper compilers on your system,\\nas reflected by the line error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\".\\n\\nInstalling Build Tools for Visual Studio should resolve this issue.', metadata={'source': 'docs/content/kb/using-streamlit/sanity-checks.md'}),\n",
       " Document(page_content='title: How to remove \"· Streamlit\" from the app title?\\nslug: /knowledge-base/using-streamlit/remove-streamlit-app-title\\n\\nHow to remove \"· Streamlit\" from the app title?\\n\\nUsing st.set_page_config to assign the page title will not append \"· Streamlit\" to that title. E.g.:\\n\\n```python\\nimport streamlit as st\\n\\nst.set_page_config(\\n   page_title=\"Ex-stream-ly Cool App\",\\n   page_icon=\"🧊\",\\n   layout=\"wide\",\\n   initial_sidebar_state=\"expanded\",\\n)\\n```', metadata={'source': 'docs/content/kb/using-streamlit/remove-streamlit-app-title.md'}),\n",
       " Document(page_content='title: Where does st.file_uploader store uploaded files and when do they get deleted?\\nslug: /knowledge-base/using-streamlit/where-file-uploader-store-when-deleted\\n\\nWhere does st.file_uploader store uploaded files and when do they get deleted?\\n\\nWhen you upload a file using st.file_uploader, the data are copied to the Streamlit backend via the browser, and contained in a BytesIO buffer in Python memory (i.e. RAM, not disk). The data will persist in RAM until the Streamlit app re-runs from top-to-bottom, which is on each widget interaction. If you need to save the data that was uploaded between runs, then you can cache it so that Streamlit persists it across re-runs.\\n\\nAs files are stored in memory, they get deleted immediately as soon as they’re not needed anymore.\\n\\nThis means Streamlit removes a file from memory when:\\n\\nThe user uploads another file, replacing the original one\\n\\nThe user clears the file uploader\\n\\nThe user closes the browser tab where they uploaded the file\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/streamlit-sharing-fileupload-where-does-it-go/9267\\n\\nhttps://discuss.streamlit.io/t/how-to-update-the-uploaded-file-using-file-uploader/13512/', metadata={'source': 'docs/content/kb/using-streamlit/where-file-uploader-store-when-deleted.md'}),\n",
       " Document(page_content=\"title: Batch elements and input widgets with st.form\\nslug: /knowledge-base/using-streamlit/batch-elements-input-widgets-form\\n\\nBatch elements and input widgets with st.form\\n\\nLet's take a look at how to use st.form to batch elements and input widgets.\\n\\nIn Streamlit, every widget interaction causes a rerun of the app. However,\\nthere are times when you might want to interact with a couple of widgets and\\nsubmit those interactions while triggering a single re-run of the app.\\n\\nUsing st.form you can batch input widgets together and along with\\nst.form_submit_button submit the state inside these widgets with the click\\nof a single button.\\n\\n```python\\n\\nForms can be declared using the 'with' syntax\\n\\nwith st.form(key='my_form'):\\n    text_input = st.text_input(label='Enter your name')\\n    submit_button = st.form_submit_button(label='Submit')\\n```\\n\\n```python\\n\\nAlternative syntax, declare a form and use the returned object\\n\\nform = st.form(key='my_form')\\nform.text_input(label='Enter some text')\\nsubmit_button = form.form_submit_button(label='Submit')\\n```\\n\\n```python\\n\\nst.form_submit_button returns True upon form submit\\n\\nif submit_button:\\n    st.write(f'hello {name}')\\n```\\n\\nForms can appear anywhere in your app (sidebar, columns etc), but there are\\nsome constraints:\\n\\nA form cannot have interdependent widgets, i.e. the output of widget1 cannot\\n  be the input to widget2 inside a form.\\n\\nBy design, interacting with widgets inside st.form does not trigger\\n  a re-run. Because of this reason, st.button cannot be declared inside st.form.\\n\\nst.form cannot be embedded inside another st.form.\\n\\nForms must have an associated st.form_submit_button. Clicking this button\\n  triggers a re-run. Streamlit throws an error if a form does not have an\\n  associated st.form_submit_button.\", metadata={'source': 'docs/content/kb/using-streamlit/batch-elements-widgets.md'}),\n",
       " Document(page_content=\"title: How can I make Streamlit watch for changes in other modules I'm importing in my app?\\nslug: /knowledge-base/using-streamlit/streamlit-watch-changes-other-modules-importing-app\\n\\nHow can I make Streamlit watch for changes in other modules I'm importing in my app?\\n\\nBy default, Streamlit only watches modules contained in the current directory of the main app module. You can track other modules by adding the parent directory of each module to the PYTHONPATH.\\n\\nbash\\nexport PYTHONPATH=$PYTHONPATH:/path/to/module\\nstreamlit run your_script.py\", metadata={'source': 'docs/content/kb/using-streamlit/streamlit-watch-changes-other-modules-importing-app.md'}),\n",
       " Document(page_content='title: How do you retrieve the filename of a file uploaded with st.file_uploader?\\nslug: /knowledge-base/using-streamlit/retrieve-filename-uploaded\\n\\nHow do you retrieve the filename of a file uploaded with st.file_uploader?\\n\\nIf you upload a single file (i.e. accept_multiple_files=False), the filename can be retrieved by using the .name attribute on the returned UploadedFile object:\\n\\n```python\\nimport streamlit as st\\n\\nuploaded_file = st.file_uploader(\"Upload a file\")\\n\\nif uploaded_file:\\n   st.write(\"Filename: \", uploaded_file.name)\\n```\\n\\nIf you upload multiple files (i.e. accept_multiple_files=True), the individual filenames can be retrieved by using the .name attribute on each UploadedFile object in the returned list:\\n\\n```python\\nimport streamlit as st\\n\\nuploaded_files = st.file_uploader(\"Upload multiple files\", accept_multiple_files=True)\\n\\nif uploaded_files:\\n   for uploaded_file in uploaded_files:\\n       st.write(\"Filename: \", uploaded_file.name)\\n```\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/is-it-possible-to-get-uploaded-file-file-name/7586', metadata={'source': 'docs/content/kb/using-streamlit/retrieve-filename-uploaded.md'}),\n",
       " Document(page_content=\"title: How do I upgrade to the latest version of Streamlit?\\nslug: /knowledge-base/using-streamlit/how-upgrade-latest-version-streamlit\\n\\nHow do I upgrade to the latest version of Streamlit?\\n\\nWe recommend upgrading to the latest official release of Streamlit so you have access to the newest, cutting-edge features. If you haven't installed Streamlit yet, please read our Installation guide. It helps you set up your virtual environment and walks you through installing Streamlit on Windows, macOS, and Linux. Regardless of which package management tool and OS you're using, we recommend running the commands on this page in a virtual environment.\\n\\nIf you've previously installed Streamlit and want to upgrade to the latest version, here's how to do it based on your dependency manager.\\n\\nPipenv\\n\\nStreamlit's officially-supported environment manager for macOS and Linux is Pipenv.\\n\\nNavigate to the project folder containing your Pipenv environment:\\n\\nbash\\ncd myproject\\n\\nActivate that environment, upgrade Streamlit, and verify you have the lastest version:\\n\\nbash\\npipenv shell\\npip install --upgrade streamlit\\nstreamlit version\\n\\nOr if you want to use an easily-reproducible environment, replace pip with pipenvevery time you install or update a package:\\n\\nbash\\npipenv update streamlit\\npipenv run streamlit version\\n\\nConda\\n\\nActivate the conda environment where Streamlit is installed:\\n\\nbash\\nconda activate $ENVIRONMENT_NAME\\n\\nBe sure to replace$ENVIRONMENT_NAME ☝️ with the name your conda environment!\\n\\nUpdate Streamlit within the active conda environment and verify you have the lastest version:\\n\\nbash\\nconda update -c conda-forge streamlit -y\\nstreamlit version\\n\\nPoetry\\n\\nIn order to get the latest version of Streamlit with Poetry and verify you have the lastest version, run:\\n\\nbash\\npoetry update streamlit\\nstreamlit version\", metadata={'source': 'docs/content/kb/using-streamlit/upgrade-version-streamlit.md'}),\n",
       " Document(page_content='title: What is serializable session state?\\nslug: /knowledge-base/using-streamlit/serializable-session-state\\n\\nWhat is serializable session state?\\n\\nSerializable Session State\\n\\nSerialization refers to the process of converting an object or data structure into a format that can be persisted and shared, and allowing you to recover the data’s original structure. Python’s built-in pickle module serializes Python objects to a byte stream (\"pickling\") and deserializes the stream into an object (\"unpickling\").\\n\\nBy default, Streamlit’s Session State allows you to persist any Python object for the duration of the session, irrespective of the object’s pickle-serializability. This property lets you store Python primitives such as integers, floating-point numbers, complex numbers and booleans, dataframes, and even lambdas returned by functions. However, some execution environments may require serializing all data in Session State, so it may be useful to detect incompatibility during development, or when the execution environment will stop supporting it in the future.\\n\\nTo that end, Streamlit provides a runner.enforceSerializableSessionState configuration option that, when set to true, only allows pickle-serializable objects in Session State. To enable the option, either create a global or project config file with the following or use it as a command-line flag:\\n\\n```toml\\n\\n.streamlit/config.toml\\n\\n[runner]\\nenforceSerializableSessionState = true\\n```\\n\\nBy \"pickle-serializable\", we mean calling pickle.dumps(obj) should not raise a PicklingError exception. When the config option is enabled, adding unserializable data to session state should result in an exception. E.g.,\\n\\n```python\\nimport streamlit as st\\n\\ndef unserializable_data():\\n        return lambda x: x\\n\\n👇 results in an exception when enforceSerializableSessionState is on\\n\\nst.session_state.unserializable = unserializable_data()\\n```', metadata={'source': 'docs/content/kb/using-streamlit/serializable-session-state.md'}),\n",
       " Document(page_content='title: What browsers does Streamlit support?\\nslug: /knowledge-base/using-streamlit/supported-browsers\\n\\nWhat browsers does Streamlit support?\\n\\nThe latest version of Streamlit is compatible with the two most recent versions of the following browsers:\\n\\nGoogle Chrome\\n\\nFirefox\\n\\nMicrosoft Edge\\n\\nSafari\\n\\nYou may not be able to use all the latest features of Streamlit with unsupported browsers or older versions of the above browsers. Streamlit will not provide bug fixes for unsupported browsers.', metadata={'source': 'docs/content/kb/using-streamlit/supported-browsers.md'}),\n",
       " Document(page_content='title: Append data to a table or chart\\nslug: /knowledge-base/using-streamlit/append-data-table-chart\\n\\nAppend data to a table or chart\\n\\nIn Streamlit, you can not only replace entire elements in your app, but also\\nmodify the data behind those elements. Here is how:\\n\\n```python\\nimport numpy as np\\nimport time\\n\\nGet some data.\\n\\ndata = np.random.randn(10, 2)\\n\\nShow the data as a chart.\\n\\nchart = st.line_chart(data)\\n\\nWait 1 second, so the change is clearer.\\n\\ntime.sleep(1)\\n\\nGrab some more data.\\n\\ndata2 = np.random.randn(10, 2)\\n\\nAppend the new data to the existing chart.\\n\\nchart.add_rows(data2)\\n```', metadata={'source': 'docs/content/kb/using-streamlit/append-data-table-chart.md'}),\n",
       " Document(page_content=\"title: How to download a file in Streamlit?\\nslug: /knowledge-base/using-streamlit/how-download-file-streamlit\\n\\nHow to download a file in Streamlit?\\n\\nUse the st.download_button widget that is natively built into Streamlit. Check out a sample app demonstrating how you can use st.download_button to download common file formats.\\n\\nExample usage\\n\\n```python\\nimport streamlit as st\\n\\nText files\\n\\ntext_contents = '''\\nFoo, Bar\\n123, 456\\n789, 000\\n'''\\n\\nDifferent ways to use the API\\n\\nst.download_button('Download CSV', text_contents, 'text/csv')\\nst.download_button('Download CSV', text_contents)  # Defaults to 'text/plain'\\n\\nwith open('myfile.csv') as f:\\n   st.download_button('Download CSV', f)  # Defaults to 'text/plain'\\n\\n---\\n\\nBinary files\\n\\nbinary_contents = b'whatever'\\n\\nDifferent ways to use the API\\n\\nst.download_button('Download file', binary_contents)  # Defaults to 'application/octet-stream'\\n\\nwith open('myfile.zip', 'rb') as f:\\n   st.download_button('Download Zip', f, file_name='archive.zip')  # Defaults to 'application/octet-stream'\\n\\nYou can also grab the return value of the button,\\n\\njust like with any other button.\\n\\nif st.download_button(...):\\n   st.write('Thanks for downloading!')\\n```\\n\\nAdditional resources:\\n\\nhttps://blog.streamlit.io/0-88-0-release-notes/\\n\\nhttps://streamlit-release-demos-0-88streamlit-app-0-88-v8ram3.streamlit.app/\\n\\nhttps://docs.streamlit.io/library/api-reference/widgets/st.download_button\", metadata={'source': 'docs/content/kb/using-streamlit/how-download-file-streamlit.md'}),\n",
       " Document(page_content=\"title: Deployment Issues\\nslug: /knowledge-base/deploy\\n\\nDeployment-related questions and errors\\n\\nHow do I deploy Streamlit on a domain so it appears to run on a regular port (i.e. port 80)?\\n\\nHow can I deploy multiple Streamlit apps on different subdomains?\\n\\nHow do I deploy Streamlit on Heroku, AWS, Google Cloud, etc...?\\n\\nInvoking a Python subprocess in a deployed Streamlit app\\n\\nDoes Streamlit support the WSGI Protocol? (aka Can I deploy Streamlit with gunicorn?)\\n\\nArgh. This app has gone over its resource limits.\\n\\nApp is not loading when running remotely\\n\\nAuthentication without SSO\\n\\nI don't have GitHub or GSuite. How do I sign in to Streamlit Community Cloud?\\n\\nHow do I share apps with viewers outside my organization?\\n\\nUpgrade the Streamlit version of your app on Streamlit Community Cloud\\n\\nOrganizing your apps with workspaces on Streamlit Community Cloud\\n\\nHow do I increase the upload limit of st.file_uploader on Streamlit Community Cloud?\\n\\nHow do I customize my app's subdomain?\\n\\nHow to update account admin settings on Streamlit Community Cloud?\\n\\nUnable to edit or delete apps in Streamlit Community Cloud after modifying GitHub username\\n\\nHuh. This is isn't supposed to happen message after trying to log in\\n\\nHuh. This isn't supposed to happen. No valid SSO connection for domain\\n\\nView-only access to app after changing GitHub username or repository name\\n\\nLogin attempt to Streamlit Community Cloud fails with error 403\\n\\nHow to submit a support case for Streamlit Community Cloud\\n\\nHow to delete your Streamlit Community Cloud account\", metadata={'source': 'docs/content/kb/deployments/index.md'}),\n",
       " Document(page_content='site_menu:\\n  - category: Streamlit library\\n    url: /library\\n    color: violet-70\\n    icon: description\\n  - category: Streamlit library / Get started\\n    url: /library/get-started\\n  - category: Streamlit library / Get started / Installation\\n    url: /library/get-started/installation\\n  - category: Streamlit library / Get started / Main concepts\\n    url: /library/get-started/main-concepts\\n  - category: Streamlit library / Get started / Create an app\\n    url: /library/get-started/create-an-app\\n  - category: Streamlit library / Get started / Multipage apps\\n    url: /library/get-started/multipage-apps\\n  - category: Streamlit library / Get started / Multipage apps / Create a multipage app\\n    url: /library/get-started/multipage-apps/create-a-multipage-app\\n  # - category: Streamlit library / Get started / Deploy an app\\n  #   url: /library/get-started/deploy-an-app\\n  # - category: Streamlit library / Get started / App gallery\\n  #   url: https://streamlit.io/gallery\\n  - category: Streamlit library / API reference\\n    url: /library/api-reference\\n  - category: Streamlit library / API reference / Write and magic\\n    url: /library/api-reference/write-magic\\n  - category: Streamlit library / API reference / Write and magic / st.write\\n    url: /library/api-reference/write-magic/st.write\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Write and magic / magic\\n    url: /library/api-reference/write-magic/magic\\n  - category: Streamlit library / API reference / Text elements\\n    url: /library/api-reference/text\\n  - category: Streamlit library / API reference / Text elements / st.markdown\\n    url: /library/api-reference/text/st.markdown\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.title\\n    url: /library/api-reference/text/st.title\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.header\\n    url: /library/api-reference/text/st.header\\n    isVersioned: true', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.subheader\\n    url: /library/api-reference/text/st.subheader\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.caption\\n    url: /library/api-reference/text/st.caption\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.code\\n    url: /library/api-reference/text/st.code\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.text\\n    url: /library/api-reference/text/st.text\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.latex\\n    url: /library/api-reference/text/st.latex\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Text elements / st.divider\\n    url: /library/api-reference/text/st.divider\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements\\n    url: /library/api-reference/data\\n  - category: Streamlit library / API reference / Data elements / st.dataframe\\n    url: /library/api-reference/data/st.dataframe\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.data_editor\\n    url: /library/api-reference/data/st.data_editor\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config\\n    url: /library/api-reference/data/st.column_config\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Column\\n    url: /library/api-reference/data/st.column_config/st.column_config.column\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Text column\\n    url: /library/api-reference/data/st.column_config/st.column_config.textcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Number column', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='url: /library/api-reference/data/st.column_config/st.column_config.numbercolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Checkbox column\\n    url: /library/api-reference/data/st.column_config/st.column_config.checkboxcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Selectbox column\\n    url: /library/api-reference/data/st.column_config/st.column_config.selectboxcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Datetime column\\n    url: /library/api-reference/data/st.column_config/st.column_config.datetimecolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Date column\\n    url: /library/api-reference/data/st.column_config/st.column_config.datecolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Time column\\n    url: /library/api-reference/data/st.column_config/st.column_config.timecolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / List column\\n    url: /library/api-reference/data/st.column_config/st.column_config.listcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Link column\\n    url: /library/api-reference/data/st.column_config/st.column_config.linkcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Image column\\n    url: /library/api-reference/data/st.column_config/st.column_config.imagecolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Line chart column\\n    url: /library/api-reference/data/st.column_config/st.column_config.linechartcolumn\\n    isVersioned: true', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Bar chart column\\n    url: /library/api-reference/data/st.column_config/st.column_config.barchartcolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.column_config / Progress column\\n    url: /library/api-reference/data/st.column_config/st.column_config.progresscolumn\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.table\\n    url: /library/api-reference/data/st.table\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.metric\\n    url: /library/api-reference/data/st.metric\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Data elements / st.json\\n    url: /library/api-reference/data/st.json\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements\\n    url: /library/api-reference/charts\\n  - category: Streamlit library / API reference / Chart elements / st.line_chart\\n    url: /library/api-reference/charts/st.line_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.area_chart\\n    url: /library/api-reference/charts/st.area_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.bar_chart\\n    url: /library/api-reference/charts/st.bar_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.pyplot\\n    url: /library/api-reference/charts/st.pyplot\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.altair_chart\\n    url: /library/api-reference/charts/st.altair_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.vega_lite_chart\\n    url: /library/api-reference/charts/st.vega_lite_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.plotly_chart', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='url: /library/api-reference/charts/st.plotly_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.bokeh_chart\\n    url: /library/api-reference/charts/st.bokeh_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.pydeck_chart\\n    url: /library/api-reference/charts/st.pydeck_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.graphviz_chart\\n    url: /library/api-reference/charts/st.graphviz_chart\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chart elements / st.map\\n    url: /library/api-reference/charts/st.map\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets\\n    url: /library/api-reference/widgets\\n  - category: Streamlit library / API reference / Input widgets / st.button\\n    url: /library/api-reference/widgets/st.button\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.download_button\\n    url: /library/api-reference/widgets/st.download_button\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.checkbox\\n    url: /library/api-reference/widgets/st.checkbox\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.radio\\n    url: /library/api-reference/widgets/st.radio\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.selectbox\\n    url: /library/api-reference/widgets/st.selectbox\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.multiselect\\n    url: /library/api-reference/widgets/st.multiselect\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.slider\\n    url: /library/api-reference/widgets/st.slider\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.select_slider', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='url: /library/api-reference/widgets/st.select_slider\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.text_input\\n    url: /library/api-reference/widgets/st.text_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.number_input\\n    url: /library/api-reference/widgets/st.number_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.text_area\\n    url: /library/api-reference/widgets/st.text_area\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.date_input\\n    url: /library/api-reference/widgets/st.date_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.time_input\\n    url: /library/api-reference/widgets/st.time_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.file_uploader\\n    url: /library/api-reference/widgets/st.file_uploader\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.camera_input\\n    url: /library/api-reference/widgets/st.camera_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Input widgets / st.color_picker\\n    url: /library/api-reference/widgets/st.color_picker\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Media elements\\n    url: /library/api-reference/media\\n  - category: Streamlit library / API reference / Media elements / st.image\\n    url: /library/api-reference/media/st.image\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Media elements / st.audio\\n    url: /library/api-reference/media/st.audio\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Media elements / st.video\\n    url: /library/api-reference/media/st.video\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers\\n    url: /library/api-reference/layout', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='url: /library/api-reference/layout\\n  - category: Streamlit library / API reference / Layouts and containers / st.sidebar\\n    url: /library/api-reference/layout/st.sidebar\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.columns\\n    url: /library/api-reference/layout/st.columns\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.tabs\\n    url: /library/api-reference/layout/st.tabs\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.expander\\n    url: /library/api-reference/layout/st.expander\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.container\\n    url: /library/api-reference/layout/st.container\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Layouts and containers / st.empty\\n    url: /library/api-reference/layout/st.empty\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chat elements\\n    url: /library/api-reference/chat\\n  - category: Streamlit library / API reference / Chat elements / st.chat_message\\n    url: /library/api-reference/chat/st.chat_message\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Chat elements / st.chat_input\\n    url: /library/api-reference/chat/st.chat_input\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements\\n    url: /library/api-reference/status\\n  - category: Streamlit library / API reference / Status elements / st.progress\\n    url: /library/api-reference/status/st.progress\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.spinner\\n    url: /library/api-reference/status/st.spinner\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.balloons\\n    url: /library/api-reference/status/st.balloons\\n    isVersioned: true', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.snow\\n    url: /library/api-reference/status/st.snow\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.error\\n    url: /library/api-reference/status/st.error\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.warning\\n    url: /library/api-reference/status/st.warning\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.info\\n    url: /library/api-reference/status/st.info\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.success\\n    url: /library/api-reference/status/st.success\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Status elements / st.exception\\n    url: /library/api-reference/status/st.exception\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Control flow\\n    url: /library/api-reference/control-flow\\n  - category: Streamlit library / API reference / Control flow / st.stop\\n    url: /library/api-reference/control-flow/st.stop\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Control flow / st.form\\n    url: /library/api-reference/control-flow/st.form\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Control flow / st.form_submit_button\\n    url: /library/api-reference/control-flow/st.form_submit_button\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Control flow / st.experimental_rerun\\n    url: /library/api-reference/control-flow/st.experimental_rerun\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Utilities\\n    url: /library/api-reference/utilities\\n  - category: Streamlit library / API reference / Utilities / st.set_page_config\\n    url: /library/api-reference/utilities/st.set_page_config\\n    isVersioned: true', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='isVersioned: true\\n  - category: Streamlit library / API reference / Utilities / st.echo\\n    url: /library/api-reference/utilities/st.echo\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Utilities / st.help\\n    url: /library/api-reference/utilities/st.help\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Utilities / st.experimental_get_query_params\\n    url: /library/api-reference/utilities/st.experimental_get_query_params\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Utilities / st.experimental_set_query_params\\n    url: /library/api-reference/utilities/st.experimental_set_query_params\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Mutate charts\\n    url: /library/api-reference/mutate\\n  - category: Streamlit library / API reference / State management\\n    url: /library/api-reference/session-state\\n  - category: Streamlit library / API reference / Performance\\n    url: /library/api-reference/performance\\n  - category: Streamlit library / API reference / Performance / st.cache_data\\n    url: /library/api-reference/performance/st.cache_data\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Performance / Clear cached data\\n    url: /library/api-reference/performance/st.cache_data.clear\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Performance / st.cache_resource\\n    url: /library/api-reference/performance/st.cache_resource\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Performance / Clear cached resources\\n    url: /library/api-reference/performance/st.cache_resource.clear\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Performance / st.cache\\n    url: /library/api-reference/performance/st.cache\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Performance / st.experimental_memo', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='url: /library/api-reference/performance/st.experimental_memo\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Performance / Clear memo\\n    url: /library/api-reference/performance/st.experimental_memo.clear\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Performance / st.experimental_singleton\\n    url: /library/api-reference/performance/st.experimental_singleton\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Performance / Clear singleton\\n    url: /library/api-reference/performance/st.experimental_singleton.clear\\n    isVersioned: true\\n    isDeprecated: true\\n  - category: Streamlit library / API reference / Personalization\\n    url: /library/api-reference/personalization\\n    isVersioned: false\\n  - category: Streamlit library / API reference / Personalization / st.experimental_user\\n    url: /library/api-reference/personalization/st.experimental_user\\n    isVersioned: false\\n  - category: Streamlit library / API reference / Connections and databases\\n    url: /library/api-reference/connections\\n  - category: Streamlit library / API reference / Connections and databases / st.experimental_connection\\n    url: /library/api-reference/connections/st.experimental_connection\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Connections and databases / SQLConnection\\n    url: /library/api-reference/connections/st.connections.sqlconnection\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Connections and databases / SnowparkConnection\\n    url: /library/api-reference/connections/st.connections.snowparkconnection\\n    isVersioned: true\\n  - category: Streamlit library / API reference / Connections and databases / Connection base class\\n    url: /library/api-reference/connections/st.connections.experimentalbaseconnection\\n    isVersioned: true\\n  - category: Streamlit library / Advanced features', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='url: /library/advanced-features\\n  - category: Streamlit library / Advanced features / ☰ App menu\\n    url: /library/advanced-features/app-menu\\n  - category: Streamlit library / Advanced features/ Command-line options\\n    url: /library/advanced-features/cli\\n  - category: Streamlit library / Advanced features/ Configuration\\n    url: /library/advanced-features/configuration\\n  - category: Streamlit library / Advanced features / Theming\\n    url: /library/advanced-features/theming\\n  - category: Streamlit library / Advanced features/ Caching\\n    url: /library/advanced-features/caching\\n  - category: Streamlit library / Advanced features/ Connecting to data\\n    url: /library/advanced-features/connecting-to-data\\n  - category: Streamlit library / Advanced features/ Optimize performance with st.cache\\n    url: /library/advanced-features/st.cache\\n    visible: false\\n  - category: Streamlit library / Advanced features/ Experimental cache primitives\\n    url: /library/advanced-features/experimental-cache-primitives\\n    visible: false\\n  - category: Streamlit library / Advanced features/ Add statefulness to apps\\n    url: /library/advanced-features/session-state\\n  - category: Streamlit library / Advanced features/ Button behavior and examples\\n    url: /library/advanced-features/button-behavior-and-examples\\n  - category: Streamlit library / Advanced features/ Dataframes\\n    url: /library/advanced-features/dataframes\\n  - category: Streamlit library / Advanced features/ Widget semantics\\n    url: /library/advanced-features/widget-semantics\\n  - category: Streamlit library / Advanced features/ Pre-release features\\n    url: /library/advanced-features/prerelease\\n  - category: Streamlit library / Advanced features/ Working with timezones\\n    url: /library/advanced-features/timezone-handling\\n  - category: Streamlit library / Advanced features/ Static file serving\\n    url: /library/advanced-features/static-file-serving\\n  - category: Streamlit library / Advanced features/ HTTPS support', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='url: /library/advanced-features/https-support\\n  - category: Streamlit library / Advanced features/ Secrets management\\n    url: /library/advanced-features/secrets-management\\n  - category: Streamlit library / Components\\n    url: /library/components\\n  - category: Streamlit library / Components / Components API\\n    url: /library/components/components-api\\n  - category: Streamlit library / Components / Create a Component\\n    url: /library/components/create\\n  - category: Streamlit library / Components / Publish a Component\\n    url: /library/components/publish\\n  - category: Streamlit library / Components / Component gallery\\n    url: https://streamlit.io/components\\n  - category: Streamlit library / Roadmap\\n    url: https://roadmap.streamlit.app\\n  - category: Streamlit library / Changelog\\n    url: /library/changelog\\n  - category: Streamlit library / Cheat sheet\\n    url: /library/cheatsheet', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='category: Streamlit Community Cloud\\n    url: /streamlit-community-cloud\\n    color: l-blue-70\\n    icon: cloud\\n\\ncategory: Streamlit Community Cloud / Get started\\n    url: /streamlit-community-cloud/get-started\\n\\ncategory: Streamlit Community Cloud / Get started / Deploy an app\\n    url: /streamlit-community-cloud/get-started/deploy-an-app\\n\\ncategory: Streamlit Community Cloud / Get started / Deploy an app / App dependencies\\n    url: /streamlit-community-cloud/get-started/deploy-an-app/app-dependencies\\n\\ncategory: Streamlit Community Cloud / Get started / Deploy an app / Connect to data sources\\n    url: /streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources\\n\\ncategory: Streamlit Community Cloud / Get started / Deploy an app / Connect to data sources / Secrets management\\n    url: /streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources/secrets-management\\n\\ncategory: Streamlit Community Cloud / Get started / Embed your app\\n    url: /streamlit-community-cloud/get-started/embed-your-app\\n\\ncategory: Streamlit Community Cloud / Get started / Share your app\\n    url: /streamlit-community-cloud/get-started/share-your-app\\n\\ncategory: Streamlit Community Cloud / Get started / Share your app / App indexability\\n    url: /streamlit-community-cloud/get-started/share-your-app/indexability\\n\\ncategory: Streamlit Community Cloud / Get started / Share your app / Share previews\\n    url: /streamlit-community-cloud/get-started/share-your-app/share-previews\\n\\ncategory: Streamlit Community Cloud / Get started / Manage your app\\n    url: /streamlit-community-cloud/get-started/manage-your-app\\n\\ncategory: Streamlit Community Cloud / Manage your account\\n    url: /streamlit-community-cloud/manage-your-account\\n\\ncategory: Streamlit Community Cloud / Manage your account / Update your email\\n    url: /streamlit-community-cloud/manage-your-account/update-your-email', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='category: Streamlit Community Cloud / Manage your account / Delete your account\\n    url: /streamlit-community-cloud/manage-your-account/delete-your-account\\n  # - category: Streamlit Community Cloud / Additional features\\n  #   url: /streamlit-community-cloud/additional-features\\n\\ncategory: Streamlit Community Cloud / Trust and Security\\n    url: /streamlit-community-cloud/trust-and-security\\n\\ncategory: Streamlit Community Cloud / Troubleshooting\\n    url: /streamlit-community-cloud/troubleshooting\\n\\ncategory: Knowledge base\\n    url: /knowledge-base\\n    color: orange-70\\n    icon: school\\n\\ncategory: Knowledge base / Tutorials\\n    url: /knowledge-base/tutorials\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources\\n    url: /knowledge-base/tutorials/databases\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / AWS S3\\n    url: /knowledge-base/tutorials/databases/aws-s3\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / BigQuery\\n    url: /knowledge-base/tutorials/databases/bigquery\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Deta Base\\n    url: /knowledge-base/tutorials/databases/deta-base\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Firestore\\n    url: https://blog.streamlit.io/streamlit-firestore/\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Google Cloud Storage\\n    url: /knowledge-base/tutorials/databases/gcs\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Microsoft SQL Server\\n    url: /knowledge-base/tutorials/databases/mssql\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / MongoDB\\n    url: /knowledge-base/tutorials/databases/mongodb\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / MySQL\\n    url: /knowledge-base/tutorials/databases/mysql\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / PostgreSQL\\n    url: /knowledge-base/tutorials/databases/postgresql', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='category: Knowledge base / Tutorials / Connect to data sources / Private Google Sheet\\n    url: /knowledge-base/tutorials/databases/private-gsheet\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Public Google Sheet\\n    url: /knowledge-base/tutorials/databases/public-gsheet\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Snowflake\\n    url: /knowledge-base/tutorials/databases/snowflake\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Supabase\\n    url: /knowledge-base/tutorials/databases/supabase\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / Tableau\\n    url: /knowledge-base/tutorials/databases/tableau\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / TiDB\\n    url: /knowledge-base/tutorials/databases/tidb\\n\\ncategory: Knowledge base / Tutorials / Connect to data sources / TigerGraph\\n    url: /knowledge-base/tutorials/databases/tigergraph\\n\\ncategory: Knowledge base / Tutorials / Deploy Streamlit apps\\n    url: /knowledge-base/tutorials/deploy\\n\\ncategory: Knowledge base / Tutorials / Deploy Streamlit apps / Docker\\n    url: /knowledge-base/tutorials/deploy/docker\\n\\ncategory: Knowledge base / Tutorials / Deploy Streamlit apps / Kubernetes\\n    url: /knowledge-base/tutorials/deploy/kubernetes\\n\\ncategory: Knowledge base / Tutorials / Session State basics\\n    url: /knowledge-base/tutorials/session-state\\n\\ncategory: Knowledge base / Tutorials / Build conversational apps\\n    url: /knowledge-base/tutorials/build-conversational-apps\\n\\ncategory: Knowledge base / Using Streamlit\\n    url: /knowledge-base/using-streamlit\\n\\ncategory: Knowledge base / Using Streamlit / How to animate elements?\\n    url: /knowledge-base/using-streamlit/animate-elements\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Append data to a table or chart\\n    url: /knowledge-base/using-streamlit/append-data-table-chart\\n    visible: false', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='category: Knowledge base / Using Streamlit / Batch elements and input widgets with st.form\\n    url: /knowledge-base/using-streamlit/batch-elements-input-widgets-form\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How do I create an anchor link?\\n    url: /knowledge-base/using-streamlit/create-anchor-link\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Enabling camera access in your browser\\n    url: /knowledge-base/using-streamlit/enable-camera\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Hide row indices when displaying a dataframe\\n    url: /knowledge-base/using-streamlit/hide-row-indices-displaying-dataframe\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How do I run my Streamlit script?\\n    url: /knowledge-base/using-streamlit/how-do-i-run-my-streamlit-script\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How to download a file in Streamlit?\\n    url: /knowledge-base/using-streamlit/how-download-file-streamlit\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How to download a Pandas DataFrame as a CSV?\\n    url: /knowledge-base/using-streamlit/how-download-pandas-dataframe-csv\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How do I upgrade to the latest version of Streamlit?\\n    url: /knowledge-base/using-streamlit/how-upgrade-latest-version-streamlit\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How to insert elements out of order?\\n    url: /knowledge-base/using-streamlit/insert-elements-out-of-order\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / What is the path of Streamlit’s config.toml file?\\n    url: /knowledge-base/using-streamlit/path-streamlit-config-toml\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How can I make st.pydeck_chart use custom Mapbox styles?\\n    url: /knowledge-base/using-streamlit/pydeck-chart-custom-mapbox-styles\\n    visible: false', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='category: Knowledge base / Using Streamlit / How to remove \"· Streamlit\" from the app title?\\n    url: /knowledge-base/using-streamlit/remove-streamlit-app-title\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How do you retrieve the filename of a file uploaded with st.file_uploader?\\n    url: /knowledge-base/using-streamlit/retrieve-filename-uploaded\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Sanity checks\\n    url: /knowledge-base/using-streamlit/sanity-checks\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How can I make Streamlit watch for changes in other modules I\\'m importing in my app?\\n    url: /knowledge-base/using-streamlit/streamlit-watch-changes-other-modules-importing-app\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / What browsers does Streamlit support?\\n    url: /knowledge-base/using-streamlit/supported-browsers\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Where does st.file_uploader store uploaded files and when do they get deleted?\\n    url: /knowledge-base/using-streamlit/where-file-uploader-store-when-deleted\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Widget updating for every second input when using session state\\n    url: /knowledge-base/using-streamlit/widget-updating-session-state\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / Why does Streamlit restrict nested st.columns?\\n    url: /knowledge-base/using-streamlit/why-streamlit-restrict-nested-columns\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / How to host static files in Streamlit?\\n    url: /knowledge-base/using-streamlit/how-host-static-files\\n    visible: false\\n\\ncategory: Knowledge base / Using Streamlit / What is serializable session state?\\n    url: /knowledge-base/using-streamlit/serializable-session-state\\n    visible: false\\n\\ncategory: Knowledge base / Streamlit Components\\n    url: /knowledge-base/components', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content=\"category: Knowledge base / Streamlit Components / How do I add a Component to the sidebar?\\n    url: /knowledge-base/components/add-component-sidebar\\n    visible: false\\n\\ncategory: Knowledge base / Streamlit Components / My Component seems to be stuttering...how do I fix that?\\n    url: /knowledge-base/components/component-blinking-stuttering-fix\\n    visible: false\\n\\ncategory: Knowledge base / Streamlit Components / How do Streamlit Components differ from functionality provided in the base Streamlit package?\\n    url: /knowledge-base/components/how-streamlit-components-differ-base-package\\n    visible: false\\n\\ncategory: Knowledge base / Streamlit Components / What types of things aren't possible with Streamlit Components?\\n    url: /knowledge-base/components/not-possibe-streamlit-components\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies\\n    url: /knowledge-base/dependencies\\n\\ncategory: Knowledge base / Installing dependencies / How to install a package not on PyPI or Conda but available on GitHub\\n    url: /knowledge-base/dependencies/install-package-not-pypi-conda-available-github\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies / ImportError libGL.so.1 cannot open shared object file No such file or directory\\n    url: /knowledge-base/dependencies/libgl\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies / ModuleNotFoundError No module named\\n    url: /knowledge-base/dependencies/module-not-found-error\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies / ERROR No matching distribution found for\\n    url: /knowledge-base/dependencies/no-matching-distribution\\n    visible: false\\n\\ncategory: Knowledge base / Installing dependencies / Install the Snowflake Connector for Python on Streamlit Community Cloud\\n    url: /knowledge-base/dependencies/snowflake-connector-python-streamlit-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues\\n    url: /knowledge-base/deploy\", metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='category: Knowledge base / Deployment issues / Authentication without SSO\\n    url: /knowledge-base/deploy/authentication-without-sso\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How can I deploy multiple Streamlit apps on different subdomains?\\n    url: /knowledge-base/deploy/deploy-multiple-streamlit-apps-different-subdomains\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How do I deploy Streamlit on a domain so it appears to run on a regular port (i.e. port 80)?\\n    url: /knowledge-base/deploy/deploy-streamlit-domain-port-80\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How do I deploy Streamlit on Heroku, AWS, Google Cloud, etc...?\\n    url: /knowledge-base/deploy/deploy-streamlit-heroku-aws-google-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Does Streamlit support the WSGI Protocol? (aka Can I deploy Streamlit with gunicorn?)\\n    url: /knowledge-base/deploy/does-streamlit-support-wsgi-protocol\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How do I increase the upload limit of st.file_uploader on Streamlit Community Cloud?\\n    url: /knowledge-base/deploy/increase-file-uploader-limit-streamlit-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Invoking a Python subprocess in a deployed Streamlit app\\n    url: /knowledge-base/deploy/invoking-python-subprocess-deployed-streamlit-app\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Organizing your apps with workspaces on Streamlit Community Cloud\\n    url: /knowledge-base/deploy/organizing-apps-workspaces-streamlit-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / App is not loading when running remotely\\n    url: /knowledge-base/deploy/remote-start\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Argh. This app has gone over its resource limits\\n    url: /knowledge-base/deploy/resource-limits\\n    visible: false', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content=\"category: Knowledge base / Deployment issues / How do I share apps with viewers outside my organization?\\n    url: /knowledge-base/deploy/share-apps-with-viewers-outside-organization\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / I don't have SSO. How do I sign in to Streamlit Community Cloud?\\n    url: /knowledge-base/deploy/sign-in-without-sso\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Upgrade the Streamlit version of your app on Streamlit Community Cloud\\n    url: /knowledge-base/deploy/upgrade-streamlit-version-on-streamlit-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Custom subdomains\\n    url: /knowledge-base/deploy/custom-subdomains\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How to update account admin settings on Streamlit Community Cloud?\\n    url: /knowledge-base/deploy/how-to-update-account-admin-settings-on-streamlit-community-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Unable to edit or delete apps in Streamlit Community Cloud after modifying GitHub username\\n    url: /knowledge-base/deploy/unable-to-edit-or-delete-apps-in-streamlit-community-cloud-after-modifying-github-username\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Huh. This is isn't supposed to happen message after trying to log in\\n    url: /knowledge-base/deploy/huh-this-isnt-supposed-to-happen-message-after-trying-to-log-in\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / Huh. This isn't supposed to happen. No valid SSO connection for domain\\n    url: /knowledge-base/deploy/huh-this-isnt-supposed-to-happen-no-valid-sso-connection-for-domain\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / View-only access to app after changing GitHub username or repository name\\n    url: /knowledge-base/deploy/view-only-access-to-app-after-changing-github-username-or-repository-name\\n    visible: false\", metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content='category: Knowledge base / Deployment issues / Login attempt to Streamlit Community Cloud fails with error 403\\n    url: /knowledge-base/deploy/login-attempt-to-streamlit-community-cloud-fails-with-error-403\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How to submit a support case for Streamlit Community Cloud\\n    url: /knowledge-base/deploy/how-to-submit-a-support-case-for-streamlit-community-cloud\\n    visible: false\\n\\ncategory: Knowledge base / Deployment issues / How to delete your Streamlit Community Cloud account\\n    url: /knowledge-base/deploy/how-to-delete-your-streamlit-community-cloud-account\\n    visible: false', metadata={'source': 'docs/content/menu.md'}),\n",
       " Document(page_content=\"title: How do I run my Streamlit script?\\nslug: /knowledge-base/using-streamlit/how-do-i-run-my-streamlit-script\\n\\nHow do I run my Streamlit script?\\n\\nWorking with Streamlit is simple. First you sprinkle a few Streamlit commands into a normal Python script, and then you run it. We list few ways to run your script, depending on your use case.\\n\\nUse streamlit run\\n\\nOnce you've created your script, say your_script.py, the easiest way to run it is with streamlit run:\\n\\nbash\\nstreamlit run your_script.py\\n\\nAs soon as you run the script as shown above, a local Streamlit server will spin up and your app will open in a new tab in your default web browser.\\n\\nPass arguments to your script\\n\\nWhen passing your script some custom arguments, they must be passed after two dashes. Otherwise the arguments get interpreted as arguments to Streamlit itself:\\n\\nbash\\nstreamlit run your_script.py [-- script args]\\n\\nPass a URL to streamlit run\\n\\nYou can also pass a URL to streamlit run! This is great when your script is hosted remotely, such as a GitHub Gist. For example:\\n\\nbash\\nstreamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n\\nRun Streamlit as a Python module\\n\\nAnother way of running Streamlit is to run it as a Python module. This is useful when configuring an IDE like PyCharm to work with Streamlit:\\n\\n```bash\\n\\nRunning\\n\\npython -m streamlit run your_script.py\\n```\\n\\n```bash\\n\\nis equivalent to:\\n\\nstreamlit run your_script.py\\n```\", metadata={'source': 'docs/content/kb/using-streamlit/how-run-my-streamlit-script.md'}),\n",
       " Document(page_content='title: View-only access to app after changing GitHub username or repository name\\nslug: /knowledge-base/deploy/view-only-access-to-app-after-changing-github-username-or-repository-name\\n\\nView-only access to app after changing GitHub username or repository name\\n\\nThis KB helps to resolve the issue of the apps becoming view-only.\\n\\nProblem\\n\\nWhen opening the dropdown menu of the apps on Streamlit Community CLoud, the options Reboot, Delete and Settings is grayed out, and there is a message \"You have view-only access to this application\".\\n\\nChanging your GitHub username or the repository name after deploying an app causes your app to become view-only.\\n\\nSolution\\n\\nPlease reach out to the Snowflake support team to delete the affected app(s) for you so you can redeploy them with your new username.', metadata={'source': 'docs/content/kb/deployments/view-only-access-to-app-after-changing-github-username-or-repository-name.md'}),\n",
       " Document(page_content=\"title: Huh. This isn't supposed to happen. No valid SSO connection for domain\\nslug: /knowledge-base/deploy/huh-this-isnt-supposed-to-happen-no-valid-sso-connection-for-domain\\n\\nHuh. This isn't supposed to happen. No valid SSO connection for domain\\n\\nThis KB helps you resolve the No valid SSO connection error while login to Streamlit Community Cloud with SSO.\\n\\nProblem\\n\\nYou have got the following screen when trying to login to your Streamlit Community Cloud account with SSO(SAML authentication):\\n\\nThis message means that you’ve logged in with both GitHub and Google in the past, but now you’re trying to log in with only GitHub.\\n\\nSolution\\n\\nYou can resolve the error by just logging in with both GitHub and Google and then, if you’d like to unlink your Google account from Streamlit Community Cloud, you can then log out via only Google.\", metadata={'source': 'docs/content/kb/deployments/huh-this-isnt-supposed-to-happen-no-valid-sso-connection-for-domain.md'}),\n",
       " Document(page_content='title: How to update account admin settings on Streamlit Community Cloud?\\nslug: /knowledge-base/deploy/how-to-update-account-admin-settings-on-streamlit-community-cloud\\n\\nHow to update account admin settings on Streamlit Community Cloud?\\n\\nSince retiring our paid Cloud plans in mid-2022, Streamlit Community Cloud no longer has account admins. Instead, app permissions are derived from GitHiub. In other words, to edit an app, you’ll need permission to make changes to the app’s GitHub repository and the changes will happen in GitHub.', metadata={'source': 'docs/content/kb/deployments/how-to-update-account-admin-settings-on-streamlit-community-cloud.md'}),\n",
       " Document(page_content='title: Organizing your apps with workspaces on Streamlit Community Cloud\\nslug: /knowledge-base/deploy/organizing-apps-workspaces-streamlit-cloud\\n\\nOrganizing your apps with workspaces on Streamlit Community Cloud\\n\\nStreamlit Community Cloud is organized into workspaces, which automatically group your apps according to the corresponding GitHub repository\\'s owner. If you are part of multiple repositories, then you will have multiple workspaces.\\n\\nPersonal workspace\\n\\nIf an app\\'s GitHub repository is owned by you, the app will appear in your personal workspace, named \"<YourGitHubHandle>\".\\n\\nOrganization workspace\\n\\nIf an app\\'s GitHub repository is owned by an organization (such as your company), the app will appear in a separate workspace, named \"<GitHubOrganizationHandle>\".\\n\\nWorkspaces with view access\\n\\nYou will also have access to any workspaces containing app(s) for which you only have view access. These apps will have a \"view-only\" tooltip when you click on their respective hamburger menus.\\n\\nSwitching between workspaces\\n\\nTo switch between workspaces, click on the workspace listed in the top right corner, then select the desired workspace name.\\n\\nIf you have further questions about workspaces on Streamlit Community Cloud, please emails us at success@streamlit.io.', metadata={'source': 'docs/content/kb/deployments/organize-apps-workspaces.md'}),\n",
       " Document(page_content='title: Invoking a Python subprocess in a deployed Streamlit app\\nslug: /knowledge-base/deploy/invoking-python-subprocess-deployed-streamlit-app\\n\\nInvoking a Python subprocess in a deployed Streamlit app\\n\\nProblem\\n\\nLet\\'s suppose you want to invoke a subprocess to run a Python script script.py in your deployed Streamlit app streamlit_app.py. For example, the machine learning library Ludwig is run using a command-line interface, or maybe you want to run a bash script or similar type of process from Python.\\n\\nYou have tried the following, but run into dependency issues for script.py, even though you have specified your Python dependencies in a requirements file:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport subprocess\\n\\nsubprocess.run([\"python\", \"script.py\"])\\n```\\n\\nSolution\\n\\nWhen you run the above code block, you will get the version of Python that is on the system path—not necessarily the Python executable installed in the virtual environment that the Streamlit code is running under.\\n\\nThe solution is to detect the Python executable directy with sys.executable:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport subprocess\\nimport sys\\n\\nsubprocess.run([f\"{sys.executable}\", \"script.py\"])\\n```\\n\\nThis ensures that script.py is running under the same Python executable as your Streamlit code—where your Python dependencies are installed.\\n\\nRelevant links\\n\\nhttps://stackoverflow.com/questions/69947867/run-portion-of-python-code-in-parallel-from-a-streamlit-app/69948545#69948545\\n\\nhttps://discuss.streamlit.io/t/modulenotfounderror-no-module-named-cv2-streamlit/18319/3?u=snehankekre\\n\\nhttps://docs.python.org/3/library/sys.html#sys.executable', metadata={'source': 'docs/content/kb/deployments/invoking-python-subprocess-deployed-streamlit-app.md'}),\n",
       " Document(page_content='title: Huh. This is isn\\'t supposed to happen message after trying to log in\\nslug: /knowledge-base/deploy/huh-this-isnt-supposed-to-happen-message-after-trying-to-log-in\\n\\nHuh. This is isn\\'t supposed to happen message after trying to log in\\n\\nThis article helps to resolve the login issue caused by email mismatching between the GitHub and the Streamlit Community Cloud.\\n\\nProblem\\n\\nYou see the following message after signing in to your Streamlit Community Cloud account:\\n\\nThis message usually indicates that our system has linked your GitHub username with an email address other than the email address you\\'re currently logged in with.\\n\\nSolution\\n\\nNo worries – all you have to do is:\\n\\nLog out of Streamlit Community Cloud completely (via both your email and GitHub accounts).\\n\\nLog in first with your email account (you can do so via either \"Continue with Google\" or \"Continue with email\").\\n\\nLog in with your GitHub account.', metadata={'source': 'docs/content/kb/deployments/huh-this-is-isnt-supposed-to-happen-message-after-trying-to-log-in.md'}),\n",
       " Document(page_content=\"title: How can I deploy multiple Streamlit apps on different subdomains?\\nslug: /knowledge-base/deploy/deploy-multiple-streamlit-apps-different-subdomains\\n\\nHow can I deploy multiple Streamlit apps on different subdomains?\\n\\nProblem\\n\\nYou want to deploy multiple Streamlit apps on different subdomains.\\n\\nSolution\\n\\nLike running your Streamlit app on more common ports such as 80, subdomains are handled by a web server like Apache or Nginx:\\n\\nSet up a web server on a machine with a public IP address, then use a DNS server to point all desired subdomains to your webserver's IP address\\n\\nConfigure your web server to route requests for each subdomain to the different ports that your Streamlit apps are running on\\n\\nCheck out these two tutorials for Apache2 and Nginx that deal with setting up a webserver to redirect subdomains to different ports:\\n\\nApache2 subdomains\\n\\nNGinx subdomains\", metadata={'source': 'docs/content/kb/deployments/deploy-multiple-streamlit-apps-different-subdomains.md'}),\n",
       " Document(page_content='title: How do I deploy Streamlit on Heroku, AWS, Google Cloud, etc...?\\nslug: /knowledge-base/deploy/deploy-streamlit-heroku-aws-google-cloud\\n\\nHow do I deploy Streamlit on Heroku, AWS, Google Cloud, etc...?\\n\\nProblem\\n\\nYou want to deploy your Streamlit app on a cloud service other than Streamlit Community Cloud.\\n\\nSolution\\n\\nDocker\\n\\nKubernetes\\n\\nWhile we work on official Streamlit deployment guides for other hosting providers, here are some user-submitted tutorials for different cloud services:\\n\\nHow to deploy Streamlit apps to Google App Engine, by Yuichiro Tachibana (Tsuchiya)\\n\\nHow to Deploy Streamlit to a Free Amazon EC2 instance, by Rahul Agarwal\\n\\nHost Streamlit on Heroku, by Maarten Grootendorst\\n\\nHost Streamlit on Azure, by Richard Peterson\\n\\nHost Streamlit on 21YunBox, by Toby Lei\\n\\nYou can find guides for other hosting providers on our community-supported deployment wiki.', metadata={'source': 'docs/content/kb/deployments/deploy-streamlit-heroku-aws-google-cloud.md'}),\n",
       " Document(page_content='title: How do I share apps with viewers outside my organization?\\nslug: /knowledge-base/deploy/share-apps-with-viewers-outside-organization\\n\\nHow do I share apps with viewers outside my organization?\\n\\nWhen you share an app with someone outside of your organization, they will receive an email inviting them to sign in with their email.\\n\\nClick here to learn more about how sign in with email works.\\n\\nViewer auth allows you to restrict the viewers of your app. To access your app, users have to authenticate using an email-based passwordless login or single sign-on (SSO). You can share your app with viewers outside your organization in two ways:\\n\\nAdding viewers from the app\\n\\nAdding viewers from your dashboard\\n\\nAdding viewers from the app\\n\\nFrom your deployed app you can easily add viewers from your developer console.\\n\\nSelect \"Manage app\" in the lower right corner.\\n\\nChoose \"Settings\" from the menu.\\n\\nAdd Viewers in Settings.\\n\\nYou can choose to allow only selected viewers based on their individual emails. Make sure to enter them as a line-separated list.\\n\\nThe viewers you have added will receive an email, like the one below, inviting them to sign in with their email.\\n\\nAdding viewers from your dashboard\\n\\nYou can also add viewers directly from your dashboard.\\n\\nOpen settings for your app\\n\\nNavigate to the app you want to add viewer to and click the hamburger icon to select \"Settings.\"\\n\\nAdd Viewers in Settings\\n\\nClick on the \"Sharing\" section in the App Settings and in the text input area, provide a line-separated list of email addresses for the users you wish to grant viewer access to your app. Click \"Save.\"\\n\\nThe viewers you have added will receive an email, like the one below, inviting them to sign in with their email.', metadata={'source': 'docs/content/kb/deployments/share-apps-with-viewers-outside-organization.md'}),\n",
       " Document(page_content=\"title: How to delete your Streamlit Community Cloud account\\nslug: /knowledge-base/deploy/how-to-delete-your-streamlit-community-cloud-account\\n\\nHow to delete your Streamlit Community Cloud account\\n\\nDeleting your Streamlit Community Cloud account is just as easy as creating it via the Settings tab. When you delete your account, your information, account, and all your hosted apps are deleted as well.\\n\\nDeleting your account is permanent and cannot be undone. Make sure you really want to delete your account and all hosted apps before proceeding.\\n\\nHow to delete your account\\n\\nFollow these steps to delete your account:\\n\\nMake sure you are logged in to Streamlit Community Cloud: https://share.streamlit.io/.\\n\\nClick 'Settings' in the top right corner of the page to go to the Settings dashboard. In the 'Account' section, click 'Delete account':\\n\\nIn the 'Delete account?' modal that appears, you will be asked to confirm that you want to delete your account by typing:\\n\\ndelete <your email address>\\n\\nType in delete followed by your email address and click 'Delete account forever':\\n\\nYou will then be logged out and your account, information, and apps will be permanently deleted.\\n\\nUpon deletion, you're shown a confirmation message that your account has been deleted, after which you will be redirected to the Streamlit Community Cloud homepage.\\n\\nIt's that simple! If you have any questions or run into issues deleting your account, please reach out to us on the Forum. We're happy to help! 🎈\", metadata={'source': 'docs/content/kb/deployments/how-to-delete-account.md'}),\n",
       " Document(page_content='title: How do I deploy Streamlit on a domain so it appears to run on a regular port (i.e. port 80)?\\nslug: /knowledge-base/deploy/deploy-streamlit-domain-port-80\\n\\nHow do I deploy Streamlit on a domain so it appears to run on a regular port (i.e. port 80)?\\n\\nProblem\\n\\nYou want to deploy a Streamlit app on a domain so it appears to run on port 80.\\n\\nSolution\\n\\nYou should use a reverse proxy to forward requests from a webserver like Apache or Nginx to the port where your Streamlit app is running. You can accomplish this in several different ways. The simplest way is to forward all requests sent to your domain so that your Streamlit app appears as the content of your website.\\n\\nAnother approach is to configure\\xa0your webserver to forward requests to designated subfolders (e.g. http://awesomestuff.net/streamlitapp) to different Streamlit apps on the same domain, as in this example config for Nginx submitted by a Streamlit community member.\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/permission-denied-in-ec2-port-80/798/3\\n\\nhttps://discuss.streamlit.io/t/how-to-use-streamlit-with-nginx/378/7', metadata={'source': 'docs/content/kb/deployments/deploy-streamlit-domain-port-80.md'}),\n",
       " Document(page_content=\"title: Does Streamlit support the WSGI Protocol? (aka Can I deploy Streamlit with gunicorn?)\\nslug: /knowledge-base/deploy/does-streamlit-support-wsgi-protocol\\n\\nDoes Streamlit support the WSGI Protocol? (aka Can I deploy Streamlit with gunicorn?)\\n\\nProblem\\n\\nYou're not sure whether your Streamlit app can be deployed with gunicorn.\\n\\nSolution\\n\\nStreamlit does not support the WSGI protocol at this time, so deploying Streamlit with (for example) gunicorn is not currently possible. Check out this forum thread regarding deploying Streamlit in a gunicorn-like manner to see how other users have accomplished this.\", metadata={'source': 'docs/content/kb/deployments/does-streamlit-support-wsgi-protocol.md'}),\n",
       " Document(page_content=\"title: Upgrade the Streamlit version of your app on Streamlit Community Cloud\\nslug: /knowledge-base/deploy/upgrade-streamlit-version-on-streamlit-cloud\\n\\nUpgrade the Streamlit version of your app on Streamlit Community Cloud\\n\\nWant to use a cool new Streamlit feature but your app on Streamlit Community Cloud is running an old version of the Streamlit library? If that's you, don't worry! All you need to do is upgrade your app's Streamlit version. Here are five ways to do this, based on how your app manages dependencies:\\n\\nNo dependency file\\n\\nWhen there is no dependency file in the repo, the app will always use the same Streamlit version that existed when the app was first deployed on Streamlit Community Cloud; even if you reboot the app! In other words, Streamlit Community Cloud automatically pins the version for you so that the app does not break suddenly when rebooted in the future.\\n\\nYou may want to avoid getting into this situation if your app depends on a specific version of Streamlit. That is why we encourage you to use a dependency file and pin your desired version of Streamlit. We cover this in more detail in the sections below.\\n\\nrequirements.txt\\n\\nIf the Streamlit version is not pinned (i.e., the requirements file contains a line with streamlit and nothing else):\\n\\nReboot the app as described above.\\n\\nIf the Streamlit version is pinned (e.g., streamlit==1.4.0):\\n\\nAdapt the pinned version in the requirements file and push it to GitHub.\\n\\nThe app on Streamlit Community Cloud will reboot automatically as soon as it detects these changes.\\n\\npipenv/poetry/conda\\n\\nIf you use any of these dependency managers, you probably know what you need to do. 😉\\n\\nOn your local computer, run the command to update the Streamlit package:\\n\\npipenv update streamlit or\\n\\npoetry update streamlit or\\n\\nWith an activated conda environment, run:\\npip install -U streamlit and\\nconda env export\\n\\nThen push any changes to Pipfile.lock or poetry.lock or environment.yml to GitHub.\", metadata={'source': 'docs/content/kb/deployments/upgrade-streamlit-version.md'}),\n",
       " Document(page_content='The app on Streamlit Community Cloud will reboot automatically as soon as it detects these changes.', metadata={'source': 'docs/content/kb/deployments/upgrade-streamlit-version.md'}),\n",
       " Document(page_content='title: App is not loading when running remotely\\nslug: /knowledge-base/deploy/remote-start\\n\\nApp is not loading when running remotely\\n\\nBelow are a few common errors that occur when users spin up their own solution\\nto host a Streamlit app remotely.\\n\\nTo learn about a deceptively simple way to host Streamlit apps that avoids all\\nthe issues below, check out Streamlit Community Cloud.\\n\\nSymptom #1: The app never loads\\n\\nWhen you enter the app\\'s URL in a browser and all you see is a blank page, a\\n\"Page not found\" error, a \"Connection refused\" error, or anything like that,\\nfirst check that Streamlit is actually running on the remote server. On a Linux\\nserver you can SSH into it and then run:\\n\\nbash\\nps -Al | grep streamlit\\n\\nIf you see Streamlit running, the most likely culprit is the Streamlit port not\\nbeing exposed. The fix depends on your exact setup. Below are three example\\nfixes:\\n\\nTry port 80: Some hosts expose port 80 by default. To\\n  set Streamlit to use that port, start Streamlit with the --server.port\\n  option:\\n\\nbash\\n  streamlit run my_app.py --server.port=80\\n\\nAWS EC2 server: First, click on your instance in the AWS Console.\\n  Then scroll down and click on Security Groups → Inbound → Edit. Next, add\\n  a Custom TCP rule that allows the Port Range 8501 with Source\\n  0.0.0.0/0.\\n\\nOther types of server: Check the firewall settings.\\n\\nIf that still doesn\\'t solve the problem, try running a simple HTTP server\\ninstead of Streamlit, and seeing if that works correctly. If it does, then\\nyou know the problem lies somewhere in your Streamlit app or configuration (in\\nwhich case you should ask for help in our\\nforums!) If not, then it\\'s definitely unrelated\\nto Streamlit.\\n\\nHow to start a simple HTTP server:\\n\\nbash\\npython -m http.server [port]\\n\\nSymptom #2: The app says \"Please wait...\" forever\\n\\nIf when you try to load your app in a browser you see a blue box in the center\\nof the page with the text \"Please wait...\", the underlying cause is likely one\\nof the following:', metadata={'source': 'docs/content/kb/deployments/remote-start.md'}),\n",
       " Document(page_content=\"Misconfigured CORS\\n  protection.\\n\\nServer is stripping headers from the Websocket connection, thereby breaking\\n  compression.\\n\\nTo diagnose the issue, try temporarily disabling CORS protection by running\\nStreamlit with the --server.enableCORS flag set to false:\\n\\nbash\\nstreamlit run my_app.py --server.enableCORS=false\\n\\nIf this fixes your issue, you should re-enable CORS protection and then set\\nbrowser.serverPort and browser.serverAddress to the URL and port of your\\nStreamlit app.\\n\\nIf the issue persists, try disabling websocket compression by running Streamlit with the\\n--server.enableWebsocketCompression flag set to false\\n\\nbash\\nstreamlit run my_app.py --server.enableWebsocketCompression=false\\n\\nIf this fixes your issue, your server setup is likely stripping the\\nSec-WebSocket-Extensions HTTP header that is used to negotiate Websocket compression.\\n\\nCompression is not required for Streamlit to work, but it's strongly recommended as it\\nimproves performance. If you'd like to turn it back on, you'll need to find which part\\nof your infrastructure is stripping the Sec-WebSocket-Extensions HTTP header and\\nchange that behavior.\\n\\nSymptom #3: Unable to upload files when running in multiple replicas\\n\\nIf the file uploader widget returns an error with status code 403, this is probably\\ndue to a misconfiguration in your app's\\nXSRF protection logic.\\n\\nTo diagnose the issue, try temporarily disabling XSRF protection by running Streamlit\\nwith the --server.enableXsrfProtection flag set to false:\\n\\nbash\\nstreamlit run my_app.py --server.enableXsrfProtection=false\\n\\nIf this fixes your issue, you should re-enable XSRF protection and then\\nconfigure your app to use the same secret across every replica by setting the\\nserver.cookieSecret config option to the same hard-to-guess string everywhere.\", metadata={'source': 'docs/content/kb/deployments/remote-start.md'}),\n",
       " Document(page_content='title: I don\\'t have GitHub or GSuite. How do I sign in to Streamlit Community Cloud?\\nslug: /knowledge-base/deploy/sign-in-without-sso\\n\\nI don\\'t have GitHub or GSuite. How do I sign in to Streamlit Community Cloud?\\n\\nIf you don\\'t have GitHub or GSuite accounts, you can sign in with your email address! Visit share.streamlit.io, enter the email address you used to sign up for Streamlit Community Cloud, and click the \"Continue with email\" button.\\n\\nOnce you do so, you will see a confirmation message (like the one below) asking you to check your email.\\n\\nCheck your inbox for an email from Streamlit, with the subject \"Sign in to Streamlit Community Cloud\". Click the link in the email to sign in to Streamlit. Note that this link will expire in 15 minutes and can only be used once.\\n\\nOnce you click the link in your email, you will be taken to your Streamlit Community Cloud workspace!🎈', metadata={'source': 'docs/content/kb/deployments/sign-in-email.md'}),\n",
       " Document(page_content='title: Argh. This app has gone over its resource limits\\nslug: /knowledge-base/deploy/resource-limits\\n\\nArgh. This app has gone over its resource limits\\n\\nSorry! It means you\\'ve hit the resource limits of your Streamlit Community Cloud account.\\n\\nThere are a few things you can change in your app to make it less resource-hungry:\\n\\nReboot your app (temporary fix)\\n\\nUse st.cache_data or st.cache_resource to load models or data only once\\n\\nRestrict the cache size with ttl or max_entries\\n\\nMove big datasets to a database\\n\\nProfile your app\\'s memory usage\\n\\nCheck out our blog post on “Common app problems: Resource limits\" for more in-depth tips prevent your app from hitting the resource limits of the Streamlit Community Cloud.\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/common-app-problems-resource-limits/16969\\n\\nhttps://blog.streamlit.io/common-app-problems-resource-limits/\\n\\nWe offer free resource increases only to support nonprofits or educational organizations on a case-by-case basis. If you are a nonprofit or educational organization, please complete this form and we will review your submission as soon as possible.\\n\\nOnce the increase is completed, you will receive an email from the Streamlit marketing team with a confirmation that the increase has been applied.', metadata={'source': 'docs/content/kb/deployments/resource-limits.md'}),\n",
       " Document(page_content='title: Login attempt to Streamlit Community Cloud fails with error 403\\nslug: /knowledge-base/deploy/login-attempt-to-streamlit-community-cloud-fails-with-error-403\\n\\nLogin attempt to Streamlit Community Cloud fails with error 403\\n\\nProblem\\n\\nStreamlit Community Cloud has monitoring jobs to detect malicious users using the platform for crypto mining. These jobs sometimes result in false positives and a normal user starts getting error 403 against a login attempt.\\n\\nSolution\\n\\nPlease contact Support by providing your GitHub username for help referring to this article.', metadata={'source': 'docs/content/kb/deployments/login-attempt-to-streamlit-community-cloud-fails-with-error-403.md'}),\n",
       " Document(page_content='title: Custom subdomains\\nslug: /knowledge-base/deploy/custom-subdomains\\n\\nCustom subdomains\\n\\nOnce you\\'ve deployed your app on Community Cloud, it\\'s given an automatically generated subdomain that follows a structure based on your GitHub repo. This subdomain is unique to your app and can be used to share your app with others. However, the default subdomain is not always the most memorable or easy to share. E.g. the following is a bit of a mouthful!\\n\\nhttps://streamlit-demo-self-driving-streamlit-app-8jya0g.streamlit.app\\n\\nYou can instead set up a custom subdomain to make your app easier to share. You can customize your subdomain to reflect your app content, personal branding, or whatever you’d like. The URL will appear as:\\n\\n<your-custom-subdomain>.streamlit.app\\n\\nTo customize your app subdomain from the dashboard:\\n\\nClick the \"︙\" overflow menu to the app\\'s right and select \"Settings\".\\n\\nView the \"General\" tab in the App settings modal. Your app\\'s unique subdomain will appear here.\\n\\nPick a custom subdomain between 6 and 63 characters in length for your app\\'s URL and hit \"Save\".\\n\\nIt\\'s that simple! You can then access your app by visiting your custom subdomain URL 🎉.\\n\\nIf a custom subdomain is not available (e.g. because it\\'s already taken), you\\'ll see an error message like this:', metadata={'source': 'docs/content/kb/deployments/custom-subdomains.md'}),\n",
       " Document(page_content='title: How to submit a support case for Streamlit Community Cloud\\nslug: /knowledge-base/deploy/how-to-submit-a-support-case-for-streamlit-community-cloud\\n\\nHow to submit a support case for Streamlit Community Cloud\\n\\nThis article describes the steps to submit a support request to Snowflake for Streamlit Community Cloud.\\n\\nFor Snowflake customers, a support case can be submitted via the support portal on Snowsight.\\n\\nNavigate to https://community.snowflake.com/s/ in your browser.\\n\\nEnsure you are registered.\\n\\na. If you are already a registered user, enter your Snowflake Community username and password into the login form. Click LOG IN.\\n\\nb. If you are not a registered user, click \"Not a member?\". Complete the form on the next screen and follow the instructions to reset your password. Return to the original \"SUBMIT A CASE\" page and log in to your account.\\n\\nScroll down to the first main section of the page (past the search bar) and locate the Support dropdown menu. Select the \"SUBMIT A CASE\" link.\\n\\nSelect the option \"I am a Streamlit Community Cloud User\"\\n\\nHit the button \"Next\" to open the case description page.\\n\\nPlease fill out your request and submit the support case.\\n\\nYou should receive a confirmation email with the case number.\\n\\nA Snowflake Support engineer will follow up directly with the next steps to resolve your case. All communication will be through email.', metadata={'source': 'docs/content/kb/deployments/how-to-submit-a-support-case-for-streamlit-community-cloud.md'}),\n",
       " Document(page_content=\"title: Streamlit Components\\nslug: /knowledge-base/components\\n\\nStreamlit Components\\n\\nBelow are some selected questions we've received about Streamlit Components. If you don't find your question here, take a look on the Streamlit community forum via the Components tag.\\n\\nHow do Streamlit Components differ from functionality provided in the base Streamlit package?\\n\\nWhat types of things aren't possible with Streamlit Components?\\n\\nHow do I add a Component to the sidebar?\\n\\nMy Component seems to be blinking/stuttering...how do I fix that?\", metadata={'source': 'docs/content/kb/components/index.md'}),\n",
       " Document(page_content='title: Unable to edit or delete apps in Streamlit Community Cloud after modifying GitHub username\\nslug: /knowledge-base/deploy/unable-to-edit-or-delete-apps-in-streamlit-community-cloud-after-modifying-github-username\\n\\nUnable to edit or delete apps in Streamlit Community Cloud after modifying GitHub username\\n\\nProblem\\n\\nAfter updating the GitHub username, apps cannot be edited or deleted from Streamlit Community Cloud.\\n\\nSolution\\n\\nSupport can delete your old applications that were deployed before updating the GitHub username.\\n\\nOnce deleted, you can redeploy the applications using the new GitHub username.\\n\\nPlease contact Support with a list of the URLs for the apps you need to be deleted.', metadata={'source': 'docs/content/kb/deployments/unable-to-edit-or-delete-apps-in-streamlit-community-cloud-after-modifying-github-username.md'}),\n",
       " Document(page_content=\"title: How do I increase the upload limit of st.file_uploader on Streamlit Community Cloud?\\nslug: /knowledge-base/deploy/increase-file-uploader-limit-streamlit-cloud\\n\\nHow do I increase the upload limit of st.file_uploader on Streamlit Community Cloud?\\n\\nOverview\\n\\nBy default, files uploaded using st.file_uploader() are limited to 200MB. You can configure this using the server.maxUploadSize config option.\\n\\nStreamlit provides four different ways to set configuration options:\\n\\nIn a global config file at ~/.streamlit/config.toml for macOS/Linux or %userprofile%/.streamlit/config.toml for Windows:\\n   toml\\n   [server]\\n   maxUploadSize = 200\\n\\nIn a per-project config file at $CWD/.streamlit/config.toml, where $CWD is the folder you're running Streamlit from.\\n\\nThrough STREAMLIT_* environment variables, such as:\\n   bash\\n   export STREAMLIT_SERVER_MAX_UPLOAD_SIZE=200\\n\\nAs flags on the command line when running streamlit run:\\n   bash\\n   streamlit run your_script.py --server.maxUploadSize 200\\n\\nWhich of the four options should you choose for an app deployed to Streamlit Community Cloud? 🤔\\n\\nSolution\\n\\nWhen deploying your app to Streamlit Community Cloud, you should use option 1. Namely, set the maxUploadSize config option in a global config file (.streamlit/config.toml) uploaded to your app's GitHub repo. 🎈\\n\\nFor example, to increase the upload limit to 400MB, upload a .streamlit/config.toml file containing the following lines to your app's GitHub repo:\\n\\ntoml\\n[server]\\nmaxUploadSize = 400\\n\\nRelevant resources\\n\\nStreamlit drag and drop capping at 200MB, need workaround\\n\\nFile uploader widget API\\n\\nHow to set Streamlit configuration options\", metadata={'source': 'docs/content/kb/deployments/increase-upload-limit-cloud.md'}),\n",
       " Document(page_content='title: My Component seems to be blinking/stuttering...how do I fix that?\\nslug: /knowledge-base/components/component-blinking-stuttering-fix\\n\\nMy Component seems to be blinking/stuttering...how do I fix that?\\n\\nCurrently, no automatic debouncing of Component updates is performed within Streamlit. The Component creator themselves can decide to rate-limit the updates they send back to Streamlit.', metadata={'source': 'docs/content/kb/components/component-blinking-stuttering-fix.md'}),\n",
       " Document(page_content='title: How do I add a Component to the sidebar?\\nslug: /knowledge-base/components/add-component-sidebar\\n\\nHow do I add a Component to the sidebar?\\n\\nYou can add a component to st.sidebar using the with syntax. For example:\\n\\npython\\nwith st.sidebar:\\n    my_component(greeting=\"hello\")\\n\\nIn fact, you can add your component to any layout container (eg st.columns, st.expander), using the with syntax!\\n\\npython\\ncol1, col2 = st.columns(2)\\nwith col2:\\n    my_component(greeting=\"hello\")', metadata={'source': 'docs/content/kb/components/add-component-sidebar.md'}),\n",
       " Document(page_content='title: How do Streamlit Components differ from functionality provided in the base Streamlit package?\\nslug: /knowledge-base/components/how-streamlit-components-differ-base-package\\n\\nHow do Streamlit Components differ from functionality provided in the base Streamlit package?\\n\\nStreamlit Components are wrapped up in an iframe, which gives you the ability to do whatever you want (within the iframe) using any web technology you like.\\n\\nThere is a strict message protocol between Components and Streamlit, which makes possible for Components to act as widgets. As Streamlit Components are wrapped in iframe, they cannot modify their parent’s DOM (a.k.a the Streamlit report), which ensures that Streamlit is always secure even with user-written components.', metadata={'source': 'docs/content/kb/components/how-streamlit-components-differ-base-package.md'}),\n",
       " Document(page_content=\"title: What types of things aren't possible with Streamlit Components?\\nslug: /knowledge-base/components/not-possibe-streamlit-components\\n\\nWhat types of things aren't possible with Streamlit Components?\\n\\nBecause each Streamlit Component gets mounted into its own sandboxed iframe, this implies a few limitations on what is possible with Components:\\n\\nCan't communicate with other Components: Components can’t contain (or otherwise communicate with) other components, so Components cannot be used to build something like grid_layout\\n\\nCan't modify CSS: A Component can’t modify the CSS that the rest of the Streamlit app uses, so you can't create something like dark_mode\\n\\nCan't add/remove elements: A Component can’t add or remove other elements of a Streamlit app, so you couldn't make something like remove_streamlit_hamburger_menu\", metadata={'source': 'docs/content/kb/components/not-possibe-streamlit-components.md'}),\n",
       " Document(page_content='title: Tutorials\\nslug: /knowledge-base/tutorials\\n\\nTutorials\\n\\nOur tutorials include step-by-step examples of building different types of apps in Streamlit.\\n\\nConnect to data sources\\n\\nSession State basics\\n\\nDeploy Streamlit apps\\n\\nBuild conversational apps', metadata={'source': 'docs/content/kb/tutorials/index.md'}),\n",
       " Document(page_content='title: Authentication without SSO\\nslug: /knowledge-base/deploy/authentication-without-sso\\n\\nAuthentication without SSO\\n\\nIntroduction\\n\\nWant to secure your Streamlit app with passwords, but cannot implement single sign-on? We got you covered! This guide shows you two simple techniques for adding basic authentication to your Streamlit app, using secrets management.\\n\\nWhile this technique adds some level of security, it is NOT comparable to proper authentication with an SSO provider.\\n\\nOption 1: One global password for all users\\n\\nThis is the easiest option! Your app will ask for a password that\\'s shared between all users. It will be stored in the app secrets using secrets management. If you want to change this password or revoke a user\\'s access, you will need to change it for everyone. If you want to have one password per user instead, jump to Option 2 below.\\n\\nStep 1: Add the password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root dir. Create this file if it doesn\\'t exist yet and add your password to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\npassword = \"streamlit123\"\\n```\\n\\nBe sure to add this file to your .gitignore so you don\\'t commit your secrets!\\n\\nStep 2: Copy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at secrets management.\\n\\nStep 3: Ask for the password in your Streamlit app\\n\\nCopy the code below to your Streamlit app, insert your normal app code in the if statement at the bottom, and run it:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\ndef check_password():\\n    \"\"\"Returns True if the user had the correct password.\"\"\"', metadata={'source': 'docs/content/kb/deployments/authentication-without-sso.md'}),\n",
       " Document(page_content='if check_password():\\n    st.write(\"Here goes your normal Streamlit app...\")\\n    st.button(\"Click me\")\\n```\\n\\nIf everything worked out, your app should look like this:\\n\\nOption 2: Individual password for each user\\n\\nThis option allows you to set a username and password for each user of your app. Like in Option 1, both values will be stored in the app secrets using secrets management.\\n\\nStep 1: Add usernames & passwords to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root dir. Create this file if it doesn\\'t exist yet and add the usernames & password to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[passwords]\\n\\nFollow the rule: username = \"password\"\\n\\nalice_foo = \"streamlit123\"\\nbob_bar = \"mycrazypw\"\\n```\\n\\nBe sure to add this file to your .gitignore so you don\\'t commit your secrets!\\n\\nAlternatively, you could set up and manage usernames & passwords via a spreadsheet or database. To use secrets to securely connect to Google Sheets, AWS, and other data providers, read our tutorials on how to Connect Streamlit to data sources.\\n\\nStep 2: Copy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at secrets management.\\n\\nStep 3: Ask for username & password in your Streamlit app\\n\\nCopy the code below to your Streamlit app, insert your normal app code in the if statement at the bottom, and run it:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\ndef check_password():\\n    \"\"\"Returns True if the user had a correct password.\"\"\"\\n\\nif check_password():\\n    st.write(\"Here goes your normal Streamlit app...\")\\n    st.button(\"Click me\")\\n```\\n\\nIf everything worked out, your app should look like this:', metadata={'source': 'docs/content/kb/deployments/authentication-without-sso.md'}),\n",
       " Document(page_content='title: Session State basics\\nslug: /knowledge-base/tutorials/session-state\\n\\nSession State basics\\n\\nCheck out this Session State basics tutorial video by Streamlit Developer Advocate Dr. Marisa Smith to get started:', metadata={'source': 'docs/content/kb/tutorials/session-state.md'}),\n",
       " Document(page_content='title: Deploy Streamlit apps\\nslug: /knowledge-base/tutorials/deploy\\n\\nDeploy Streamlit apps\\n\\nThis sections contains step-by-step guides on how to deploy Streamlit apps to various cloud platforms and services. We have deployment guides for:\\n\\nStreamlit Community Cloud\\n\\nDocker\\n\\nKubernetes\\n\\nWhile we work on official Streamlit deployment guides for other hosting providers, here are some user-submitted tutorials for different cloud services:\\n\\nHow to deploy Streamlit apps to Google App Engine, by Yuichiro Tachibana (Tsuchiya)\\n\\nHow to Deploy Streamlit to a Free Amazon EC2 instance, by Rahul Agarwal\\n\\nHost Streamlit on Heroku, by Maarten Grootendorst\\n\\nHost Streamlit on Azure, by Richard Peterson\\n\\nHost Streamlit on 21YunBox, by Toby Lei\\n\\nCommunity-supported deployment wiki.', metadata={'source': 'docs/content/kb/tutorials/deploy/index.md'}),\n",
       " Document(page_content=\"title: Connect to data sources\\nslug: /knowledge-base/tutorials/databases\\n\\nConnect Streamlit to data sources\\n\\nThese step-by-step guides demonstrate how to connect Streamlit apps to various databases & APIs.\\nThey use Streamlit's secrets management and\\ncaching to provide secure and fast data access.\\n\\nAWS S3\\n\\nBigQuery\\n\\nDeta Base\\n\\nFirestore (blog)\\n\\nGoogle Cloud Storage\\n\\nMicrosoft SQL Server\\n\\nMongoDB\\n\\nMySQL\\n\\nPostgreSQL\\n\\nPrivate Google Sheet\\n\\nPublic Google Sheet\\n\\nSnowflake\\n\\nSupabase\\n\\nTableau\\n\\nTiDB\\n\\nTigerGraph\", metadata={'source': 'docs/content/kb/tutorials/databases/index.md'}),\n",
       " Document(page_content=\"title: Deploy Streamlit using Kubernetes\\nslug: /knowledge-base/tutorials/deploy/kubernetes\\n\\nDeploy Streamlit using Kubernetes\\n\\nIntroduction\\n\\nSo you have an amazing app and you want to start sharing it with other people, what do you do? You have a few options. First, where do you want to run your Streamlit app, and how do you want to access it?\\n\\nOn your corporate network\\xa0- Most corporate networks are closed to the outside world. You typically use a VPN to log onto your corporate network and access resources there. You could run your Streamlit app on a server in your corporate network for security reasons, to ensure that only folks internal to your company can access it.\\n\\nOn the cloud\\xa0- If you'd like to access your Streamlit app from outside of a corporate network, or share your app with folks outside of your home network or laptop, you might choose this option. In this case, it'll depend on your hosting provider. We have community-submitted guides from Heroku, AWS, and other providers.\\n\\nWherever you decide to deploy your app, you will first need to containerize it. This guide walks you through using Kubernetes to deploy your app. If you prefer Docker see Deploy Streamlit using Docker.\\n\\nPrerequisites\\n\\nInstall Docker Engine\\n\\nInstall the gcloud CLI\\n\\nInstall Docker Engine\\n\\nIf you haven't already done so, install Docker on your server. Docker provides\\xa0.deb and\\xa0.rpm packages from many Linux distributions, including:\\n\\nDebian\\n\\nUbuntu\\n\\nVerify that Docker Engine is installed correctly by running the\\xa0hello-world Docker image:\\n\\nbash\\nsudo docker run hello-world\\n\\nFollow Docker's official post-installation steps for Linux to run Docker as a non-root user, so that you don't have to preface the docker command with sudo.\\n\\nInstall the gcloud CLI\\n\\nIn this guide, we will orchestrate Docker containers with Kubernetes and host docker images on the Google Container Registry (GCR). As GCR is a Google-supported Docker registry, we need to register\\xa0gcloud as the Docker credential helper.\", metadata={'source': 'docs/content/kb/tutorials/deploy/kubernetes.md'}),\n",
       " Document(page_content='Follow the official documentation to Install the gcloud CLI and initialize it.\\n\\nCreate a Docker container\\n\\nWe need to create a docker container which contains all the dependencies and the application code. Below you can see the entrypoint, i.e. the command run when the container starts, and the Dockerfile definition.\\n\\nCreate an entrypoint script\\n\\nCreate a run.sh script containing the following:\\n\\n```bash\\n\\n!/bin/bash\\n\\nAPP_PID=\\nstopRunningProcess() {\\n    # Based on https://linuxconfig.org/how-to-propagate-a-signal-to-child-processes-from-a-bash-script\\n    if test ! \"${APP_PID}\" = \\'\\' && ps -p ${APP_PID} > /dev/null ; then\\n       > /proc/1/fd/1 echo \"Stopping ${COMMAND_PATH} which is running with process ID ${APP_PID}\"\\n\\ntrap stopRunningProcess EXIT TERM\\n\\nsource ${VIRTUAL_ENV}/bin/activate\\n\\nstreamlit run ${HOME}/app/streamlit_app.py &\\nAPP_ID=${!}\\n\\nwait ${APP_ID}\\n```\\n\\nCreate a Dockerfile\\n\\nDockerfile reference. The\\n\\ndocker build command builds an image from a\\n\\ndocker run command first creates a container over the specified image, and then starts it using the specified command.\\n\\nHere\\'s an example Dockerfile that you can add to the root of your directory.\\n\\n```docker\\nFROM python:3.8-slim\\n\\nRUN groupadd --gid 1000 appuser \\\\\\n    && useradd --uid 1000 --gid 1000 -ms /bin/bash appuser\\n\\nRUN pip3 install --no-cache-dir --upgrade \\\\\\n    pip \\\\\\n    virtualenv\\n\\nRUN apt-get update && apt-get install -y \\\\\\n    build-essential \\\\\\n    software-properties-common \\\\\\n    git\\n\\nUSER appuser\\nWORKDIR /home/appuser\\n\\nRUN git clone https://github.com/streamlit/streamlit-example.git app\\n\\nENV VIRTUAL_ENV=/home/appuser/venv\\nRUN virtualenv ${VIRTUAL_ENV}\\nRUN . ${VIRTUAL_ENV}/bin/activate && pip install -r app/requirements.txt\\n\\nEXPOSE 8501\\n\\nCOPY run.sh /home/appuser\\nENTRYPOINT [\"./run.sh\"]\\n```', metadata={'source': 'docs/content/kb/tutorials/deploy/kubernetes.md'}),\n",
       " Document(page_content='As mentioned in Development flow, for Streamlit version 1.10.0 and higher, Streamlit apps cannot be run from the root directory of Linux distributions. Your main script should live in a directory other than the root directory. If you try to run a Streamlit app from the root directory, Streamlit will throw a FileNotFoundError: [Errno 2] No such file or directory error. For more information, see GitHub issue #5239.\\n\\nIf you are using Streamlit version 1.10.0 or higher, you must set the WORKDIR to a directory other than the root directory. For example, you can set the WORKDIR to /home/appuser as shown in the example Dockerfile above.\\n\\nBuild a Docker image\\n\\nPut the above files (run.sh and Dockerfile) in the same folder and build the docker image:\\n\\ndocker\\ndocker build --platform linux/amd64 -t gcr.io/$GCP_PROJECT_ID/k8s-streamlit:test .\\n\\nReplace $GCP_PROJECT_ID in the above command with the name of your Google Cloud project.\\n\\nUpload the Docker image to a container registry\\n\\nThe next step is to upload the Docker image to a container registry. In this example, we will use the Google Container Registry (GCR). Start by enabling the Container Registry API. Sign in to Google Cloud and navigate to your project’s Container Registry and click Enable.\\n\\nWe can now build the Docker image from the previous step and push it to our project’s GCR. Be sure to replace $GCP_PROJECT_ID in the docker push command with the name of your project:\\n\\nbash\\ngcloud auth configure-docker\\ndocker push gcr.io/$GCP_PROJECT_ID/k8s-streamlit:test\\n\\nCreate a Kubernetes deployment\\n\\nFor this step you will need a:\\n\\nRunning Kubernetes service\\n\\nCustom domain for which you can generate a TLS certificate\\n\\nDNS service where you can configure your custom domain to point to the application IP\\n\\nAs the image was uploaded to the container registry in the previous step, we can run it in Kubernetes using the below configurations.\\n\\nInstall and run Kubernetes', metadata={'source': 'docs/content/kb/tutorials/deploy/kubernetes.md'}),\n",
       " Document(page_content='Install and run Kubernetes\\n\\nMake sure your Kubernetes client, kubectl, is installed and running on your machine.\\n\\nConfigure a Google OAuth Client and oauth2-proxy\\n\\nFor configuring the Google OAuth Client, please see Google Auth Provider. Configure oauth2-proxy to use the desired OAuth Provider Configuration and update the oath2-proxy config in the config map.\\n\\nThe below configuration contains a ouath2-proxy sidecar container which handles the authentication with Google. You can learn more from the oauth2-proxy repository.\\n\\nCreate a Kubernetes configuration file\\n\\nCreate a YAML configuration file named k8s-streamlit.yaml:\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: streamlit-configmap\\ndata:\\n  oauth2-proxy.cfg: |-\\n    http_address = \"0.0.0.0:4180\"\\n    upstreams = [\"http://127.0.0.1:8501/\"]\\n    email_domains = [\"*\"]\\n    client_id = \"\"\\n    client_secret = \"\"\\n    cookie_secret = \"<16, 24, or 32 bytes>\"\\n    redirect_url =', metadata={'source': 'docs/content/kb/tutorials/deploy/kubernetes.md'}),\n",
       " Document(page_content='apiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: streamlit-deployment\\n  labels:\\n    app: streamlit\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: streamlit\\n  template:\\n    metadata:\\n      labels:\\n        app: streamlit\\n    spec:\\n      containers:\\n        - name: oauth2-proxy\\n          image: quay.io/oauth2-proxy/oauth2-proxy:v7.2.0\\n          args: [\"--config\", \"/etc/oauth2-proxy/oauth2-proxy.cfg\"]\\n          ports:\\n            - containerPort: 4180\\n          livenessProbe:\\n            httpGet:\\n              path: /ping\\n              port: 4180\\n              scheme: HTTP\\n          readinessProbe:\\n            httpGet:\\n              path: /ping\\n              port: 4180\\n              scheme: HTTP\\n          volumeMounts:\\n            - mountPath: \"/etc/oauth2-proxy\"\\n              name: oauth2-config\\n        - name: streamlit\\n          image: gcr.io/GCP_PROJECT_ID/k8s-streamlit:test\\n          imagePullPolicy: Always\\n          ports:\\n            - containerPort: 8501\\n          livenessProbe:\\n            httpGet:\\n              path: /_stcore/health\\n              port: 8501\\n              scheme: HTTP\\n            timeoutSeconds: 1\\n          readinessProbe:\\n            httpGet:\\n              path: /_stcore/health\\n              port: 8501\\n              scheme: HTTP\\n            timeoutSeconds: 1\\n          resources:\\n            limits:\\n              cpu: 1\\n              memory: 2Gi\\n            requests:\\n              cpu: 100m\\n              memory: 745Mi\\n      volumes:\\n        - name: oauth2-config\\n          configMap:\\n            name: streamlit-configmap\\n\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: streamlit-service\\nspec:\\n  type: LoadBalancer\\n  selector:\\n    app: streamlit\\n  ports:\\n    - name: streamlit-port\\n      protocol: TCP\\n      port: 80\\n      targetPort: 4180\\n```', metadata={'source': 'docs/content/kb/tutorials/deploy/kubernetes.md'}),\n",
       " Document(page_content=\"While the above configurations can be copied verbatim, you will have to configure the oauth2-proxy yourself and use the correct GOOGLE_CLIENT_ID, GOOGLE_CLIENT_ID, GCP_PROJECT_ID, and REDIRECT_URL.\\n\\nNow create the configuration from the file in Kubernetes with the kubectl create command:\\n\\nbash\\nkubctl create -f k8s-streamlit.yaml\\n\\nSet up TLS support\\n\\nSince you are using the Google authentication, you will need to set up TLS support. Find out how in TLS Configuration.\\n\\nVerify the deployment\\n\\nOnce the deployment and the service are created, we need to wait a couple of minutes for the public IP address to become available. We can check when that is ready by running:\\n\\nbash\\nkubectl get service streamlit-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\\n\\nAfter the public IP is assigned, you will need to configure in your DNS service an A record pointing to the above IP address.\", metadata={'source': 'docs/content/kb/tutorials/deploy/kubernetes.md'}),\n",
       " Document(page_content='title: Connect Streamlit to a public Google Sheet\\nslug: /knowledge-base/tutorials/databases/public-gsheet\\n\\nConnect Streamlit to a public Google Sheet\\n\\nIntroduction\\n\\nThis guide explains how to securely access a public Google Sheet from Streamlit Community Cloud. It uses the pandas library and Streamlit\\'s secrets management.\\n\\nThis method requires you to enable link sharing for your Google Sheet. While the sharing link will not appear in your code (and actually acts as sort of a password!), someone with the link can get all the data in the Sheet. If you don\\'t want this, follow the (more complicated) guide Connect Streamlit to a private Google Sheet.\\n\\nCreate a Google Sheet and turn on link sharing\\n\\nIf you already have a Sheet that you want to access, feel free to skip to the next\\nstep.\\n\\nAdd the Sheets URL to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the share link of your Google Sheet to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\npublic_gsheets_url = \"https://docs.google.com/spreadsheets/d/xxxxxxx/edit#gid=0\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport pandas as pd\\nimport streamlit as st\\n\\nRead in data from the Google Sheet.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.', metadata={'source': 'docs/content/kb/tutorials/databases/public-gsheet.md'}),\n",
       " Document(page_content='@st.cache_data(ttl=600)\\ndef load_data(sheets_url):\\n    csv_url = sheets_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\\n    return pd.read_csv(csv_url)\\n\\ndf = load_data(st.secrets[\"public_gsheets_url\"])\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/public-gsheet.md'}),\n",
       " Document(page_content=\"title: Deploy Streamlit using Docker\\nslug: /knowledge-base/tutorials/deploy/docker\\n\\nDeploy Streamlit using Docker\\n\\nIntroduction\\n\\nSo you have an amazing app and you want to start sharing it with other people, what do you do? You have a few options. First, where do you want to run your Streamlit app, and how do you want to access it?\\n\\nOn your corporate network\\xa0- Most corporate networks are closed to the outside world. You typically use a VPN to log onto your corporate network and access resources there. You could run your Streamlit app on a server in your corporate network for security reasons, to ensure that only folks internal to your company can access it.\\n\\nOn the cloud\\xa0- If you'd like to access your Streamlit app from outside of a corporate network, or share your app with folks outside of your home network or laptop, you might choose this option. In this case, it'll depend on your hosting provider. We have community-submitted guides from Heroku, AWS, and other providers.\\n\\nWherever you decide to deploy your app, you will first need to containerize it. This guide walks you through using Docker to deploy your app. If you prefer Kubernetes see Deploy Streamlit using Kubernetes.\\n\\nPrerequisites\\n\\nInstall Docker Engine\\n\\nCheck network port accessibility\\n\\nInstall Docker Engine\\n\\nIf you haven't already done so, install Docker on your server. Docker provides\\xa0.deb and\\xa0.rpm packages from many Linux distributions, including:\\n\\nDebian\\n\\nUbuntu\\n\\nVerify that Docker Engine is installed correctly by running the\\xa0hello-world Docker image:\\n\\nbash\\nsudo docker run hello-world\\n\\nFollow Docker's official post-installation steps for Linux to run Docker as a non-root user, so that you don't have to preface the docker command with sudo.\\n\\nCheck network port accessibility\", metadata={'source': 'docs/content/kb/tutorials/deploy/docker.md'}),\n",
       " Document(page_content='Check network port accessibility\\n\\nAs you and your users are behind your corporate VPN, you need to make sure all of you can access a certain network port. Let\\'s say port 8501, as it is the default port used by Streamlit. Contact your IT team and request access to port 8501 for you and your users.\\n\\nCreate a Dockerfile\\n\\nDockerfile reference. The\\n\\ndocker build command builds an image from a\\n\\ndocker run command first creates a container over the specified image, and then starts it using the specified command.\\n\\nHere\\'s an example Dockerfile that you can add to the root of your directory. i.e. in /app/\\n\\n```docker\\n\\napp/Dockerfile\\n\\nFROM python:3.9-slim\\n\\nWORKDIR /app\\n\\nRUN apt-get update && apt-get install -y \\\\\\n    build-essential \\\\\\n    curl \\\\\\n    software-properties-common \\\\\\n    git \\\\\\n    && rm -rf /var/lib/apt/lists/*\\n\\nRUN git clone https://github.com/streamlit/streamlit-example.git .\\n\\nRUN pip3 install -r requirements.txt\\n\\nEXPOSE 8501\\n\\nHEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\\n\\nENTRYPOINT [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\\n```\\n\\nDockerfile walkthrough\\n\\nLet’s walk through each line of the Dockerfile :\\n\\nA Dockerfile must start with a\\xa0FROM instruction. It sets the Base Image (think OS) for the container:\\n\\ndocker\\n   FROM python:3.9-slim\\n\\nDocker has a number of official Docker base images based on various Linux distributions. They also have base images that come with language-specific modules such as Python. The python images come in many flavors, each designed for a specific use case. Here, we use the python:3.9-slim image which is a lightweight image that comes with the latest version of Python 3.9.\\n\\nYou can also use your own base image, provided the image you use contains a supported version of Python for Streamlit. There is no one-size-fits-all approach to using any specific base image, nor is there an official Streamlit-specific base image.', metadata={'source': 'docs/content/kb/tutorials/deploy/docker.md'}),\n",
       " Document(page_content='The\\xa0WORKDIR instruction sets the working directory for any\\xa0RUN,\\xa0CMD,\\xa0ENTRYPOINT,\\xa0COPY and\\xa0ADD instructions that follow it in the\\xa0Dockerfile . Let’s set it to app/ :\\n\\ndocker\\n   WORKDIR /app\\n\\nAs mentioned in Development flow, for Streamlit version 1.10.0 and higher, Streamlit apps cannot be run from the root directory of Linux distributions. Your main script should live in a directory other than the root directory. If you try to run a Streamlit app from the root directory, Streamlit will throw a FileNotFoundError: [Errno 2] No such file or directory error. For more information, see GitHub issue #5239.\\n\\nIf you are using Streamlit version 1.10.0 or higher, you must set the WORKDIR to a directory other than the root directory. For example, you can set the WORKDIR to /app as shown in the example Dockerfile above.\\n\\nInstall git so that we can clone the app code from a remote repo:\\n\\ndocker\\n   RUN apt-get update && apt-get install -y \\\\\\n       build-essential \\\\\\n       curl \\\\\\n       software-properties-common \\\\\\n       git \\\\\\n       && rm -rf /var/lib/apt/lists/*\\n\\nClone your code that lives in a remote repo to WORKDIR:\\n\\na. If your code is in a public repo:\\n\\ndocker\\n   RUN git clone https://github.com/streamlit/streamlit-example.git .\\n\\nOnce cloned, the directory of WORKDIR will look like the following:\\n\\nbash\\n   app/\\n   - requirements.txt\\n   - streamlit_app.py\\n\\nwhere requirements.txt file contains all your Python dependencies. E.g\\n\\naltair\\n   pandas\\n   streamlit\\n\\nand streamlit_app.py is your main script. E.g.\\n\\n```python\\n   from collections import namedtuple\\n   import altair as alt\\n   import math\\n   import pandas as pd\\n   import streamlit as st\\n\\n\"\"\"\\n   # Welcome to Streamlit!\\n\\nEdit /streamlit_app.py to customize this app to your heart\\'s desire :heart:\\n\\nIf you have any questions, checkout our documentation and community\\n   forums.\\n\\nIn the meantime, below is an example of what you can do with just a few lines of code:\\n   \"\"\"', metadata={'source': 'docs/content/kb/tutorials/deploy/docker.md'}),\n",
       " Document(page_content='with st.echo(code_location=\\'below\\'):\\n      total_points = st.slider(\"Number of points in spiral\", 1, 5000, 2000)\\n      num_turns = st.slider(\"Number of turns in spiral\", 1, 100, 9)\\n\\n```\\n\\nb. If your code is in a private repo, please read Using SSH to access private data in builds and modify the Dockerfile accordingly -- to install an SSH client, download the public key for github.com, and clone your private repo. If you use an alternative VCS such as GitLab or Bitbucket, please consult the documentation for that VCS on how to copy your code to the WORKDIR of the Dockerfile.\\n\\nc. If your code lives in the same directory as the Dockerfile, copy all your app files from your server into the container, including streamlit_app.py, requirements.txt, etc, by replacing the git clone line with:\\n\\ndocker\\n   COPY . .\\n\\nMore generally, the idea is copy your app code from wherever it may live on your server into the container. If the code is not in the same directory as the Dockerfile, modify the above command to include the path to the code.\\n\\nInstall your app’s Python dependencies from the cloned requirements.txt in the container:\\n\\ndocker\\n   RUN pip3 install -r requirements.txt\\n\\nThe\\xa0EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. Your container needs to listen to Streamlit’s (default) port 8501:\\n\\ndocker\\n   EXPOSE 8501\\n\\nThe\\xa0HEALTHCHECK instruction tells Docker how to test a container to check that it is still working. Your container needs to listen to Streamlit’s (default) port 8501:\\n\\ndocker\\n   HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\\n\\nAn\\xa0ENTRYPOINT\\xa0allows you to configure a container that will run as an executable. Here, it also contains the entire streamlit run command for your app, so you don’t have to call it from the command line:\\n\\ndocker\\n   ENTRYPOINT [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\\n\\nBuild a Docker image', metadata={'source': 'docs/content/kb/tutorials/deploy/docker.md'}),\n",
       " Document(page_content=\"Build a Docker image\\n\\nThe\\xa0docker build command builds an image from a\\xa0Dockerfile . Run the following command from the app/ directory on your server to build the image:\\n\\ndocker\\ndocker build -t streamlit .\\n\\nThe -t flag is used to tag the image. Here, we have tagged the image streamlit. If you run:\\n\\ndocker\\ndocker images\\n\\nYou should see a streamlit image under the REPOSITORY column. For example:\\n\\nREPOSITORY   TAG       IMAGE ID       CREATED              SIZE\\nstreamlit    latest    70b0759a094d   About a minute ago   1.02GB\\n\\nRun the Docker container\\n\\nNow that you have built the image, you can run the container by executing:\\n\\ndocker\\ndocker run -p 8501:8501 streamlit\\n\\nThe -p flag publishes the container’s port 8501 to your server’s 8501 port.\\n\\nIf all went well, you should see an output similar to the following:\\n\\n```\\ndocker run -p 8501:8501 streamlit\\n\\nYou can now view your Streamlit app in your browser.\\n\\nURL: http://0.0.0.0:8501\\n```\\n\\nTo view your app, users can browse to http://0.0.0.0:8501 or http://localhost:8501\\n\\nBased on your server's network configuration, you could map to port 80/443 so that users can view your app using the server IP or hostname. For example: http://your-server-ip:80 or http://your-hostname:443.\", metadata={'source': 'docs/content/kb/tutorials/deploy/docker.md'}),\n",
       " Document(page_content=\"title: Connect Streamlit to Snowflake\\nslug: /knowledge-base/tutorials/databases/snowflake\\n\\nConnect Streamlit to Snowflake\\n\\nIntroduction\\n\\nThis guide explains how to securely access a Snowflake database from Streamlit. It uses st.experimental_connection, the Snowpark Python library and Streamlit's secrets management. The below example code will only work on Streamlit version >= 1.22, when st.experimental_connection was added.\\n\\nSkip to the bottom for information about connecting using Snowflake Connector for Python.\\n\\nCreate a Snowflake database\\n\\nIf you already have a database that you want to use, feel free to skip to the next step.\\n\\nFirst, sign up for Snowflake and log into the Snowflake web interface (note down your username, password, and account identifier!):\\n\\nEnter the following queries into the SQL editor in the Worksheets page to create a database and a table with some example values:\\n\\n```sql\\nCREATE DATABASE PETS;\\n\\nCREATE TABLE MYTABLE (\\n    NAME            varchar(80),\\n    PET             varchar(80)\\n);\\n\\nINSERT INTO MYTABLE VALUES ('Mary', 'dog'), ('John', 'cat'), ('Robert', 'bird');\\n\\nSELECT * FROM MYTABLE;\\n```\\n\\nBefore you execute the queries, first determine which Snowflake UI / web interface you're using. The examples below use Snowsight. You can also use Classic Console Worksheets or any other means of running Snowflake SQL statements.\\n\\nExecute queries in a Worksheet\\n\\nTo execute the queries in a Worksheet, highlight or select all the queries with your mouse, and click the play button in the top right corner.\\n\\nBe sure to highlight or select all the queries (lines 1-10) before clicking the play button.\\n\\nOnce you have executed the queries, you should see a preview of the table in the Results panel at the bottom of the page. Additionally, you should see your newly created database and schema by expanding the accordion on the left side of the page. Lastly, the warehouse name is displayed on the button to the left of the Share button.\", metadata={'source': 'docs/content/kb/tutorials/databases/snowflake.md'}),\n",
       " Document(page_content='Make sure to note down the name of your warehouse, database, and schema. ☝️\\n\\nInstall snowflake-snowpark-python\\n\\nYou can find the instructions and prerequisites for installing snowflake-snowpark-python in the Snowpark Developer Guide.\\n\\nbash\\npip install \"snowflake-snowpark-python[pandas]\"\\n\\nParticular prerequisites to highlight:\\n\\nCurrently, only python 3.8 is supported.\\n\\nEnsure you have the correct pyarrow version installed for your version of snowflake-snowpark-python. When in doubt, try uninstalling pyarrow before installing snowflake-snowpark-python.\\n\\nAdd connection parameters to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app’s root directory. Learn more about Streamlit secrets management here. Create this file if it doesn’t exist yet and add your Snowflake username, password, account identifier, and the name of your warehouse, database, and schema as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowpark]\\naccount = \"xxx\"\\nuser = \"xxx\"\\npassword = \"xxx\"\\nrole = \"xxx\"\\nwarehouse = \"xxx\"\\ndatabase = \"xxx\"\\nschema = \"xxx\"\\nclient_session_keep_alive = true\\n```\\n\\nIf you created the database from the previous step, the names of your database and schema are PETS and PUBLIC, respectively. Streamlit will also use Snowflake config and credentials from a SnowSQL config file if available.\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the query to use the name of your table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'snowpark\\')\\n\\nPerform query.\\n\\ndf = conn.query(\\'SELECT * from mytable;\\', ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.NAME} has a :{row.PET}:\")\\n```', metadata={'source': 'docs/content/kb/tutorials/databases/snowflake.md'}),\n",
       " Document(page_content='See st.experimental_connection above? This handles secrets retrieval, setup, query caching and retries. By default, query() results are cached without expiring. In this case, we set ttl=600 to ensure the query result is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:\\n\\nUsing a Snowpark Session\\n\\nThe same SnowparkConnection used above also provides access to the Snowpark Session for DataFrame-style operations that run natively inside Snowflake. Using this approach, you can rewrite the app above as follows:\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'snowpark\\')\\n\\nLoad the table as a dataframe using the Snowpark Session.\\n\\n@st.cache_data\\ndef load_table():\\n    with conn.safe_session() as session:\\n        return session.table(\\'mytable\\').to_pandas()\\n\\ndf = load_table()\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.NAME} has a :{row.PET}:\")\\n```\\n\\nThis example uses with conn.safe_session() to provide thread safety. conn.session also works directly, but does not guarantee thread safety. If everything worked out (and you used the example table we created above), your app should look the same as the screenshot from the first example above.\\n\\nUsing the Snowflake Connector for Python\\n\\nIn some cases, you may prefer to use the Snowflake Connector for Python instead of Snowpark Python. Streamlit supports this natively through the SQLConnection and the snowflake-sqlalchemy library.\\n\\nbash\\npip install snowflake-sqlalchemy\\n\\nInstalling snowflake-sqlalchemy will also install all necessary dependencies.\\n\\nConfiguring credentials follows the SQLConnection format which is slightly different. See the Snowflake SQLAlchemy Configuration Parameters documentation for more details.\\n\\n```toml\\n\\n.streamlit/secrets.toml', metadata={'source': 'docs/content/kb/tutorials/databases/snowflake.md'}),\n",
       " Document(page_content='```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowflake]\\nurl = \"snowflake://:@//?warehouse=&role=\"\\n```\\n\\nAlternatively, specify connection parameters like authenticator or key pair authentication using create_engine_kwargs, as shown below.\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowflake]\\nurl = \"snowflake://@/\"\\n\\n[connections.snowflake.create_engine_kwargs.connect_args]\\nauthenticator = \"externalbrowser\"\\nwarehouse = \"xxx\"\\nrole = \"xxx\"\\nclient_session_keep_alive = true\\n```\\n\\nInitializing and using the connection in your app is similar. Note that SQLConnection.query() supports extra arguments like params and chunksize which may be useful for more advanced apps.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'snowflake\\', type=\\'sql\\')\\n\\nPerform query.\\n\\ndf = conn.query(\\'SELECT * from mytable;\\', ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```\\n\\nIf everything worked out (and you used the example table we created above), your app should look the same as the screenshot from the first example above.\\n\\nConnecting to Snowflake from Community Cloud\\n\\nThis tutorial assumes a local Streamlit app, however you can also connect to Snowflake from apps hosted in Community Cloud. The main additional steps are:\\n\\nInclude information about dependencies using a requirements.txt file with snowflake-snowpark-python and any other dependencies.\\n\\nAdd your secrets to your Community Cloud app.\\n\\nFor apps using snowflake-snowpark-python, you should also ensure the app is running on python 3.8.', metadata={'source': 'docs/content/kb/tutorials/databases/snowflake.md'}),\n",
       " Document(page_content=\"title: Build conversational apps\\nslug: /knowledge-base/tutorials/build-conversational-apps\\n\\nBuild conversational apps\\n\\nIntroduction\\n\\nThe advent of large language models like GPT has revolutionized the ease of developing chat-based applications. Streamlit offers several Chat elements, enabling you to build Graphical User Interfaces (GUIs) for conversational agents or chatbots. Leveraging session state along with these elements allows you to construct anything from a basic chatbot to a more advanced, ChatGPT-like experience using purely Python code.\\n\\nIn this tutorial, we'll start by walking through Streamlit's chat elements, st.chat_message and st.chat_input. Then we'll proceed to construct three distinct applications, each showcasing an increasing level of complexity and functionality:\\n\\nFirst, we'll Build a bot that mirrors your input to get a feel for the chat elements and how they work. We'll also introduce session state and how it can be used to store the chat history. This section will serve as a foundation for the rest of the tutorial.\\n\\nNext, you'll learn how to Build a simple chatbot GUI with streaming.\\n\\nFinally, we'll Build a ChatGPT-like app that leverages session state to remember conversational context, all within less than 50 lines of code.\\n\\nHere's a sneak peek of the simple chatbot GUI with streaming we'll build in this tutorial:\\n\\nPlay around with the above demo to get a feel for what we'll build in this tutorial. A few things to note:\\n\\nThere's a chat input at the bottom of the screen that's always visible. It contains some placeholder text. You can type in a message and press Enter or click the run button to send it.\\n\\nWhen you enter a message, it appears as a chat message in the container above. The container is scrollable, so you can scroll up to see previous messages. A default avatar is displayed to your messages' left.\", metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='The assistant responds to your messages with a random message from a list of responses. The responses are streamed to the frontend and are displayed with a different default avatar.\\n\\nBefore we start building, let\\'s take a closer look at the chat elements we\\'ll use.\\n\\nChat elements\\n\\nStreamlit offers several commands to help you build conversational apps. These chat elements are designed to be used in conjunction with each other, but you can also use them separately.\\n\\nst.chat_message lets you insert a chat message container into the app so you can display messages from the user or the app. Chat containers can contain other Streamlit elements, including charts, tables, text, and more. st.chat_input lets you display a chat input widget so the user can type in a message.\\n\\nst.chat_message\\n\\nst.chat_message lets you insert a multi-element chat message container into your app. The returned container can contain any Streamlit element, including charts, tables, text, and more. To add elements to the returned container, you can use with notation.\\n\\nst.chat_message\\'s first parameter is the name of the message author, which can be either \"user\" or \"assistant\" to enable preset styling and avatars, like in the demo above. You can also pass in a custom string to use as the author name. Currently, the name is not shown in the UI but is only set as an accessibility label. For accessibility reasons, you should not use an empty string.\\n\\nHere\\'s an minimal example of how to use st.chat_message to display a welcome message:\\n\\n```python\\nimport streamlit as st\\n\\nwith st.chat_message(\"user\"):\\n    st.write(\"Hello 👋\")\\n```\\n\\nNotice the message is displayed with a default avatar and styling since we passed in \"user\" as the author name. You can also pass in \"assistant\" as the author name to use a different default avatar and styling, or pass in a custom name and avatar. See the API reference for more details.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='with st.chat_message(\"assistant\"):\\n    st.write(\"Hello human\")\\n    st.bar_chart(np.random.randn(30, 3))\\n```\\n\\nWhile we\\'ve used the preferred with notation in the above examples, you can also just call methods directly in the returned objects. The below example is equivalent to the one above:\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\n\\nmessage = st.chat_message(\"assistant\")\\nmessage.write(\"Hello human\")\\nmessage.bar_chart(np.random.randn(30, 3))\\n```\\n\\nSo far, we\\'ve displayed predefined messages. But what if we want to display messages based on user input?\\n\\nst.chat_input\\n\\nst.chat_input lets you display a chat input widget so the user can type in a message. The returned value is the user\\'s input, which is None if the user hasn\\'t sent a message yet. You can also pass in a default prompt to display in the input widget. Here\\'s an example of how to use st.chat_input to display a chat input widget and show the user\\'s input:\\n\\n```python\\nimport streamlit as st\\n\\nprompt = st.chat_input(\"Say something\")\\nif prompt:\\n    st.write(f\"User has sent the following prompt: {prompt}\")\\n```\\n\\nPretty straightforward, right? Now let\\'s combine st.chat_message and st.chat_input to build a bot the mirrors or echoes your input.\\n\\nBuild a bot that mirrors your input\\n\\nIn this section, we\\'ll build a bot that mirrors or echoes your input. More specifically, the bot will respond to your input with the same message. We\\'ll use st.chat_message to display the user\\'s input and st.chat_input to accept user input. We\\'ll also use session state to store the chat history so we can display it in the chat message container.\\n\\nFirst, let\\'s think about the different components we\\'ll need to build our bot:\\n\\nTwo chat message containers to display messages from the user and the bot, respectively.\\n\\nA chat input widget so the user can type in a message.', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='A way to store the chat history so we can display it in the chat message containers. We can use a list to store the messages, and append to it every time the user or bot sends a message. Each entry in the list will be a dictionary with the following keys: role (the author of the message), and content (the message content).\\n\\n```python\\nimport streamlit as st\\n\\nst.title(\"Echo Bot\")\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n```\\n\\nIn the above snippet, we\\'ve added a title to our app and a for loop to iterate through the chat history and display each message in the chat message container (with the author role and message content). We\\'ve also added a check to see if the messages key is in st.session_state. If it\\'s not, we initialize it to an empty list. This is because we\\'ll be adding messages to the list later on, and we don\\'t want to overwrite the list every time the app reruns.\\n\\nNow let\\'s accept user input with st.chat_input, display the user\\'s message in the chat message container, and add it to the chat history.\\n\\n```python\\n\\nReact to user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Display user message in chat message container\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n```\\n\\nWe used the := operator to assign the user\\'s input to the prompt variable and checked if it\\'s not None in the same line. If the user has sent a message, we display the message in the chat message container and append it to the chat history.', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='All that\\'s left to do is add the chatbot\\'s responses within the if block. We\\'ll use the same logic as before to display the bot\\'s response (which is just the user\\'s prompt) in the chat message container and add it to the history.\\n\\n```python\\nresponse = f\"Echo: {prompt}\"\\n\\nDisplay assistant response in chat message container\\n\\nwith st.chat_message(\"assistant\"):\\n    st.markdown(response)\\n\\nAdd assistant response to chat history\\n\\nst.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\\n```\\n\\nPutting it all together, here\\'s the full code for our simple chatbot GUI and the result:\\n\\n```python\\nimport streamlit as st\\n\\nst.title(\"Echo Bot\")\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nReact to user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Display user message in chat message container\\n    st.chat_message(\"user\").markdown(prompt)\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n\\n```\\n\\nWhile the above example is very simple, it\\'s a good starting point for building more complex conversational apps. Notice how the bot responds instantly to your input. In the next section, we\\'ll add a delay to simulate the bot \"thinking\" before responding.\\n\\nBuild a simple chatbot GUI with streaming\\n\\nIn this section, we\\'ll build a simple chatbot GUI that responds to user input with a random message from a list of pre-determind responses. In the next section, we\\'ll convert this simple toy example into a ChatGPT-like experience using OpenAI.', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='Just like previously, we still require the same components to build our chatbot. Two chat message containers to display messages from the user and the bot, respectively. A chat input widget so the user can type in a message. And a way to store the chat history so we can display it in the chat message containers.\\n\\nLet\\'s just copy the code from the previous section and add a few tweaks to it.\\n\\n```python\\nimport streamlit as st\\nimport random\\nimport time\\n\\nst.title(\"Simple chat\")\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nAccept user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Display user message in chat message container\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n```\\n\\nThe only difference so far is we\\'ve changed the title of our app and added imports for random and time. We\\'ll use random to randomly select a response from a list of responses and time to add a delay to simulate the chatbot \"thinking\" before responding.\\n\\nAll that\\'s left to do is add the chatbot\\'s responses within the if block. We\\'ll use a list of responses and randomly select one to display. We\\'ll also add a delay to simulate the chatbot \"thinking\" before responding (or stream its response).\\n\\n```python\\n\\nDisplay assistant response in chat message container', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='with st.chat_message(\"assistant\"):\\n    message_placeholder = st.empty()\\n    full_response = \"\"\\n    assistant_response = random.choice(\\n        [\\n            \"Hello there! How can I assist you today?\",\\n            \"Hi, human! Is there anything I can help you with?\",\\n            \"Do you need help?\",\\n        ]\\n    )\\n    # Simulate stream of response with milliseconds delay\\n    for chunk in assistant_response.split():\\n        full_response += chunk + \" \"\\n        time.sleep(0.05)\\n        # Add a blinking cursor to simulate typing\\n        message_placeholder.markdown(full_response + \"▌\")\\n    message_placeholder.markdown(full_response)\\n\\nAdd assistant response to chat history\\n\\nst.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\\n```\\n\\nAbove, we\\'ve added a placeholder to display the chatbot\\'s response. We\\'ve also added a for loop to iterate through the response and display it one word at a time. We\\'ve added a delay of 0.05 seconds between each word to simulate the chatbot \"thinking\" before responding. Finally, we append the chatbot\\'s response to the chat history. As you\\'ve probably guessed, this is a naive implementation of streaming. We\\'ll see how to implement streaming with OpenAI in the next section.\\n\\nPutting it all together, here\\'s the full code for our simple chatbot GUI and the result:\\n\\n```python\\nimport streamlit as st\\nimport random\\nimport time\\n\\nst.title(\"Simple chat\")\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nAccept user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n    # Display user message in chat message container\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n\\n```', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='```\\n\\nPlay around with the above demo to get a feel for what we\\'ve built. It\\'s a very simple chatbot GUI, but it has all the components of a more sophisticated chatbot. In the next section, we\\'ll see how to build a ChatGPT-like app using OpenAI.\\n\\nBuild a ChatGPT-like app\\n\\nNow that you\\'ve understood the basics of Streamlit\\'s chat elements, let\\'s make a few tweaks to it to build our own ChatGPT-like app. You\\'ll need to install the OpenAI Python library and get an API key to follow along.\\n\\nInstall dependencies\\n\\nFirst let\\'s install the dependencies we\\'ll need for this section:\\n\\nbash\\npip install openai streamlit\\n\\nAdd OpenAI API key to Streamlit secrets\\n\\nNext, let\\'s add our OpenAI API key to Streamlit secrets. We do this by creating .streamlit/secrets.toml file in our project directory and adding the following lines to it:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nOPENAI_API_KEY = \"YOUR_API_KEY\"\\n```\\n\\nWrite the app\\n\\nNow let\\'s write the app. We\\'ll use the same code as before, but we\\'ll replace the list of responses with a call to the OpenAI API. We\\'ll also add a few more tweaks to make the app more ChatGPT-like.\\n\\n```python\\nimport streamlit as st\\nimport openai\\n\\nst.title(\"ChatGPT-like clone\")\\n\\nSet OpenAI API key from Streamlit secrets\\n\\nopenai.api_key = st.secrets[\"OPENAI_API_KEY\"]\\n\\nSet a default model\\n\\nif \"openai_model\" not in st.session_state:\\n    st.session_state[\"openai_model\"] = \"gpt-3.5-turbo\"\\n\\nInitialize chat history\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nDisplay chat messages from history on app rerun\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nAccept user input', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='Accept user input\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    # Add user message to chat history\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n    # Display user message in chat message container\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n    # Display assistant response in chat message container\\n    with st.chat_message(\"assistant\"):\\n        message_placeholder = st.empty()\\n        full_response = \"\"\\n```\\n\\nAll that\\'s changed is that we\\'ve added a default model to st.session_state and set our OpenAI API key from Streamlit secrets. Here\\'s where it gets interesting. We can replace our logic from earlier to emulate streaming predetermind responses with the model\\'s responses from OpenAI:\\n\\npython\\n    for response in openai.ChatCompletion.create(\\n        model=st.session_state[\"openai_model\"],\\n        messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in st.session_state.messages],\\n        stream=True,\\n    ):\\n        full_response += response.choices[0].delta.get(\"content\", \"\")\\n        message_placeholder.markdown(full_response + \"▌\")\\n    message_placeholder.markdown(full_response)\\nst.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\\n\\nAbove, we\\'ve replaced the list of responses with a call to openai.ChatCompletion.create. We\\'ve set stream=True to stream the responses to the frontend. In the API call, we pass the model name we hardcoded in session state and pass the chat history as a list of messages. We also pass the role and content of each message in the chat history. Finally, OpenAI returns a stream of responses (split into chunks of tokens), which we iterate through and display each chunk.\\n\\nPutting it all together, here\\'s the full code for our ChatGPT-like app and the result:\\n\\n```python\\nimport openai\\nimport streamlit as st\\n\\nst.title(\"ChatGPT-like clone\")\\n\\nopenai.api_key = st.secrets[\"OPENAI_API_KEY\"]', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content='openai.api_key = st.secrets[\"OPENAI_API_KEY\"]\\n\\nif \"openai_model\" not in st.session_state:\\n    st.session_state[\"openai_model\"] = \"gpt-3.5-turbo\"\\n\\nif \"messages\" not in st.session_state:\\n    st.session_state.messages = []\\n\\nfor message in st.session_state.messages:\\n    with st.chat_message(message[\"role\"]):\\n        st.markdown(message[\"content\"])\\n\\nif prompt := st.chat_input(\"What is up?\"):\\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\\n    with st.chat_message(\"user\"):\\n        st.markdown(prompt)\\n\\n```\\n\\nCongratulations! You\\'ve built your own ChatGPT-like app in less than 50 lines of code.\\n\\nWe\\'re very excited to see what you\\'ll build with Streamlit\\'s chat elements. Experiment with different models and tweak the code to build your own conversational apps. If you build something cool, let us know on the Forum or check out some other Generative AI apps for inspiration. 🎈', metadata={'source': 'docs/content/kb/tutorials/chat.md'}),\n",
       " Document(page_content=\"title: Connect Streamlit to Google BigQuery\\nslug: /knowledge-base/tutorials/databases/bigquery\\n\\nConnect Streamlit to Google BigQuery\\n\\nIntroduction\\n\\nThis guide explains how to securely access a BigQuery database from Streamlit Community Cloud. It uses the\\ngoogle-cloud-bigquery library and\\nStreamlit's secrets management.\\n\\nCreate a BigQuery database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFor this example, we will use one of the sample datasets from BigQuery (namely the shakespeare table). If you want to create a new dataset instead, follow Google's quickstart guide.\\n\\nEnable the BigQuery API\\n\\nProgrammatic access to BigQuery is controlled through Google Cloud Platform. Create an account or sign in and head over to the APIs & Services dashboard (select or create a project if asked). As shown below, search for the BigQuery API and enable it:\\n\\nCreate a service account & key file\\n\\nTo use the BigQuery API from Streamlit Community Cloud, you need a Google Cloud Platform service account (a special account type for programmatic data access). Go to the Service Accounts page and create an account with the Viewer permission (this will let the account access data but not change it):\\n\\nIf the button CREATE SERVICE ACCOUNT is gray, you don't have the correct permissions. Ask the\\nadmin of your Google Cloud project for help.\\n\\nAfter clicking DONE, you should be back on the service accounts overview. Create a JSON key file for the new account and download it:\\n\\nAdd the key file to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app's root\\ndirectory. Create this file if it doesn't exist yet and add the content of the key file you just\\ndownloaded to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\", metadata={'source': 'docs/content/kb/tutorials/databases/bigquery.md'}),\n",
       " Document(page_content='```toml\\n\\n.streamlit/secrets.toml\\n\\n[gcp_service_account]\\ntype = \"service_account\"\\nproject_id = \"xxx\"\\nprivate_key_id = \"xxx\"\\nprivate_key = \"xxx\"\\nclient_email = \"xxx\"\\nclient_id = \"xxx\"\\nauth_uri = \"https://accounts.google.com/o/oauth2/auth\"\\ntoken_uri = \"https://oauth2.googleapis.com/token\"\\nauth_provider_x509_cert_url = \"https://www.googleapis.com/oauth2/v1/certs\"\\nclient_x509_cert_url = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd google-cloud-bigquery to your requirements file\\n\\nAdd the google-cloud-bigquery package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ngoogle-cloud-bigquery==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the query if you don\\'t use the sample table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom google.oauth2 import service_account\\nfrom google.cloud import bigquery\\n\\nCreate API client.\\n\\ncredentials = service_account.Credentials.from_service_account_info(\\n    st.secrets[\"gcp_service_account\"]\\n)\\nclient = bigquery.Client(credentials=credentials)\\n\\nPerform query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query(query):\\n    query_job = client.query(query)\\n    rows_raw = query_job.result()\\n    # Convert to list of dicts. Required for st.cache_data to hash the return value.\\n    rows = [dict(row) for row in rows_raw]\\n    return rows\\n\\nrows = run_query(\"SELECT word FROM bigquery-public-data.samples.shakespeare LIMIT 10\")', metadata={'source': 'docs/content/kb/tutorials/databases/bigquery.md'}),\n",
       " Document(page_content='Print results.\\n\\nst.write(\"Some wise words from Shakespeare:\")\\nfor row in rows:\\n    st.write(\"✍️ \" + row[\\'word\\'])\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nAlternatively, you can use pandas to read from BigQuery right into a dataframe! Follow all the above steps, install the pandas-gbq library (don\\'t forget to add it to requirements.txt!), and call pandas.read_gbq(query, credentials=credentials). More info in the pandas docs.\\n\\nIf everything worked out (and you used the sample table), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/bigquery.md'}),\n",
       " Document(page_content='title: Connect Streamlit to MongoDB\\nslug: /knowledge-base/tutorials/databases/mongodb\\n\\nConnect Streamlit to MongoDB\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote MongoDB database from Streamlit Community Cloud. It uses the PyMongo library and Streamlit\\'s secrets management.\\n\\nCreate a MongoDB Database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, follow the official tutorials to install MongoDB, set up authentication (note down the username and password!), and connect to the MongoDB instance. Once you are connected, open the mongo shell and enter the following two commands to create a collection with some example values:\\n\\nsql\\nuse mydb\\ndb.mycollection.insertMany([{\"name\" : \"Mary\", \"pet\": \"dog\"}, {\"name\" : \"John\", \"pet\": \"cat\"}, {\"name\" : \"Robert\", \"pet\": \"bird\"}])\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the database information as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[mongo]\\nhost = \"localhost\"\\nport = 27017\\nusername = \"xxx\"\\npassword = \"xxx\"\\n```\\n\\nWhen copying your app secrets to Streamlit Community Cloud, be sure to replace the values of host, port, username, and password with those of your remote MongoDB database!\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd PyMongo to your requirements file', metadata={'source': 'docs/content/kb/tutorials/databases/mongodb.md'}),\n",
       " Document(page_content='Add PyMongo to your requirements file\\n\\nAdd the PyMongo package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\npymongo==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the name of your database and collection.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport pymongo\\n\\nInitialize connection.\\n\\nUses st.cache_resource to only run once.\\n\\n@st.cache_resource\\ndef init_connection():\\n    return pymongo.MongoClient(**st.secrets[\"mongo\"])\\n\\nclient = init_connection()\\n\\nPull data from the collection.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef get_data():\\n    db = client.mydb\\n    items = db.mycollection.find()\\n    items = list(items)  # make hashable for st.cache_data\\n    return items\\n\\nitems = get_data()\\n\\nPrint results.\\n\\nfor item in items:\\n    st.write(f\"{item[\\'name\\']} has a :{item[\\'pet\\']}:\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example data we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/mongodb.md'}),\n",
       " Document(page_content='title: Connect Streamlit to AWS S3\\nslug: /knowledge-base/tutorials/databases/aws-s3\\n\\nConnect Streamlit to AWS S3\\n\\nIntroduction\\n\\nThis guide explains how to securely access files on AWS S3 from Streamlit Community Cloud. It uses Streamlit FilesConnection, the s3fs library and optionally Streamlit\\'s secrets management.\\n\\nCreate an S3 bucket and add a file\\n\\nIf you already have a bucket that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, sign up for AWS or log in. Go to the S3 console and create a new bucket:\\n\\nNavigate to the upload section of your new bucket:\\n\\nAnd note down the \"AWS Region\" for later. In this example, it\\'s us-east-1, but it may differ for you.\\n\\nNext, upload the following CSV file, which contains some example data:\\n\\nmyfile.csv\\n\\nCreate access keys\\n\\nGo to the AWS console, create access keys as shown below and copy the \"Access Key ID\" and \"Secret Access Key\":\\n\\nAccess keys created as a root user have wide-ranging permissions. In order to make your AWS account\\nmore secure, you should consider creating an IAM account with restricted permissions and using its\\naccess keys. More information here.\\n\\nSet up your AWS credentials locally\\n\\nStreamlit FilesConnection and s3fs will read and use your existing AWS credentials and configuration if available - such as from an ~/.aws/credentials file or environment variables.\\n\\nIf you don\\'t already have this set up, or plan to host the app on Streamlit Community Cloud, you should specify the credentials from a file .streamlit/secrets.toml in your app\\'s root directory or your home directory. Create this file if it doesn\\'t exist yet and add to it the access key ID, access key secret, and the AWS default region you noted down earlier, as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nAWS_ACCESS_KEY_ID = \"xxx\"\\nAWS_SECRET_ACCESS_KEY = \"xxx\"\\nAWS_DEFAULT_REGION = \"xxx\"\\n```\\n\\nBe sure to replace xxx above with the values you noted down earlier, and add this file to .gitignore so you don\\'t commit it to your GitHub repo!', metadata={'source': 'docs/content/kb/tutorials/databases/aws-s3.md'}),\n",
       " Document(page_content='Copy your app secrets to the cloud\\n\\nTo host your app on Streamlit Community Cloud, you will need to pass your credentials to your deployed app via secrets. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml above into the text area. More information is available at Secrets Management.\\n\\nAdd FilesConnection and s3fs to your requirements file\\n\\nAdd the FilesConnection and s3fs packages to your requirements.txt file, preferably pinning the versions (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ns3fs==x.x.x\\n\\nDirect pypi install coming soon\\n\\ngit+https://github.com/streamlit/files-connection\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the name of your bucket and file. Note that Streamlit automatically turns the access keys from your secrets file into environment variables, where s3fs searches for them by default.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom st_files_connection import FilesConnection\\n\\nCreate connection object and retrieve file contents.\\n\\nSpecify input format is a csv and to cache the result for 600 seconds.\\n\\nconn = st.experimental_connection(\\'s3\\', type=FilesConnection)\\ndf = conn.read(\"testbucket-jrieke/myfile.csv\", input_format=\"csv\", ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.Owner} has a :{row.Pet}:\")\\n```\\n\\nSee st.experimental_connection above? This handles secrets retrieval, setup, result caching and retries. By default, read() results are cached without expiring. In this case, we set ttl=600 to ensure the file contents is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example file given above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/aws-s3.md'}),\n",
       " Document(page_content=\"title: Connect Streamlit to a private Google Sheet\\nslug: /knowledge-base/tutorials/databases/private-gsheet\\n\\nConnect Streamlit to a private Google Sheet\\n\\nIntroduction\\n\\nThis guide explains how to securely access a private Google Sheet from Streamlit Community Cloud. It uses the gsheetsdb library and Streamlit's secrets management.\\n\\nIf you are fine with enabling link sharing for your Google Sheet (i.e. everyone with the link can view it), the guide Connect Streamlit to a public Google Sheet shows a simpler method of doing this. If your Sheet contains sensitive information and you cannot enable link sharing, keep on reading.\\n\\nCreate a Google Sheet\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nEnable the Sheets API\\n\\nProgrammatic access to Google Sheets is controlled through Google Cloud Platform. Create an account or sign in and head over to the APIs & Services dashboard (select or create a project if asked). As shown below, search for the Sheets API and enable it:\\n\\nCreate a service account & key file\\n\\nTo use the Sheets API from Streamlit Community Cloud, you need a Google Cloud Platform service account (a special account type for programmatic data access). Go to the Service Accounts page and create an account with the Viewer permission (this will let the account access data but not change it):\\n\\nThe button CREATE SERVICE ACCOUNT is gray, you don't have the correct permissions. Ask the admin of your Google Cloud project for help.\\n\\nAfter clicking DONE, you should be back on the service accounts overview. First, note down the email address of the account you just created (important for next step!). Then, create a JSON key file for the new account and download it:\\n\\nShare the Google Sheet with the service account\", metadata={'source': 'docs/content/kb/tutorials/databases/private-gsheet.md'}),\n",
       " Document(page_content='Share the Google Sheet with the service account\\n\\nBy default, the service account you just created cannot access your Google Sheet. To give it access, click on the Share button in the Google Sheet, add the email of the service account (noted down in step 2), and choose the correct permission (if you just want to read the data, Viewer is enough):\\n\\nAdd the key file to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the URL of your Google Sheet plus the content of the key file you downloaded to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nprivate_gsheets_url = \"https://docs.google.com/spreadsheets/d/12345/edit?usp=sharing\"\\n\\n[gcp_service_account]\\ntype = \"service_account\"\\nproject_id = \"xxx\"\\nprivate_key_id = \"xxx\"\\nprivate_key = \"xxx\"\\nclient_email = \"xxx\"\\nclient_id = \"xxx\"\\nauth_uri = \"https://accounts.google.com/o/oauth2/auth\"\\ntoken_uri = \"https://oauth2.googleapis.com/token\"\\nauth_provider_x509_cert_url = \"https://www.googleapis.com/oauth2/v1/certs\"\\nclient_x509_cert_url = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd gsheetsdb to your requirements file\\n\\nAdd the gsheetsdb package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ngsheetsdb==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it.\\n\\n```python\\n\\nstreamlit_app.py', metadata={'source': 'docs/content/kb/tutorials/databases/private-gsheet.md'}),\n",
       " Document(page_content='```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom google.oauth2 import service_account\\nfrom gsheetsdb import connect\\n\\nCreate a connection object.\\n\\ncredentials = service_account.Credentials.from_service_account_info(\\n    st.secrets[\"gcp_service_account\"],\\n    scopes=[\\n        \"https://www.googleapis.com/auth/spreadsheets\",\\n    ],\\n)\\nconn = connect(credentials=credentials)\\n\\nPerform SQL query on the Google Sheet.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query(query):\\n    rows = conn.execute(query, headers=1)\\n    rows = rows.fetchall()\\n    return rows\\n\\nsheet_url = st.secrets[\"private_gsheets_url\"]\\nrows = run_query(f\\'SELECT * FROM \"{sheet_url}\"\\')\\n\\nPrint results.\\n\\nfor row in rows:\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/private-gsheet.md'}),\n",
       " Document(page_content='title: Connect Streamlit to PostgreSQL\\nslug: /knowledge-base/tutorials/databases/postgresql\\n\\nConnect Streamlit to PostgreSQL\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote PostgreSQL database from Streamlit Community Cloud. It uses the psycopg2 library and Streamlit\\'s secrets management.\\n\\nCreate a PostgreSQL database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, follow this tutorial to install PostgreSQL and create a database (note down the database name, username, and password!). Open the SQL Shell (psql) and enter the following two commands to create a table with some example values:\\n\\n```sql\\nCREATE TABLE mytable (\\n    name            varchar(80),\\n    pet             varchar(80)\\n);\\n\\nINSERT INTO mytable VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\');\\n```\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the name, user, and password of your database as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[postgres]\\nhost = \"localhost\"\\nport = 5432\\ndbname = \"xxx\"\\nuser = \"xxx\"\\npassword = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd psycopg2 to your requirements file\\n\\nAdd the psycopg2 package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\npsycopg2-binary==x.x.x\\n```\\n\\nWrite your Streamlit app', metadata={'source': 'docs/content/kb/tutorials/databases/postgresql.md'}),\n",
       " Document(page_content='Write your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt query to use the name of your table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport psycopg2\\n\\nInitialize connection.\\n\\nUses st.cache_resource to only run once.\\n\\n@st.cache_resource\\ndef init_connection():\\n    return psycopg2.connect(**st.secrets[\"postgres\"])\\n\\nconn = init_connection()\\n\\nPerform query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query(query):\\n    with conn.cursor() as cur:\\n        cur.execute(query)\\n        return cur.fetchall()\\n\\nrows = run_query(\"SELECT * from mytable;\")\\n\\nPrint results.\\n\\nfor row in rows:\\n    st.write(f\"{row[0]} has a :{row[1]}:\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/postgresql.md'}),\n",
       " Document(page_content='title: Connect Streamlit to MySQL\\nslug: /knowledge-base/tutorials/databases/mysql\\n\\nConnect Streamlit to MySQL\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote MySQL database from Streamlit Community Cloud. It uses st.experimental_connection and Streamlit\\'s secrets management. The below example code will only work on Streamlit version >= 1.22, when st.experimental_connection was added.\\n\\nCreate a MySQL database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, follow this tutorial to install MySQL and start the MySQL server (note down the username and password!). Once your MySQL server is up and running, connect to it with the mysql client and enter the following commands to create a database and a table with some example values:\\n\\n```sql\\nCREATE DATABASE pets;\\n\\nUSE pets;\\n\\nCREATE TABLE mytable (\\n    name            varchar(80),\\n    pet             varchar(80)\\n);\\n\\nINSERT INTO mytable VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\');\\n```\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Learn more about Streamlit secrets management here. Create this file if it doesn\\'t exist yet and add the database name, user, and password of your MySQL server as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.mysql]\\ndialect = \"mysql\"\\nhost = \"localhost\"\\nport = 3306\\ndatabase = \"xxx\"\\nusername = \"xxx\"\\npassword = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.', metadata={'source': 'docs/content/kb/tutorials/databases/mysql.md'}),\n",
       " Document(page_content='Add dependencies to your requirements file\\n\\nAdd the mysqlclient and SQLAlchemy packages to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\nmysqlclient==x.x.x\\nSQLAlchemy==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt query to use the name of your table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'mysql\\', type=\\'sql\\')\\n\\nPerform query.\\n\\ndf = conn.query(\\'SELECT * from mytable;\\', ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```\\n\\nSee st.experimental_connection above? This handles secrets retrieval, setup, query caching and retries. By default, query() results are cached without expiring. In this case, we set ttl=600 to ensure the query result is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/mysql.md'}),\n",
       " Document(page_content=\"title: Connect Streamlit to Microsoft SQL Server\\nslug: /knowledge-base/tutorials/databases/mssql\\n\\nConnect Streamlit to Microsoft SQL Server\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote Microsoft SQL Server database from Streamlit Community Cloud. It uses the pyodbc library and Streamlit's secrets management.\\n\\nCreate an SQL Server database\\n\\nIf you already have a remote database that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, follow the Microsoft documentation to install SQL Server and the sqlcmd Utility. They have detailed installation guides on how to:\\n\\nInstall SQL Server on Windows\\n\\nInstall on Red Hat Enterprise Linux\\n\\nInstall on SUSE Linux Enterprise Server\\n\\nInstall on Ubuntu\\n\\nRun on Docker\\n\\nProvision a SQL VM in Azure\\n\\nOnce you have SQL Server installed, note down your SQL Server name, username, and password during setup.\\n\\nConnect locally\\n\\nIf you are connecting locally, use sqlcmd to connect to your new local SQL Server instance.\\n\\nIn your terminal, run the following command:\\n\\nbash\\n   sqlcmd -S localhost -U SA -P '<YourPassword>'\\n\\nAs you are connecting locally, the SQL Server name is localhost, the username is SA, and the password is the one you provided during the SA account setup.\\n\\nYou should see a sqlcmd command prompt 1>, if successful.\\n\\nIf you run into a connection failure, review Microsoft's connection troubleshooting recommendations for your OS (Linux & Windows).\\n\\nWhen connecting remotely, the SQL Server name is the machine name or IP address. You might also need to open the SQL Server TCP port (default 1433) on your firewall.\\n\\nCreate a SQL Server database\\n\\nBy now, you have SQL Server running and have connected to it with sqlcmd! 🥳 Let's put it to use by creating a database containing a table with some example values.\\n\\nFrom the sqlcmd command prompt, run the following Transact-SQL command to create a test database mydb:\\n\\nsql\\n   CREATE DATABASE mydb\\n\\nTo execute the above command, type GO on a new line:\\n\\nsql\\n   GO\", metadata={'source': 'docs/content/kb/tutorials/databases/mssql.md'}),\n",
       " Document(page_content='sql\\n   GO\\n\\nInsert some data\\n\\nNext create a new table, mytable, in the mydb database with three columns and two rows.\\n\\nSwitch to the new mydb database:\\n\\nsql\\n   USE mydb\\n\\nCreate a new table with the following schema:\\n\\nsql\\n   CREATE TABLE mytable (name varchar(80), pet varchar(80))\\n\\nInsert some data into the table:\\n\\nsql\\n   INSERT INTO mytable VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\')\\n\\nType GO to execute the above commands:\\n\\nsql\\n   GO\\n\\nTo end your sqlcmd session, type QUIT on a new line.\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the SQL Server name, database name, username, and password as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nserver = \"localhost\"\\ndatabase = \"mydb\"\\nusername = \"SA\"\\npassword = \"xxx\"\\n```\\n\\nWhen copying your app secrets to Streamlit Community Cloud, be sure to replace the values of server, database, username, and password with those of your remote SQL Server!\\n\\nAnd add this file to .gitignore and don\\'t commit it to your GitHub repo.\\n\\nCopy your app secrets to Streamlit Community Cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd pyodbc to your requirements file\\n\\nTo connect to SQL Server locally with Streamlit, you need to pip install pyodbc, in addition to the Microsoft ODBC driver you installed during the SQL Server installation.', metadata={'source': 'docs/content/kb/tutorials/databases/mssql.md'}),\n",
       " Document(page_content='On Streamlit Cloud, we have built-in support for SQL Server. On popular demand, we directly added SQL Server tools including the ODBC drivers and the executables sqlcmd and bcp to the container image for Cloud apps, so you don\\'t need to install them.\\n\\nAll you need to do is add the pyodbc Python package to your requirements.txt file, and you\\'re ready to go! 🎈\\n\\n```bash\\n\\nrequirements.txt\\n\\npyodbc==x.x.x\\n```\\n\\nReplace x.x.x ☝️ with the version of pyodbc you want installed on Cloud.\\n\\nAt this time, Streamlit Community Cloud does not support Azure Active Directory authentication. We will update this tutorial when we add support for Azure Active Directory.\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt query to use the name of your table.\\n\\n```python\\nimport streamlit as st\\nimport pyodbc\\n\\nInitialize connection.\\n\\nUses st.cache_resource to only run once.\\n\\n@st.cache_resource\\ndef init_connection():\\n    return pyodbc.connect(\\n        \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\"\\n        + st.secrets[\"server\"]\\n        + \";DATABASE=\"\\n        + st.secrets[\"database\"]\\n        + \";UID=\"\\n        + st.secrets[\"username\"]\\n        + \";PWD=\"\\n        + st.secrets[\"password\"]\\n    )\\n\\nconn = init_connection()\\n\\nPerform query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query(query):\\n    with conn.cursor() as cur:\\n        cur.execute(query)\\n        return cur.fetchall()\\n\\nrows = run_query(\"SELECT * from mytable;\")\\n\\nPrint results.\\n\\nfor row in rows:\\n    st.write(f\"{row[0]} has a :{row[1]}:\")\\n\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.', metadata={'source': 'docs/content/kb/tutorials/databases/mssql.md'}),\n",
       " Document(page_content='If everything worked out (and you used the example table we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/mssql.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Deta Base\\nslug: /knowledge-base/tutorials/databases/deta-base\\n\\nConnect Streamlit to Deta Base\\n\\nIntroduction\\n\\nThis guide explains how to securely access and write to a Deta Base database from Streamlit Community Cloud. Deta Base is a fully-managed and fast NoSQL database with a focus on end-user simplicity. The data is stored in your own \"personal cloud\" on Deta Space. This guide uses the Deta Python SDK for Deta Base and Streamlit\\'s Secrets Management.\\n\\nCreate an account and sign in to Deta Space\\n\\nFirst, you need to create a Deta Space account for using Deta Base. Make sure the \"Developer Mode\" option is enabled when signing up. Once you have an account, sign in to Deta Space. After signing in, open the Collections app by clicking on it.\\n\\nDeta Collections is a pre-installed app on Space that stores different types of data that can be connected to other apps or services.\\n\\nNow click on the Get Started button and then click on the Create Collection button after giving your Collection a name.\\n\\nAfter that, click on the Collection Settings option in the top corner, which will show the modal for creating a Data Key. Click on the Create New Data Key button, then give your key a name, and click the Generate button. Copy the key shown to your clipboard by clicking on the copy button.\\n\\nData Keys allow you to read and manipulate data within your Collections.\\n\\nBe sure to store your Data Key securely. It is shown only once, and you will need it to connect to your Deta Base.\\n\\nAdd Data Key to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the Data Key (from the previous step) of your Deta Base as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\ndata_key = \"xxx\"\\n```\\n\\nReplace xxx above ☝️ with your Data Key from the previous step.\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!', metadata={'source': 'docs/content/kb/tutorials/databases/detabase.md'}),\n",
       " Document(page_content='Copy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd deta to your requirements file\\n\\nAdd the deta Python SDK for Deta Base to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ndeta==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. The example app below writes data from a Streamlit form to a Deta Base database example-db.\\n\\n```python\\nimport streamlit as st\\nfrom deta import Deta\\n\\nData to be written to Deta Base\\n\\nwith st.form(\"form\"):\\n    name = st.text_input(\"Your name\")\\n    age = st.number_input(\"Your age\")\\n    submitted = st.form_submit_button(\"Store in database\")\\n\\nConnect to Deta Base with your Data Key\\n\\ndeta = Deta(st.secrets[\"data_key\"])\\n\\nCreate a new database \"example-db\"\\n\\nIf you need a new database, just use another name.\\n\\ndb = deta.Base(\"example-db\")\\n\\nIf the user clicked the submit button,\\n\\nwrite the data from the form to the database.\\n\\nYou can store any data you want here. Just modify that dictionary below (the entries between the {}).\\n\\nif submitted:\\n    db.put({\"name\": name, \"age\": age})\\n\\n\"---\"\\n\"Here\\'s everything stored in the database:\"\\n\\nThis reads all items from the database and displays them to your app.\\n\\ndb_content is a list of dictionaries. You can do everything you want with it.\\n\\ndb_content = db.fetch().items\\nst.write(db_content)\\n```\\n\\nIf everything worked out (and you used the example we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/detabase.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Supabase\\nslug: /knowledge-base/tutorials/databases/supabase\\n\\nConnect Streamlit to Supabase\\n\\nIntroduction\\n\\nThis guide explains how to securely access a Supabase instance from Streamlit Community Cloud. It uses the Supabase Python Client Library and Streamlit\\'s secrets management. Supabase is the open source Firebase alternative and is based on PostgreSQL.\\n\\nSign in to Supabase and create a project\\n\\nFirst, head over to Supabase and sign up for a free account using your GitHub.\\n\\nOnce you\\'re signed in, you can create a project.\\n\\nYour screen should look like this once your project has been created:\\n\\nMake sure to note down your Project API Key and Project URL highlighted in the above screenshot. ☝️\\n\\nYou will need these to connect to your Supabase instance from Streamlit.\\n\\nCreate a Supabase database\\n\\nNow that you have a project, you can create a database and populate it with some sample data. To do so, click on the SQL editor button on the same project page, followed by the New query button in the SQL editor.\\n\\nIn the SQL editor, enter the following queries to create a database and a table with some example values:\\n\\n```sql\\nCREATE TABLE mytable (\\n    name            varchar(80),\\n    pet             varchar(80)\\n);\\n\\nINSERT INTO mytable VALUES (\\'Mary\\', \\'dog\\'), (\\'John\\', \\'cat\\'), (\\'Robert\\', \\'bird\\');\\n```\\n\\nClick Run to execute the queries. To verify that the queries were executed successfully, click on the Table Editor button on the left menu, followed by your newly created table mytable.\\n\\nWith your Supabase database created, you can now connect to it from Streamlit!\\n\\nAdd Supabase Project URL and API key to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the supabase_url and supabase_key here:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\nsupabase_url = \"xxxx\"\\nsupabase_key = \"xxxx\"\\n```', metadata={'source': 'docs/content/kb/tutorials/databases/supabase.md'}),\n",
       " Document(page_content='supabase_url = \"xxxx\"\\nsupabase_key = \"xxxx\"\\n```\\n\\nReplace xxxx above with your Project URL and API key from Step 1.\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd supabase to your requirements file\\n\\nAdd the supabase Python Client Library to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\nsupabase==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom supabase import create_client, Client\\n\\nInitialize connection.\\n\\nUses st.cache_resource to only run once.\\n\\n@st.cache_resource\\ndef init_connection():\\n    url = st.secrets[\"supabase_url\"]\\n    key = st.secrets[\"supabase_key\"]\\n    return create_client(url, key)\\n\\nsupabase = init_connection()\\n\\nPerform query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query():\\n    return supabase.table(\"mytable\").select(\"*\").execute()\\n\\nrows = run_query()\\n\\nPrint results.\\n\\nfor row in rows.data:\\n    st.write(f\"{row[\\'name\\']} has a :{row[\\'pet\\']}:\")\\n\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.', metadata={'source': 'docs/content/kb/tutorials/databases/supabase.md'}),\n",
       " Document(page_content='If everything worked out (and you used the example table we created above), your app should look like this:\\n\\nAs Supabase uses PostgresSQL under the hood, you can also connect to Supabase by using the connection string Supabase provides under Settings > Databases. From there, you can refer to the PostgresSQL tutorial to connect to your database.', metadata={'source': 'docs/content/kb/tutorials/databases/supabase.md'}),\n",
       " Document(page_content='title: Connect Streamlit to TigerGraph\\nslug: /knowledge-base/tutorials/databases/tigergraph\\n\\nConnect Streamlit to TigerGraph\\n\\nIntroduction\\n\\nThis guide explains how to securely access a TigerGraph database from Streamlit Community Cloud. It uses the pyTigerGraph library and Streamlit\\'s secrets management.\\n\\nCreate a TigerGraph Cloud Database\\n\\nFirst, follow the official tutorials to create a TigerGraph instance in TigerGraph Cloud, either as a blog or a video. Note your username, password, and subdomain.\\n\\nFor this tutorial, we will be using the COVID-19 starter kit. When setting up your solution, select the “COVID-19 Analysis\" option.\\n\\nOnce it is started, ensure your data is downloaded and queries are installed.\\n\\nAdd username and password to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app’s root directory. Create this file if it doesn’t exist yet and add your TigerGraph Cloud instance username, password, graph name, and subdomain as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[tigergraph]\\nhost = \"https://xxx.i.tgcloud.io/\"\\nusername = \"xxx\"\\npassword = \"xxx\"\\ngraphname = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd PyTigerGraph to your requirements file\\n\\nAdd the pyTigerGraph package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\npyTigerGraph==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the name of your graph and query.\\n\\n```python', metadata={'source': 'docs/content/kb/tutorials/databases/tigergraph.md'}),\n",
       " Document(page_content='```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport pyTigerGraph as tg\\n\\nInitialize connection.\\n\\nconn = tg.TigerGraphConnection(**st.secrets[\"tigergraph\"])\\nconn.apiToken = conn.getToken(conn.createSecret())\\n\\nPull data from the graph by running the \"mostDirectInfections\" query.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef get_data():\\n    most_infections = conn.runInstalledQuery(\"mostDirectInfections\")[0][\"Answer\"][0]\\n    return most_infections[\"v_id\"], most_infections[\"attributes\"]\\n\\nitems = get_data()\\n\\nPrint results.\\n\\nst.title(f\"Patient {items[0]} has the most direct infections\")\\nfor key, val in items[1].items():\\n    st.write(f\"Patient {items[0]}\\'s {key} is {val}.\")\\n```\\n\\nSee st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that\\'s what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out (and you used the example data we created above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/tigergraph.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Tableau\\nslug: /knowledge-base/tutorials/databases/tableau\\n\\nConnect Streamlit to Tableau\\n\\nIntroduction\\n\\nThis guide explains how to securely access data on Tableau from Streamlit Community Cloud. It uses the tableauserverclient library and Streamlit\\'s secrets management.\\n\\nCreate a Tableau site\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nFor simplicity, we are using the cloud version of Tableau here but this guide works equally well for self-hosted deployments. First, sign up for Tableau Online or log in. Create a workbook or run one of the example workbooks under \"Dashboard Starters\".\\n\\nCreate personal access tokens\\n\\nWhile the Tableau API allows authentication via username and password, you should use personal access tokens for a production app.\\n\\nGo to your Tableau Online homepage, create an access token and note down the token name and secret.\\n\\nPersonal access tokens will expire if not used after 15 consecutive days.\\n\\nAdd token to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add your token, the site name you created during setup, and the URL of your Tableau server like below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[tableau]\\ntoken_name = \"xxx\"\\ntoken_secret = \"xxx\"\\nserver_url = \"https://abc01.online.tableau.com/\"\\nsite_id = \"streamlitexample\"  # in your site\\'s URL behind the server_url\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.', metadata={'source': 'docs/content/kb/tutorials/databases/tableau.md'}),\n",
       " Document(page_content='Add tableauserverclient to your requirements file\\n\\nAdd the tableauserverclient package to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ntableauserverclient==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Note that this code just shows a few options of data you can get – explore the tableauserverclient library to find more!\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nimport tableauserverclient as TSC\\n\\nSet up connection.\\n\\ntableau_auth = TSC.PersonalAccessTokenAuth(\\n    st.secrets[\"tableau\"][\"token_name\"],\\n    st.secrets[\"tableau\"][\"personal_access_token\"],\\n    st.secrets[\"tableau\"][\"site_id\"],\\n)\\nserver = TSC.Server(st.secrets[\"tableau\"][\"server_url\"], use_server_version=True)\\n\\nGet various data.\\n\\nExplore the tableauserverclient library for more options.\\n\\nUses st.cache_data to only rerun when the query changes or after 10 min.\\n\\n@st.cache_data(ttl=600)\\ndef run_query():\\n    with server.auth.sign_in(tableau_auth):\\n\\nworkbooks_names, views_names, view_name, view_image, view_csv = run_query()\\n\\nPrint results.\\n\\nst.subheader(\"📓 Workbooks\")\\nst.write(\"Found the following workbooks:\", \", \".join(workbooks_names))\\n\\nst.subheader(\"👁️ Views\")\\nst.write(\\n    f\"Workbook {workbooks_names[0]} has the following views:\",\\n    \", \".join(views_names),\\n)\\n\\nst.subheader(\"🖼️ Image\")\\nst.write(f\"Here\\'s what view {view_name} looks like:\")\\nst.image(view_image, width=300)\\n\\nst.subheader(\"📊 Data\")\\nst.write(f\"And here\\'s the data for view {view_name}:\")\\nst.write(pd.read_csv(StringIO(view_csv)))\\n```', metadata={'source': 'docs/content/kb/tutorials/databases/tableau.md'}),\n",
       " Document(page_content=\"See st.cache_data above? Without it, Streamlit would run the query every time the app reruns (e.g. on a widget interaction). With st.cache_data, it only runs when the query changes or after 10 minutes (that's what ttl is for). Watch out: If your database updates more frequently, you should adapt ttl or remove caching so viewers always see the latest data. Learn more in Caching.\\n\\nIf everything worked out, your app should look like this (can differ based on your workbooks):\", metadata={'source': 'docs/content/kb/tutorials/databases/tableau.md'}),\n",
       " Document(page_content=\"title: Streamlit Community Cloud\\nslug: /streamlit-community-cloud\\n\\nWelcome to Streamlit Community Cloud\\n\\nStreamlit's Community Cloud is an open and free platform for the community to deploy, discover, and share Streamlit apps and code with each other. If you're just getting started and have not yet built your first Streamlit app, check out the main Get started page first. When you're ready to share it, create a Community Cloud account and you can launch your app in just a few minutes! Deploy, manage, and share your apps with the world, directly from Streamlit — all for free.\\n\\nInterested in our security model? Check out our Trust and Security page.\\n\\nQuestions? Reach out to us on the Community forum!\", metadata={'source': 'docs/content/streamlit-cloud/index.md'}),\n",
       " Document(page_content=\"title: Connect Streamlit to TiDB\\nslug: /knowledge-base/tutorials/databases/tidb\\n\\nConnect Streamlit to TiDB\\n\\nIntroduction\\n\\nThis guide explains how to securely access a remote TiDB database from Streamlit Community Cloud. It uses st.experimental_connection and Streamlit's secrets management. The below example code will only work on Streamlit version >= 1.22, when st.experimental_connection was added.\\n\\nTiDB is an open-source, MySQL-compatible database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. TiDB Cloud is a fully managed cloud database service that simplifies the deployment and management of TiDB databases for developers.\\n\\nSign in to TiDB Cloud and create a cluster\\n\\nFirst, head over to TiDB Cloud and sign up for a free account, using either Google, GitHub, Microsoft or E-mail:\\n\\nOnce you've signed in, you will already have a TiDB cluster:\\n\\nYou can create more clusters if you want to. Click the cluster name to enter cluster overview page:\\n\\nThen click Connect to easily get the connection arguments to access the cluster. On the popup, click Create password to set the password.\\n\\nMake sure to note down the password. It won't be available on TiDB Cloud after this step.\\n\\nCreate a TiDB database\\n\\nIf you already have a database that you want to use, feel free\\nto skip to the next step.\\n\\nOnce your TiDB cluster is up and running, connect to it with the mysql client(or with Chat2Query tab on the console) and enter the following commands to create a database and a table with some example values:\\n\\n```sql\\nCREATE DATABASE pets;\\n\\nUSE pets;\\n\\nCREATE TABLE mytable (\\n    name            varchar(80),\\n    pet             varchar(80)\\n);\\n\\nINSERT INTO mytable VALUES ('Mary', 'dog'), ('John', 'cat'), ('Robert', 'bird');\\n```\\n\\nAdd username and password to your local app secrets\", metadata={'source': 'docs/content/kb/tutorials/databases/tidb.md'}),\n",
       " Document(page_content='Your local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Learn more about Streamlit secrets management here. Create this file if it doesn\\'t exist yet and add host, username and password of your TiDB cluster as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.tidb]\\ndialect = \"mysql\"\\nhost = \"\"\\nport = 4000\\ndatabase = \"pets\"\\nusername = \"\"\\npassword = \"\"\\n```\\n\\nWhen copying your app secrets to Streamlit Community Cloud, be sure to replace the values of host, username and password with those of your remote TiDB cluster!\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd dependencies to your requirements file\\n\\nAdd the mysqlclient and SQLAlchemy packages to your requirements.txt file, preferably pinning its version (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\nmysqlclient==x.x.x\\nSQLAlchemy==x.x.x\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt query to use the name of your table.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nInitialize connection.\\n\\nconn = st.experimental_connection(\\'tidb\\', type=\\'sql\\')\\n\\nPerform query.\\n\\ndf = conn.query(\\'SELECT * from mytable;\\', ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.name} has a :{row.pet}:\")\\n```', metadata={'source': 'docs/content/kb/tutorials/databases/tidb.md'}),\n",
       " Document(page_content='See st.experimental_connection above? This handles secrets retrieval, setup, query caching and retries. By default, query() results are cached without expiring. In this case, we set ttl=600 to ensure the query result is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example table we created above), your app should look like this:\\n\\nConnect with PyMySQL\\n\\nOther than mysqlclient, PyMySQL is another popular MySQL Python client. To use PyMySQL, first you need to adapt your requirements file:\\n\\n```bash\\n\\nrequirements.txt\\n\\nPyMySQL==x.x.x\\nSQLAlchemy==x.x.x\\n```\\n\\nThen adapt your secrets file:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.tidb]\\ndialect = \"mysql\"\\ndriver = \"pymysql\"\\nhost = \"\"\\nport = 4000\\ndatabase = \"pets\"\\nusername = \"\"\\npassword = \"\"\\ncreate_engine_kwargs = { connect_args = { ssl = { ca = \"\" }}}\\n```', metadata={'source': 'docs/content/kb/tutorials/databases/tidb.md'}),\n",
       " Document(page_content='title: Connect Streamlit to Google Cloud Storage\\nslug: /knowledge-base/tutorials/databases/gcs\\n\\nConnect Streamlit to Google Cloud Storage\\n\\nIntroduction\\n\\nThis guide explains how to securely access files on Google Cloud Storage from Streamlit Community Cloud. It uses Streamlit FilesConnection, the gcsfs library and Streamlit\\'s secrets management.\\n\\nCreate a Google Cloud Storage bucket and add a file\\n\\nIf you already have a bucket that you want to use, feel free\\nto skip to the next step.\\n\\nFirst, sign up for Google Cloud Platform or log in. Go to the Google Cloud Storage console and create a new bucket.\\n\\nNavigate to the upload section of your new bucket:\\n\\nAnd upload the following CSV file, which contains some example data:\\n\\nmyfile.csv\\n\\nEnable the Google Cloud Storage API\\n\\nThe Google Cloud Storage API is enabled by default when you create a project through the Google Cloud Console or CLI. Feel free to skip to the next step.\\n\\nIf you do need to enable the API for programmatic access in your project, head over to the APIs & Services dashboard (select or create a project if asked). Search for the Cloud Storage API and enable it. The screenshot below has a blue \"Manage\" button and indicates the \"API is enabled\" which means no further action needs to be taken. This is very likely what you have since the API is enabled by default. However, if that is not what you see and you have an \"Enable\" button, you\\'ll need to enable the API:\\n\\nCreate a service account and key file\\n\\nTo use the Google Cloud Storage API from Streamlit, you need a Google Cloud Platform service account (a special type for programmatic data access). Go to the Service Accounts page and create an account with Viewer permission.\\n\\nIf the button CREATE SERVICE ACCOUNT is gray, you don\\'t have the correct permissions. Ask the\\nadmin of your Google Cloud project for help.\\n\\nAfter clicking DONE, you should be back on the service accounts overview. Create a JSON key file for the new account and download it:', metadata={'source': 'docs/content/kb/tutorials/databases/gcs.md'}),\n",
       " Document(page_content='Add the key to your local app secrets\\n\\nYour local Streamlit app will read secrets from a file .streamlit/secrets.toml in your app\\'s root directory. Create this file if it doesn\\'t exist yet and add the access key to it as shown below:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.gcs]\\ntype = \"service_account\"\\nproject_id = \"xxx\"\\nprivate_key_id = \"xxx\"\\nprivate_key = \"xxx\"\\nclient_email = \"xxx\"\\nclient_id = \"xxx\"\\nauth_uri = \"https://accounts.google.com/o/oauth2/auth\"\\ntoken_uri = \"https://oauth2.googleapis.com/token\"\\nauth_provider_x509_cert_url = \"https://www.googleapis.com/oauth2/v1/certs\"\\nclient_x509_cert_url = \"xxx\"\\n```\\n\\nAdd this file to .gitignore and don\\'t commit it to your GitHub repo!\\n\\nCopy your app secrets to the cloud\\n\\nAs the secrets.toml file above is not committed to GitHub, you need to pass its content to your deployed app (on Streamlit Community Cloud) separately. Go to the app dashboard and in the app\\'s dropdown menu, click on Edit Secrets. Copy the content of secrets.toml into the text area. More information is available at Secrets Management.\\n\\nAdd FilesConnection and gcsfs to your requirements file\\n\\nAdd the FilesConnection and gcsfs packages to your requirements.txt file, preferably pinning the versions (replace x.x.x with the version you want installed):\\n\\n```bash\\n\\nrequirements.txt\\n\\ngcsfs==x.x.x\\n\\nDirect pypi install coming soon\\n\\ngit+https://github.com/streamlit/files-connection\\n```\\n\\nWrite your Streamlit app\\n\\nCopy the code below to your Streamlit app and run it. Make sure to adapt the name of your bucket and file. Note that Streamlit automatically turns the access keys from your secrets file into environment variables.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\nfrom st_files_connection import FilesConnection\\n\\nCreate connection object and retrieve file contents.\\n\\nSpecify input format is a csv and to cache the result for 600 seconds.', metadata={'source': 'docs/content/kb/tutorials/databases/gcs.md'}),\n",
       " Document(page_content='conn = st.experimental_connection(\\'gcs\\', type=FilesConnection)\\ndf = conn.read(\"streamlit-bucket/myfile.csv\", input_format=\"csv\", ttl=600)\\n\\nPrint results.\\n\\nfor row in df.itertuples():\\n    st.write(f\"{row.Owner} has a :{row.Pet}:\")\\n```\\n\\nSee st.experimental_connection above? This handles secrets retrieval, setup, result caching and retries. By default, read() results are cached without expiring. In this case, we set ttl=600 to ensure the file contents is cached for no longer than 10 minutes. You can also set ttl=0 to disable caching. Learn more in Caching.\\n\\nIf everything worked out (and you used the example file given above), your app should look like this:', metadata={'source': 'docs/content/kb/tutorials/databases/gcs.md'}),\n",
       " Document(page_content='title: Manage your account\\nslug: /streamlit-community-cloud/manage-your-account\\n\\nManage your account\\n\\nYou can update the email associated with your account or delete your account entirely through \"Settings.\" When using Streamlit Community Cloud, you have two kinds of logins: a primary identity (email) and source control (GitHub). Your primary identity allows other users to share private apps with you. Your source control identity allows you to deploy apps from GitHub repositories and manage them through the Streamlit Community Cloud dashboard.\\n\\nAccess your account settings\\n\\nTo manage your account, sign in to https://share.streamlit.io and click \"Settings\" in the top right corner.\\n\\nDoing so opens your \"Workspace settings,\" where you can update your email or delete your account.\\n\\nYou can find the detailed steps for each option in Update your email and Delete your account.', metadata={'source': 'docs/content/streamlit-cloud/manage-your-account/index.md'}),\n",
       " Document(page_content='title: Update your email\\nslug: /streamlit-community-cloud/manage-your-account/update-your-email\\n\\nUpdate your email\\n\\nIf you wish to update your email on Streamlit Community Cloud, you can do so via \"Settings.\" Updating your email changes the primary identity of your account. Once updated, if your account\\'s email is associated with a Google account, you can Sign in with Google OAuth. Otherwise, you have the alternative to Sign in with Email. The latter involves typing in your email, after which we\\'ll send you a unique link (valid for 15 minutes) to that email. Click the link in the email to sign in to Streamlit.\\n\\nHow to update your email\\n\\nSign in to Streamlit Community Cloud: https://share.streamlit.io/\\n\\nClick \"Settings\" in the page\\'s top-right corner.\\n\\nClick \"Update email\" within the \"Linked accounts\" section.\\n\\nEnter your new email and click \"Update email.\"\\n\\nYou\\'ll see a confirmation dialog asking you to check your email for a confirmation link. Click \"Done.\"\\n\\nYour account settings will show \"Update pending\" until you complete the next step.\\n\\nCheck your inbox for an email from Streamlit containing a \"Change email\" button and a confirmation link. This one-time link expires in 15 minutes. Click either one to confirm your new email address for Streamlit Community Cloud. Before doing so, ensure you access the link from the same browser session where you are logged in to Streamlit Community Cloud.\\n\\nIf you access the confirmation link from a browser session where you are not logged in to Streamlit Community Cloud, the confirmation link will not complete the process. You will be prompted to sign in. If you try to sign in with your new email, you will create a second account instead. See Troubleshooting.\\n\\nA confirmation will display to confirm your email update is complete! 🎈\\n\\nResend your confirmation link\\n\\nIf your confirmation link expires, don\\'t worry! You can resend it by following these steps:', metadata={'source': 'docs/content/streamlit-cloud/manage-your-account/update-your-email.md'}),\n",
       " Document(page_content='Sign in to Streamlit Community Cloud: https://share.streamlit.io/ and click \"Settings\" in the page\\'s top-right corner.\\n\\nClick \"Update pending\"\\n\\nClick \"Resend email\"\\n\\nContinue from step 4 of How to update your email.\\n\\nTroubleshooting\\n\\nIf you click the confirmation link in a browser session where you are not signed in, you will be informed that \"Sign in is required.\" If you try to sign in with your new email, you will create a second account instead. You cannot resend your confirmation link while you have this second account. If you accidentally created a second account, you can Delete your account, then Resend your confirmation link from your first account.', metadata={'source': 'docs/content/streamlit-cloud/manage-your-account/update-your-email.md'}),\n",
       " Document(page_content='title: Streamlit Trust and Security\\nslug: /streamlit-community-cloud/trust-and-security\\n\\nStreamlit Trust and Security\\n\\nStreamlit is a framework that turns Python scripts into interactive apps, giving data scientists the ability to quickly create data and model-based apps for the entire company.\\n\\nA simple Streamlit app is:\\n\\npython\\nimport streamlit as st\\nnumber = st.slider(\"Pick a number: \", min_value=1, max_value=10)\\nst.text(\"Your number is \" + str(number))\\n\\nWhen you streamlit run my_app.py, you start a web server that runs the interactive application on your local computer at http://localhost:8501. This is great for local development. When you want to share with your colleagues, Streamlit Community Cloud enables you to deploy and run these applications in the cloud. Streamlit Community Cloud handles all the details of scaling, reliability, and security as well as providing you an interface for easily managing your deployed apps.\\n\\nThis document is an overview of how we provide best-in-industry security for you. We\\'ll cover all the important areas in the lifecycle of your data:\\n\\nProduct Security: how we ensure only you can create and view apps that access your data\\n\\nNetwork and Application Security: how we ensure your data is protected when it is in our cloud\\n\\nOngoing Operations: how we stay good stewards of security best practices\\n\\nProduct Security\\n\\nSSO\\n\\nAll access and sign-ins to Streamlit are conducted via an SSO provider: GitHub and GSuite. We do not store customer passwords.\\n\\nCredential Storage\\n\\nWe encrypt sensitive customer data (e.g. secrets, authentication tokens) at-rest with AES256 as described in Google\\'s documentation.\\n\\nPermissions and Role-Based Access Control\\n\\nOur permission levels inherit from the permissions you have assigned in GitHub. Users with write access to a GitHub repository for a given app will be able to make changes in the Streamlit administrative console.\\n\\nOnly users with admin access to a repository are able to deploy and delete apps.', metadata={'source': 'docs/content/streamlit-cloud/security-model.md'}),\n",
       " Document(page_content=\"Network and Application Security\\n\\nData Hosting\\n\\nOur physical infrastructure is hosted and managed within Google Cloud Platform (GCP) using their secure data centers. Streamlit leverages many of the platform's built-in security, privacy, and redundancy features. GCP continually monitors its data centers for risk and undergoes assessments to ensure compliance with industry standards. GCP's data centers have numerous accreditations, including ISO-27001, SOC 1 and SOC 2.\\n\\nVirtual Private Cloud\\n\\nAll of our servers are within a virtual private cloud (VPC) with firewalls and network access control lists (ACLs) to allow external access to a select few API endpoints; all other internal services are only accessible within the VPC.\\n\\nEncryption\\n\\nAll Streamlit apps are served entirely over HTTPS. All data sent to or from Streamlit over the public internet is encrypted in transit using 256-bit encryption. Our API and application endpoints are TLS only (v1.2). We use only strong cipher suites and HTTP Strict Transport Security (HSTS) to ensure browsers interact with Streamlit apps over HTTPS. We also encrypt data at rest using AES-256.\\n\\nPermissions and Authentication\\n\\nAccess to customer data is limited to authorized employees who require it for their job. We run a zero-trust corporate network so there are no corporate resources or additional privileges gained from being on Streamlit's internal network. We utilize single sign-on, 2-factor authentication (2FA), and enforce strong password policies to ensure access to all cloud-related services are protected.\\n\\nIncident Response\\n\\nWe have an internal protocol for handling security events which includes escalation procedures, rapid mitigation, and documented post-mortems. We notify customers promptly and publicize security advisories at https://streamlit.io/advisories.\\n\\nPenetration Testing\", metadata={'source': 'docs/content/streamlit-cloud/security-model.md'}),\n",
       " Document(page_content='Penetration Testing\\n\\nStreamlit uses third-party security tools to scan for vulnerabilities on a regular basis. Our security partners conduct periodic, intensive penetration tests on the Streamlit platform. Our product development team immediately responds to any identified issues or potential vulnerabilities to ensure the quality and security of Streamlit applications.\\n\\nSecurity and Compliance Programs\\n\\nPeople\\n\\nBackground Checks\\n\\nAll Streamlit employees go through a thorough background check before hiring.\\n\\nTraining\\n\\nWe take a least-privilege approach to the access and handling of data. While we retain a minimal amount of customer data and limit internal access on a need-to-know basis, all employees are required to review related security policies and are trained on proper data handling to ensure they uphold our strict commitment to the privacy and security of your data.\\n\\nConfidentiality\\n\\nAll employees sign a confidentiality agreement before they start at Streamlit.\\n\\nVulnerability Control\\n\\nVulnerability Management\\n\\nWe keep our systems up-to-date with the latest security patches and continuously monitor for new vulnerabilities through compliance and security mailing lists. This includes automatic scanning of our code repositories for vulnerable dependencies.\\n\\nIf you have further questions about Community Cloud and our security approach, please reach out to us on the Community forum.', metadata={'source': 'docs/content/streamlit-cloud/security-model.md'}),\n",
       " Document(page_content='title: Delete your account\\nslug: /streamlit-community-cloud/manage-your-account/delete-your-account\\n\\nDelete your account\\n\\nDeleting your Streamlit Community Cloud account is just as easy as creating it via \"Settings\". When you delete your account, your information, account, and all your hosted apps are deleted as well.\\n\\nDeleting your account is permanent and cannot be undone. Make sure you really want to delete your account and all hosted apps before proceeding.\\n\\nHow to delete your account\\n\\nFollow these steps to delete your account:\\n\\nSign in to Streamlit Community Cloud: https://share.streamlit.io/\\n\\nClick \"Settings\" in the top right corner of the page to go to the Settings dashboard. In the \\'Account\\' section, click \\'Delete account\\':\\n\\nIn the \"Delete account?\" dialog that appears, you will be asked to confirm that you want to delete your account by typing:\\n\\ndelete <your email address>\\n\\nType in delete followed by your email address and click \"Delete account forever\":\\n\\nYou will then be logged out and your account, information, and apps will be permanently deleted.\\n\\nUpon deletion, you\\'re shown a confirmation message that your account has been deleted, after which you will be redirected to the Streamlit Community Cloud homepage.\\n\\nIt\\'s that simple! If you have any questions or run into issues deleting your account, please reach out to us on the Forum. We\\'re happy to help! 🎈', metadata={'source': 'docs/content/streamlit-cloud/manage-your-account/delete-your-account.md'}),\n",
       " Document(page_content=\"title: Troubleshooting\\nslug: /streamlit-community-cloud/troubleshooting\\n\\nTroubleshooting\\n\\nSorry to hear you're having issues! Please take a look at some frequently asked questions and issues below. If you cannot find an answer to your issue, please post on our Community forum so that our engineers or community members can help you.\\n\\nTable of contents\\n\\nGeneral help\\n\\nDeploying apps\\n\\nSharing and accessing apps\\n\\nData and app security\\n\\nGitHub integration\\n\\nLimitations and known issues\\n\\nGeneral help\\n\\nHow can I get help with my app?\\n\\nIf you have any questions, feedback, run into any issues, or need to reach us, you can ask on our Community forum. This is best suited for any questions related to the open source library and Community Cloud - debugging code, deployment, resource limits, etc.\\n\\nDeploying apps\\n\\nMy repo isn't showing on the Deploy page\\n\\nIt's possible it just isn't showing up even though it is already there. Try typing it in. If we don't recognize it, you'll see the message below with a link to click and give access.\\n\\nIf for some reason that doesn't work, please try logging out and back in again to make sure the change took effect. And if that doesn't work - please let us know and we'll get you sorted!\\n\\nIt won't let me deploy the app\\n\\nTo deploy an app for the first time you must have admin-level access to the repo in GitHub. Please check with your administrator to make sure you have that access. If not, please ask them to deploy for the first time (we need this in order to establish webhooks for continuous integration) and from there you can then push updates to the app.\\n\\nI need to set a specific Python version for my app\\n\\nWhen deploying an app, under advanced settings, you can choose which version of Python you wish your app to use.\\n\\nHow do I store files locally?\\n\\nIf you want to store your data locally as opposed to in a database, you can store the file in your GitHub repository. Streamlit is just python, so you can read the file using:\", metadata={'source': 'docs/content/streamlit-cloud/troubleshooting.md'}),\n",
       " Document(page_content='pandas.read_csv(\"data.csv\") or open(\"data.csv\")\\n\\nIf you have really big or binary data that you change frequently, and git is feeling slow, you might want to check out Git Large File Store (LFS) as a better way to store large files in GitHub. You don\\'t need to make any changes to your app to start using it. If your GitHub repo uses LFS, it will now just work with Streamlit.\\n\\nMy app is running into issues while deploying\\n\\nCheck your Cloud logs by clicking on the \"Manage app\" expander in the bottom right corner of your screen. Often the trouble is due to a dependency not being declared. See here for more information on dependency management.\\n\\nIf that\\'s not the issue, then please send the logs and warning you are seeing to our Community forum and we\\'ll help get you sorted!\\n\\nMy app is hitting resource limits / my app is running very slowly\\n\\nIf your app is running slowly or you\\'re hitting the \\'Argh\\' page, we first highly recommend going through and implementing the suggestions in the following blog posts to prevent your app from hitting the resource limits and to detect if your Streamlit app leaks memory:\\n\\nCommon app problems: Resource limits\\n\\n3 steps to fix app memory leaks\\n\\nIf you\\'re still having issues, click here to learn more about resource limits.\\n\\nCan I get a custom URL for my app?\\n\\nYes! You can find instructions for setting a custom subdomain here.\\n\\nSharing and accessing apps\\n\\nI don\\'t have SSO. How do I sign in to Streamlit?\\n\\nDon\\'t have SSO? No problem! You can sign in to Streamlit with your email address. Click here for step-by-step instructions on how to sign in with email.\\n\\nHow do I add viewers to my Streamlit apps?\\n\\nViewer auth allows you to restrict the viewers of your private app. To access your app, users have to authenticate using an email-based passwordless login or Google OAuth. To learn more about how to share your public and private apps with viewers, click here.\\n\\nDo viewers need access to the GitHub repo?', metadata={'source': 'docs/content/streamlit-cloud/troubleshooting.md'}),\n",
       " Document(page_content=\"Do viewers need access to the GitHub repo?\\n\\nNope! You only need access to the GitHub repo if you want to push changes to the app.\\n\\nWhat will unauthorized/logged out viewers see when they view my app?\\n\\nA 404 error is displayed to unauthorized viewers to avoid providing any unnecessary information about your app to unintended viewers. Users who satisfy any of the following conditions will see a 404 error when attempting to view your app after you have configured viewer auth:\\n\\nUser is not logged in with Google SSO.\\n\\nUser is not included in the list of viewers provided in the app settings.\\n\\nUser lacks read access to your app's GitHub repo.\\n\\nUser has read access to your app's GitHub repo but is not enrolled in Community Cloud.\\n\\nI've added someone to the viewer list but they still see a 404 error when attempting to view the app\\n\\nIf a user is still seeing a 404 error after their email address has been added to the viewer list, we recommend that you:\\n\\nCheck that the user did not log into a different Google account via Single Sign-On (if you have added their work email address to the viewer list, ask the user to check that they are not logged into their personal Google account, and vice versa).\\n\\nCheck that the user has navigated to the correct URL.\\n\\nCheck that the user's email address has been entered correctly in the viewer list.\\n\\nReach out on our Community forum and we will be happy to help.\\n\\nData and app security\\n\\nHow will Streamlit secure my data?\\n\\nStreamlit takes a number of industry best-practice measures to ensure your code, data, and apps are all secure. Read more in our Trust and Security memo.\\n\\nHow do I set up SSO for my organization?\\n\\nCommunity Cloud uses Google OAuth, by default. If you use Google for authentication you're all set.\\n\\nBilling and administration\\n\\nThe Community Cloud is a free service. You don't have to worry about setting up billing or being charged.\\n\\nGitHub integration\\n\\nWhy does Streamlit require additional OAuth scope?\", metadata={'source': 'docs/content/streamlit-cloud/troubleshooting.md'}),\n",
       " Document(page_content=\"In order to deploy your app, Streamlit requires access to your app's source code in GitHub and also the ability to manage the public keys associated with the repositories. The default GitHub OAuth scopes are sufficient to work with apps in public GitHub repositories. However, in order to work with apps in private GitHub repositories, Streamlit requires the additional repo OAuth scope from GitHub. We recognize that this scope provides Streamlit with extra permissions that we do not really need, and which, as people who prize security, we'd rather not even be granted. Alas, we need to work with the APIs we are provided by GitHub.\\n\\nAfter deploying my private-repo app, I received an email from GitHub saying a new public key was added to my repo. Is this expected?\\n\\nThis is the expected behavior. When you try to deploy an app that lives in a private repo, Streamlit Community Cloud needs to get access to that repo somehow. For this, we create a read-only GitHub Deploy Key then access your repo using a public SSH key. When we set this up, GitHub notifies admins of the repo that the key was created as a security measure.\\n\\nWhat happens when a user's permissions change on GitHub?\\n\\nOnce a user is added to a repository on GitHub, it will take at most 15 minutes before they can deploy the app on Cloud. If a user is removed from a repository on GitHub, it will take at most 15 minutes before their permissions to manage the app from that repository are revoked.\\n\\nLimitations and known issues\\n\\nHere are some limitations and known issues that we're actively working to resolve. If you find an issue\\xa0please let us know!\\n\\nWhen you print something to the Cloud logs, you may need to do a\\xa0sys.stdout.flush()\\xa0before it shows up.\\n\\nMatplotlib\\xa0doesn't work well with threads. So if you're using Matplotlib you should wrap your code with locks as shown in the snippet below. This Matplotlib bug is more prominent when you share your app apps since you're more likely to get more concurrent users then.\", metadata={'source': 'docs/content/streamlit-cloud/troubleshooting.md'}),\n",
       " Document(page_content=\"```python\\n  from matplotlib.backends.backend_agg import RendererAgg\\n  _lock = RendererAgg.lock\\n\\nwith _lock:\\n    fig.title('This is a figure)')\\n    fig.plot([1,20,3,40])\\n    st.pyplot(fig)\\n  ```\\n\\nAll apps are hosted in the United States. This is currently not configurable.\", metadata={'source': 'docs/content/streamlit-cloud/troubleshooting.md'}),\n",
       " Document(page_content=\"title: Embed your app\\nslug: /streamlit-community-cloud/get-started/embed-your-app\\n\\nEmbed your app\\n\\nEmbedding Streamlit Community Cloud apps enriches your content by integrating interactive, data-driven applications directly within your pages. Whether you're writing a blog post, a technical document, or sharing resources on platforms like Medium, Notion, or even StackOverflow, embedding Streamlit apps adds a dynamic component to your content. This allows your audience to interact with your ideas, rather than merely reading about them or looking at screenshots.\\n\\nStreamlit Community Cloud supports both iframe and oEmbed methods for embedding public apps. This flexibility enables you to share your apps across a wide array of platforms, broadening your app's visibility and impact. In this guide, we'll cover how to use both methods effectively to share your Streamlit apps with the world.\\n\\nEmbedding with iframes\\n\\nStreamlit Community Cloud supports embedding\\xa0public\\xa0apps using the subdomain scheme. To embed a public app, add the query parameter\\xa0/?embed=true\\xa0to the end of the\\xa0*streamlit.app\\xa0URL.\\n\\nFor example, say you want to embed the 30DaysOfStreamlit app. The URL to include in your iframe is: https://30days.streamlit.app/?embed=true:\\n\\n```javascript\\n\\n```\\n\\nThere will be no official support for embedding private apps.\\n\\nIn addition to allowing you to embed apps via iframes, the\\xa0?embed=true\\xa0query parameter also does the following:\\n\\nRemoves the toolbar with the hamburger menu\\n\\nRemoves the padding at the top and bottom of the app\\n\\nRemoves the footer\\n\\nRemoves the colored line from the top of the app\\n\\nFor granular control over the embedding behavior, Streamlit allows you to specify one or more instances of the ?embed_options query parameter (e.g. to show the toolbar, open the app in dark theme, etc). Click here for a full list of Embed options.\\n\\nEmbedding with oEmbed\", metadata={'source': 'docs/content/streamlit-cloud/get-started/embed-your-app.md'}),\n",
       " Document(page_content='Embedding with oEmbed\\n\\nThe new oEmbed support allows for a simpler embedding experience. You can now directly drop a Streamlit app\\'s URL into a Medium, Ghost, or Notion page (or any site that supports oEmbed or embed.ly, which includes over 700 content providers), and the embedded app will automatically appear. This helps Streamlit Community Cloud apps seamlessly integrate into these platforms, improving the visibility and accessibility of your apps.\\n\\nExample\\n\\nWhen creating content in a Notion page, Medium article, or Ghost blog, you only need to paste the app\\'s URL and hit Enter. The app will then render automatically at that spot in your content. Use the same URL as for iframe embedding, but without the ?embed=true query parameter.\\n\\nhttps://30days.streamlit.app/\\n\\nHere\\'s an example of @chrieke\\'s Prettymapp app embedded in a Medium article:\\n\\nEnsure the platform hosting the embedded Streamlit app supports oEmbed or embed.ly.\\n\\nKey Sites for oEmbed\\n\\noEmbed should work out of the box for several platforms including but not limited to:\\n\\nMedium\\n\\nNotion\\n\\nLooker\\n\\nTableau\\n\\nGhost\\n\\nDiscourse\\n\\nStackOverflow\\n\\nW3\\n\\nReddit\\n\\nPlease check the specific platform\\'s documentation to verify support for oEmbed.\\n\\niframe versus oEmbed\\n\\nThe only noteworthy differences between the methods is that iframing allows you customize the app\\'s embedding behavior (e.g. showing the toolbar, opening the app in dark theme, etc) using the various ?embed_options described in the next section.\\n\\nEmbed options\\n\\nWhen Embedding with iframes, Streamlit allows you to specify one or more instances of the ?embed_options query parameter for granular control over the embedding behavior. The supported values for\\xa0?embed_options\\xa0are listed below:\\n\\nShow the toolbar at the top right of the app (menu, running man, ...).\\n\\njavascript\\n   /?embed=true&embed_options=show_toolbar\\n\\nShow padding at the top and bottom of the app.\\n\\njavascript\\n   /?embed=true&embed_options=show_padding\\n\\nShow the footer reading \"Made with Streamlit.\"', metadata={'source': 'docs/content/streamlit-cloud/get-started/embed-your-app.md'}),\n",
       " Document(page_content='Show the footer reading \"Made with Streamlit.\"\\n\\njavascript\\n   /?embed=true&embed_options=show_footer\\n\\nShow the colored line at the top of the app.\\n\\njavascript\\n   /?embed=true&embed_options=show_colored_line\\n\\nDisable scrolling for the main body of the app. (The sidebar will still be scrollable.)\\n\\njavascript\\n   /?embed=true&embed_options=disable_scrolling\\n\\nOpen the app with light theme.\\n\\njavascript\\n   /?embed=true&embed_options=light_theme\\n\\nOpen the app with dark theme.\\n\\njavascript\\n   /?embed=true&embed_options=dark_theme\\n\\nYou can also combine the params:\\n\\njavascript\\n/?embed=true&embed_options=show_toolbar&embed_options=show_padding&embed_options=show_footer&embed_options=show_colored_line&embed_options=disable_scrolling\\n\\nBoth\\xa0?embed\\xa0and\\xa0?embed_options\\xa0are invisible to\\xa0st.experimental_get_query_params and\\xa0st.experimental_set_query_params. The former ignores the embed query parameters and does not return them, while the latter disallows setting embed query parameters.', metadata={'source': 'docs/content/streamlit-cloud/get-started/embed-your-app.md'}),\n",
       " Document(page_content=\"title: Share your app\\nslug: /streamlit-community-cloud/get-started/share-your-app\\n\\nShare your app\\n\\nNow that your app is deployed you can easily share it and collaborate on it. But first, let's take a moment and do a little joy dance for getting that app deployed! 🕺💃\\n\\nYour app is now live at that fixed URL, so go wild and share it with whomever you want. Your app will inherit permissions from your GitHub repo, meaning that if your repo is private your app will be private and if your repo is public your app will be public. If you want to change that you can simply do so from the app menu.\\n\\nSharing private apps\\n\\nSharing public apps\\n\\nAdding developers\\n\\nThere are three primary ways to share your app with viewers. You can either directly add viewers from the in-app share menu, or do so from the Cloud logs menu, or from your app dashboard.\\n\\nSharing private apps\\n\\nBy default all apps deployed from private source code are private to the developers in the workspace. Your apps will not be visible to anyone else unless you grant them explicit permission. You can grant permission either in your workspace or from the app itself.\\n\\nWhat is viewer auth?\\n\\nViewer auth allows you to restrict the viewers of your app. To access your app, users have to authenticate using an email-based passwordless login or Google OAuth.\\n\\nConfiguring single sign-on\\n\\nGoogle OAuth is enabled by default, so if you use Google, you're good to go.\\n\\nOnce you have added someone's email address to your app's viewer list, that person will be able to sign in via Google Single Sign-On and view your app. They will also receive an email inviting them to view your app. If they are already logged in with that account in their browser (the usual case for most people) then they will automatically be able to view the app. If they are not logged in, or they have not been given access, then they will see a page asking them to first log in.\", metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/index.md'}),\n",
       " Document(page_content='Having trouble granting access? Is a viewer having trouble logging on? See our troubleshooting section for help.\\n\\nAdding viewers from the in-app share menu\\n\\nYou can add viewers from the in-app share menu by clicking the \"Share\" button in the top right corner of your deployed app.\\n\\nClick \"Share\" in the top right corner.\\n\\nEnter the email addresses of the viewers.\\n\\nClick \"Invite\".\\n\\nIt\\'s that easy! The viewers you have added will receive an email inviting them to visit your app. The most recently added viewers will appear at the top of the list in the in-app share menu.\\n\\nTo remove a viewer, simply hover over their email address and click \"X\" that appears to the right:\\n\\nDevelopers, invited viewers, and members of your workspace can all see the in-app share menu, read the list of viewers, and add and remove viewers.\\n\\nOnly developers are allowed to toggle whether the app is public or private. App viewers don\\'t have permission to change this setting.\\n\\nAdding viewers from the Cloud logs menu\\n\\nFrom your deployed app you can easily add viewers from your Cloud logs menu.\\n\\nSelect \"Manage app\" in the lower right corner.\\n\\nChoose \"Settings\" from the menu.\\n\\nAdd Viewers in Settings.\\n\\nYou can choose to allow only selected viewers based on their individual emails. Make sure to enter them as a line-separated list.\\n\\nAdding viewers from the app dashboard\\n\\nYou can also add viewers directly from your dashboard.\\n\\nOpen settings for your app\\n\\nNavigate to the app you want to add viewer to and click the hamburger icon to select \"Settings.\"\\n\\nAdd Viewers in Settings\\n\\nClick on the \"Sharing\" section in the App Settings and in the text input area, provide a line-separated list of email addresses for the users you wish to grant viewer access to your app. Click \"Save.\"\\n\\nSharing public apps', metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/index.md'}),\n",
       " Document(page_content='Sharing public apps\\n\\nFrom your deployed app you can click on the \"☰\" menu on the top right and select \\'Share this app\\' to post it directly into social media or to share with the community on our Community forum. We\\'d love to see what you make and perhaps feature your app as our app of the month ❤️.\\n\\nAdd a GitHub badge\\n\\nTo help others find and play with your Streamlit app, you can add Streamlit\\'s GitHub badge to your repo. Below is an example of what the badge looks like. Clicking on the badge takes you to, in this case, Streamlit\\'s Face-GAN Demo.\\n\\nOnce you deploy your app, you can embed this badge right into your GitHub README.md by adding the following Markdown:\\n\\nmarkdown\\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://<your-custom-subdomain>.streamlit.app)\\n\\nBe sure to replace https://<your-custom-subdomain>.streamlit.app with the URL of your deployed app!\\n\\nAdding developers\\n\\nInviting other developers is simple, just invite them to your GitHub repository so that you can code on apps together, and then have them log in to share.streamlit.io. If you are working as a team, you likely are already in the same repos, so skip step 1 and go straight to having them log into share.streamlit.io\\n\\nStreamlit Community Cloud inherits developer permissions from GitHub, so when your teammates log in, they will automatically view the workspaces you share. From there you can all deploy, manage, and share apps together.\\n\\nPushing new code\\n\\nYou can also collaborate with other developers by having multiple contributors pushing to the same GitHub repo. Whenever anyone on the team updates the code on GitHub, the app will also automatically update for you!\\n\\nIf you want to try out something new while still keeping your original app running, just create a new branch, make some changes, and deploy a new version of the Streamlit app.\\n\\nFinding app code', metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/index.md'}),\n",
       " Document(page_content='Finding app code\\n\\nEvery deployed app has its GitHub source code linked in the \"☰\" menu on the top right. So if you are looking to understand the code of another Streamlit app, you can navigate to the GitHub page from there and read or fork the app.', metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/index.md'}),\n",
       " Document(page_content=\"title: Get started\\nslug: /streamlit-community-cloud/get-started\\n\\nGet started\\n\\nWelcome to Streamlit Community Cloud! First things first, before you get started with Streamlit Community Cloud, you need to have a Streamlit app to deploy. If you haven't built one yet, read our Get started docs or start with an Example app. Either way, it only takes a few minutes to create your first app.\\n\\nHow Streamlit Community Cloud works\\n\\nStreamlit Community Cloud is a workspace for your team to deploy, manage, and collaborate on your Streamlit apps. You connect your Streamlit Community Cloud account directly to your GitHub repository (public or private) and then Streamlit Community Cloud launches the apps directly from the code you've stored on GitHub. Most apps will launch in only a few minutes, and any time you update the code on GitHub, your app will automatically update for you. This creates a fast iteration cycle for your deployed apps, so that developers and viewers of apps can rapidly prototype, explore, and update apps.\\n\\nUnder the hood Streamlit Community Cloud handles all of the containerization, authentication, scaling, security and everything else so that all you need to worry about is creating the app. Maintaining Streamlit apps is easy. Containers get the latest security patches, are actively monitored for container health. We are also building the capability to observe and monitor apps.\\n\\nGetting started\\n\\nGetting your workspace set up with Streamlit Community Cloud only takes a few minutes.\\n\\nSign up for Streamlit Community Cloud\\n\\nLog in to your account\\n\\nConnect your Streamlit Community Cloud account to GitHub\\n\\nExplore your Streamlit Community Cloud workspace\\n\\nInvite other developers on your team\\n\\nSign up for Streamlit Community Cloud\\n\\nStreamlit's Community Cloud allows you to deploy, manage, and share your apps with the world, directly from Streamlit — all for free. Sign up on the Community Cloud homepage.\", metadata={'source': 'docs/content/streamlit-cloud/get-started/index.md'}),\n",
       " Document(page_content='Once you\\'ve signed up, login to share.streamlit.io and follow the steps below.\\n\\nLog in to share.streamlit.io\\n\\nYou can login to Streamlit Community Cloud with:\\n\\nGoogle\\n\\nGitHub\\n\\nEmail based sign-in link: These are single-use links that are valid for up to 15 minutes.\\n\\nIf you\\'re a developer, we recommend starting with GitHub the first time you login. You can later setup your account to login using Google.\\n\\nIf you\\'re sharing your app, your app\\'s users can use any of the above methods to login.\\n\\nSign in with Google\\n\\nVisit share.streamlit.io and click the \"Continue with Google\" button.\\n\\nOn the next page, choose an account to sign in with and enter your Google account credentials.\\n\\nOnce you have signed in to Google, you will be taken to your Streamlit Community Cloud workspace!🎈\\n\\nSign in with GitHub\\n\\nVisit share.streamlit.io and click the \"Continue with GitHub\" button.\\n\\nOn the next page, enter your GitHub credentials to sign in.\\n\\nOnce you have signed in to GitHub, you will be taken to your Streamlit Community Cloud workspace!🎈\\n\\nSign in with Email\\n\\nIf you don\\'t have SSO, you can sign in with your email address! Visit share.streamlit.io, enter the email address you used to sign up for Streamlit Community Cloud, and click the \"Continue with email\" button.\\n\\nOnce you do so, you will see a confirmation message (like the one below) asking you to check your email.\\n\\nCheck your inbox for an email from Streamlit, with the subject \"Sign in to Streamlit Community Cloud\". Click the link in the email to sign in to Streamlit. Note that this link will expire in 15 minutes and can only be used once.\\n\\nOnce you click the link in your email, you will be taken to your Streamlit Community Cloud workspace!🎈\\n\\nConnect your GitHub account', metadata={'source': 'docs/content/streamlit-cloud/get-started/index.md'}),\n",
       " Document(page_content='Connect your GitHub account\\n\\nNext you need to authorize Streamlit to connect to your GitHub account. This lets your Streamlit Community Cloud workspace launch apps directly from the app files you store in your repos, as well as let the system check for updates to those app files so that your apps can automatically update. You will see two different authorization screens to give this access. Click \"authorize\" on both. Questions about GitHub permissions? Read more here!\\n\\nYou must have admin permissions to your repo in order to deploy apps. If you don\\'t have admin access, talk to your IT team or manager about helping you set up your Streamlit Community Cloud account or reach out to us on the Community forum.\\n\\nOnce a user is added to a repository on GitHub, it will take at most 15 minutes before they can deploy the app on Cloud. If a user is removed from a repository on GitHub, it will take at most 15 minutes before their permissions to manage the app from that repository are revoked.\\n\\nExplore your Streamlit Community Cloud workspace\\n\\nCongrats! You are now logged in and ready to go. If you are joining someone else\\'s workspace you may already see apps populated in your workspace. If not, then you need to deploy an app! Check out our next section on how to deploy an app. And if you need an app to deploy check out our example apps that include apps for machine learning, data science, and business use cases.\\n\\nYou may also find that you already have multiple Streamlit Community Cloud workspaces. Streamlit Community Cloud automatically groups your apps according to the corresponding GitHub repository\\'s owner. In the upper right corner you can see the workspaces you have access to. If your team has already launched apps, then you will see those apps in your workspace. Read more about workspaces here.\\n\\nInvite other developers to your workspace', metadata={'source': 'docs/content/streamlit-cloud/get-started/index.md'}),\n",
       " Document(page_content='Invite other developers to your workspace\\n\\nInviting other developers is simple, just invite them to your GitHub repository so that you can code on apps together, and then have them log in to share.streamlit.io. If you are working as a team, you likely are already in the same repos, so skip step 1 and go straight to having them log into share.streamlit.io\\n\\nStreamlit Community Cloud inherits developer permissions from GitHub, so when your teammates log in, they will automatically view the workspaces you share. From there you can all deploy, manage, and share apps together.\\n\\nAnd remember, whenever anyone on the team updates the code on GitHub, the app will also automatically update for you!', metadata={'source': 'docs/content/streamlit-cloud/get-started/index.md'}),\n",
       " Document(page_content='title: App indexability\\nslug: /streamlit-community-cloud/get-started/share-your-app/indexability\\n\\nApp indexability\\n\\nWhen you deploy a public app to Community Cloud, it is automatically indexed by search engines like Google and Bing on a weekly basis. 🎈 This means that anyone can find your app by searching for its custom subdomain (e.g. https://traingenerator.streamlit.app) or by searching for the app\\'s title.\\n\\nGet the most out of app indexability\\n\\nHere are some tips to help you get the most out of app indexability:\\n\\nMake sure your app is public\\n\\nChoose a custom subdomain early\\n\\nChoose a descriptive app title\\n\\nCustomize your app\\'s meta description\\n\\nMake sure your app is public\\n\\nAll public apps hosted on Community Cloud are indexed by search engines. If your app is private, it will not be indexed by search engines. To make your private app public, read Share your app.\\n\\nChoose a custom subdomain early\\n\\nCommunity Cloud automatically generates a random subdomain for your app based on the structure of the app\\'s GitHub repo. To learn more about app URLs, see Your app URL. However, subdomains are customizable! Custom subdomains modify your app URLs to reflect your app content, personal branding, or whatever you’d like. Read more about custom subdomains in Custom subdomains.\\n\\nBy choosing a custom subdomain, you can use it to help people find your app. For example, if you\\'re deploying an app that generates training data, you might choose a subdomain like traingenerator.streamlit.app. This makes it easy for people to find your app by searching for \"training generator\" or \"train generator streamlit app\"', metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/indexability.md'}),\n",
       " Document(page_content='We recommend choosing a custom subdomain early, right after you deploy your app. This ensures that your app is indexed by search engines using your custom subdomain, rather than the automatically generated one. If you choose a custom subdomain later, your app may first be indexed by search engines using the default subdomain. This means that your app will be indexed multiple times, once using the default subdomain (which will lead to a 404) and once using your custom subdomain. This can confuse users who are searching for your app.\\n\\nChoose a descriptive app title\\n\\nThe meta title of your app is the text that appears in search engine results. It is also the text that appears in the browser tab when your app is open. By default, the meta title of your app is the same as the title of your app. However, you can customize the meta title of your app by setting the st.set_page_config parameter page_title to a custom string. For example:\\n\\npython\\nst.set_page_config(page_title=\"Traingenerator\")\\n\\nThis will change the meta title of your app to \"Traingenerator.\" This makes it easier for people to find your app by searching for \"Traingenerator\" or \"train generator streamlit app\":\\n\\nCustomize your app\\'s meta description\\n\\nMeta descriptions are the short descriptions that appear in search engine results. Search engines use the meta description to help users understand what your app is about.\\n\\nFrom our observations, search engines seem to favor the content in both st.header and st.text over st.title. If you put a description at the top of your app under st.header or st.text, there’s a good chance search engines will use this for the meta description.\\n\\nWhat does my indexed app look like?\\n\\nIf you\\'re curious about what your app looks like in search engine results, you can type the following into Google Search:\\n\\nsite:<your-app-url>\\n\\nExample: site:traingenerator.streamlit.app\\n\\nWhat if I don\\'t want my app to be indexed?', metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/indexability.md'}),\n",
       " Document(page_content=\"What if I don't want my app to be indexed?\\n\\nIf you don't want your app to be indexed by search engines, you can make it private. Read Share your app to learn more about making your app private. Note: each workspace can only have one private app. If you want to make your app private, you must first delete any other private apps in your workspace.\\n\\nThat said, Community Cloud is an open and free platform for the community to deploy, discover, and share Streamlit apps and code with each other. As such, we encourage you to make your app public so that it can be indexed by search engines and discovered by other Streamlit users and community members.\\n\\nHave questions or feedback?\\n\\nIf you run into issues with app indexability or have any questions or feedback about it, we’d love to hear from you! Please post on our Community forum so that our team and community members can help you. 🤗\", metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/indexability.md'}),\n",
       " Document(page_content='title: Share previews\\nslug: /streamlit-community-cloud/get-started/share-your-app/share-previews\\n\\nShare previews\\n\\nSocial media sites generate a card with a title, preview image, and description when you share a link. This feature is called a \"share preview.\" In the same way, when you share a link to a public Streamlit app on social media, a share preview is also generated. Here\\'s an example of a share preview for a public Streamlit app posted on Twitter:\\n\\nShare previews are generated only for public apps deployed on Community Cloud.\\n\\nTitles\\n\\nThe title is the text that appears at the top of the share preview. The text also appears in the browser tab when you visit the app. You should set the title to something that will make sense to your app\\'s audience and describe what the app does. It is best practice to keep the title concise, ideally under 60 characters.\\n\\nThere are two ways to set the title of a share preview:\\n\\nSet the page_title parameter in st.set_page_config() to your desired title. E.g.:\\n\\n```python\\n   import streamlit as st\\n\\nst.set_page_config(page_title=\"My App\")\\n\\n# ... rest of your app\\n   ```\\n\\nIf you don\\'t set the page_title parameter, the title of the share preview will be the name of your app\\'s GitHub repository. E.g., if you don\\'t set the page_title parameter in st.set_page_config(), the title of the share preview for an app hosted on GitHub at https://github.com/jrieke/traingenerator will be \"traingenerator\".\\n\\nDescriptions\\n\\nThe description is the text that appears below the title in the share preview. The description should summarize what the app does and ideally should be under 100 characters.\\n\\nStreamlit pulls the description from the README in the app\\'s GitHub repository. If there is no README, the description will default to:\\n\\nThis app was built in Streamlit! Check it out and visit https://streamlit.io for more awesome community apps. 🎈', metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/share-previews.md'}),\n",
       " Document(page_content=\"If you want your share previews to look great and want users to share your app and click on your links, you should write a good description in the README of your app’s GitHub repository.\\n\\nPreview images\\n\\nCommunity Cloud takes a screenshot of your app once a day and uses it as the preview image, unlike titles and descriptions, which are pulled directly from your app's code or GitHub repository. This screenshot may take up to 24 hours to update.\\n\\nSwitching your app from public to private\\n\\nIf you initially made your app public and later decided to make it private, we will stop generating share previews for the app. However, it may take up to 24 hours for the share previews to stop appearing.\", metadata={'source': 'docs/content/streamlit-cloud/get-started/share-your-app/share-previews.md'}),\n",
       " Document(page_content='title: Manage your app\\nslug: /streamlit-community-cloud/get-started/manage-your-app\\n\\nManage your app\\n\\nYou can manage your app directly from the deployed app in your developer view or you can log in to your app dashboard at\\xa0share.streamlit.io\\xa0to view, deploy, delete, reboot, or favorite an app.\\n\\nManage apps from your developer view\\n\\nManage apps from your app dashboard\\n\\nManage apps in GitHub\\n\\nApp resources and limits\\n\\nApp favoriting\\n\\nAnalytics Modal\\n\\nManage apps from your developer view\\n\\nOnce you have deployed an app you will have a developer view for that app.\\n\\nDeveloper view\\n\\nClick on the bottom right where it says \"Manage app\" to view your Cloud logs and other settings.\\n\\nCloud logs\\n\\nOnce you\\'ve clicked on \"Manage app\", you will be able to view your app\\'s logs. This is your primary place to troubleshoot any issues with your app.\\n\\nYou can also click on the \"︙\" overflow menu at the bottom of the Cloud logs to view other options for your app including the ability to download logs, reboot the app, delete the app, navigate to settings (which includes managing viewer access and app secrets), go to your app dashboard, go to documentation, contact support, or sign out.\\n\\nManage apps from your app dashboard\\n\\nWhen you first log into\\xa0share.streamlit.io\\xa0you will land on your app dashboard, which gives you a list of all your deployed apps. This list does include apps deployed by other developers in your workspace, since you\\'re all managers of those apps. Such apps are indicated with an icon like this one:\\n\\nApp workspaces\\n\\nStreamlit Community Cloud is organized into workspaces, which automatically group your apps according to the corresponding GitHub repository\\'s owner. If you are part of multiple repositories, then you will have multiple workspaces.\\n\\nIf an app\\'s GitHub repository is owned by you, the app will appear in your personal workspace, named \"<YourGitHubHandle>\".', metadata={'source': 'docs/content/streamlit-cloud/get-started/manage-your-app.md'}),\n",
       " Document(page_content='If an app\\'s GitHub repository is owned by an organization (such as your company), the app will appear in a separate workspace, named \"<GitHubOrganizationHandle>\".\\n\\nYou will also have access to any workspaces containing app(s) for which you only have view access. These apps will have a \"view-only\" tooltip when you click on their respective hamburger menus.\\n\\nTo switch between workspaces, click on the workspace listed in the top right corner, then select the desired workspace name.\\n\\nReboot an app\\n\\nIf your app needs a hard reboot, click on the \"︙\" overflow menu to the right of the app and click to Reboot. This will interrupt any user that may currently be using that app. It may also take a few minutes for your app to re-deploy, and in that time you — and anyone visiting the app — will see the \\'Your app is in the oven\\' screen.\\n\\nApp settings\\n\\nThe app settings let you pick a custom subdomain for your app, manage viewers of your apps and secrets of your apps. Click on the links to lean more about these features.\\n\\nManage apps in GitHub\\n\\nUpdate your app\\n\\nYour GitHub repository is the source for the app, so that means that any time you push an update to your repo you\\'ll see it reflected in the app in almost real time. Try it out!\\n\\nStreamlit also smartly detects whether you touched your dependencies, in which case it will automatically do a full redeploy for you—which will take a little more time. But since most updates don\\'t involve dependency changes, you should usually see your app update in real time.\\n\\nAdd or remove dependencies\\n\\nYou can add/remove dependencies at any point by updating\\xa0requirements.txt\\xa0(Python deps) or\\xa0packages.txt\\xa0(Debian deps) and doing a\\xa0git push\\xa0to your remote repo. This will cause Streamlit to detect there was a change in its dependencies, which will automatically trigger its installation.', metadata={'source': 'docs/content/streamlit-cloud/get-started/manage-your-app.md'}),\n",
       " Document(page_content='It is best practice to pin your Streamlit version in\\xa0requirements.txt. Otherwise, the version may be auto-upgraded at any point without your knowledge, which could lead to undesired results (e.g. when we deprecate a feature in Streamlit).\\n\\nApp resources and limits\\n\\nResource limits\\n\\nAll Community Cloud users have access to the same resources and are subject to the same limits (1 GB of RAM).\\nIf your app is running slowly or you\\'re hitting the \\'Argh\\' page, we first highly recommend going through and implementing the suggestions in the following blog posts to prevent your app from hitting the resource limits and to detect if your Streamlit app leaks memory:\\n\\nCommon app problems: Resource limits\\n\\n3 steps to fix app memory leaks\\n\\nDeveloper view\\n\\nIf your app exceeds its resource limits, you will see one of the following messages when you visit your app. If your app uses an older version of Streamlit (<1.1.0) without memory fixes, you will see the message on the left. If your app uses a newer version of Streamlit (>=1.1.0), you will see the message on the right:\\n\\nSimilarly, you will receive one of the following two emails from alert@streamlit.io with the subject \"Your Streamlit app has gone over its resource limits 🤯\":\\n\\nNon-developer view\\n\\nIf your app exceeds its resource limits, users with view-only access will see one of the following messages when they visit your app. They will see the message on the left if your app uses an older version of Streamlit (<1.1.0) without memory fixes, and the message on the right if your app uses a newer version of Streamlit (>=1.1.0). Viewers have the option to notify you when the app exceeds its resource limits:\\n\\nApp hibernation\\n\\nPrivate apps will not hibernate, but public Community Cloud apps without traffic for 7 consecutive days will automatically go to sleep. This is done to alleviate resources and allow the best communal use of the platform! Here are some need to know\\'s about how this works:', metadata={'source': 'docs/content/streamlit-cloud/get-started/manage-your-app.md'}),\n",
       " Document(page_content='As the app developer, you will receive an email after 5 days of no traffic on your app.\\n\\nIf you would like to keep your app awake, you have one of two choices:\\n\\nVisit the app (create traffic).\\n\\nPush a commit to the app (this can be empty!).\\n\\nIf left alone the app will go to sleep at the 7 day mark (2 days after you receive the email). When someone visits the app after this, they will see the sleeping page:\\n\\nTo wake the app up, press the \"Yes, get this app back up!\" button. This can be done by\\xa0anyone\\xa0who wants to view the app, not just the app developer!\\n\\nYou can also wake apps through your Streamlit Community Cloud dashboard. You will know which apps are sleeping because a moon icon will appear next to the app settings. To wake an app from the dashboard, click the moon.\\n\\nApp favoriting\\n\\nStreamlit Community Cloud supports a \"favorite\" feature that lets you quickly access your apps from the app dashboard. Favorited apps appear at the top of the app dashboard with a yellow star (⭐) beside them. You can favorite and unfavorite apps in any workspace to which you have access.\\n\\nFavorites are specific to your account. Other members of your workspace cannot see which apps you have favorited.\\n\\nFavorite an app from your app dashboard\\n\\nThere are two ways to favorite an app from the app dashboard:\\n\\nHover over an app and click the star (☆) that appears.\\n\\nClick on the \"︙\" overflow menu to the app\\'s right and click to Favorite.\\n\\nTo unfavorite an app, either hover over the app and click the star (⭐) again, or click on the \"︙\" overflow menu to the app\\'s right and click to Unfavorite.\\n\\nIn-app favoriting\\n\\nYou can also favorite an app from right within the app! Currently, in-app favoriting is available for apps that use Streamlit v1.4.0 or later. Note that in-app favoriting is not available on apps in your workspaces for which you only have view access.\\n\\nWhen viewing any app in your workspace, click the star (☆) in the top-right corner of the app, besides the \"☰\" hamburger menu.', metadata={'source': 'docs/content/streamlit-cloud/get-started/manage-your-app.md'}),\n",
       " Document(page_content='To unfavorite an app, click the star (⭐) again.\\n\\nClick here to learn more about upgrading the Streamlit version of your app on Streamlit Community Cloud.\\n\\nAnalytics Modal\\n\\nOnce you have access to a Streamlit workspace, you have access to 2 types of analytics:\\n\\nWorkspace analytics: shows you how many viewers in total have visited all the apps in your workspace.\\n\\nApp viewers: shows you who has recently viewed your workspace’s individual apps and when.\\n\\nThe Analytics Modal is visible to everyone with access to your workspace, including admins, developers, or anyone with viewer access to a workspace.\\n\\nWorkspace analytics\\n\\nStreamlit Community Cloud enables you to view analytics data for all apps in your workspace in one central dashboard. At a glance, you get an overview of how active your workspace is and how popular your apps are.\\n\\nTo view your Workspace analytics:\\n\\nSelect the \"Analytics\" option on the dashboard header\\n\\nView the \"Workspace\" tab in the Analytics modal\\n\\nYou\\'re presented with a graph that you can hover over to see the number of users who have viewed at least one app in your workspace that month. This viewers count includes apps that anyone in your workspace created.\\n\\nSolid lines indicate fully-complete months on the dashboard, while dotted lines indicate the current in-progress month.\\n\\nViewers data on your dashboard starts from April 2022 and onward. April 2022 data was our first month comprehensively tracking user analytics in Streamlit workspaces, and our tracking is even more refined starting in May 2022 and onward.\\n\\nApp viewers\\n\\nIn addition to a general overview of the activity of your workspace and the popularity of your apps, Streamlit Community Cloud allows you to drill down to the level of individual apps and understand their viewership better.', metadata={'source': 'docs/content/streamlit-cloud/get-started/manage-your-app.md'}),\n",
       " Document(page_content='As an app developer or a viewer with access to a given workspace, you can see who has viewed a given app and when. Specifically, you can see the total viewers count of your app (since April 2022 and onward), the most recent unique viewers (capped up to your last 20 viewers), and a relative timestamp of their last view.\\n\\nThere are three ways to access the app viewers data:\\n\\nFrom the app dashboard, click the \"︙\" overflow menu to the app\\'s right and select Analytics:\\n\\nDoing so opens the \"App viewers\" tab of the \"Analytics\" modal.\\n\\nThe dropdown selects your app by default and displays:\\n\\nThe total (all time) number of unique viewers for the app.\\n\\nA list of the most recent viewers\\' names and a relative timestamp of their last view, sorted by the time since the last view (newest first).\\n\\nClick the \"Analytics\" option on the dashboard header and select the \"App viewers\" tab:\\n\\nDoing so opens the \"App viewers\" tab of the \"Analytics\" modal.\\n\\nThe first app in your workspace is pre-selected in the dropdown by default. You can select the app you want to see the analytics for by clicking the corresponding app in the dropdown.\\n\\nYou can also access app viewer analytics from right within individual apps! This is a capability if you have GitHub push access for a given app. Just view any app in your workspace as a developer, click the \"︙\" overflow menu at the bottom of the Cloud logs and select \"Analytics\":\\n\\nApp viewers for public vs private apps\\n\\nFor public apps, we anonymize all viewers outside your workspace to protect their privacy and display anonymous viewers as random pseudonyms. You\\'ll still be able to see the identities of fellow members in your workspace, though.\\n\\nMeanwhile, for private apps that are only accessible to your own workspace\\'s viewers, you will be able to see the specific users who recently viewed your apps.', metadata={'source': 'docs/content/streamlit-cloud/get-started/manage-your-app.md'}),\n",
       " Document(page_content=\"Additionally, you may occasionally see anonymous users in private apps. Rest assured, these anonymous users do have authorized view access granted by you or your workspace members.\\n\\nCommon reasons why users show up anonymously are:\\n\\nThe app was previously public\\n\\nGiven viewer viewed app in April 2022, when the Streamlit team was honing user identification for this feature\\n\\nGiven viewer disconnected their SSO and GitHub accounts previously\\n\\nSee Streamlit's general Privacy Notice.\", metadata={'source': 'docs/content/streamlit-cloud/get-started/manage-your-app.md'}),\n",
       " Document(page_content=\"title: App dependencies\\nslug: /streamlit-community-cloud/get-started/deploy-an-app/app-dependencies\\n\\nApp dependencies\\n\\nThe main reason that apps fail to build properly is because Streamlit Community Cloud can't find your dependencies! So make sure you:\\n\\nAdd a\\xa0requirements file\\xa0for Python dependencies.\\n\\n(optional) Add a\\xa0packages.txt\\xa0file to manage any external dependencies (i.e Linux dependencies outside Python environment).\\n\\nPython requirements files should be placed either in the root of your repository or in the same\\ndirectory as your Streamlit app.\\n\\nAdd Python dependencies\\n\\nStreamlit looks at your requirements file's filename to determine which Python dependency manager to use in the order below. Streamlit will stop and install the first requirements file found.\\n\\ndocs\\n\\ndocs\\n\\ndocs\\n\\ndocs\\n\\nOnly include packages in your requirements file that are not distributed with a standard Python\\ninstallation. If any of the modules from base Python\\nare included in the requirements file, you will get an error when you try to deploy. Additionally, we recommend that you\\nuse the latest version of Streamlit to ensure full Streamlit Community Cloud functionality. Be sure to take note of\\nStreamlit's current requirements\\nfor package compatibility when planning your environment, especially protobuf>=3.20,<5.\\n\\nYou should only use one requirements file for your app. If you include more than one (e.g.\\nrequirements.txt and Pipfile). Streamlit will first look in the directory of your Streamlit app;\\nhowever, if no requirements file is found, Streamlit will then look at the root of the repo.\\n\\napt-get dependencies\\n\\nIf packages.txt exists in the root directory of your repository we automatically detect it, parse it, and install the listed packages as described below. You can read more about apt-get in their docs.\\n\\nAdd apt-get dependencies to\\xa0packages.txt, one package name per line. For example:\\n\\nbash\\n    freeglut3-dev\\n    libgtk2.0-dev\", metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/app-dependencies.md'}),\n",
       " Document(page_content='title: Deploy an app\\nslug: /streamlit-community-cloud/get-started/deploy-an-app\\n\\nDeploy an app\\n\\nStreamlit Community Cloud lets you deploy your apps in just one click, and most apps will deploy in only a few minutes. If you don\\'t have an app ready to deploy, fork or clone one of our example apps — you can find apps for machine learning, data visualization, data exploration, A/B testing and more.\\n\\nIf you want to deploy your app on a different cloud service, check out the Deploy Streamlit apps article in our Knowledge Base.\\n\\nAdd your app to GitHub\\n\\nStreamlit Community Cloud launches apps directly from your GitHub repo, so your app code and dependencies need to be on GitHub before you try to deploy the app. See App dependencies for more information.\\n\\nOptionally, add a configuration file\\n\\nStreamlit allows you to optionally set configuration options via four different methods. Among other things, you can use custom configs to customize your app\\'s theme, enable logging, or set the port on which your app runs. For more information, see\\xa0Configuration\\xa0and\\xa0Theming. On Streamlit Community Cloud, however, you can only set configuration options via a configuration file in your GitHub repo.\\n\\nSpecifically, you can add a configuration file to the root (top-level) directory of your repo: create a\\xa0.streamlit\\xa0folder, and then add a\\xa0config.toml\\xa0file to that folder. E.g., if your app is in a repo called\\xa0my-app, you would add a file called\\xa0my-app/.streamlit/config.toml. Say you want to set the theme of your app to \"dark\". You would add the following to your\\xa0.streamlit/config.toml\\xa0file:\\n\\ntoml\\n[theme]\\nbase=\"dark\"\\n\\nThere can be only one configuration file, regardless of the number of apps in the repo.\\n\\nDeploy your app\\n\\nTo deploy an app, click \"New app\" from the upper right corner of your workspace.', metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/index.md'}),\n",
       " Document(page_content='Fill in your repo, branch, and file path. As a shortcut, you can also click \"Paste GitHub URL\". Optionally, you can specify a custom subdomain. In the example below, the app would be deployed to https://red-balloon.streamlit.app/. You can always set or change your subdomain later. See more about custom subdomains at the end of this page.\\n\\nAdvanced settings for deployment\\n\\nIf you are connecting to a data source or want to select a Python version for your app, you can do that by clicking \"Advanced settings\" before you deploy the app.\\n\\nYou can connect to private data sources by using secrets management. Read more on how to connect to data sources.\\n\\nStreamlit Community Cloud supports Python 3.8 - Python 3.11, and defaults to version 3.9. You can select a version of your choice from the \"Python version\" dropdown in the \"Advanced settings\" modal.\\n\\nWatch your app launch\\n\\nYour app is now deploying and you can watch while it launches. Most apps take only a couple of minutes to deploy, but if your app has a lot of dependencies it may take some time to deploy the first time. After the initial deployment, any change that does not touch your dependencies should show up immediately.\\n\\nThe Cloud logs on the right hand side are only viewable to the developer and is how you can grab logs and debug any issues with the app. Learn more about Cloud logs.\\n\\nYour app URL\\n\\nThat\\'s it — you\\'re done! Your app now has a unique subdomain URL that you can share with others. Click here to read about how to share your app with viewers.\\n\\nUnique subdomains\\n\\nIf a custom subdomain was not set, an app URL follows a structure based on your GitHub repo. The URL begins with your GitHub username or organization owning your repo, followed by your repo name, app path, and a short hash. If you deploy from a branch other than main or master, the URL also includes the branch name.\\n\\nbash\\nhttps://[GitHub username or organization]-[repo name]-[app path]-[branch name]-[short hash].streamlit.app', metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/index.md'}),\n",
       " Document(page_content='For example, this is an app deployed from the streamlit organization. The repo is demo-self-driving and the app name is streamlit_app.py in the root directory. The branch name is master and therefore not included.\\n\\nbash\\nhttps://streamlit-demo-self-driving-streamlit-app-8jya0g.streamlit.app\\n\\nCustom subdomains\\n\\nThe default subdomain is not always the most memorable or easy to share. That\\'s why you can also set a custom domain for your app. The URL will appear as:\\n\\nbash\\nhttps://<your-custom-subdomain>.streamlit.app\\n\\nTo view or customize your app subdomain from the dashboard:\\n\\nClick the \"︙\" overflow menu to the app\\'s right and select \"Settings\".\\n\\nView the \"General\" tab in the App settings modal. Your app\\'s unique subdomain will appear here.\\n\\nPick a custom subdomain between 6 and 63 characters in length for your app\\'s URL and hit \"Save\".\\n\\nIt\\'s that simple! You can then access your app by visiting your custom subdomain URL 🎉.\\n\\nIf a custom subdomain is not available (e.g. because it\\'s already taken), you\\'ll see an error message like this:\\n\\nEmbed apps\\n\\nDocumentation for embedding apps has moved to Embed your app. Please update your bookmarks.', metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/index.md'}),\n",
       " Document(page_content='title: Streamlit Library\\nslug: /library\\n\\nStreamlit Library', metadata={'source': 'docs/content/library/index.md'}),\n",
       " Document(page_content='title: Secrets management\\nslug: /streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources/secrets-management\\n\\nSecrets management\\n\\nIntroduction\\n\\nIt\\'s generally considered bad practice to store unencrypted secrets in a git repository. If your application needs access to sensitive credentials the recommended solution is to store those credentials in a file that is not committed to the repository and to pass them as environment variables.\\n\\nSecrets Management allows you to store secrets securely and access them in your Streamlit app as environment variables.\\n\\nHow to use Secrets Management\\n\\nDeploy an app and set up secrets\\n\\nGo to http://share.streamlit.io/ and click \"New app\" to deploy a new app with secrets.\\n\\nClick \"Advanced settings...\"\\n\\nYou will see a modal appear with an input box for your secrets.\\n\\nProvide your secrets in the \"Secrets\" field using TOML format. For example:\\n\\n```toml\\n   # Everything in this section will be available as an environment variable\\n   db_username = \"Jane\"\\n   db_password = \"12345qwerty\"\\n\\n# You can also add other sections if you like.\\n   # The contents of sections as shown below will not become environment variables,\\n   # but they\\'ll be easily accessible from within Streamlit anyway as we show\\n   # later in this doc.\\n   [my_cool_secrets]\\n   things_i_like = [\"Streamlit\", \"Python\"]\\n   ```\\n\\nUse secrets in your app\\n\\nAccess your secrets as environment variables or by querying the st.secrets dict. For example, if you enter the secrets from the section above, the code below shows you how you can access them within your Streamlit app.\\n\\n```python\\nimport streamlit as st\\n\\nEverything is accessible via the st.secrets dict:\\n\\nst.write(\"DB username:\", st.secrets[\"db_username\"])\\nst.write(\"DB password:\", st.secrets[\"db_password\"])\\nst.write(\"My cool secrets:\", st.secrets[\"my_cool_secrets\"][\"things_i_like\"])\\n\\nAnd the root-level secrets are also accessible as environment variables:\\n\\nimport os', metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/connect-data-sources/secrets-management.md'}),\n",
       " Document(page_content='import os\\n\\nst.write(\\n    \"Has environment variables been set:\",\\n    os.environ[\"db_username\"] == st.secrets[\"db_username\"],\\n)\\n```\\n\\nYou can access st.secrets via attribute notation (e.g. st.secrets.key), in addition to key notation (e.g. st.secrets[\"key\"])—like st.session_state.\\n\\nYou can even use TOML sections to compactly pass multiple secrets as a single attribute.\\n\\nConsider the following secrets:\\n\\ntoml\\n[db_credentials]\\nusername = \"my_username\"\\npassword = \"my_password\"\\n\\nRather than passing each secret as attributes in a function, you can more compactly pass the section to achieve the same result. See the notional code below which uses the secrets above:\\n\\n```python\\n\\nVerbose version\\n\\nmy_db.connect(username=st.secrets.db_credentials.username, password=st.secrets.db_credentials.password)\\n\\nFar more compact version!\\n\\nmy_db.connect(**st.secrets.db_credentials)\\n```\\n\\nEdit your app\\'s secrets\\n\\nGo to https://share.streamlit.io/\\n\\nOpen the menu for your app, and click \"Settings\".\\n\\nYou will see a modal appear. Click on the \"Secrets\" section and edit your secrets.\\n\\nAfter you edit your secrets, click \"Save\". It might take a minute for the update to be propagated to your app, but the new values will be reflected when the app re-runs.\\n\\nDevelop locally with secrets\\n\\nWhen developing your app locally, add a file called secrets.toml in a folder called .streamlit at the root of your app repo, and copy/paste your secrets into that file. Further instructions are available in the Streamlit library Secrets management documentation.\\n\\nBe sure to add this file to your .gitignore so you don\\'t commit your secrets!', metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/connect-data-sources/secrets-management.md'}),\n",
       " Document(page_content=\"title: Connect to data sources\\nslug: /streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources\\n\\nConnect data sources\\n\\nYour app probably connects to some data source, and it's important to make sure that connection is secure. That data might just be a csv that you have in your GitHub repo, but in many cases it'll be a private data source you connect with via API, on a cloud service, or maybe in your company's VPN.\\n\\nStreamlit has one primary way of securely connecting to private data:\\n\\nSecrets management: securely store secrets like API keys and TOML files that you can then access as environment variables in your app.\\n\\nWe also have a series of guides on how to connect to:\\n\\nAWS S3\\n\\nBigQuery\\n\\nDeta Base\\n\\nFirestore (blog)\\n\\nGoogle Cloud Storage\\n\\nMicrosoft SQL Server\\n\\nMongoDB\\n\\nMySQL\\n\\nPostgreSQL\\n\\nPrivate Google Sheet\\n\\nPublic Google Sheet\\n\\nSnowflake\\n\\nSupabase\\n\\nTableau\\n\\nTiDB\\n\\nTigerGraph\\n\\nTrouble connecting to data? Need a different way to securely connect? Reach out on our Community forum to chat through options!\", metadata={'source': 'docs/content/streamlit-cloud/get-started/deploy-an-app/connect-data-sources/index.md'}),\n",
       " Document(page_content=\"title: Advanced features\\nslug: /library/advanced-features\\n\\nAdvanced features\\n\\nThis section gives you background on how different parts of Streamlit work.\\n\\n☰ App menu\\n\\nStreamlit provides a configurable menu within your app to access convenient tools for developers and viewers. These options can modify the appearance of your app while running.\\n\\nModify your app's theme while running\\n\\nRecord a screencast of your app\\n\\nDeploy a local app to Streamlit Community Cloud\\n\\nCustomize or hide the app menu\\n\\nCommand-line options\\n\\nWhen you install Streamlit, a command-line (CLI) tool gets installed as well. The purpose of this tool is to run Streamlit apps, change Streamlit configuration options, and help you diagnose and fix issues.\\n\\nWhat is the command-line interface (CLI)?\\n\\nHow to run Streamlit apps from the CLI?\\n\\nView Streamlit version from the CLI?\\n\\nView documentation from the CLI\\n\\nClear cache from the CLI\\n\\nStreamlit configuration\\n\\nStreamlit provides four different ways to set configuration options. Learn how to use each of them to change the behavior of Streamlit.\\n\\nHow to set configuration options?\\n\\nOpt out of telemetry collection\\n\\nView all configuration options\\n\\nTheming\\n\\nThis section provides examples of how Streamlit page elements are affected by the various theme config options.\\n\\nprimaryColor\\n\\nbackgroundcolor\\n\\nsecondarybackgroundcolor\\n\\ntextcolor\\n\\nfont\\n\\nbase\\n\\nCaching\\n\\nThe Streamlit cache allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations. To cache a function in Streamlit, you need to decorate it with one of two decorators: st.cache_data and st.cache_resource.\\n\\nMinimal example\\n\\nBasic usage\\n\\nst.cache_data\\n\\nst.cache_resource\\n\\nDeciding which caching decorator to use\\n\\nAdvanced usage\\n\\nExcluding input parameters\\n\\nControlling cache size and duration\\n\\nCustomizing the spinner\\n\\nUsing Streamlit commands in cached functions\\n\\nMutation and concurrency issues\\n\\nMigrating from st.cache\", metadata={'source': 'docs/content/library/advanced-features/index.md'}),\n",
       " Document(page_content=\"Migrating from st.cache\\n\\nAdd statefulness to apps\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks.\\n\\nWhat is Session State?\\n\\nHow to initialize Session State items?\\n\\nHow to read and update Session State items?\\n\\nHow to use callbacks in Session State?\\n\\nHow to use args and kwargs in callbacks?\\n\\nHow to use callbacks in forms?\\n\\nHow is Session State related to Widget State?\\n\\nCaveats and limitations\\n\\nPre-release features\\n\\nAt Streamlit, we like to move quick while keeping things stable. In our latest effort to move even faster without sacrificing stability, we're offering our bold and fearless users two ways to try out Streamlit's bleeding-edge features.\\n\\nExperimental features\\n\\nNightly releases\\n\\nSecrets management\\n\\nThis section provides examples of how to use secrets management to store and retrieve sensitive information in your Streamlit app.\\n\\nDevelop locally and set up secrets\\n\\nUse secrets in your app\\n\\nError handling\\n\\nUse secrets on Streamlit Community Cloud\\n\\nWorking with timezones\\n\\nWorking with timezones can be tricky. This section provides a high-level description of how to handle timezones in Streamlit to avoid unexpected behavior.\\n\\nOverview\\n\\nHow Streamlit handles timezones\\n\\ndatetime instance without a timezone (naive)\\n\\ndatetime instance with a timezone\\n\\nAdvanced notes on widget behavior\\n\\nWidgets are magical and often work how you want. But they can have surprising behavior in some situations. This section provides is a high-level, abstract description of widget behavior, including some common edge-cases.\", metadata={'source': 'docs/content/library/advanced-features/index.md'}),\n",
       " Document(page_content=\"title: Experimental cache primitives\\nslug: /library/advanced-features/experimental-cache-primitives\\n\\nThe experimental cache primitives described on this page were deprecated in version 1.18.0. Use st.cache_data or st.cache_resource instead. Learn more in Caching.\\n\\nExperimental cache primitives\\n\\nOverview\\n\\nStreamlit's unique execution model is a part of what makes it a joy to use: your code executes from top to bottom like a simple script for every interaction. There's no need to think about models, views, controllers, or anything of the sort.\\n\\nWhenever your code re-executes, a decorator called @st.cache—which is a powerful primitive for memoization and state storage capabilities—provides a caching mechanism that allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations.\\n\\nHowever, we've found that @st.cache is hard to use and not fast. You're either faced with cryptic errors like InternalHashError or UnhashableTypeError. Or you need to understand concepts like hash_funcs and allow_output_mutation.\\n\\nOur solutions include two new primitives: st.experimental_memo and st.experimental_singleton. They're conceptually simpler and much, much faster. In some of our internal tests on caching large dataframes, @st.experimental_memo has outperformed @st.cache by an order of magnitude. That's over 10X faster! 🚀\\n\\nLet's take a look at the use-cases these two experimental APIs serve, and how they're a significant improvement over @st.cache.\\n\\nProblem\\n\\n@st.cache was serving the following use-cases:\\n\\nStoring computation results given different kinds of inputs. In Computer Science literature, this is called memoization.\\n\\nInitializing an object exactly once, and reusing that same instance on each rerun for the Streamlit server's lifetime. This is called the singleton pattern.\", metadata={'source': 'docs/content/library/advanced-features/experimental-cache-primitives.md'}),\n",
       " Document(page_content='Storing global state to be shared and modified across multiple Streamlit sessions (and, since Streamlit is threaded, you need to pay special attention to thread-safety).\\n\\nAs a result of @st.cache trying to cover too many use-cases under a single unified API, it\\'s both slow and complex.\\n\\nSolution\\n\\nWhile @st.cache tries to solve two very different problems simultaneously (caching data and sharing global singleton objects), these new primitives simplify things by dividing the problem across two different APIs. As a result, they are faster and simpler.\\n\\n@st.experimental_memo\\n\\nUse @st.experimental_memo to store expensive computation which can be \"cached\" or \"memoized\" in the traditional sense. It has almost the exact same API as the existing @st.cache, so you can often blindly replace one for the other:\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_memo\\ndef factorial(n):\\n    if n < 1:\\n        return 1\\n    return n * factorial(n - 1)\\n\\nf10 = factorial(10)\\nf9 = factorial(9)  # Returns instantly!\\n```\\n\\nProperties\\n\\nUnlike @st.cache, this returns cached items by value, not by reference. This means that you no longer have to worry about accidentally mutating the items stored in the cache. Behind the scenes, this is done by using Python\\'s pickle() function to serialize/deserialize cached values.\\n\\nAlthough this uses a custom hashing solution for generating cache keys (like @st.cache), it does not use hash_funcs as an escape hatch for unhashable parameters. Instead, we allow you to ignore unhashable parameters (e.g. database connections) by prefixing them with an underscore.\\n\\nFor example:\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nfrom sqlalchemy.orm import sessionmaker\\n\\n@st.experimental_memo\\ndef get_page(_sessionmaker, page_size, page):\\n    \"\"\"Retrieve rows from the RNA database, and cache them.\\n\\n```\\n\\n@st.experimental_singleton', metadata={'source': 'docs/content/library/advanced-features/experimental-cache-primitives.md'}),\n",
       " Document(page_content='```\\n\\n@st.experimental_singleton\\n\\n@st.experimental_singleton is a key-value store that\\'s shared across all sessions of a Streamlit app. It\\'s great for storing heavyweight singleton objects across sessions (like TensorFlow/Torch/Keras sessions and/or database connections).\\n\\nExample usage:\\n\\n```python\\nimport streamlit as st\\nfrom sqlalchemy.orm import sessionmaker\\n\\n@st.experimental_singleton\\ndef get_db_sessionmaker():\\n    # This is for illustration purposes only\\n    DB_URL = \"your-db-url\"\\n    engine = create_engine(DB_URL)\\n    return sessionmaker(engine)\\n\\ndbsm = get_db_sessionmaker()\\n```\\n\\nHow this compares to @st.cache:\\n\\nLike @st.cache, this returns items by reference.\\n\\nYou can return any object type, including objects that are not serializable.\\n\\nUnlike @st.cache, this decorator does not have additional logic to check whether you are unexpectedly mutating the cached object. That logic was slow and produced confusing error messages. So, instead, we\\'re hoping that by calling this decorator \"singleton,\" we\\'re nudging you to the correct behavior.\\n\\nThis does not follow the computation graph.\\n\\nYou don\\'t have to worry about hash_funcs! Just prefix your arguments with an underscore to ignore them.\\n\\nSingleton objects can be used concurrently by every user connected to your app, and you are responsible for ensuring that @st.singleton objects are thread-safe. (Most objects you\\'d want to stick inside an @st.singleton annotation are probably already safe—but you should verify this.)\\n\\nWhich to use: memo or singleton?\\n\\nFor example:\\n\\nDataframe computation (pandas, numpy, etc): this is data—use memo\\n\\nStoring downloaded data: memo\\n\\nCalculating pi to n digits: memo\\n\\nTensorflow session: this is a non-data object—use singleton\\n\\nDatabase connection: singleton\\n\\nClear memo and singleton caches procedurally\\n\\nYou can clear caches of functions decorated with @st.experimental_memo and @st.experimental_singleton in code. For example, you can do the following:', metadata={'source': 'docs/content/library/advanced-features/experimental-cache-primitives.md'}),\n",
       " Document(page_content='```python\\n@st.experimental_memo\\ndef square(x):\\n    return x**2\\n\\nif st.button(\"Clear Square\"):\\n    # Clear square\\'s memoized values:\\n    square.clear()\\n\\nif st.button(\"Clear All\"):\\n    # Clear values from all memoized functions:\\n    st.experimental_memo.clear()\\n```\\n\\nPressing the \"Clear Square\" button will clear square()\\'s memoized values. Pressing the \"Clear All\" button will clear memoized values from all functions decorated with @st.experimental_memo.\\n\\nIn summary:\\n\\nAny function annotated with @st.experimental_memo or @st.experimental_singleton gets its own clear() function automatically.\\n\\nAdditionally, you can use st.experimental_memo.clear() and st.experimental_singleton.clear() to clear all memo and singleton caches, respectively.\\n\\nThe commands are experimental, so they\\'re governed by our experimental API process.\\n\\nThese specialized memoization and singleton commands represent a big step in Streamlit\\'s evolution, with the potential to entirely replace @st.cache at some point in 2022.\\n\\nYes, today you may use @st.cache for storing data you pulled in from a database connection (for a Tensorflow session, for caching the results of a long computation like changing the datetime values on a pandas dataframe, etc.). But these are very different things, so we made two new functions that will make it much faster! 💨\\n\\nPlease help us out by testing these commands in real apps and leaving comments in the Streamlit forums.', metadata={'source': 'docs/content/library/advanced-features/experimental-cache-primitives.md'}),\n",
       " Document(page_content='title: Cheat sheet\\nslug: /library/cheatsheet\\n\\nCheat Sheet\\n\\nThis is a summary of the docs, as of Streamlit v1.24.0.\\n\\nInstall & Import\\n\\n```python\\nstreamlit run first_app.py\\n\\nImport convention\\n\\nimport streamlit as st\\n```\\n\\nCommand line\\n\\npython\\nstreamlit --help\\nstreamlit run your_script.py\\nstreamlit hello\\nstreamlit config show\\nstreamlit cache clear\\nstreamlit docs\\nstreamlit --version\\n\\nPre-release features\\n\\npython\\npip uninstall streamlit\\npip install streamlit-nightly --upgrade\\n\\nLearn more about experimental features\\n\\nMagic commands\\n\\n```python\\n\\nMagic commands implicitly\\n\\ncall st.write().\\n\\n\\'This is some Markdown*\\'\\nmy_variable\\n\\'dataframe:\\', my_data_frame\\n\\n```\\n\\nDisplay text\\n\\npython\\nst.text(\\'Fixed width text\\')\\nst.markdown(\\'_Markdown_\\') # see *\\nst.latex(r\\'\\'\\' e^{i\\\\pi} + 1 = 0 \\'\\'\\')\\nst.write(\\'Most objects\\') # df, err, func, keras!\\nst.write([\\'st\\', \\'is <\\', 3]) # see *\\nst.title(\\'My title\\')\\nst.header(\\'My header\\')\\nst.subheader(\\'My sub\\')\\nst.code(\\'for i in range(8): foo()\\')\\n* optional kwarg unsafe_allow_html = True\\n\\nDisplay data\\n\\n```python\\nst.dataframe(my_dataframe)\\nst.table(data.iloc[0:10])\\nst.json({\\'foo\\':\\'bar\\',\\'fu\\':\\'ba\\'})\\nst.metric(\\'My metric\\', 42, 2)\\n\\n```\\n\\nDisplay media\\n\\npython\\nst.image(\\'./header.png\\')\\nst.audio(data)\\nst.video(data)\\n\\nAdd widgets to sidebar\\n\\n```python\\n\\nJust add it after st.sidebar:\\n\\na = st.sidebar.radio(\\'Select one:\\', [1, 2])\\n\\nOr use \"with\" notation:\\n\\nwith st.sidebar:\\n  st.radio(\\'Select one:\\', [1, 2])\\n```\\n\\nColumns\\n\\n```python\\n\\nTwo equal columns:\\n\\ncol1, col2 = st.columns(2)\\ncol1.write(\"This is column 1\")\\ncol2.write(\"This is column 2\")\\n\\nThree different columns:\\n\\ncol1, col2, col3 = st.columns([3, 1, 1])\\n\\ncol1 is larger.\\n\\nYou can also use \"with\" notation:\\n\\nwith col1:\\n  st.radio(\\'Select one:\\', [1, 2])\\n```\\n\\nTabs\\n\\n```python\\n\\nInsert containers separated into tabs:\\n\\ntab1, tab2 = st.tabs([\"Tab 1\", \"Tab2\"])\\ntab1.write(\"this is tab 1\")\\ntab2.write(\"this is tab 2\")\\n\\nYou can also use \"with\" notation:\\n\\nwith tab1:\\n  st.radio(\\'Select one:\\', [1, 2])\\n```\\n\\nControl flow\\n\\n```python', metadata={'source': 'docs/content/library/api-cheat-sheet.md'}),\n",
       " Document(page_content='Control flow\\n\\n```python\\n\\nStop execution immediately:\\n\\nst.stop()\\n\\nRerun script immediately:\\n\\nst.experimental_rerun()\\n\\nGroup multiple widgets:\\n\\nwith st.form(key=\\'my_form\\'):\\n  username = st.text_input(\\'Username\\')\\n  password = st.text_input(\\'Password\\')\\n  st.form_submit_button(\\'Login\\')\\n```\\n\\nDisplay interactive widgets\\n\\n```python\\nst.button(\\'Click me\\')\\nst.data_editor(\\'Edit data\\', data)\\nst.checkbox(\\'I agree\\')\\nst.radio(\\'Pick one\\', [\\'cats\\', \\'dogs\\'])\\nst.selectbox(\\'Pick one\\', [\\'cats\\', \\'dogs\\'])\\nst.multiselect(\\'Buy\\', [\\'milk\\', \\'apples\\', \\'potatoes\\'])\\nst.slider(\\'Pick a number\\', 0, 100)\\nst.select_slider(\\'Pick a size\\', [\\'S\\', \\'M\\', \\'L\\'])\\nst.text_input(\\'First name\\')\\nst.number_input(\\'Pick a number\\', 0, 10)\\nst.text_area(\\'Text to translate\\')\\nst.date_input(\\'Your birthday\\')\\nst.time_input(\\'Meeting time\\')\\nst.file_uploader(\\'Upload a CSV\\')\\nst.download_button(\\'Download file\\', data)\\nst.camera_input(\"Take a picture\")\\nst.color_picker(\\'Pick a color\\')\\n\\nUse widgets\\' returned values in variables:\\n\\nfor i in range(int(st.number_input(\\'Num:\\'))):\\n  foo()\\nif st.sidebar.selectbox(\\'I:\\',[\\'f\\']) == \\'f\\':\\n  b()\\nmy_slider_val = st.slider(\\'Quinn Mallory\\', 1, 88)\\nst.write(slider_val)\\n\\nDisable widgets to remove interactivity:\\n\\nst.slider(\\'Pick a number\\', 0, 100, disabled=True)\\n```\\n\\nBuild chat-based apps\\n\\n```python\\n\\nInsert a chat message container.\\n\\nwith st.chat_message(\"user\"):\\n   st.write(\"Hello 👋\")\\n   st.line_chart(np.random.randn(30, 3))\\n\\nDisplay a chat input widget.\\n\\nst.chat_input(\"Say something\")\\n```\\n\\nLearn how to build chat-based apps\\n\\nMutate data\\n\\n```python\\n\\nAdd rows to a dataframe after\\n\\nshowing it.\\n\\nelement = st.dataframe(df1)\\nelement.add_rows(df2)\\n\\nAdd rows to a chart after\\n\\nshowing it.\\n\\nelement = st.line_chart(df1)\\nelement.add_rows(df2)\\n```\\n\\nDisplay code\\n\\n```python\\n\\nwith st.echo():\\n  st.write(\\'Code will be executed and printed\\')\\n```\\n\\nPlaceholders, help, and options\\n\\n```python\\n\\nReplace any single element.\\n\\nelement = st.empty()\\nelement.line_chart(...)\\nelement.text_input(...)  # Replaces previous.', metadata={'source': 'docs/content/library/api-cheat-sheet.md'}),\n",
       " Document(page_content='Insert out of order.\\n\\nelements = st.container()\\nelements.line_chart(...)\\nst.write(\"Hello\")\\nelements.text_input(...)  # Appears above \"Hello\".\\n\\nst.help(pandas.DataFrame)\\nst.get_option(key)\\nst.set_option(key, value)\\nst.set_page_config(layout=\\'wide\\')\\nst.experimental_get_query_params()\\nst.experimental_set_query_params(**params)\\n```\\n\\nConnect to data sources\\n\\n```python\\nst.experimental_connection(\\'pets_db\\', type=\\'sql\\')\\nconn = st.experimental_connection(\\'sql\\')\\nconn = st.experimental_connection(\\'snowpark\\')\\n\\nclass MyConnection(ExperimentalBaseConnection[myconn.MyConnection]):\\n   def _connect(self, kwargs) -> MyConnection:\\n       return myconn.connect(self._secrets, **kwargs)\\n   def query(self, query):\\n      return self._instance.query(query)\\n```\\n\\nOptimize performance\\n\\nCache data objects\\n\\n```python\\n\\nE.g. Dataframe computation, storing downloaded data, etc.\\n\\n@st.cache_data\\n... def foo(bar):\\n...   # Do something expensive and return data\\n...   return data\\n\\nExecutes foo\\n\\nd1 = foo(ref1)\\n\\nDoes not execute foo\\n\\nReturns cached item by value, d1 == d2\\n\\nd2 = foo(ref1)\\n\\nDifferent arg, so function foo executes\\n\\nd3 = foo(ref2)\\n\\nClear all cached entries for this function\\n\\nfoo.clear()\\n\\nClear values from all in-memory or on-disk cached functions\\n\\nst.cache_data.clear()\\n```\\n\\nCache global resources\\n\\n```python\\n\\nE.g. TensorFlow session, database connection, etc.\\n\\n@st.cache_resource\\n... def foo(bar):\\n...   # Create and return a non-data object\\n...   return session\\n\\nExecutes foo\\n\\ns1 = foo(ref1)\\n\\nDoes not execute foo\\n\\nReturns cached item by reference, s1 == s2\\n\\ns2 = foo(ref1)\\n\\nDifferent arg, so function foo executes\\n\\ns3 = foo(ref2)\\n\\nClear all cached entries for this function\\n\\nfoo.clear()\\n\\nClear all global resources from cache\\n\\nst.cache_resource.clear()\\n```\\n\\nDeprecated caching\\n\\n```python\\n\\n@st.cache\\n... def foo(bar):\\n...   # Do something expensive in here...\\n...   return data\\n\\nExecutes foo\\n\\nd1 = foo(ref1)\\n\\nDoes not execute foo\\n\\nReturns cached item by reference, d1 == d2\\n\\nd2 = foo(ref1)', metadata={'source': 'docs/content/library/api-cheat-sheet.md'}),\n",
       " Document(page_content='d2 = foo(ref1)\\n\\nDifferent arg, so function foo executes\\n\\nd3 = foo(ref2)\\n```\\n\\nDisplay progress and status\\n\\n```python\\n\\nwith st.spinner(text=\\'In progress\\'):\\n  time.sleep(5)\\n  st.success(\\'Done\\')\\n\\nst.progress(progress_variable_1_to_100)\\nst.balloons()\\nst.snow()\\nst.error(\\'Error message\\')\\nst.warning(\\'Warning message\\')\\nst.info(\\'Info message\\')\\nst.success(\\'Success message\\')\\nst.exception(e)\\n```\\n\\nPersonalize apps for users\\n\\n```python\\n\\nShow different content based on the user\\'s email address.\\n\\nif st.user.email == \\'jane@email.com\\':\\n   display_jane_content()\\nelif st.user.email == \\'adam@foocorp.io\\':\\n   display_adam_content()\\nelse:\\n   st.write(\"Please contact us to get access!\")\\n```', metadata={'source': 'docs/content/library/api-cheat-sheet.md'}),\n",
       " Document(page_content='title: Static file serving\\nslug: /library/advanced-features/static-file-serving\\n\\nStatic file serving\\n\\nStreamlit apps can host and serve small, static media files to support media embedding use cases that\\nwon\\'t work with the normal media elements.\\n\\nTo enable this feature, set enableStaticServing = true under [server] in your config file,\\nor environment variable STREAMLIT_SERVER_ENABLE_STATIC_SERVING=true.\\n\\nMedia stored in the folder ./static/ relative to the running app file is served at path\\napp/static/[filename], such as http://localhost:8501/app/static/cat.png.\\n\\nDetails on usage\\n\\nFiles with the following extensions will be served normally: \".jpg\", \".jpeg\", \".png\", \".gif\". Any other\\n  file will be sent with header Content-Type:text/plain which will cause browsers to render in plain text.\\n  This is included for security - other file types that need to render should be hosted outside the app.\\n\\nStreamlit also sets X-Content-Type-Options:nosniff for all files rendered from the static directory.\\n\\nFor apps running on Streamlit Community Cloud:\\n\\nFiles available in the Github repo will always be served. Any files generated while the app is running,\\n    such as based on user interaction (file upload, etc), are not guaranteed to persist across user sessions.\\n\\nApps which store and serve many files, or large files, may run into resource limits and be shut down.\\n\\nExample usage\\n\\nPut an image cat.png in the folder ./static/\\n\\nAdd enableStaticServing = true under [server] in your .streamlit/config.toml\\n\\nAny media in the ./static/ folder is served at the relative URL like app/static/cat.png\\n\\n```toml\\n\\n.streamlit/config.toml\\n\\n[server]\\nenableStaticServing = true\\n```\\n\\n```python\\n\\napp.py\\n\\nimport streamlit as st\\n\\nwith st.echo():\\n    st.title(\"CAT\")\\n\\n```\\n\\nAdditional resources:\\n\\nhttps://docs.streamlit.io/library/advanced-features/configuration\\n\\nhttps://static-file-serving.streamlit.app/', metadata={'source': 'docs/content/library/advanced-features/static-file-serving.md'}),\n",
       " Document(page_content=\"title: Add statefulness to apps\\nslug: /library/advanced-features/session-state\\n\\nAdd statefulness to apps\\n\\nWhat is State?\\n\\nWe define access to a Streamlit app in a browser tab as a session. For each browser tab that connects to the Streamlit server, a new session is created. Streamlit reruns your script from top to bottom every time you interact with your app. Each reruns takes place in a blank slate: no variables are shared between runs.\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks. Session state also persists across apps inside a multipage app.\\n\\nIn this guide, we will illustrate the usage of Session State and Callbacks as we build a stateful Counter app.\\n\\nFor details on the Session State and Callbacks API, please refer to our Session State API Reference Guide.\\n\\nAlso, check out this Session State basics tutorial video by Streamlit Developer Advocate Dr. Marisa Smith to get started:\\n\\nBuild a Counter\\n\\nLet's call our script counter.py. It initializes a count variable and has a button to increment the value stored in the count variable:\\n\\n```python\\nimport streamlit as st\\n\\nst.title('Counter Example')\\ncount = 0\\n\\nincrement = st.button('Increment')\\nif increment:\\n    count += 1\\n\\nst.write('Count = ', count)\\n```\\n\\nNo matter how many times we press the Increment button in the above app, the count remains at 1. Let's understand why:\\n\\nEach time we press the Increment button, Streamlit reruns counter.py from top to bottom, and with every run, count gets initialized to 0 .\\n\\nPressing Increment subsequently adds 1 to 0, thus count=1 no matter how many times we press Increment.\\n\\nAs we'll see later, we can avoid this issue by storing count as a Session State variable. By doing so, we're indicating to Streamlit that it should maintain the value stored inside a Session State variable across app reruns.\", metadata={'source': 'docs/content/library/advanced-features/session-state.md'}),\n",
       " Document(page_content=\"Let's learn more about the API to use Session State.\\n\\nInitialization\\n\\nThe Session State API follows a field-based API, which is very similar to Python dictionaries:\\n\\n```python\\nimport streamlit as st\\n\\nCheck if 'key' already exists in session_state\\n\\nIf not, then initialize it\\n\\nif 'key' not in st.session_state:\\n    st.session_state['key'] = 'value'\\n\\nSession State also supports the attribute based syntax\\n\\nif 'key' not in st.session_state:\\n    st.session_state.key = 'value'\\n```\\n\\nReads and updates\\n\\nRead the value of an item in Session State by passing the item to st.write :\\n\\n```python\\nimport streamlit as st\\n\\nif 'key' not in st.session_state:\\n    st.session_state['key'] = 'value'\\n\\nReads\\n\\nst.write(st.session_state.key)\\n\\nOutputs: value\\n\\n```\\n\\nUpdate an item in Session State by assigning it a value:\\n\\n```python\\nimport streamlit as st\\n\\nif 'key' not in st.session_state:\\n    st.session_state['key'] = 'value'\\n\\nUpdates\\n\\nst.session_state.key = 'value2'     # Attribute API\\nst.session_state['key'] = 'value2'  # Dictionary like API\\n```\\n\\nStreamlit throws an exception if an uninitialized variable is accessed:\\n\\n```python\\nimport streamlit as st\\n\\nst.write(st.session_state['value'])\\n\\nThrows an exception!\\n\\n```\\n\\nLet's now take a look at a few examples that illustrate how to add Session State to our Counter app.\\n\\nExample 1: Add Session State\\n\\nNow that we've got a hang of the Session State API, let's update our Counter app to use Session State:\\n\\n```python\\nimport streamlit as st\\n\\nst.title('Counter Example')\\nif 'count' not in st.session_state:\\n    st.session_state.count = 0\\n\\nincrement = st.button('Increment')\\nif increment:\\n    st.session_state.count += 1\\n\\nst.write('Count = ', st.session_state.count)\\n```\\n\\nAs you can see in the above example, pressing the Increment button updates the count each time.\\n\\nExample 2: Session State and Callbacks\", metadata={'source': 'docs/content/library/advanced-features/session-state.md'}),\n",
       " Document(page_content=\"Example 2: Session State and Callbacks\\n\\nNow that we've built a basic Counter app using Session State, let's move on to something a little more complex. The next example uses Callbacks with Session State.\\n\\nSession State API Reference Guide.\\n\\n```python\\nimport streamlit as st\\n\\nst.title('Counter Example using Callbacks')\\nif 'count' not in st.session_state:\\n    st.session_state.count = 0\\n\\ndef increment_counter():\\n    st.session_state.count += 1\\n\\nst.button('Increment', on_click=increment_counter)\\n\\nst.write('Count = ', st.session_state.count)\\n```\\n\\nNow, pressing the Increment button updates the count each time by calling the increment_counter() function.\\n\\nExample 3: Use args and kwargs in Callbacks\\n\\nCallbacks also support passing arguments using the args parameter in a widget:\\n\\n```python\\nimport streamlit as st\\n\\nst.title('Counter Example using Callbacks with args')\\nif 'count' not in st.session_state:\\n    st.session_state.count = 0\\n\\nincrement_value = st.number_input('Enter a value', value=0, step=1)\\n\\ndef increment_counter(increment_value):\\n    st.session_state.count += increment_value\\n\\nincrement = st.button('Increment', on_click=increment_counter,\\n    args=(increment_value, ))\\n\\nst.write('Count = ', st.session_state.count)\\n```\\n\\nAdditionally, we can also use the kwargs parameter in a widget to pass named arguments to the callback function as shown below:\\n\\n```python\\nimport streamlit as st\\n\\nst.title('Counter Example using Callbacks with kwargs')\\nif 'count' not in st.session_state:\\n    st.session_state.count = 0\\n\\ndef increment_counter(increment_value=0):\\n    st.session_state.count += increment_value\\n\\ndef decrement_counter(decrement_value=0):\\n    st.session_state.count -= decrement_value\\n\\nst.button('Increment', on_click=increment_counter,\\n    kwargs=dict(increment_value=5))\\n\\nst.button('Decrement', on_click=decrement_counter,\\n    kwargs=dict(decrement_value=1))\\n\\nst.write('Count = ', st.session_state.count)\\n```\\n\\nExample 4: Forms and Callbacks\", metadata={'source': 'docs/content/library/advanced-features/session-state.md'}),\n",
       " Document(page_content='Example 4: Forms and Callbacks\\n\\nSay we now want to not only increment the count, but also store when it was last updated. We illustrate doing this using Callbacks and st.form:\\n\\n```python\\nimport streamlit as st\\nimport datetime\\n\\nst.title(\\'Counter Example\\')\\nif \\'count\\' not in st.session_state:\\n    st.session_state.count = 0\\n    st.session_state.last_updated = datetime.time(0,0)\\n\\ndef update_counter():\\n    st.session_state.count += st.session_state.increment_value\\n    st.session_state.last_updated = st.session_state.update_time\\n\\nwith st.form(key=\\'my_form\\'):\\n    st.time_input(label=\\'Enter the time\\', value=datetime.datetime.now().time(), key=\\'update_time\\')\\n    st.number_input(\\'Enter a value\\', value=0, step=1, key=\\'increment_value\\')\\n    submit = st.form_submit_button(label=\\'Update\\', on_click=update_counter)\\n\\nst.write(\\'Current Count = \\', st.session_state.count)\\nst.write(\\'Last Updated = \\', st.session_state.last_updated)\\n```\\n\\nAdvanced concepts\\n\\nSession State and Widget State association\\n\\nSession State provides the functionality to store variables across reruns. Widget state (i.e. the value of a widget) is also stored in a session.\\n\\nFor simplicity, we have unified this information in one place. i.e. the Session State. This convenience feature makes it super easy to read or write to the widget\\'s state anywhere in the app\\'s code. Session State variables mirror the widget value using the key argument.\\n\\nWe illustrate this with the following example. Let\\'s say we have an app with a slider to represent temperature in Celsius. We can set and get the value of the temperature widget by using the Session State API, as follows:\\n\\n```python\\nimport streamlit as st\\n\\nif \"celsius\" not in st.session_state:\\n    # set the initial default value of the slider widget\\n    st.session_state.celsius = 50.0\\n\\nst.slider(\\n    \"Temperature in Celsius\",\\n    min_value=-100.0,\\n    max_value=100.0,\\n    key=\"celsius\"\\n)\\n\\nThis will get the value of the slider widget\\n\\nst.write(st.session_state.celsius)\\n```', metadata={'source': 'docs/content/library/advanced-features/session-state.md'}),\n",
       " Document(page_content='st.write(st.session_state.celsius)\\n```\\n\\nThere is a limitation to setting widget values using the Session State API.\\n\\nStreamlit does not allow setting widget values via the Session State API for st.button and st.file_uploader.\\n\\nThe following example will raise a StreamlitAPIException on trying to set the state of st.button via the Session State API:\\n\\n```python\\nimport streamlit as st\\n\\nif \\'my_button\\' not in st.session_state:\\n    st.session_state.my_button = True\\n    # Streamlit will raise an Exception on trying to set the state of button\\n\\nst.button(\\'Submit\\', key=\\'my_button\\')\\n```\\n\\nSerializable Session State\\n\\nSerialization refers to the process of converting an object or data structure into a format that can be persisted and shared, and allowing you to recover the data’s original structure. Python’s built-in pickle module serializes Python objects to a byte stream (\"pickling\") and deserializes the stream into an object (\"unpickling\").\\n\\nBy default, Streamlit’s Session State allows you to persist any Python object for the duration of the session, irrespective of the object’s pickle-serializability. This property lets you store Python primitives such as integers, floating-point numbers, complex numbers and booleans, dataframes, and even lambdas returned by functions. However, some execution environments may require serializing all data in Session State, so it may be useful to detect incompatibility during development, or when the execution environment will stop supporting it in the future.\\n\\nTo that end, Streamlit provides a runner.enforceSerializableSessionState configuration option that, when set to true, only allows pickle-serializable objects in Session State. To enable the option, either create a global or project config file with the following or use it as a command-line flag:\\n\\n```toml\\n\\n.streamlit/config.toml\\n\\n[runner]\\nenforceSerializableSessionState = true\\n```', metadata={'source': 'docs/content/library/advanced-features/session-state.md'}),\n",
       " Document(page_content='By \"pickle-serializable\", we mean calling pickle.dumps(obj) should not raise a PicklingError exception. When the config option is enabled, adding unserializable data to session state should result in an exception. E.g.,\\n\\n```python\\nimport streamlit as st\\n\\ndef unserializable_data():\\n        return lambda x: x\\n\\n👇 results in an exception when enforceSerializableSessionState is on\\n\\nst.session_state.unserializable = unserializable_data()\\n```\\n\\nCaveats and limitations\\n\\nHere are some limitations to keep in mind when using Session State:\\n\\nSession State exists for as long as the tab is open and connected to the Streamlit server. As soon as you close the tab, everything stored in Session State is lost.\\n\\nSession State is not persisted. If the Streamlit server crashes, then everything stored in Session State gets wiped\\n\\nFor caveats and limitations with the Session State API, please see the API limitations.', metadata={'source': 'docs/content/library/advanced-features/session-state.md'}),\n",
       " Document(page_content='title: Working with timezones\\nslug: /library/advanced-features/timezone-handling\\n\\nWorking with timezones\\n\\nIn general, working with timezones can be tricky. Your Streamlit app users are not necessarily in the same timezone as the server running your app. It is especially true of public apps, where anyone in the world (in any timezone) can access your app. As such, it is crucial to understand how Streamlit handles timezones, so you can avoid unexpected behavior when displaying datetime information.\\n\\nHow Streamlit handles timezones\\n\\nStreamlit always shows datetime information on the frontend with the same information as its corresponding datetime instance in the backend. I.e., date or time information does not automatically adjust to the users\\' timezone. We distinguish between the following two cases:\\n\\ndatetime instance without a timezone (naive)\\n\\nWhen you provide a datetime instance without specifying a timezone, the frontend shows the datetime instance without timezone information. For example (this also applies to other widgets like st.dataframe):\\n\\n```python\\nimport streamlit as st\\nfrom datetime import datetime\\n\\nst.write(datetime(2020, 1, 10, 10, 30))\\n\\nOutputs: 2020-01-10 10:30:00\\n\\n```\\n\\nUsers of the above app always see the output as 2020-01-10 10:30:00.\\n\\ndatetime instance with a timezone\\n\\nWhen you provide a datetime instance and specify a timezone, the frontend shows the datetime instance in that same timezone. For example (this also applies to other widgets like st.dataframe):\\n\\n```python\\nimport streamlit as st\\nfrom datetime import datetime\\nimport pytz\\n\\nst.write(datetime(2020, 1, 10, 10, 30, tzinfo=pytz.timezone(\"EST\")))\\n\\nOutputs: 2020-01-10 10:30:00-05:00\\n\\n```\\n\\nUsers of the above app always see the output as 2020-01-10 10:30:00-05:00.', metadata={'source': 'docs/content/library/advanced-features/timezone-handling.md'}),\n",
       " Document(page_content='In both cases, neither the date nor time information automatically adjusts to the users\\' timezone on the frontend. What users see is identical to the corresponding datetime instance in the backend. It is currently not possible to automatically adjust the date or time information to the timezone of the users viewing the app.\\n\\nThe legacy version of the st.dataframe has issues with timezones. We do not plan to roll out additional fixes or enhancements for the legacy dataframe. If you need stable timezone support, please consider switching to the arrow serialization by changing the config setting, config.dataFrameSerialization = \"arrow\".', metadata={'source': 'docs/content/library/advanced-features/timezone-handling.md'}),\n",
       " Document(page_content='title: Secrets management\\nslug: /library/advanced-features/secrets-management\\n\\nSecrets management\\n\\nStoring unencrypted secrets in a git repository is a bad practice. For applications that require access to sensitive credentials, the recommended solution is to store those credentials outside the repository - such as using a credentials file not committed to the repository or passing them as environment variables.\\n\\nStreamlit provides native file-based secrets management to easily store and securely access your secrets in your Streamlit app.\\n\\nExisting secrets management tools, such as dotenv files, AWS credentials files, Google Cloud Secret Manager, or Hashicorp Vault, will work fine in Streamlit. We just add native secrets management for times when it\\'s useful.\\n\\nHow to use secrets management\\n\\nDevelop locally and set up secrets\\n\\nStreamlit provides two ways to set up secrets locally using\\xa0TOML\\xa0format:\\n\\nIn a global secrets file at ~/.streamlit/secrets.toml for macOS/Linux or %userprofile%/.streamlit/secrets.toml for Windows:\\n\\n```toml\\n   # Everything in this section will be available as an environment variable\\n   db_username = \"Jane\"\\n   db_password = \"mypassword\"\\n\\n# You can also add other sections if you like.\\n   # The contents of sections as shown below will not become environment variables,\\n   # but they\\'ll be easily accessible from within Streamlit anyway as we show\\n   # later in this doc.\\n   [my_other_secrets]\\n   things_i_like = [\"Streamlit\", \"Python\"]\\n   ```\\n\\nIf you use the global secrets file, you don\\'t have to duplicate secrets across several project-level files if multiple Streamlit apps share the same secrets.\\n\\nIn a per-project secrets file at $CWD/.streamlit/secrets.toml, where $CWD is the folder you\\'re running Streamlit from. If both a global secrets file and a per-project secrets file exist, secrets in the per-project file overwrite those defined in the global file.\\n\\nAdd this file to your .gitignore so you don\\'t commit your secrets!\\n\\nUse secrets in your app', metadata={'source': 'docs/content/library/advanced-features/secrets-management.md'}),\n",
       " Document(page_content='Use secrets in your app\\n\\nAccess your secrets by querying the\\xa0st.secrets\\xa0dict, or as environment variables. For example, if you enter the secrets from the section above, the code below shows you how to access them within your Streamlit app.\\n\\n```python\\nimport streamlit as st\\n\\nEverything is accessible via the st.secrets dict:\\n\\nst.write(\"DB username:\", st.secrets[\"db_username\"])\\nst.write(\"DB password:\", st.secrets[\"db_password\"])\\n\\nAnd the root-level secrets are also accessible as environment variables:\\n\\nimport os\\n\\nst.write(\\n    \"Has environment variables been set:\",\\n    os.environ[\"db_username\"] == st.secrets[\"db_username\"],\\n)\\n```\\n\\nYou can access st.secrets via attribute notation (e.g. st.secrets.key), in addition to key notation (e.g. st.secrets[\"key\"]) — like st.session_state.\\n\\nYou can even compactly use TOML sections to pass multiple secrets as a single attribute. Consider the following secrets:\\n\\ntoml\\n[db_credentials]\\nusername = \"my_username\"\\npassword = \"my_password\"\\n\\nRather than passing each secret as attributes in a function, you can more compactly pass the section to achieve the same result. See the notional code below, which uses the secrets above:\\n\\n```python\\n\\nVerbose version\\n\\nmy_db.connect(username=st.secrets.db_credentials.username, password=st.secrets.db_credentials.password)\\n\\nFar more compact version!\\n\\nmy_db.connect(**st.secrets.db_credentials)\\n```\\n\\nError handling\\n\\nHere are some common errors you might encounter when using secrets management.\\n\\nIf a .streamlit/secrets.toml is created while the app is running, the server needs to be restarted for changes to be reflected in the app.\\n\\nIf you try accessing a secret, but no secrets.toml file exists, Streamlit will raise a FileNotFoundError exception:\\n\\nIf you try accessing a secret that doesn\\'t exist, Streamlit will raise a KeyError exception:\\n\\n```python\\n  import streamlit as st\\n\\nst.write(st.secrets[\"nonexistent_key\"])\\n  ```\\n\\nUse secrets on Streamlit Community Cloud', metadata={'source': 'docs/content/library/advanced-features/secrets-management.md'}),\n",
       " Document(page_content=\"Use secrets on Streamlit Community Cloud\\n\\nWhen you deploy your app to Streamlit Community Cloud, you can use the same secrets management workflow as you would locally. However, you'll need to also set up your secrets in the Community Cloud Secrets Management console. Learn how to do so via the Cloud-specific Secrets management documentation.\", metadata={'source': 'docs/content/library/advanced-features/secrets-management.md'}),\n",
       " Document(page_content=\"title: Changelog\\nslug: /library/changelog\\n\\nChangelog\\n\\nThis page lists highlights, bug fixes, and known issues for official Streamlit releases. If you're looking for information about nightly releases, beta features, or experimental features, see Try pre-release features.\\n\\nTo upgrade to the latest version of Streamlit, run:\\n\\nbash\\npip install --upgrade streamlit\\n\\nVersion 1.24.0\\n\\nRelease date: June 27, 2023\\n\\nHighlights\\n\\n💬 Introducing st.chat_message and st.chat_input — two new chat elements that let you build conversational apps. Learn how to use these features in your LLM-powered chat apps in our tutorial.\\n\\n💾\\xa0Streamlit's caching decorators now allow you to customize Streamlit's hashing of input parameters with the keyword-only argument hash_funcs.\\n\\nNotable Changes\\n\\n🐍\\xa0We've deprecated support for Python 3.7 in the core library and Streamlit Community Cloud (#6868).\\n\\n📅\\xa0st.cache_data and st.cache_resource can hash timezone-aware datetime objects (#6812, #6690, #5110).\\n\\nOther Changes\\n\\n✨\\xa0Visual design tweaks to Streamlit's input widgets (#6817).\\n\\n🐛\\xa0Bug fix: st.write pretty-prints dataclasses using st.help (#6750).\\n\\n🪲\\xa0Bug fix: st.button's height is consistent with that of other widgets (#6738).\\n\\n🐜\\xa0Bug fix: Upgraded the react-range frontend dependency to fix the memory usage of sliders (#6764, #5436). Thanks @wolfd!\\n\\n🐝\\xa0Bug fix: Pydantic validators no longer result in exceptions on app reruns (#6664, #3218).\\n\\n🐞\\xa0Bug fix: streamlit config show honors newlines (#6758, #2868).\\n\\n🪰\\xa0Bug fix: Fixed a race condition to ensure Streamlit reruns the latest code when the file changes (#6884).\\n\\n🦋\\xa0Bug fix: Apps no longer rerun when users click anchor links (#6834, #6500).\\n\\n🕸️\\xa0Bug fix: Added robust out-of-bounds checks for min_value and max_value in st.number_input (#6847, #6797).\\n\\nVersion 1.23.0\\n\\nRelease date: June 1, 2023\\n\\nHighlights\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Release date: June 1, 2023\\n\\nHighlights\\n\\n✂️ Announcing the general availability of st.data_editor, a widget that allows you to edit DataFrames and many other data structures in a table-like UI. Breaking change: the data editor\\'s representation used in st.session_state was altered. Find out more about the new format in Access edited data.\\n\\n⚙️ Introducing the Column configuration API with a suite of methods to configure the display and editing behavior of st.dataframe and st.data_editor columns (e.g. their title, visibility, type, or format). Keep an eye out for a detailed blog post and in-depth documentation upcoming in the next two weeks.\\n\\n🔌 Learn to use st.experimental_connection to create and manage data connections in your apps with the new Connecting to data docs and video tutorial.\\n\\nNotable Changes\\n\\n📊\\xa0Streamlit now supports Protobuf 4 and Altair 5 (#6215, #6618, #5626, #6622).\\n\\n☎️ st.dataframe and st.data_editor can hide index columns with hide_index, specify the display order of columns with column_order, and disable editing for individual columns with the disabled parameter.\\n\\n⏱️ The ttl parameter in st.cache_data and st.cache_resource accepts formatted strings, so you can simply say ttl=\"30d\", ttl=\"1h30m\" and any other combination of w, d, h, m, s supported by Pandas\\'s Timedelta constructor (#6560).\\n\\n📂 st.file_uploader now interprets the type parameter more accurately. For example, \"jpg\" or \".jpg\" now accept both \"jpg\" and \"jpeg\" extensions. This functionality has also been extended to \"mpeg/mpg\", \"tiff/tif\", \"html/htm\", and \"mpeg4/mp4\".\\n\\n🤫\\xa0The new global.disableWidgetStateDuplicationWarning configuration option allows the silencing of warnings triggered by setting widget default values and keyed session state values concurrently (#3605, #6640). Thanks, @antonAce!\\n\\nOther Changes\\n\\n🏃\\u200d♀️Improved startup time by lazy loading some dependencies (#6531).\\n\\n👋 Removed st.beta_* and st.experimental_show due to deprecation and low-use (#6558)', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"🚀\\xa0Further improvements to st.dataframe and st.data_editor:\\n\\nImproved editing on mobile devices for the data editor (#6548).\\n\\nAll editable columns have an icon in their column header and support tooltips (#6550, #6561).\\n\\nEnable editing for columns containing datetime, date, or time values (#6025).\\n\\nNew input validation options for columns in the data editor, such as max_chars and validate for text columns, and min_value, max_value and step for number columns (#6563).\\n\\nImproved type parsing capabilities in the data editor (#6551).\\n\\nUnified missing values to None in returned data structures (#6544).\\n\\nA warning is shown in cells when integers exceed the maximum safe value of (2^53) -1 (#6311, #6549).\\n\\nPrevented editing the sessions state by showing a warning (#6634).\\n\\nFixed issues with list columns sometimes breaking the frontend (#6644).\\n\\nFixed a display issue with index columns using category dtype (#6680, #6598).\\n\\nFixed an issue that prevented a rerun when adding empty rows (#6598).\\n\\nUnified the behavior between st.data_editor and st.dataframe related to auto-hiding the index column(s) based on the input data (#6659, #6598)\\n\\n🛡️\\xa0Streamlit's Security Policy can be found in its GitHub repository (#6666).\\n\\n🤏 Documented the integer size limit for st.number_input and st.slider (#6724).\\n\\n🐍\\xa0The majority of Streamlit's Python dependencies have set a maximum allowable version, with the standard upper limit set to the next major version, but not inclusive of it (#6691).\\n\\n💅\\xa0UI design improvements to in-app modals (#6688).\\n\\n🐞\\xa0Bug fix: st.date_input's date selector is equally visible in dark mode (#6072, #6630).\\n\\n🐜\\xa0Bug fix: the sidebar navigation expansion indicator in multipage apps is restored (#6731).\\n\\n🐛\\xa0Bug fix: The docstring and exception message for st.set_page_config have been updated to clarify that this command can be invoked once for each page within a multipage app, rather than once per entire app (#6594).\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='🐝\\xa0Bug fix: st.json\\xa0no longer collapses multiple spaces in both keys and values with single space when rendered (#6657, #6663).\\n\\nVersion 1.22.0\\n\\nRelease date: April 27, 2023\\n\\nHighlights\\n\\n🔌\\xa0Introducing st.experimental_connection: Easily connect your app to data sources and APIs using our new connection feature. Find more details in the API reference, and stay tuned for an upcoming blog post and in-depth documentation! In the meantime, explore our updated MySQL and Snowflake connection tutorials for examples of this feature.\\n\\nNotable Changes\\n\\n🐼\\xa0Streamlit now supports Pandas 2.0 (#6413, #6378, #6507). Thanks, connortann!\\n\\n🍔\\xa0Customize the visibility of items in the toolbar, options menu, and the settings dialog using the client.toolbarMode config option (#6174).\\n\\n🪵\\xa0Streamlit logs now reside in the \"streamlit\" namespace instead of the root logger, enabling app developers to better manage log handling (#3978, #6377).\\n\\nOther Changes\\n\\n🔏\\xa0CLI parameters can no longer be used to set sensitive configuration values (#6376).\\n\\n🤖\\xa0Improved the debugging experience by reducing log noise (#6391).\\n\\n🐞\\xa0Bug fix:\\xa0@st.cache_data decorated functions support UUID objects as parameters (#6440, #6459).\\n\\n🐛\\xa0Bug fix: Tabbing through buttons and other elements now displays a red border only when focused, not when clicked (#6373).\\n\\n🪲\\xa0Bug fix: st.multiselect\\'s clear icon is larger and includes a hover effect (#6471).\\n\\n🐜\\xa0Bug fix: Custom theme font settings no longer apply to code blocks (#6484, #6535).\\n\\n©️\\xa0Bug fix: st.code\\'s copy-to-clipboard button appears when you hover on code blocks (#6490, #6498).\\n\\nVersion 1.21.0\\n\\nRelease date: April 6, 2023\\n\\nHighlights\\n\\n📏 Introducing st.divider — a command that displays a horizontal line in your app. Learn how to use this command in its API reference.\\n\\n🔏 Streamlit now supports the use of a global secrets.toml file, in addition to a project-level file, to easily store and securely access your secrets. Learn more in Secrets management.', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"🚀 st.help has been revamped to show more information about object methods, attributes, classes, and more, which is great for debugging (#5857, #6382)!\\n\\nNotable Changes\\n\\n🪜 st.time_input supports adding a stepping interval with the keyword-only step parameter (#6071).\\n\\n❓ Most text elements can include tooltips with the help parameter (#6043).\\n\\n↔️ st.pyplot has a use_container_width parameter to set the chart to the container width (now all chart elements support this parameter) (#6067).\\n\\n👩\\u200d💻 st.code supports optionally displaying line numbers to the code block's left with the boolean line_numbers parameter (#5756, #6042).\\n\\n⚓ Anchors in header elements can be turned off by setting anchor=False (#6158).\\n\\nOther Changes\\n\\n🐼\\xa0st.table and st.dataframe support pandas.Period, and number and boolean types in categorical columns (#2547, #5429, #5329, #6248).\\n\\n🕸️\\xa0Added .webp to the list of allowed static file extensions (#6331)\\n\\n🐞\\xa0Bug fix: stop script execution on websocket close to immediately clear session information (#6166, #6204).\\n\\n🐜\\xa0Bug fixes: updated allowed/disallowed label markdown behavior such that unsupported elements are unwrapped and only their children (text contents) render (#5872, #6036, #6054, #6163).\\n\\n🪲\\xa0Bug fixes: don't push browser history states on rerun, use HTTPS to load external resources in streamlit hello, and make the browser back button work for multipage apps (#5292, #6266, #6232). Thanks, whitphx!\\n\\n🐝\\xa0Bug fix: avoid showing emoji on non-UTF-8 terminals. (#2284, #6088). Thanks, kcarnold!\\n\\n📁\\xa0Bug fix: override default use of\\xa0File System Access API for\\xa0react-dropzone so that st.file_uploader's File Selection Dialog only shows file types corresponding to those included in the type parameter (#6176, #6315).\\n\\n💾\\xa0Bug fix: make the .clear() method on cache-decorated functions work (#6310, #6321).\\n\\n🏃\\xa0Bug fix: st.experimental_get_query_params doesn't need reruns to work (#6347, #6348). Thanks, PaleNeutron!\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='🐛\\xa0Bug fix: CachedStFunctionWarning mentions experimental_allow_widgets instead of the deprecated suppress_st_warning (#6216, #6217).\\n\\nVersion 1.20.0\\n\\nRelease date: March 09, 2023\\n\\nNotable Changes\\n\\n🔐\\xa0Added support for configuring SSL to\\xa0serve apps directly over HTTPS\\xa0(#5969).\\n\\n🖼️\\xa0Granular control over app embedding behavior with the /?embed and /?embed_options query parameters. Learn how to use this feature in our docs (#6011, #6019).\\n\\n⚡\\xa0Enabled the runner.fastReruns configuration option by default to make apps much more responsive to user interaction (#6200).\\n\\nOther Changes\\n\\n🍔\\xa0Cleaned up the hamburger menu by removing the least used options (#6080).\\n\\n🖨️\\xa0Design changes to ensure apps being printed or saved as a PDF look good (#6180).\\n\\n🐞\\xa0Bug fix: improved dtypes checking in st.experimental_data_editor (#6185, #6188).\\n\\n🐛\\xa0Bug fix: properly position st.metric\\'s help tooltip when not inside columns (#6168).\\n\\n🪲\\xa0Bug fix: regression in retrieving messages from the server\\'s ForwardMsgCache (#6210).\\n\\n🌀\\xa0Bug fix: st.cache_data docstring for the show_spinner param now lists str as a supported type (#6207, #6213).\\n\\n⏱️\\xa0Made ping and websocket timeouts far more forgiving (#6212).\\n\\n🗺️\\xa0st.map and st.pydeck_chart docs state that Streamlit\\'s Mapbox token will not work indefinitely (#6143).\\n\\nVersion 1.19.0\\n\\nRelease date: February 23, 2023\\n\\nHighlights\\n\\n✂️\\xa0Introducing st.experimental_data_editor, a widget that allows you to edit DataFrames and many other data structures in a table-like UI. Read more in our documentation and blog post.\\n\\nOther Changes\\n\\n✨ Streamlit\\'s GitHub README got a new look (#6016).\\n\\n🌚\\xa0Improved readability of styled dataframe cells in dark mode (#6060, #6098).\\n\\n🐛\\xa0Bug fix: make apps work again in the latest versions of Safari, and in Chrome with third-party cookies blocked (#6092, #6094, #6087, #6100).\\n\\n🐞\\xa0Bug fix: refer to new cache primitives in the “Clear cache\" dialog and error messages (#6082, #6128).', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='🐝\\xa0Bug fix: properly cache class member functions and instance methods (#6109, #6114).\\n\\n🐜\\xa0Bug fix: regression in st.metric tooltip position (#6093, #6129).\\n\\n🪲\\xa0Bug fix: allow fullscreen button to show for dataframes, charts, etc, in expander (#6083, #6148).\\n\\nVersion 1.18.0\\n\\nRelease date: February 09, 2023\\n\\nHighlights\\n\\n🎊\\xa0Introducing\\xa0@st.cache_data\\xa0and\\xa0@st.cache_resource\\xa0— two new caching commands to replace\\xa0st.cache! Check out our\\xa0blog post\\xa0and\\xa0documentation\\xa0for more information.\\n\\nNotable Changes\\n\\n🪆\\xa0st.columns supports up to one level of column nesting (i.e., columns inside columns) in the main area of the app.\\n\\n⏳\\xa0st.progress supports adding a message to display above the progress bar with the text keyword parameter.\\n\\n↔️ st.button has an optional\\xa0use_container_width\\xa0parameter to allow you to stretch buttons across the full container width.\\n\\n🐍 We formally added support for Python 3.11.\\n\\n🖨️\\xa0Save your app as a PDF via the “Print\" option in your app\\'s hamburger menu.\\n\\n🛎️\\xa0Apps can serve small, static media files via the enableStaticServing config option. See our documentation on how to use this feature and our demo app for an example.\\n\\nOther Changes\\n\\n🏁\\xa0All Streamlit endpoints (including /healthz) have been renamed to have a consistent pattern and avoid any clashes with reserved endpoints of GCP (notably Cloud Run and App Engine) (#5534).\\n\\n⚡\\xa0Improved caching performance when multiple sessions access an uncomputed cached value simultaneously (#6017).\\n\\n🚧\\xa0Streamlit only displays deprecation warnings in the browser when the client.showErrorDetails config option is set to True. Deprecation warnings always get logged to the console, regardless of whether they\\'re displayed in-browser (#5945).\\n\\n🏓\\xa0Refactored the st.dataframe internals to improve dataframe handling and conversion, such as detecting more types, converting key-value dicts to dataframes, and more (#6026, #6023).\\n\\n💽 The behavior of widget labels when they are passed unsupported Markdown elements is documented (#5978).', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"📊\\xa0Bug fix: Plotly improvements — upgraded multiple frontend dependencies, including Plotly, to the latest version to properly redraw cached charts, make Plotly mapbox animations work, and allow users to update the figure layout when using the Streamlit theme (#5885, #5967, #6055).\\n\\n📶\\xa0Bug fix: allow browser tabs that transiently disconnect (due to a network blip, load balancer timeout, etc.) to avoid losing all of their state (#5856).\\n\\n📱 Bug fix: the keyboard is hidden on mobile when st.selectbox and st.multiselect have less than 10 options (#5979).\\n\\n🐝\\xa0Bug fix: design tweaks to st.metric, st.multiselect, st.tabs , and menu items to prevent label overflow and scrolling issues, especially with small viewport sizes (#5933, #6034).\\n\\n🐞\\xa0Bug fix: switched to a functioning Twemoji URL from which page favicons are loaded in st.set_page_config (#5943).\\n\\n✍️ More type hints (#5986). Thanks, harahu!\\n\\nVersion 1.17.0\\n\\nRelease date: January 12, 2023\\n\\nNotable Changes\\n\\n🪄\\xa0@st.experimental_singleton supports an optional validate parameter that accepts a validation function for cached data and is called each time the cached value is accessed.\\n\\n💾\\xa0 @st.experimental_memo's persist parameter can also accept booleans.\\n\\nOther Changes\\n\\n📟\\xa0Multipage apps exclude __init__.py from the page selector (#5890).\\n\\n📐\\xa0The iframes of embedded apps have the ability to dynamically resize their height (#5894).\\n\\n🐞\\xa0Bug fix: thumb values of range sliders respect the container width (#5913).\\n\\n🪲\\xa0Bug fix: all examples in docstrings of Streamlit commands contain relevant imports to make them reproducible (#5877).\\n\\nVersion 1.16.0\\n\\nRelease date: December 14, 2022\\n\\nHighlights\\n\\n👩\\u200d🎨\\xa0Introducing a new Streamlit theme for Altair, Plotly, and Vega-Lite charts! Check out our blog post for more information.\\n\\n🎨\\xa0Streamlit now supports colored text in all commands that accept Markdown, including st.markdown, st.header, and more. Learn more in our documentation.\\n\\nNotable Changes\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Notable Changes\\n\\n🔁\\xa0Functions cached with st.experimental_memo or st.experimental_singleton can contain Streamlit media elements and forms.\\n\\n⛄\\xa0All Streamlit commands that accept pandas DataFrames as input also support Snowpark and PySpark DataFrames.\\n\\n🏷\\xa0st.checkbox and st.metric can customize how to hide their labels with the label_visibility parameter.\\n\\nOther Changes\\n\\n🗺️\\xa0st.map improvements: support for upper case columns and better exception messages (#5679, #5792).\\n\\n🐞\\xa0Bug fix: st.plotly_chart respects the figure\\'s height attribute and the use_container_width parameter (#5779).\\n\\n🪲\\xa0Bug fix: all commands with the icon parameter such as st.error, st.warning, etc, can contain emojis with variant selectors (#5583).\\n\\n🐝\\xa0Bug fix: prevent st.camera_input from jittering when resizing the browser window (#5661).\\n\\n🐜\\xa0Bug fix: update exception layout to avoid overflow of stack traces (#5700).\\n\\nVersion 1.15.0\\n\\nRelease date: November 17, 2022\\n\\nNotable Changes\\n\\n💅\\xa0Widget labels can contain inline Markdown. See our docs and demo app for more info.\\n\\n🎵 st.audio now supports playing audio data passed in as NumPy arrays with the keyword-only sample_rate parameter.\\n\\n🔁\\xa0Functions cached with st.experimental_memo or st.experimental_singleton can contain Streamlit widgets using the experimental_allow_widgets parameter. This allows caching checkboxes, sliders, radio buttons, and more!\\n\\nOther Changes\\n\\n👩\\u200d🎨\\xa0Design tweak to prevent jittering in sliders (#5612).\\n\\n🐛\\xa0Bug fix: links in headers are red, not blue (#5609).\\n\\n🐞\\xa0Bug fix: properly resize Plotly charts when exiting fullscreen (#5645).\\n\\n🐝: Bug fix: don\\'t accidentally trigger st.balloons and st.snow (#5401).\\n\\nVersion 1.14.0\\n\\nRelease date: October 27, 2022\\n\\nHighlights\\n\\n🎨\\xa0st.button and st.form_submit_button support designating buttons as \"primary\" (for additional emphasis) or \"secondary\" (for normal buttons) with the type keyword-only parameter.\\n\\nNotable Changes', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Notable Changes\\n\\n🤏\\xa0st.multiselect has a keyword-only max_selections parameter to limit the number of options that can be selected at a time.\\n\\n📄\\xa0st.form_submit_button now has the disabled parameter that removes interactivity.\\n\\nOther Changes\\n\\n🏓\\xa0st.dataframe and st.table accept categorical intervals as input (#5395).\\n\\n⚡\\xa0Performance improvements to Plotly charts (#5542).\\n\\n🪲\\xa0Bug fix: st.download_button supports non-latin1 characters in filenames (#5465).\\n\\n🐞\\xa0Bug fix: Allow st.image to render a local GIF as a GIF, not as a static PNG (#5438).\\n\\n📱\\xa0Design tweaks to the sidebar in multipage apps (#5538, #5445, #5559).\\n\\n📊\\xa0Improvements to the axis configuration for built-in charts (#5412).\\n\\n🔧\\xa0Memo and singleton improvements: support text values for show_spinner, use datetime.timedelta objects as ttl parameter value, properly hash PIL images and Enum classes, show better error messages when returning unevaluated dataframes (#5447, #5413, #5504, #5426, #5515).\\n\\n🔍\\xa0Zoom buttons in maps created with st.map and st.pydeck_chart use light or dark style based on the app\\'s theme (#5479).\\n\\n🗜\\xa0Websocket headers from the current session\\'s incoming WebSocket request can be obtained from a new \"internal\" (i.e.: subject to change without deprecation) API (#5457).\\n\\n📝\\xa0Improve the text that gets printed when you first install and use Streamlit (#5473).\\n\\nVersion 1.13.0\\n\\nRelease date: September 22, 2022\\n\\nNotable Changes\\n\\n🏷\\xa0Widgets can customize how to hide their labels with the label_visibility parameter.\\n\\n🔍 st.map adds zoom buttons to the map by default.\\n\\n↔️\\xa0st.dataframe\\xa0supports the\\xa0use_container_width\\xa0parameter to stretch across the full container width.\\n\\n🪄 Improvements to\\xa0st.dataframe\\xa0sizing: Column width calculation respects column headers, supports double click between column headers to autosize, better fullscreen support, and fixes the issue with the\\xa0width\\xa0parameter.\\n\\nOther Changes\\n\\n⌨️ st.time_input allows for keyboard-only input (#5194).', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"💿 st.memo will warn the user when using\\xa0ttl\\xa0and\\xa0persist\\xa0keyword argument together (#5032).\\n\\n🔢\\xa0st.number_input returns consistent type after rerun (#5359).\\n\\n🚒\\xa0st.sidebar UI fixes including a fix for scrollbars in Firefox browsers (#5157, #5324).\\n\\n👩\\u200d💻\\xa0Improvements to usage metrics to guide API development.\\n\\n✍️\\xa0More type hints! (#5191, #5192, #5242, #5243, #5244, #5245, #5246) Thanks harahu!\\n\\nVersion 1.12.0\\n\\nRelease date: August 11, 2022\\n\\nHighlights\\n\\n📊\\xa0Built-in charts (e.g. st.line_chart) get a brand-new look and parameters x and y! Check out our blog post for more information.\\n\\nNotable Changes\\n\\n⏯\\xa0Functions cached with st.experimental_memo or st.experimental_singleton can now contain static st commands. This allows caching text, charts, dataframes, and more!\\n\\n↔️\\xa0The sidebar is now resizable via drag and drop.\\n\\n☎️\\xa0st.info, st.success, st.error, and st.warning got a redesign and have a new keyword-only parameter: icon.\\n\\nOther Changes\\n\\n🎚️\\xa0st.select_slider correctly handles all floats now (#4973, #4978).\\n\\n🔢\\xa0st.multi_select can take values from enums (#4987).\\n\\n🍊\\xa0st.slider range values can now be set through st.session_state (#5007).\\n\\n🎨\\xa0st.progress got a redesign (#5011, #5086).\\n\\n🔘\\xa0st.radio better deals with list-like dataframes (#5021).\\n\\n🧞\\u200d♂️\\xa0st.cache properly handles JSON files now (#5023).\\n\\n⚓️ Headers render markdown now when the anchor parameter is set (#5038).\\n\\n🗻\\xa0st.image can now load SVGs from Inkscape (#5040).\\n\\n🗺️\\xa0st.map and st.pydeck_chart use light or dark style based on the app's theme (#5074, #5108).\\n\\n🎈\\xa0Clicks on elements below\\xa0st.balloons and st.snow don't get blocked anymore (#5098).\\n\\n🔝\\xa0Embedded apps have lower top padding (#5111).\\n\\n💅\\xa0Adjusted padding and alignment for widgets, charts, and dataframes (#4995, #5061, #5081).\\n\\n✍️\\xa0More type hints! (#4926, #4932, #4933)\\n\\nVersion 1.11.0\\n\\nRelease date: July 14, 2022\\n\\nHighlights\\n\\n🗂\\xa0Introducing st.tabs to have tab containers in your app. See our documentation on how to use this feature.\\n\\nNotable Changes\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Notable Changes\\n\\nℹ️\\xa0st.metric supports tooltips with the help keyword parameter.\\n\\n🚇\\xa0st.columns supports setting the gap size between columns with the gap keyword parameter.\\n\\nOther Changes\\n\\n💅\\xa0Design tweaks to st.selectbox, st.expander, st.spinner (#4801).\\n\\n📱\\xa0The sidebar will close when users select a page from the navigation menu on mobile devices (#4851).\\n\\n🧠\\xa0st.memo supports dataclasses! (#4850)\\n\\n🏎\\xa0Bug fix for a race condition that destroyed widget state with rapid interaction (#4882).\\n\\n🏓\\xa0st.table presents overflowing content to be scrollable when placed inside columns and expanders (#4934).\\n\\n🐍\\xa0Types: More updated type annotations across Streamlit! (#4808, #4809, #4856)\\n\\nVersion 1.10.0\\n\\nRelease date: June 2, 2022\\n\\nHighlights\\n\\n📖 Introducing native support for multipage apps! Check out our blog post and try out our new streamlit hello.\\n\\nNotable Changes\\n\\n✨ st.dataframe has been redesigned.\\n\\n🔘 st.radio has a horizontal keyword-only parameter to display options horizontally.\\n\\n⚠️ Streamlit Community Cloud will support richer exception formatting.\\n\\n🏂 Get user information on private apps using st.experimental_user.\\n\\nOther Changes\\n\\n📊 Upgraded Vega-Lite library to support even more interactive charting improvements. See their release notes to find out more. (#4751).\\n\\n📈 st.vega_lite_chart will respond to updates, particularly in response to input widgets (#4736).\\n\\n💬 st.markdown with long text will always wrap (#4696).\\n\\n📦 Support for PDM (#4724).\\n\\n✍️ Types: Updated type annotations across Streamlit! (#4679, #4680, #4681, #4682, #4683, #4684, #4685, #4686, #4687, #4688, #4690, #4703, #4704, #4705, #4706, #4707, #4708, #4710, #4723, #4733).\\n\\nVersion 1.9.0\\n\\nRelease date: May 4, 2022\\n\\nNotable Changes\\n\\n🪗 st.json now supports a keyword-only argument, expanded on whether the JSON should be expanded by default (defaults to True).\\n\\n🏃\\u200d♀️ More performance improvements from reducing redundant work each script run.\\n\\nOther Changes', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Other Changes\\n\\n🏇 Widgets when disabled is set/unset will maintain its value (#4527).\\n\\n🧪 Experimental feature to increase the speed of reruns using configuration runner.fastReruns. See #4628 for the known issues in enabling this feature.\\n\\n🗺️ DataFrame timestamps support UTC offset (in addition to time zone notation) (#4669).\\n\\nVersion 1.8.0\\n\\nRelease date: March 24, 2022\\n\\nNotable Changes\\n\\n🏃\\u200d♀️\\xa0Dataframes should see performance improvements (#4463).\\n\\nOther Changes\\n\\n🕰\\xa0st.slider handles timezones better by removing timezone conversions on the backend (#4348).\\n\\n👩\\u200d🎨\\xa0Design improvements to our header (#4496).\\n\\nVersion 1.7.0\\n\\nRelease date: March 3, 2022\\n\\nHighlights\\n\\nIntroducing st.snow, celebrating our acquisition by Snowflake! See more information in our blog post.\\n\\nVersion 1.6.0\\n\\nRelease date: Feb 24, 2022\\n\\nOther Changes\\n\\n🗜\\xa0WebSocket compression is now disabled by default, which will improve CPU and latency performance for large dataframes. You can use the\\xa0server.enableWebsocketCompression configuration option to re-enable it if you find the increased network traffic more impactful.\\n\\n☑️\\xa0🔘\\xa0Radio and checkboxes improve focus on Keyboard navigation (#4308).\\n\\nVersion 1.5.0\\n\\nRelease date: Jan 27, 2022\\n\\nNotable Changes\\n\\n🌟 Favicon defaults to a PNG to allow for transparency (#4272).\\n\\n🚦 Select Slider Widget now has the disabled parameter that removes interactivity (completing all of our widgets) (#4314).\\n\\nOther Changes\\n\\n🔤 Improvements to our markdown library to provide better support for HTML (specifically nested HTML) (#4221).\\n\\n📖 Expanders maintain their expanded state better when multiple expanders are present (#4290).\\n\\n🗳 Improved file uploader and camera input to call its on_change handler only when necessary (#4270).\\n\\nVersion 1.4.0\\n\\nRelease date: Jan 13, 2022\\n\\nHighlights\\n\\n📸 Introducing st.camera_input for uploading images straight from your camera.\\n\\nNotable Changes\\n\\n🚦 Widgets now have the disabled parameter that removes interactivity.', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"🚮 Clear st.experimental_memo and st.experimental_singleton programmatically by using the clear() method on a cached function.\\n\\n📨 Developers can now configure the maximum size of a message to accommodate larger messages within the Streamlit application. See server.maxMessageSize.\\n\\n🐍 We formally added support for Python 3.10.\\n\\nOther Changes\\n\\n😵\\u200d💫 Calling str or repr on threading.current_thread() does not cause a RecursionError (#4172).\\n\\n📹 Gracefully stop screencast recording when user removes permission to record (#4180).\\n\\n🌇 Better scale images by using a higher-quality image bilinear resampling algorithm (#4159).\\n\\nVersion 1.3.0\\n\\nRelease date: Dec 16, 2021\\n\\nNotable Changes\\n\\n💯 Support for NumPy values in st.metric.\\n\\n🌐 Support for Mesh Layers in PyDeck.\\n\\n📊 Updated Plotly chart version to support the latest features.\\n\\n🏀 st.spinner element has visual animated spinner.\\n\\n🍰 st.caption supports HTML in text with unsafe_allow_html parameter.\\n\\nOther Changes\\n\\n🪲 Bug fix: Allow st.session_state to be used to set number_input values with no warning (#4047).\\n\\n🪲 Bug fix: Fix footer alignment in wide mode (#4035).\\n\\n🐞 Bug fix: Better support for Graphviz and Bokeh charts in containers (columns, expanders, etc.) (#4039).\\n\\n🐞 Bug fix: Support inline data values in Vega-Lite (#4070).\\n\\n✍️ Types: Updated type annotations for experimental memo and singleton decorators.\\n\\n✍️ Types: Improved type annotations for st.selectbox, st.select_slider, st.radio, st.number_input, and st.multiselect.\\n\\nVersion 1.2.0\\n\\nRelease date: Nov 11, 2021\\n\\nNotable Changes\\n\\n✏️\\xa0st.text_input\\xa0and st.text_area now have a\\xa0placeholder\\xa0parameter to display text when the field is empty.\\n\\n📏 Viewers can now resize the input box in st.text_area.\\n\\n📁 Streamlit can auto-reload when files in sub-directories change.\\n\\n🌈 We've upgraded Bokeh support to 2.4.1! We recommend updating your Bokeh library to 2.4.1 to maintain functionality. Going forward, we'll let you know if there's a mismatch in your Bokeh version via an error prompt.\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='🔒 Developers can access secrets via attribute notation (e.g. st.secrets.key vs st.secrets[\"key\"]) just like session state.\\n\\n✍️ Publish type annotations according to PEP 561. Users now get type annotations for Streamlit when running mypy (#4025).\\n\\nOther Changes\\n\\n👀 Visual fixes (#3863, #3995, #3926, #3975).\\n\\n🍔 Fixes to the hamburger menu (#3968).\\n\\n🖨️ Ability to print session state (#3970).\\n\\nVersion 1.1.0\\n\\nRelease date: Oct 21, 2021\\n\\nHighlights\\n\\n🧠 Memory improvements: Streamlit apps allocate way less memory over time now.\\n\\nNotable Changes\\n\\n♻️ Apps automatically rerun now when the content of secrets.toml changes (before this you had to refresh the page manually).\\n\\nOther Changes\\n\\n🔗 Redirected some links to our brand-new docs site, e.g. in exceptions.\\n\\n🪲 Bug fix: Allow initialization of range slider with session state (#3586).\\n\\n🐞 Bug fix: Refresh chart when using add_rows with datetime index (#3653).\\n\\n✍️ Added some more type annotation in our codebase (#3908).\\n\\nVersion 1.0.0\\n\\nRelease date: Oct 5, 2021\\n\\nHighlights\\n\\n🎈Announcing Streamlit 1.0! To read more about check out our 1.0 blog post.\\n\\nOther Changes\\n\\n🐞 Fixed an issue where using df.dtypes to show datatypes for a DF fails while using Arrow (#3709), Image captions stay within image width and are readable (#3530).\\n\\nVersion 0.89.0\\n\\nRelease date: Sep 22, 2021\\n\\nHighlights\\n\\n💰 Introducing st.experimental_memo and experimental_singleton, a new primitive for caching! See our blog post.\\n\\n🍔 Streamlit allows developers to configure their hamburger menu to be more user-centric.\\n\\nNotable Changes\\n\\n💅 We updated our UI to a more polished look with a new font.\\n\\n🎨 We now support theme.base in the theme object when it\\'s sent to custom components.\\n\\n🧠 We\\'ve modified session state to reset widgets if any of their arguments changed even if they provide a key.\\n\\nSome widget behavior may have changed, but we believe this change makes the most sense. We have added a section to our documentation describing how they behave.\\n\\nOther Changes', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Other Changes\\n\\n🐞 Bug fixes: Support svgs from a URL (#3809) and that do not start with <svg> tag (#3789).\\n\\nVersion 0.88.0\\n\\nRelease date: Sep 2, 2021\\n\\nHighlights\\n\\n⬇️ Introducing st.download_button, a new button widget for easily downloading files.\\n\\nNotable Changes\\n\\n🛑 We made changes to improve the redacted exception experience on Streamlit Community Cloud. When client.showErrorDetails=true exceptions display the Error Type and the Traceback, but redact the actual error text to prevent data leaks.\\n\\nVersion 0.87.0\\n\\nRelease date: Aug 19, 2021\\n\\nHighlights\\n\\n🔢 Introducing st.metric, an API for displaying KPIs. Check out the demo app showcasing the functionality.\\n\\nOther Changes\\n\\n🐞 Bug Fixes: File uploader retains state upon expander closing (#3557), setIn Error with st.empty (#3659), Missing IFrame embeds in docs (#3706), Fix error writing certain PNG files (#3597).\\n\\nVersion 0.86.0\\n\\nRelease date: Aug 5, 2021\\n\\nHighlights\\n\\n🎓 Our layout primitives are graduating from beta! You can now use st.columns, st.container and st.expander without the beta_ prefix.\\n\\nNotable Changes\\n\\n📱 When using st.columns, columns will stack vertically when viewport size <640px so that column layout on smaller viewports is consistent and cleaner. (#3594).\\n\\nOther Changes\\n\\n🐞 Bug fixes: Fixed st.date_input crashes if its empty (#3194), Opening files with utf-8(#3022), st.select_slider resets its state upon interaction (#3600).\\n\\nVersion 0.85.0\\n\\nRelease date: Jul 22, 2021\\n\\nHighlights\\n\\n🏹 Streamlit now uses Apache Arrow for serializing data frames when they are sent from Streamlit server to the front end. See our blog post.\\n\\n(Users who wish to continue using the legacy data frame serialization can do so by setting the dataFrameSerialization config option to \"legacy\" in their config.toml).\\n\\nOther Changes\\n\\n🐞 Bug fixes: Unresponsive pydeck example (#3395), JSON parse error message (#2324), Tooltips rendering (#3300), Colorpicker not working on Streamlit Sharing (#2689).\\n\\nVersion 0.84.0', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Version 0.84.0\\n\\nRelease date: Jul 1, 2021\\n\\nHighlights\\n\\n🧠 Introducing st.session_state and widget callbacks to allow you to add statefulness to your apps. Check out the blog post\\n\\nNotable Changes\\n\\n🪄 st.text_input now has an autocomplete parameter to allow password managers to be used\\n\\nOther Changes\\n\\nUsing st.set_page_config to assign the page title no longer appends “Streamlit\" to that title (#3467)\\n\\nNumberInput: disable plus/minus buttons when the widget is already at its max (or min) value (#3493)\\n\\nVersion 0.83.0\\n\\nRelease date: Jun 17, 2021\\n\\nHighlights\\n\\n🛣️ Updates to Streamlit docs to include step-by-step guides which demonstrate how to connect Streamlit apps to various databases & APIs\\n\\nNotable Changes\\n\\n📄 st.form now has a clear_on_submit parameter which \"resets\" all the form\\'s widgets when the form is submitted.\\n\\nOther Changes\\n\\nFixed bugs regarding file encodings (#3320, #3108, #2731)\\n\\nVersion 0.82.0\\n\\nRelease date: May 13, 2021\\n\\nNotable Changes\\n\\n♻️ Improvements to memory management by forcing garbage collection between script runs.\\n\\nVersion 0.81.1\\n\\nRelease date: Apr 29, 2021\\n\\nHighlights\\n\\n📝 Introducing st.form and st.form_submit_button to allow you to batch input widgets. Check out our blog post\\n\\n🔤 Introducing st.caption so you can add explainer text anywhere in you apps.\\n\\n🎨 Updates to Theming, including ability to build a theme that inherits from any of our default themes.\\n\\n🚀 Improvements to deployment experience to Streamlit sharing from the app menu.\\n\\nOther changes\\n\\nSupport for binary files in Custom Components (#3144)\\n\\nVersion 0.80.0\\n\\nRelease date: Apr 8, 2021\\n\\nHighlights\\n\\n🔐 Streamlit now support Secrets management for apps deployed to Streamlit Sharing!\\n\\n⚓️ Titles and headers now come with automatically generated anchor links. Just hover over any title and click the 🔗 to get the link!\\n\\nOther changes\\n\\nAdded allow-downloads capability to custom components (#3040)\\n\\nFixed markdown tables in dark theme (#3020)', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"Fixed markdown tables in dark theme (#3020)\\n\\nImproved color picker widget in the Custom Theme dialog (#2970)\\n\\nVersion 0.79.0\\n\\nRelease date: Mar 18, 2021\\n\\nHighlights\\n\\n🌈 Introducing support for custom themes. Check out our blog post\\n\\n🌚 This release also introduces dark mode!\\n\\n🛠️ Support for tooltips on all input widgets\\n\\nOther changes\\n\\nFixed bugs regarding file encodings (#1936, #2606) and caching functions (#2728)\\n\\nVersion 0.78.0\\n\\nRelease date: Mar 4, 2021\\n\\nFeatures\\n\\nIf you're in the Streamlit for Teams beta, we made a few updates to how secrets work. Check the beta docs for more info!\\n\\nDataframes now displays timezones for all DateTime and Time columns, and shows the time with the timezone applied, rather than in UTC\\n\\nNotable Bug Fixes\\n\\nVarious improvement to column alignment in st.beta_columns\\n\\nRemoved the long-deprecated format param from st.image, and replaced with output_format.\\n\\nVersion 0.77.0\\n\\nRelease date: Feb 23, 2021\\n\\nFeatures\\n\\nAdded a new config option client.showErrorDetails allowing the developer to control the granularity of error messages. This is useful for when you deploy an app, and want to conceal from your users potentially-sensitive information contained in tracebacks.\\n\\nNotable bug fixes\\n\\nFixed bug where st.image wasn't rendering certain kinds of SVGs correctly.\\n\\nFixed regression where the current value of an st.slider was only shown on hover.\\n\\nVersion 0.76.0\\n\\nRelease date: February 4, 2021\\n\\nNotable Changes\\n\\n🎨 st.color_picker is now out of beta. This means the old beta_color_picker function, which was marked as deprecated for the past 3 months, has now been replaced with color_picker.\\n\\n🐍 Display a warning when a Streamlit script is run directly as python script.py.\\n\\nst.image's use_column_width now defaults to an auto option which will resize the image to the column width if the image exceeds the column width.\\n\\n✂️ Fixed bugs (2437 and 2247) with content getting cut off within a st.beta_expander\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='📜 Fixed a bug in st.dataframe where the scrollbar overlapped with the contents in the last column.\\n\\n💾 Fixed a bug for st.file_uploader where file data returned was not the most recently uploaded file.\\n\\n➕ Fixed bugs (2086 and 2556) where some LaTeX commands were not rendering correctly.\\n\\nVersion 0.75.0\\n\\nRelease date: January 21, 2021\\n\\nNotable Changes\\n\\n🕳 st.empty\\n  previously would clear the component at the end of the script. It has now been\\n  updated to clear the component instantly.\\n\\n🛹 Previously in wide mode, we had thin margins around the webpage. This has\\n  now been increased to provide a better visual experience.\\n\\nVersion 0.74.0\\n\\nRelease date: January 6, 2021\\n\\nNotable Changes\\n\\n💾 st.file_uploader. has been stabilized and the deprecation warning\\n  and associated configuration option (deprecation.showfileUploaderEncoding) has been removed.\\n\\n📊 st.bokeh_chart is no longer duplicated when the page loads.\\n\\n🎈 Fixed page icon to support emojis with variants (i.e. 🤦\\u200d♀️ vs 🤦🏼\\u200d♀️) or dashes (i.e 🌙 - crescent-moon).\\n\\nVersion 0.73.0\\n\\nRelease date: December 17, 2020\\n\\nNotable Changes\\n\\n🐍 Streamlit can now be installed on Python 3.9. Streamlit components are not\\n  yet compatible with Python 3.9 and must use version 3.8 or earlier.\\n\\n🧱 Streamlit Components now allows same origin, enabling features provided by\\n  the browser such as a webcam component.\\n\\n🐙 Fix Streamlit sharing deploy experience for users running on Git versions\\n  2.7.0 or earlier.\\n\\n🧰 Handle unexpected closing of uploaded files for st.file_uploader.\\n\\nVersion 0.72.0\\n\\nRelease date: December 2, 2020\\n\\nNotable Changes\\n\\n🌈 Establish a framework for theming and migrate existing components.\\n\\n📱 Improve the sidebar experience for mobile devices.\\n\\n🧰 Update st.file_uploader to reduce reruns.\\n\\nVersion 0.71.0\\n\\nRelease date: November 11, 2020\\n\\nNotable Changes\\n\\n📁 Updated st.file_uploader\\n  to automatically reset buffer on app reruns.\\n\\n📊 Optimize the default rendering of charts and reduce issues with the initial render.', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Version 0.70.0\\n\\nRelease date: October 28, 2020\\n\\nNotable Changes\\n\\n🧪 st.set_page_config and st.color_picker have now been moved into the\\n  Streamlit namespace. These will be removed from beta January 28th, 2021. Learn\\n  more about our beta process here.\\n\\n📊 Improve display of bar charts for discrete values.\\n\\nVersion 0.69.0\\n\\nRelease date: October 15, 2020\\n\\nHighlights:\\n\\n🎁 Introducing Streamlit sharing, the best way to deploy, manage, and share your public Streamlit apps—for free. Read more about it on our blog post or sign up here!\\n\\nAdded st.experimental_rerun to programatically re-run your app. Thanks SimonBiggs!\\n\\nNotable Changes\\n\\n📹 Better support across browsers for start and stop times for st.video.\\n\\n🖼 Bug fix for intermittently failing media files\\n\\n📦 Bug fix for custom components compatibility with Safari. Make sure to upgrade to the latest streamlit-component-lib.\\n\\nVersion 0.68.0\\n\\nRelease date: October 8, 2020\\n\\nHighlights:\\n\\n⌗ Introducing new layout options for Streamlit! Move aside, vertical layout.\\n  Make a little space for... horizontal layout! Check out our\\n  blog post.\\n\\n💾 File uploader redesigned with new functionality for multiple files uploads\\n  and better support for working with uploaded files. This may cause breaking\\n  changes. Please see the new api in our\\n  documentation\\n\\nNotable Changes\\n\\n🎈 st.balloon has gotten a facelift with nicer balloons and smoother animations.\\n\\n🚨 Breaking Change: Following the deprecation of st.deck_gl_chart in\\n  January 2020, we have now removed the API completely. Please use\\n  st.pydeck_chart instead.\\n\\n🚨 Breaking Change: Following the deprecation of width and height for\\n  st.altair_chart, st.graphviz_chart, st.plotly_chart, and\\n  st.vega_lite_chart in January 2020, we have now removed the args completely.\\n  Please set the width and height in the respective charting library.\\n\\nVersion 0.67.0\\n\\nRelease date: September 16, 2020\\n\\nHighlights:', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Release date: September 16, 2020\\n\\nHighlights:\\n\\n🦷 Streamlit Components can now return bytes to your Streamlit App. To create a\\n  component that returns bytes, make sure to upgrade to the latest\\n  streamlit-component-lib.\\n\\nNotable Changes\\n\\n📈 Deprecation warning: Beginning December 1st, 2020 st.pyplot() will require a figure to\\n  be provided. To disable the deprecation warning, please set deprecation.showPyplotGlobalUse\\n  to False\\n\\n🎚 st.multiselect and st.select are now lightning fast when working with large datasets. Thanks masa3141!\\n\\nVersion 0.66.0\\n\\nRelease date: September 1, 2020\\n\\nHighlights:\\n\\n✏️ st.write is now available for use in the sidebar!\\n\\n🎚 A slider for distinct or non-numerical values is now available with st.select_slider.\\n\\n⌗ Streamlit Components can now return dataframes to your Streamlit App. Check out our SelectableDataTable example.\\n\\n📦 The Streamlit Components library used in our Streamlit Component template is\\n  now available as a npm package (streamlit-component-lib) to simplify future upgrades to the latest version.\\n  Existing components do not need to migrate.\\n\\nNotable Changes\\n\\n🐼 Support StringDtype from pandas version 1.0.0\\n\\n🧦 Support for running Streamlit on Unix sockets\\n\\nVersion 0.65.0\\n\\nRelease date: August 12, 2020\\n\\nHighlights:\\n\\n⚙️ Ability to set page title, favicon, sidebar state, and wide mode via st.beta_set_page_config(). See our documentation for details.\\n\\n📝 Add stateful behaviors through the use of query parameters with st.experimental_set_query_params and st.experimental_get_query_params. Thanks @zhaoooyue!\\n\\n🐼 Improved pandas dataframe support for st.radio, st.selectbox, and st.multiselect.\\n\\n🛑 Break out of your Streamlit app with st.stop.\\n\\n🖼 Inline SVG support for st.image.\\n\\nCallouts:\\n\\n🚨Deprecation Warning: The st.image parameter format has been renamed to output_format.\\n\\nVersion 0.64.0\\n\\nRelease date: July 23, 2020\\n\\nHighlights:', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"Release date: July 23, 2020\\n\\nHighlights:\\n\\n📊 Default matplotlib to display charts with a tight layout. To disable this,\\n  set bbox_inches to None, inches as a string, or a Bbox\\n\\n🗃 Deprecation warning for automatic encoding on st.file_uploader\\n\\n🙈 If gatherUserStats is False, do not even load the Segment library.\\n  Thanks @tanmaylaud!\\n\\nVersion 0.63.0\\n\\nRelease date: July 13, 2020\\n\\nHighlights:\\n\\n🧩 Support for Streamlit Components!!! See\\n  documentation for more info.\\n\\n🕗 Support for datetimes in\\n  st.slider. And, of course, just\\n  like any other value you use in st.slider, you can also pass in two-element lists to get a\\n  datetime range slider.\\n\\nVersion 0.62.0\\n\\nRelease date: June 21, 2020\\n\\nHighlights:\\n\\n📨 Ability to turn websocket compression on/off via the config option\\n  server.enableWebsocketCompression. This is useful if your server strips HTTP headers and you do\\n  not have access to change that behavior.\\n\\n🗝️ Out-of-the-box support for CSRF protection using the\\n  Cookie-to-header token\\n  technique. This means that if you're serving your Streamlit app from multiple replicas you'll need\\n  to configure them to to use the same cookie secret with the server.cookieSecret config option.\\n  To turn XSRF protection off, set server.enableXsrfProtection=false.\\n\\nNotable bug fixes:\\n\\n🖼️ Added a grace period to the image cache expiration logic in order to fix multiple related bugs\\n  where images sent with st.image or st.pyplot were sometimes missing.\\n\\nVersion 0.61.0\\n\\nRelease date: June 2, 2020\\n\\nHighlights:\\n\\n📅 Support for date ranges in st.date_picker. See\\n  docs\\n  for more info, but the TLDR is: just pass a list/tuple as the default date and it will be\\n  interpreted as a range.\\n\\n🗣️ You can now choose whether st.echo prints the code above or below the output of the echoed\\n  block. To learn more, refer to the code_location argument in the\\n  docs.\\n\\n📦 Improved @st.cache support for Keras models and Tensorflow saved_models.\\n\\nVersion 0.60.0\\n\\nRelease date: May 18, 2020\\n\\nHighlights:\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"Release date: May 18, 2020\\n\\nHighlights:\\n\\n↕️ Ability to set the height of an st.text_area with the height argument\\n  (expressed in pixels). See\\n  docs for more.\\n\\n🔡 Ability to set the maximimum number of characters allowed in st.text_area\\n  or st.text_input. Check out the max_chars argument in the\\n  docs.\\n\\n🗺️ Better DeckGL support for the H3 geospatial indexing\\n  system. So now you can use things like H3HexagonLayer in\\n  st.pydeck_chart.\\n\\n📦 Improved @st.cache support for PyTorch TensorBase and Model.\\n\\nVersion 0.59.0\\n\\nRelease date: May 05, 2020\\n\\nHighlights:\\n\\n🎨 New color-picker widget! Use it with\\n  st.beta_color_picker()\\n\\n🧪 Introducing st.beta_* and st.experimental_* function prefixes, for faster\\n  Streamlit feature releases. See\\n  docs for more info.\\n\\n📦 Improved @st.cache support for SQL Alchemy objects, CompiledFFI, PyTorch\\n  Tensors, and builtins.mappingproxy.\\n\\nVersion 0.58.0\\n\\nRelease date: April 22, 2020\\n\\nHighlights:\\n\\n💼 Made st.selectbox filtering case-insensitive.\\n\\n㈬ Better support for Tensorflow sessions in @st.cache.\\n\\n📊 Changed behavior of st.pyplot to auto-clear the figure only when using\\n  the global Matplotlib figure (i.e. only when calling st.pyplot() rather\\n  than st.pyplot(fig)).\\n\\nVersion 0.57.0\\n\\nRelease date: March 26, 2020\\n\\nHighlights:\\n\\n⏲️ Ability to set expiration options for @st.cache'ed functions by setting\\n  the max_entries and ttl arguments. See\\n  docs.\\n\\n🆙 Improved the machinery behind st.file_uploader, so it's much more\\n  performant now! Also increased the default upload limit to 200MB\\n  (configurable via server.max_upload_size).\\n\\n🔒 The server.address config option now binds the server to that address\\n  for added security.\\n\\n📄 Even more details added to error messages for @st.cache for easier\\n  debugging.\\n\\nVersion 0.56.0\\n\\nRelease date: February 15, 2020\\n\\nHighlights:\\n\\n📄 Improved error messages for st.cache. The errors now also point to the new\\n  caching docs we just released. Read more\\n  here!\\n\\nBreaking changes:\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Breaking changes:\\n\\n🐍 As announced last month,\\n  Streamlit no longer supports Python 2. To use Streamlit you\\'ll need\\n  Python 3.5 or above.\\n\\nVersion 0.55.0\\n\\nRelease date: February 4, 2020\\n\\nHighlights:\\n\\n📺 Ability to record screencasts directly from Streamlit! This allows\\n  you to easily record and share explanations about your models, analyses,\\n  data, etc. Just click ☰ then \"Record a screencast\". Give it a try!\\n\\nVersion 0.54.0\\n\\nRelease date: January 29, 2020\\n\\nHighlights:\\n\\n⌨️ Support for password fields! Just pass type=\"password\" to\\n  st.text_input().\\n\\nNotable fixes:\\n\\n✳️ Numerous st.cache improvements, including better support for complex objects.\\n\\n🗣️ Fixed cross-talk in sidebar between multiple users.\\n\\nBreaking changes:\\n\\nIf you\\'re using the SessionState hack Gist, you should re-download it!\\n  Depending on which hack you\\'re using, here are some links to save you some\\n  time:\\n\\nSessionState.py\\n\\nst_state_patch.py\\n\\nVersion 0.53.0\\n\\nRelease date: January 14, 2020\\n\\nHighlights:\\n\\n🗺️ Support for all DeckGL features! Just use\\n  Pydeck instead of\\n  st.deck_gl_chart.\\n  To do that, simply pass a PyDeck object to\\n  st.pydeck_chart,\\n  st.write,\\n  or magic.\\n\\nNote that as a preview release things may change in the near future.\\n  Looking forward to hearing input from the community before we stabilize the\\n  API!\\n\\nThe goals is for this to replace st.deck_gl_chart, since it\\n  is does everything the old API did and much more!\\n\\n🆕 Better handling of Streamlit upgrades while developing. We now auto-reload\\n  the browser tab if the app it is displaying uses a newer version of Streamlit\\n  than the one the tab is running.\\n\\n👑 New favicon, with our new logo!\\n\\nNotable fixes:\\n\\nMagic now works correctly in Python 3.8. It no longer causes\\n  docstrings to render in your app.\\n\\nBreaking changes:\\n\\nUpdated how we calculate the default width and height of all chart types.\\n  We now leave chart sizing up to your charting library itself, so please refer\\n  to the library\\'s documentation.', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='As a result, the width and height arguments have been deprecated\\n  from most chart commands, and use_container_width has been introduced\\n  everywhere to allow you to make charts fill as much horizontal space as\\n  possible (this used to be the default).\\n\\nVersion 0.52.0\\n\\nRelease date: December 20, 2019\\n\\nHighlights:\\n\\n📤 Preview release of the file uploader widget. To try it out just call\\n  st.file_uploader!\\n\\nNote that as a preview release things may change in the near future.\\n  Looking forward to hearing input from the community before we stabilize the\\n  API!\\n\\n👋 Support for emoji codes in\\n  st.write and st.markdown! Try it out with st.write(\"Hello :wave:\").\\n\\nBreaking changes:\\n\\n🧹 st.pyplot now clears figures by default, since that\\'s what you want 99% of\\n  the time. This allows you to create two or more Matplotlib charts without\\n  having to call\\n  pyplot.clf\\n  every time. If you want to turn this behavior off, use\\n  st.pyplot(clear_figure=False)\\n\\n📣 st.cache no longer checks for input mutations. This is the first change\\n  of our ongoing effort to simplify the caching system and prepare Streamlit\\n  for the launch of other caching primitives like Session State!\\n\\nVersion 0.51.0\\n\\nRelease date: November 30, 2019\\n\\nHighlights:\\n\\n🐕 You can now tweak the behavior of the file watcher with the config option server.fileWatcherType. Use it to switch between:\\n\\nauto (default) : Streamlit will attempt to use the watchdog module, and\\n    falls back to polling if watchdog is not available.\\n\\nwatchdog : Force Streamlit to use the watchdog module.\\n\\npoll : Force Streamlit to always use polling.\\n\\nnone : Streamlit will not watch files.\\n\\nNotable bug fixes:\\n\\nFix the \"keyPrefix\" option in static report sharing #724\\n\\nAdd support for getColorX and getTargetColorX to DeckGL Chart #718\\n\\nFixing Tornado on Windows + Python 3.8 #682\\n\\nFall back on webbrowser if xdg-open is not installed on Linux #701\\n\\nFixing number input spin buttons for Firefox #683\\n\\nFixing CTRL+ENTER on Windows #699', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Fixing CTRL+ENTER on Windows #699\\n\\nDo not automatically create credential file when in headless mode #467\\n\\nVersion 0.50.1\\n\\nRelease date: November 10, 2019\\n\\nHighlights:\\n\\n👩\\u200d🎓 SymPy support and ability to draw mathematical expressions using LaTeX! See\\n  st.latex,\\n  st.markdown,\\n  and\\n  st.write.\\n\\n🌄 You can now set config options using environment variables. For example,\\n  export STREAMLIT_SERVER_PORT=9876.\\n\\n🐱 Ability to call streamlit run directly with Github and Gist URLs. No\\n  need to grab the \"raw\" URL first!\\n\\n📃 Cleaner exception stack traces. We now remove all Streamlit-specific code\\n  from stack traces originating from the user\\'s app.\\n\\nVersion 0.49.0\\n\\nRelease date: October 23, 2019\\n\\nHighlights:\\n\\n💯 New input widget for entering numbers with the keyboard: st.number_input()\\n\\n📺 Audio/video improvements: ability to load from a URL, to embed YouTube\\n  videos, and to set the start position.\\n\\n🤝 You can now (once again) share static snapshots of your apps to S3! See\\n  the S3 section of streamlit config show to set it up. Then share from\\n  top-right menu.\\n\\n⚙️ Use server.baseUrlPath config option to set Streamlit\\'s URL to something\\n  like http://domain.com/customPath.\\n\\nNotable bug fixes:\\n\\nFixes numerous Windows bugs, including Issues\\n  #339 and\\n  #401.\\n\\nVersion 0.48.0\\n\\nRelease date: October 12, 2019\\n\\nHighlights:\\n\\n🔧 Ability to set config options as command line flags or in a local config file.\\n\\n↕️ You can now maximize charts and images!\\n\\n⚡ Streamlit is now much faster when writing data in quick succession to your app.\\n\\n✳️ Ability to blacklist folder globs from \"run on save\" and @st.cache hashing.\\n\\n🎛️ Improved handling of widget state when Python file is modified.\\n\\n🙈 Improved HTML support in st.write and st.markdown. HTML is still unsafe, though!\\n\\nNotable bug fixes:\\n\\nFixes @st.cache bug related to having your Python environment on current\\n  working directory. Issue #242\\n\\nFixes loading of root url / on Windows. Issue #244\\n\\nVersion 0.47.0\\n\\nRelease date: October 1, 2019', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='Version 0.47.0\\n\\nRelease date: October 1, 2019\\n\\nHighlights:\\n\\n🌄 New hello.py showing off 4 glorious Streamlit apps. Try it out!\\n\\n🔄 Streamlit now automatically selects an unused port when 8501 is already in use.\\n\\n🎁 Sidebar support is now out of beta! Just start any command with st.sidebar. instead of st.\\n\\n⚡ Performance improvements: we added a cache to our websocket layer so we no longer re-send data to the browser when it hasn\\'t changed between runs\\n\\n📈 Our \"native\" charts st.line_chart, st.area_chart and st.bar_chart now use Altair behind the scenes\\n\\n🔫 Improved widgets: custom st.slider labels; default values in multiselect\\n\\n🕵️\\u200d♀️ The filesystem watcher now ignores hidden folders and virtual environments\\n\\n💅 Plus lots of polish around caching and widget state management\\n\\nBreaking change:\\n\\n🛡️ We have temporarily disabled support for sharing static \"snapshots\" of Streamlit apps. Now that we\\'re no longer in a limited-access beta, we need to make sure sharing is well thought through and abides by laws like the DMCA. But we\\'re working on a solution!\\n\\nVersion 0.46.0\\n\\nRelease date: September 19, 2019\\n\\nHighlights:\\n\\n✨ Magic commands! Use st.write without typing st.write. See\\n  https://docs.streamlit.io/en/latest/api.html#magic-commands\\n\\n🎛️ New st.multiselect widget.\\n\\n🐍 Fixed numerous install issues so now you can use pip install streamlit\\n  even in Conda! We\\'ve therefore deactivated our Conda repo.\\n\\n🐞 Multiple bug fixes and additional polish in preparation for our launch!\\n\\nBreaking change:\\n\\n🛡️ HTML tags are now blacklisted in st.write/st.markdown by default. More\\n  information and a temporary work-around at:\\n  https://github.com/streamlit/streamlit/issues/152\\n\\nVersion 0.45.0\\n\\nRelease date: August 28, 2019\\n\\nHighlights:\\n\\n😱 Experimental support for sidebar! Let us know if you want to be a beta\\n  tester.', metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"🎁 Completely redesigned st.cache! Much more performant, has a cleaner API,\\n  support for caching functions called by @st.cached functions,\\n  user-friendly error messages, and much more!\\n\\n🖼️ Lightning fast st.image, ability to choose between JPEG and PNG\\n  compression, and between RGB and BGR (for OpenCV).\\n\\n💡 Smarter API for st.slider, st.selectbox, and st.radio.\\n\\n🤖 Automatically fixes the Matplotlib backend -- no need to edit .matplotlibrc\\n\\nVersion 0.44.0\\n\\nRelease date: July 28, 2019\\n\\nHighlights:\\n\\n⚡ Lightning-fast reconnect when you do a ctrl-c/rerun on your Streamlit code\\n\\n📣 Useful error messages when the connection fails\\n\\n💎 Fixed multiple bugs and improved polish of our newly-released interactive widgets\\n\\nVersion 0.43.0\\n\\nRelease date: July 9, 2019\\n\\nHighlights:\\n\\n⚡ Support for interactive widgets! 🎈🎉\\n\\nVersion 0.42.0\\n\\nRelease date: July 1, 2019\\n\\nHighlights:\\n\\n💾 Ability to save Vega-Lite and Altair charts to SVG or PNG\\n\\n🐇 We now cache JS files in your browser for faster loading\\n\\n⛔ Improvements to error-handling inside Streamlit apps\\n\\nVersion 0.41.0\\n\\nRelease date: June 24, 2019\\n\\nHighlights:\\n\\n📈 Greatly improved our support for named datasets in Vega-Lite and Altair\\n\\n🙄 Added ability to ignore certain folders when watching for file changes. See the server.folderWatchBlacklist config option.\\n\\n☔ More robust against syntax errors on the user's script and imported modules\\n\\nVersion 0.40.0\\n\\nRelease date: June 10, 2019\\n\\nHighlights:\\n\\nStreamlit is more than 10x faster. Just save and watch your analyses update instantly.\\n\\nWe changed how you run Streamlit apps:\\n  $ streamlit run your_script.py [script args]\\n\\nUnlike the previous versions of Streamlit, streamlit run [script] [script args] creates a server (now you don't need to worry if the proxy is up). To kill the server, all you need to do is hit Ctrl+c.\\n\\nWhy is this so much faster?\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content=\"Why is this so much faster?\\n\\nNow, Streamlit keeps a single Python session running until you kill the server. This means that Streamlit can re-run your code without kicking off a new process; imported libraries are cached to memory. An added bonus is that st.cache now caches to memory instead of to disk.\\n\\nWhat happens if I run Streamlit the old way?\\n\\nIf you run $ python your_script.py the script will execute from top to bottom, but won't produce a Streamlit app.\\n\\nWhat are the limitations of the new architecture?\\n\\nTo switch Streamlit apps, first you have to kill the Streamlit server with Ctrl-c. Then, you can use streamlit run to generate the next app.\\n\\nStreamlit only works when used inside Python files, not interactively from the Python REPL.\\n\\nWhat else do I need to know?\\n\\nThe strings we print to the command line when liveSave is on have been cleaned up. You may need to adjust any RegEx that depends on those.\\n\\nA number of config options have been renamed:\\n\\nWhat if something breaks?\\n\\nIf the new Streamlit isn't working, please let us know by Slack or email. You can downgrade at any time with these commands:\\n\\nbash\\npip install --upgrade streamlit==0.37\\n\\nbash\\nconda install streamlit=0.37\\n\\nWhat's next?\\n\\nThank you for staying with us on this journey! This version of Streamlit lays the foundation for interactive widgets, a new feature of Streamlit we're really excited to share with you in the next few months.\\n\\nVersion 0.36.0\\n\\nRelease date: May 03, 2019\\n\\nHighlights\\n\\n🚣\\u200d♀️ st.progress() now also accepts floats from 0.0–1.0\\n\\n🤯 Improved rendering of long headers in DataFrames\\n\\n🔐 Shared apps now default to HTTPS\\n\\nVersion 0.35.0\\n\\nRelease date: April 26, 2019\\n\\nHighlights\\n\\n📷 Bokeh support! Check out docs for st.bokeh_chart\\n\\n⚡️ Improved the size and load time of saved apps\\n\\n⚾️ Implemented better error-catching throughout the codebase\", metadata={'source': 'docs/content/library/changelog.md'}),\n",
       " Document(page_content='title: Dataframes\\nslug: /library/advanced-features/dataframes\\n\\nDataframes\\n\\nDataframes are a great way to display and edit data in a tabular format. Working with Pandas DataFrames and other tabular data structures is key to data science workflows. If developers and data scientists want to display this data in Streamlit, they have multiple options: st.dataframe and st.data_editor. If you want to solely display data in a table-like UI, st.dataframe is the way to go. If you want to interactively edit data, use st.data_editor. We explore the use cases and advantages of each option in the following sections.\\n\\nDisplay dataframes with st.dataframe\\n\\nStreamlit can display dataframes in a table-like UI via st.dataframe :\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\ndf = pd.DataFrame(\\n    [\\n        {\"command\": \"st.selectbox\", \"rating\": 4, \"is_widget\": True},\\n        {\"command\": \"st.balloons\", \"rating\": 5, \"is_widget\": False},\\n        {\"command\": \"st.time_input\", \"rating\": 3, \"is_widget\": True},\\n    ]\\n)\\n\\nst.dataframe(df, use_container_width=True)\\n```\\n\\nAdditional UI features\\n\\nst.dataframe also provides some additional functionality by using glide-data-grid under the hood:\\n\\nColumn sorting: sort columns by clicking on their headers.\\n\\nColumn resizing: resize columns by dragging and dropping column header borders.\\n\\nTable resizing: resize tables by dragging and dropping the bottom right corner.\\n\\nSearch: search through data by clicking a table, using hotkeys (⌘ Cmd + F\\xa0or\\xa0Ctrl + F) to bring up the search bar, and using the search bar to filter data.\\n\\nCopy to clipboard: select one or multiple cells, copy them to the clipboard and paste them into your favorite spreadsheet software.\\n\\nTry out all the addition UI features using the embedded app from the prior section.', metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content='In addition to Pandas DataFrames, st.dataframe also supports other common Python types, e.g., list, dict, or numpy array. It also supports Snowpark and PySpark DataFrames, which allow you to lazily evaluate and pull data from databases. This can be useful for working with large datasets.\\n\\nEdit data with st.data_editor\\n\\nStreamlit supports editable dataframes via the st.data_editor command. Check out its API in st.data_editor. It shows the dataframe in a table, similar to st.dataframe. But in contrast to st.dataframe, this table isn\\'t static! The user can click on cells and edit them. The edited data is then returned on the Python side. Here\\'s an example:\\n\\n```python\\ndf = pd.DataFrame(\\n    [\\n        {\"command\": \"st.selectbox\", \"rating\": 4, \"is_widget\": True},\\n        {\"command\": \"st.balloons\", \"rating\": 5, \"is_widget\": False},\\n        {\"command\": \"st.time_input\", \"rating\": 3, \"is_widget\": True},\\n    ]\\n)\\n\\ndf = load_data()\\nedited_df = st.data_editor(df) # 👈 An editable dataframe\\n\\nfavorite_command = edited_df.loc[edited_df[\"rating\"].idxmax()][\"command\"]\\nst.markdown(f\"Your favorite command is {favorite_command} 🎈\")\\n```\\n\\nTry it out by double-clicking on any cell. You\\'ll notice you can edit all cell values. Try editing the values in the rating column and observe how the text output at the bottom changes:\\n\\nst.data_editor also supports a few additional things:\\n\\nCopy and paste support from and to Excel and Google Sheets.\\n\\nAdd and delete rows. You can do this by setting num_rows= \"dynamic\" when calling st.data_editor. This will allow users to add and delete rows as needed.\\n\\nAccess edited data. Only access the individual edits instead of the entire edited data structure via session state.\\n\\nBulk edits (similar to Excel, just drag a handle to edit neighboring cells).\\n\\nAutomatic input validation, a strong data type support. e.g. There\\'s no way to enter letters into a number cell and many other configurable input validation options. e.g. min-/max-value.', metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content='Edit common data structures such as lists, dicts, NumPy ndarray, etc.\\n\\nCopy and paste support\\n\\nThe data editor supports pasting in tabular data from Google Sheets, Excel, Notion, and many other similar tools. You can also copy-paste data between\\xa0st.data_editor instances. This functionality, powered by the Clipboard API, can be a huge time saver for users who need to work with data across multiple platforms. To try it out:\\n\\nCopy data from\\xa0this Google Sheets document\\xa0to clipboard\\n\\nSelect any cell in the\\xa0name\\xa0column of the table below and paste it in (via\\xa0ctrl/cmd + v).\\n\\nEvery cell of the pasted data will be evaluated individually and inserted into the cells if the data is compatible with the column type. E.g., pasting in non-numerical text data into a number column will be ignored.\\n\\nDid you notice that although the initial dataframe had just five rows, pasting all those rows from the spreadsheet added additional rows to the dataframe? 👀\\xa0Let\\'s find out how that works in the next section.\\n\\nIf you embed your apps with iframes, you\\'ll need to allow the iframe to access the clipboard if you want to use the copy-paste functionality. To do so, give the iframe clipboard-write and clipboard-read permissions. E.g.\\n\\n```javascript\\n\\n```\\n\\nAs developers, ensure the app is served with a valid, trusted certificate when using TLS. If users encounter issues with copying and pasting data, direct them to check if their browser has activated clipboard access permissions for the Streamlit application, either when prompted or through the browser\\'s site settings.\\n\\nAdd and delete rows\\n\\nWith st.data_editor, viewers can add or delete rows via the table UI. This mode can be activated by setting the\\xa0num_rows parameter to\\xa0\"dynamic\". E.g.\\n\\npython\\nedited_df = st.data_editor(df, num_rows=\"dynamic\")\\n\\nTo add new rows, scroll to the bottom-most row and click on the “+\" sign in any cell.\\n\\nTo delete rows, select one or more rows and press the delete key on your keyboard.\\n\\nAccess edited data', metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content='Access edited data\\n\\nSometimes, it is more convenient to know which cells have been changed rather than getting the entire edited dataframe back. Streamlit makes this easy through the use of session state. If a key parameter is set, Streamlit will store any changes made to the dataframe in the session state.\\n\\nThis snippet shows how you can access changed data using session state:\\n\\npython\\nst.data_editor(df, key=\"data_editor\") # 👈 Set a key\\nst.write(\"Here\\'s the session state:\")\\nst.write(st.session_state[\"data_editor\"]) # 👈 Access the edited data\\n\\nIn this code snippet, the key parameter is set to \"data_editor\". Any changes made to the data in the st.data_editor instance will be tracked by Streamlit and stored in session state under the key \"data_editor\".\\n\\nAfter the data editor is created, the contents of the \"data_editor\" key in session state are printed to the screen using st.write(st.session_state[\"data_editor\"]). This allows you to see the changes made to the original dataframe without having to return the entire dataframe from the data editor.\\n\\nThis can be useful when working with large dataframes and you only need to know which cells have changed, rather than the entire edited dataframe.\\n\\nUse all we\\'ve learned so far and apply them to the above embedded app. Try editing cells, adding new rows, and deleting rows.\\n\\nNotice how edits to the table are reflected in session state: when you make any edits, a rerun is triggered which sends the edits to the backend via st.data_editor\\'s keyed widget state. Its widget state is a JSON object containing three properties: edited_rows, added_rows, and deleted rows:.\\n\\nedited_rows is a dictionary containing all edits. Keys are zero-based row indices and values are dictionaries that map column names to edits (e.g. {0: {\"col1\": ..., \"col2\": ...}}).\\n\\nadded_rows is a list of newly added rows. Each value is a dictionary with the same format as above (e.g. [{\"col1\": ..., \"col2\": ...}]).', metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content='deleted_rows is a list of row numbers that have been deleted from the table (e.g. [0, 2]).\\n\\nBulk edits\\n\\nThe data editor includes a feature that allows for bulk editing of cells. Similar to Excel, you can drag a handle across a selection of cells to edit their values in bulk. You can even apply commonly used keyboard shortcuts in spreadsheet software. This is useful when you need to make the same change across multiple cells, rather than editing each cell individually:\\n\\nAutomatic input validation\\n\\nColumn configuration API. Such as\\n\\ntext columns, or\\n\\nnumber columns. You can also set\\n\\nEdit common data structures\\n\\nEditing doesn\\'t just work for Pandas DataFrames! You can also edit lists, tuples, sets, dictionaries, NumPy arrays, or Snowpark & PySpark DataFrames. Most data types will be returned in their original format. But some types (e.g. Snowpark and PySpark) are converted to Pandas DataFrames. To learn about all the supported types, read the st.data_editor API.\\n\\nE.g. you can easily let the user add items to a list:\\n\\npython\\nedited_list = st.data_editor([\"red\", \"green\", \"blue\"], num_rows= \"dynamic\")\\nst.write(\"Here are all the colors you entered:\")\\nst.write(edited_list)\\n\\nOr numpy arrays:\\n\\n```python\\nimport numpy as np\\n\\nst.data_editor(np.array([\\n    [\"st.text_area\", \"widget\", 4.92],\\n    [\"st.markdown\", \"element\", 47.22]\\n]))\\n```\\n\\nOr lists of records:\\n\\npython\\nst.data_editor([\\n    {\"name\": \"st.text_area\", \"type\": \"widget\"},\\n    {\"name\": \"st.markdown\", \"type\": \"element\"},\\n])\\n\\nOr dictionaries and many more types!\\n\\npython\\nst.data_editor({\\n    \"st.text_area\": \"widget\",\\n    \"st.markdown\": \"element\"\\n})\\n\\nConfiguring columns', metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content='Configuring columns\\n\\nYou are able configure the display and editing behavior of columns via st.dataframe and st.data_editor via the Column configuration API. We have developed the API to let you add images, charts, and clickable URLs in dataframe and data editor columns. Additionally, you can make individual columns editable, set columns as categorical and specify which options they can take, hide the index of the dataframe, and much more.\\n\\nColumn configuration\\n\\nWhen working with data in Streamlit, the st.column_config class is a powerful tool for configuring data display and interaction. Specifically designed for the column_config parameter in st.dataframe and st.data_editor, it provides a suite of methods to tailor your columns to various data types - from simple text and numbers to lists, URLs, images, and more.\\n\\nWhether it\\'s translating temporal data into user-friendly formats or utilizing charts and progress bars for clearer data visualization, column configuration not only provides the user with an enriched data viewing experience but also ensures that you\\'re equipped with the tools to present and interact with your data, just the way you want it.\\n\\nColumn\\n\\nConfigure a generic column.\\n\\npython\\nColumn(\"Streamlit Widgets\", width=\"medium\", help=\"Streamlit **widget** commands 🎈\")\\n\\nText column\\n\\nConfigure a text column.\\n\\npython\\nTextColumn(\"Widgets\", max_chars=50, validate=\"^st\\\\.[a-z_]+$\")\\n\\nNumber column\\n\\nConfigure a number column.\\n\\npython\\nNumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nCheckbox column\\n\\nConfigure a checkbox column.\\n\\npython\\nCheckboxColumn(\"Your favorite?\", help=\"Select your **favorite** widgets\")\\n\\nSelectbox column\\n\\nConfigure a selectbox column.\\n\\npython\\nSelectboxColumn(\"App Category\", options=[\"🤖 LLM\", \"📈 Data Viz\"])\\n\\nDatetime column\\n\\nConfigure a datetime column.\\n\\npython\\nDatetimeColumn(\"Appointment\", min_value=datetime(2023, 6, 1), format=\"D MMM YYYY, h:mm a\")\\n\\nDate column\\n\\nConfigure a date column.', metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content='Date column\\n\\nConfigure a date column.\\n\\npython\\nDateColumn(\"Birthday\", max_value=date(2005, 1, 1), format=\"DD.MM.YYYY\")\\n\\nTime column\\n\\nConfigure a time column.\\n\\npython\\nTimeColumn(\"Appointment\", min_value=time(8, 0, 0), format=\"hh:mm a\")\\n\\nList column\\n\\nConfigure a list column.\\n\\npython\\nListColumn(\"Sales (last 6 months)\", width=\"medium\")\\n\\nLink column\\n\\nConfigure a link column.\\n\\npython\\nLinkColumn(\"Trending apps\", max_chars=100, validate=\"^https://.*$\")\\n\\nImage column\\n\\nConfigure an image column.\\n\\npython\\nImageColumn(\"Preview Image\", help=\"The preview screenshots\")\\n\\nLine chart column\\n\\nConfigure a line chart column.\\n\\npython\\nLineChartColumn(\"Sales (last 6 months)\" y_min=0, y_max=100)\\n\\nBar chart column\\n\\nConfigure a bar chart column.\\n\\npython\\nBarChartColumn(\"Marketing spend\" y_min=0, y_max=100)\\n\\nProgress column\\n\\nConfigure a progress column.\\n\\npython\\nProgressColumn(\"Sales volume\", min_value=0, max_value=1000, format=\"$%f\")\\n\\nWe will release in-depth documentation and a blog post on how to configure columns in the next two weeks. Keep at an eye out for updates on this page and the Streamlit blog.\\n\\nThere are techniques you can use with Pandas today to render columns as checkboxes, selectboxes, and change the type of columns, that don\\'t involve the column configuration API. We explore these techniques in the following sections.\\n\\nBoolean columns (checkboxes)\\n\\nTo render columns as checkboxes and clickable checkboxes in st.dataframe and st.data_editor, respectively, set the type of the Pandas column as bool.\\n\\nHere’s an example of creating a Pandas DataFrame with column A containing boolean values. When we display it using st.dataframe, the boolean values are rendered as checkboxes, where True and False values are checked and unchecked, respectively.\\n\\n```python\\nimport pandas as pd\\n\\ncreate a dataframe with a boolean column\\n\\ndf = pd.DataFrame({\"A\": [True, False, True, False]})\\n\\nshow the dataframe with checkboxes\\n\\nst.dataframe(df)\\n```', metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content='st.dataframe(df)\\n```\\n\\nNotice you cannot change their values from the frontend. To let users check and uncheck values, we display the dataframe with st.data_editor instead:\\n\\n```python\\nimport pandas as pd\\n\\ncreate a dataframe with a boolean column\\n\\ndf = pd.DataFrame({\"A\": [True, False, True, False]})\\n\\nshow the data editor with checkboxes\\n\\nst.data_editor(df)\\n```\\n\\nCategorical columns (selectboxes)\\n\\nTo render columns as selectboxes with st.data_editor, set the type of the Pandas column as category:\\n\\n```python\\nimport pandas as pd\\n\\ndf = pd.DataFrame(\\n    {\"command\": [\"st.selectbox\", \"st.slider\", \"st.balloons\", \"st.time_input\"]}\\n)\\ndf[\"command\"] = df[\"command\"].astype(\"category\")\\n\\nedited_df = st.data_editor(df)\\n```\\n\\nIn some cases, you may want users to select categories that aren’t in the original Pandas DataFrame. Let’s say we use df from above. Currently, the command column can take on four unique values. What should we do if we want users to see additional options such as st.button and st.radio?\\n\\nTo add additional categories to the selection, use pandas.Series.cat.add_categories:\\n\\n```python\\nimport pandas as pd\\n\\ndf = pd.DataFrame(\\n    {\"command\": [\"st.selectbox\", \"st.slider\", \"st.balloons\", \"st.time_input\"]}\\n)\\ndf[\"command\"] = (\\n    df[\"command\"].astype(\"category\").cat.add_categories([\"st.button\", \"st.radio\"])\\n)\\n\\nedited_df = st.data_editor(df)\\n```\\n\\nHandling large datasets\\n\\nst.dataframe and st.data_editor have been designed to theoretically handle tables with millions of rows thanks to their highly performant implementation using the glide-data-grid library and HTML canvas. However, the maximum amount of data that an app can realistically handle will depend on several other factors, including:\\n\\nThe maximum size of WebSocket messages: Streamlit\\'s WebSocket messages are configurable via the server.maxMessageSize config option, which limits the amount of data that can be transferred via the WebSocket connection at once.', metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content=\"The server memory: The amount of data that your app can handle will also depend on the amount of memory available on your server. If the server's memory is exceeded, the app may become slow or unresponsive.\\n\\nThe user's browser memory: Since all the data needs to be transferred to the user's browser for rendering, the amount of memory available on the user's device can also affect the app's performance. If the browser's memory is exceeded, it may crash or become unresponsive.\\n\\nIn addition to these factors, a slow network connection can also significantly slow down apps that handle large datasets.\\n\\nWhen handling large datasets with more than 150,000 rows, Streamlit applies additional optimizations and disables column sorting. This can help to reduce the amount of data that needs to be processed at once and improve the app's performance.\\n\\nLimitations\\n\\nWhile Streamlit's data editing capabilities offer a lot of functionality, there are some limitations to be aware of:\\n\\nEditing is enabled for a limited set of column types (TextColumn, NumberColumn, LinkColumn, CheckboxColumn, SelectboxColumn, DateColumn, TimeColumn, and DatetimeColumn). We are actively working on supporting editing for other column types as well, such as images, lists, and charts.\\n\\nEditing of Pandas DataFrames only supports the following index types:\\xa0RangeIndex, (string)\\xa0Index,\\xa0Float64Index,\\xa0Int64Index, and\\xa0UInt64Index.\\n\\nSome actions like deleting rows or searching data can only be triggered via keyboard hotkeys.\\n\\nWe are working to fix the above limitations in future releases, so keep an eye out for updates.\", metadata={'source': 'docs/content/library/advanced-features/dataframes.md'}),\n",
       " Document(page_content=\"title: Widget semantics\\nslug: /library/advanced-features/widget-semantics\\n\\nAdvanced notes on widget behavior\\n\\nWidgets are magical and often work how you want. But they can have surprising behavior in some situations. Here is a high-level, abstract description of widget behavior, including some common edge-cases:\\n\\nIf you call a widget function before the widget state exists, the widget state defaults to a value. This value depends on the widget and its arguments.\\n\\nA widget function call returns the current widget state value. The return value is a simple Python type, and the exact type depends on the widget and its arguments.\\n\\nWidget states depend on a particular session (browser connection). The actions of one user do not affect the widgets of any other user.\\n\\nA widget's identity depends on the arguments passed to the widget function. If those change, the call will create a new widget (with a default value, per 1).\\n\\nIf you don't call a widget function in a script run, we neither store the widget state nor render the widget. If you call a widget function with the same arguments later, Streamlit treats it as a new widget.\\n\\n4 and 5 are the most likely to be surprising and may pose a problem for some application designs. When you want to persist widget state for recreating a widget, use Session State to work around 5.\", metadata={'source': 'docs/content/library/advanced-features/advanced-widget-behavior.md'}),\n",
       " Document(page_content=\"title: Command-line options\\nslug: /library/advanced-features/cli\\n\\nCommand-line interface\\n\\nWhen you install Streamlit, a command-line (CLI) tool gets installed\\nas well. The purpose of this tool is to run Streamlit apps, change Streamlit configuration options,\\nand help you diagnose and fix issues.\\n\\nTo see all of the supported commands:\\n\\nbash\\nstreamlit --help\\n\\nRun Streamlit apps\\n\\nbash\\nstreamlit run your_script.py [-- script args]\\n\\nRuns your app. At any time you can stop the server with Ctrl+c.\\n\\nWhen passing your script some custom arguments, they must be passed after\\ntwo dashes. Otherwise the arguments get interpreted as arguments to Streamlit\\nitself.\\n\\nTo see the Streamlit 'Hello, World!' example app, run streamlit hello.\\n\\nView Streamlit version\\n\\nTo see what version of Streamlit is installed, just type:\\n\\nbash\\nstreamlit version\\n\\nView documentation\\n\\nbash\\nstreamlit docs\\n\\nOpens the Streamlit documentation (i.e. this website) in a web browser.\\n\\nClear cache\\n\\nbash\\nstreamlit cache clear\\n\\nClears persisted files from the on-disk Streamlit cache, if\\npresent.\\n\\nView all configuration options\\n\\nAs described in Configuration, Streamlit has several\\nconfiguration options. To view them all, including their current values, just type:\\n\\nbash\\nstreamlit config show\", metadata={'source': 'docs/content/library/advanced-features/cli.md'}),\n",
       " Document(page_content=\"title: Connecting to data\\nslug: /library/advanced-features/connecting-to-data\\n\\nConnecting to data\\n\\nMost Streamlit apps need some kind of data or API access to be useful - either retrieving data to view or saving the results of some user action. This data or API is often part of some remote service, database, or other data source.\\n\\nAnything you can do with Python, including data connections, will generally work in Streamlit. Streamlit's tutorials are a great starting place for many data sources. However:\\n\\nConnecting to data in a Python application is often tedious and annoying.\\n\\nThere are specific considerations for connecting to data from streamlit apps, such as caching and secrets management.\\n\\nStreamlit provides st.experimental_connection() to more easily connect your Streamlit apps to data and APIs with just a few lines of code. This page provides a basic example of using the feature and then focuses on advanced usage.\\n\\nFor a comprehensive overview of this feature, check out this video tutorial by Joshua Carroll, Streamlit's Product Manager for Developer Experience. You'll learn about the feature's utility in creating and managing data connections within your apps by using real-world examples.\\n\\nBasic usage\\n\\nFor basic startup and usage examples, read up on the relevant data source tutorial or our blog post introducing st.experimental_connection. Streamlit has built-in connections to SQL dialects and Snowflake Snowpark. We also maintain installable connections for Cloud File Storage and Google Sheets.\\n\\nIf you are just starting, the best way to learn is to pick a data source you can access and get a minimal example working from one of the pages above 👆. Here, we will provide an ultra-minimal usage example for using a SQLite database. From there, the rest of this page will focus on advanced usage.\\n\\nA simple starting point - using a local SQLite database\\n\\nA local SQLite database could be useful for your app's semi-persistent data storage.\", metadata={'source': 'docs/content/library/advanced-features/connecting-to-data.md'}),\n",
       " Document(page_content='Community Cloud apps do not guarantee the persistence of local file storage, so the platform may delete data stored using this technique at any time.\\n\\nTo see the example below running live, check out the interactive demo below:\\n\\nStep 1: Install prerequisite library - SQLAlchemy\\n\\nAll SQLConnections in Streamlit use SQLAlchemy. For most other SQL dialects, you also need to install the driver. But the SQLite driver ships with python3, so it isn\\'t necessary.\\n\\nbash\\npip install SQLAlchemy==1.4.0\\n\\nStep 2: Set a database URL in your Streamlit secrets.toml file\\n\\nCreate a directory and file .streamlit/secrets.toml in the same directory your app will run from. Add the following to the file.\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.pets_db]\\nurl = \"sqlite:///pets.db\"\\n```\\n\\nStep 3: Use the connection in your app\\n\\nThe following app creates a connection to the database, uses it to create a table and insert some data, then queries the data back and displays it in a data frame.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nCreate the SQL connection to pets_db as specified in your secrets file.\\n\\nconn = st.experimental_connection(\\'pets_db\\', type=\\'sql\\')\\n\\nInsert some data with conn.session.\\n\\nwith conn.session as s:\\n    s.execute(\\'CREATE TABLE IF NOT EXISTS pet_owners (person TEXT, pet TEXT);\\')\\n    s.execute(\\'DELETE FROM pet_owners;\\')\\n    pet_owners = {\\'jerry\\': \\'fish\\', \\'barbara\\': \\'cat\\', \\'alex\\': \\'puppy\\'}\\n    for k in pet_owners:\\n        s.execute(\\n            \\'INSERT INTO pet_owners (person, pet) VALUES (:owner, :pet);\\',\\n            params=dict(owner=k, pet=pet_owners[k])\\n        )\\n    s.commit()\\n\\nQuery and display the data you inserted\\n\\npet_owners = conn.query(\\'select * from pet_owners\\')\\nst.dataframe(pet_owners)\\n```\\n\\nIn this example, we didn\\'t set a ttl= value on the call to conn.query(), meaning Streamlit caches the result indefinitely as long as the app server runs.\\n\\nNow, on to more advanced topics! 🚀\\n\\nAdvanced topics', metadata={'source': 'docs/content/library/advanced-features/connecting-to-data.md'}),\n",
       " Document(page_content='Advanced topics\\n\\nGlobal secrets, managing multiple apps and multiple data stores\\n\\nStreamlit supports a global secrets file specified in the user\\'s home directory, such as ~/.streamlit/secrets.toml. If you build or manage multiple apps, we recommend using a global credential or secret file for local development across apps. With this approach, you only need to set up and manage your credentials in one place, and connecting a new app to your existing data sources is effectively a one-liner. It also reduces the risk of accidentally checking in your credentials to git since they don\\'t need to exist in the project repository.\\n\\nFor cases where you have multiple similar data sources that you connect to during local development (such as a local vs. staging database), you can define different connection sections in your secrets or credentials file for different environments and then decide which to use at runtime. st.experimental_connection supports this with the name=env:<MY_NAME_VARIABLE> syntax.\\n\\nE.g., say I have a local and a staging MySQL database and want to connect my app to either at different times. I could create a global secrets file like this:\\n\\n```toml\\n\\n~/.streamlit/secrets.toml\\n\\n[connections.local]\\nurl = \"mysql://me:****@localhost:3306/local_db\"\\n\\n[connections.staging]\\nurl = \"mysql://jdoe:**@staging.acmecorp.com:3306/staging_db\"\\n```\\n\\nThen I can configure my app connection to take its name from a specified environment variable\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nconn = st.experimental_connection(\"env:DB_CONN\", \"sql\")\\ndf = conn.query(\"select * from mytable\")\\n\\n...\\n\\n```\\n\\nNow I can specify whether to connect to local or staging at runtime by setting the DB_CONN environment variable.\\n\\n```bash\\n\\nconnect to local\\n\\nDB_CONN=local streamlit run streamlit_app.py\\n\\nconnect to staging\\n\\nDB_CONN=staging streamlit run streamlit_app.py\\n```\\n\\nAdvanced SQLConnection configuration', metadata={'source': 'docs/content/library/advanced-features/connecting-to-data.md'}),\n",
       " Document(page_content='Advanced SQLConnection configuration\\n\\nThe SQLConnection configuration uses SQLAlchemy create_engine() function. It will take a single URL argument or attempt to construct a URL from several parts (username, database, host, and so on) using SQLAlchemy.engine.URL.create().\\n\\nSeveral popular SQLAlchemy dialects, such as Snowflake and Google BigQuery, can be configured using additional arguments to create_engine() besides the URL. These can be passed as **kwargs to the st.experimental_connection call directly or specified in an additional secrets section called create_engine_kwargs.\\n\\nE.g. snowflake-sqlalchemy takes an additional connect_args argument as a dictionary for configuration that isn’t supported in the URL. These could be specified as follows:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowflake]\\nurl = \"snowflake://@/\"\\n\\n[connections.snowflake.create_engine_kwargs.connect_args]\\nauthenticator = \"externalbrowser\"\\nwarehouse = \"xxx\"\\nrole = \"xxx\"\\n```\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nurl and connect_args from secrets.toml above are picked up and used here\\n\\nconn = st.experimental_connection(\"snowflake\", \"sql\")\\n\\n...\\n\\n```\\n\\nAlternatively, this could be specified entirely in **kwargs.\\n\\n```python\\n\\nstreamlit_app.py\\n\\nimport streamlit as st\\n\\nsecrets.toml is not needed\\n\\nconn = st.experimental_connection(\\n    \"snowflake\",\\n    \"sql\",\\n    url = \"snowflake://@/\",\\n    connect_args = dict(\\n        authenticator = \"externalbrowser\",\\n        warehouse = \"xxx\",\\n        role = \"xxx\",\\n    )\\n)\\n\\n...\\n\\n```\\n\\nYou can also provide both kwargs and secrets.toml values, and they will be merged (typically, kwargs take precedence).\\n\\nConnection considerations in frequently used or long-running apps\\n\\nBy default, connection objects are cached without expiration using st.cache_resource. In most cases this is desired. You can do st.experimental_connection(\\'myconn\\', type=MyConnection, ttl=<N>) if you want the connection object to expire after some time.', metadata={'source': 'docs/content/library/advanced-features/connecting-to-data.md'}),\n",
       " Document(page_content='Many connection types are expected to be long-running or completely stateless, so expiration is unnecessary. Suppose a connection becomes stale (such as a cached token expiring or a server-side connection being closed). In that case, every connection has a reset() method, which will invalidate the cached version and cause Streamlit to recreate the connection the next time it is retrieved\\n\\nst.cache_data without an expiration. When an app can run many different read operations with large results, it can cause high memory usage over time and results to become stale in a long-running app, the same as with any other usage of\\n\\nCaching for more information.\\n\\nFor apps that could get significant concurrent usage, ensure that you understand any thread safety implications of your connection, particularly when using a connection built by a third party. Connections built by Streamlit should provide thread-safe operations by default.\\n\\nBuild your own connection\\n\\nBuilding your own basic connection implementation using an existing driver or SDK is quite straightforward in most cases. However, you can add more complex functionality with further effort. This custom implementation can be a great way to extend support to a new data source and contribute to the Streamlit ecosystem.\\n\\nMaintaining a tailored internal Connection implementation across many apps can be a powerful practice for organizations with frequently used access patterns and data sources.\\n\\nCheck out the Build your own Connection page in the st.experimental connection demo app below for a quick tutorial and working implementation. This demo builds a minimal but very functional Connection on top of DuckDB.\\n\\nThe typical steps are:\\n\\nDeclare the Connection class, extending ExperimentalBaseConnection with the type parameter bound to the underlying connection object:\\n\\n```python\\n   from streamlit.connections import ExperimentalBaseConnection\\n   import duckdb', metadata={'source': 'docs/content/library/advanced-features/connecting-to-data.md'}),\n",
       " Document(page_content=\"class DuckDBConnection(ExperimentalBaseConnection[duckdb.DuckDBPyConnection])\\n   ```\\n\\nImplement the _connect method that reads any kwargs, external config/credential locations, and Streamlit secrets to initialize the underlying connection:\\n\\npython\\n   def _connect(self, **kwargs) -> duckdb.DuckDBPyConnection:\\n       if 'database' in kwargs:\\n           db = kwargs.pop('database')\\n       else:\\n           db = self._secrets['database']\\n       return duckdb.connect(database=db, **kwargs)\\n\\nAdd useful helper methods that make sense for your connection (wrapping them in st.cache_data where caching is desired)\\n\\nConnection-building best practices\\n\\nWe recommend applying the following best practices to make your Connection consistent with the Connections built into Streamlit and the wider Streamlit ecosystem. These practices are especially important for Connections that you intend to distribute publicly.\\n\\nExtend existing drivers or SDKs, and default to semantics that makes sense for their existing users.\\n\\nYou should rarely need to implement complex data access logic from scratch when building a Connection. Use existing popular Python drivers and clients whenever possible. Doing so makes your Connection easier to maintain, more secure, and enables users to get the latest features. E.g. SQLConnection extends SQLAlchemy, FileConnection extends fsspec, GsheetsConnection extends gspread, etc.\\n\\nConsider using access patterns, method/argument naming, and return values that are consistent with the underlying package and familiar to existing users of that package.\\n\\nIntuitive, easy to use read methods.\\n\\nMuch of the power of st.experimental_connection is providing intuitive, easy-to-use read methods that enable app developers to get started quickly. Most connections should expose at least one read method that is:\\n\\nNamed with a simple verb, like read(), query(), or get()\\n\\nWrapped by st.cache_data by default, with at least ttl= argument supported\", metadata={'source': 'docs/content/library/advanced-features/connecting-to-data.md'}),\n",
       " Document(page_content='If the result is in a tabular format, it returns a pandas DataFrame\\n\\nProvides commonly used keyword arguments (such as paging or formatting) with sensible defaults - ideally, the common case requires only 1-2 arguments.\\n\\nConfig, secrets, and precedence in _connect method.\\n\\nEvery Connection should support commonly used connection parameters provided via Streamlit secrets and keyword arguments. The names should match the ones used when initializing or configuring the underlying package.\\n\\nAdditionally, where relevant, Connections should support data source specific configuration through existing standard environment variables or config / credential files. In many cases, the underlying package provides constructors or factory functions that already handle this easily.\\n\\nWhen you can specify the same connection parameters in multiple places, we recommend using the following precedence order when possible (highest to lowest):\\n\\nKeyword arguments specified in the code\\n\\nStreamlit secrets\\n\\ndata source specific configuration (if relevant)\\n\\nHandling thread safety and stale connections.\\n\\nConnections should provide thread-safe operations when practical (which should be most of the time) and clearly document any considerations around this. Most underlying drivers or SDKs should provide thread-safe objects or methods - use these when possible.\\n\\nIf the underlying driver or SDK has a risk of stateful connection objects becoming stale or invalid, consider building a low impact health check or reset/retry pattern into the access methods. The SQLConnection built into Streamlit has a good example of this pattern using tenacity and the built-in Connection.reset() method. An alternate approach is to encourage developers to set an appropriate TTL on the st.experimental_connection() call to ensure it periodically reinitializes the connection object.', metadata={'source': 'docs/content/library/advanced-features/connecting-to-data.md'}),\n",
       " Document(page_content='title: Theming\\nslug: /library/advanced-features/theming\\n\\nTheming\\n\\nIn this guide, we provide examples of how Streamlit page elements are affected\\nby the various theme config options. For a more high-level overview of\\nStreamlit themes, see the Themes section of the\\nmain concepts documentation.\\n\\nStreamlit themes are defined using regular config options: a theme can be set\\nvia command line flag when starting your app using streamlit run or by\\ndefining it in the [theme] section of a .streamlit/config.toml file. For\\nmore information on setting config options, please refer to the\\nStreamlit configuration documentation.\\n\\nThe following config options show the default Streamlit Light theme recreated\\nin the [theme] section of a .streamlit/config.toml file.\\n\\ntoml\\n[theme]\\nprimaryColor=\"#F63366\"\\nbackgroundColor=\"#FFFFFF\"\\nsecondaryBackgroundColor=\"#F0F2F6\"\\ntextColor=\"#262730\"\\nfont=\"sans serif\"\\n\\nLet\\'s go through each of these options, providing screenshots to demonstrate\\nwhat parts of a Streamlit app they affect where needed.\\n\\nprimaryColor\\n\\nprimaryColor defines the accent color most often used throughout a Streamlit\\napp. A few examples of Streamlit widgets that use primaryColor include\\nst.checkbox, st.slider, and st.text_input (when focused).\\n\\nAny CSS color can be used as the value for primaryColor and the other color\\noptions below. This means that theme colors can be specified in hex or with\\nbrowser-supported color names like \"green\", \"yellow\", and\\n\"chartreuse\". They can even be defined in the RGB and HSL formats!\\n\\nbackgroundColor\\n\\nDefines the background color used in the main content area of your app.\\n\\nsecondaryBackgroundColor\\n\\nThis color is used where a second background color is needed for added\\ncontrast. Most notably, it is the sidebar\\'s background color. It is also used \\nas the background color for most interactive widgets.\\n\\ntextColor\\n\\nThis option controls the text color for most of your Streamlit app.\\n\\nfont', metadata={'source': 'docs/content/library/advanced-features/theming.md'}),\n",
       " Document(page_content='font\\n\\nSelects the font used in your Streamlit app. Valid values are \"sans serif\",\\n\"serif\", and \"monospace\". This option defaults to \"sans serif\" if unset\\nor invalid.\\n\\nNote that code blocks are always rendered using the monospace font regardless of\\nthe font selected here.\\n\\nbase\\n\\nAn easy way to define custom themes that make small changes to one of the\\npreset Streamlit themes is to use the base option. Using base, the\\nStreamlit Light theme can be recreated as a custom theme by writing the\\nfollowing:\\n\\ntoml\\n[theme]\\nbase=\"light\"\\n\\nThe base option allows you to specify a preset Streamlit theme that your\\ncustom theme inherits from. Any theme config options not defined in your theme\\nsettings have their values set to those of the base theme. Valid values for\\nbase are \"light\" and \"dark\".\\n\\nFor example, the following theme config defines a custom theme nearly identical\\nto the Streamlit Dark theme, but with a new primaryColor.\\n\\ntoml\\n[theme]\\nbase=\"dark\"\\nprimaryColor=\"purple\"\\n\\nIf base itself is omitted, it defaults to \"light\", so you can define a\\ncustom theme that changes the font of the Streamlit Light theme to serif with\\nthe following config\\n\\ntoml\\n[theme]\\nfont=\"serif\"', metadata={'source': 'docs/content/library/advanced-features/theming.md'}),\n",
       " Document(page_content=\"title: Configuration\\nslug: /library/advanced-features/configuration\\n\\nConfiguration\\n\\nStreamlit provides four different ways to set configuration options. This list is in reverse order of precedence, i.e. command line flags take precedence over environment variables when the same configuration option is provided multiple times.\\n\\nIf changes to .streamlit/config.toml are made while the app is running, the server needs to be restarted for changes to be reflected in the app.\\n\\nIn a global config file at ~/.streamlit/config.toml for macOS/Linux or %userprofile%/.streamlit/config.toml for Windows:\\n\\ntoml\\n   [server]\\n   port = 80\\n\\nIn a per-project config file at $CWD/.streamlit/config.toml, where\\n   $CWD is the folder you're running Streamlit from.\\n\\nThrough STREAMLIT_* environment variables, such as:\\n\\nbash\\n   export STREAMLIT_SERVER_PORT=80\\n   export STREAMLIT_SERVER_COOKIE_SECRET=dontforgottochangeme\\n\\nAs flags on the command line when running streamlit run:\\n\\nbash\\n   streamlit run your_script.py --server.port 80\\n\\nTo set configuration options on Streamlit Community Cloud, read Optionally, add a configuration file in the Streamlit Community Cloud docs.\\n\\nTelemetry\\n\\nAs mentioned during the installation process, Streamlit collects usage statistics. You can find out\\nmore by reading our Privacy Notice, but the high-level\\nsummary is that although we collect telemetry data we cannot see and do not store information\\ncontained in Streamlit apps.\\n\\nIf you'd like to opt out of usage statistics, add the following to your config file:\\n\\ntoml\\n[browser]\\ngatherUsageStats = false\\n\\nTheming\\n\\nYou can change the base colors of your app using the [theme] section of the configuration system.\\nTo learn more, see Theming.\\n\\nView all configuration options\\n\\nAs described in Command-line options, you can\\nview all available configuration option using:\\n\\nbash\\nstreamlit config show\\n\\nBelow are all the sections and options you can have in your .streamlit/config.toml file:\\n\\nGlobal\\n\\n```toml\\n[global]\", metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='Global\\n\\n```toml\\n[global]\\n\\nBy default, Streamlit checks if the Python watchdog module is available\\n\\nand, if not, prints a warning asking for you to install it. The watchdog\\n\\nmodule is not required, but highly recommended. It improves Streamlit\\'s\\n\\nability to detect changes to files in your filesystem.\\n\\nIf you\\'d like to turn off this warning, set this to True.\\n\\nDefault: false\\n\\ndisableWatchdogWarning = false\\n\\nBy default, Streamlit displays a warning when a user sets both a widget\\n\\ndefault value in the function defining the widget and a widget value via\\n\\nthe widget\\'s key in st.session_state.\\n\\nIf you\\'d like to turn off this warning, set this to True.\\n\\nDefault: false\\n\\ndisableWidgetStateDuplicationWarning = false\\n\\nIf True, will show a warning when you run a Streamlit-enabled script\\n\\nvia \"python my_script.py\".\\n\\nDefault: true\\n\\nshowWarningOnDirectExecution = true\\n\\nDataFrame serialization.\\n\\nAcceptable values:\\n\\n\\'legacy\\' : Serialize DataFrames using Streamlit\\'s custom format. Slow\\n\\nbut battle-tested.\\n\\n\\'arrow\\'  : Serialize DataFrames using Apache Arrow. Much faster and\\n\\nversatile.\\n\\nDefault: \"arrow\"\\n\\ndataFrameSerialization = \"arrow\"\\n```\\n\\nLogger\\n\\n```toml\\n[logger]\\n\\nLevel of logging: \\'error\\', \\'warning\\', \\'info\\', or \\'debug\\'.\\n\\nDefault: \\'info\\'\\n\\nlevel = \"info\"\\n\\nString format for logging messages. If logger.datetimeFormat is set,\\n\\nlogger messages will default to %(asctime)s.%(msecs)03d %(message)s. See\\n\\nPython\\'s documentation for available attributes:\\n\\nhttps://docs.python.org/2.6/library/logging.html#formatter-objects\\n\\nDefault: \"%(asctime)s %(message)s\"\\n\\nmessageFormat = \"%(asctime)s %(message)s\"\\n```\\n\\nClient\\n\\n```toml\\n[client]\\n\\nWhether to enable st.cache. This does not affect st.cache_data or\\n\\nst.cache_resource.\\n\\nDefault: true\\n\\ncaching = true\\n\\nIf false, makes your Streamlit script not draw to a\\n\\nStreamlit app.\\n\\nDefault: true\\n\\ndisplayEnabled = true\\n\\nControls whether uncaught app exceptions and deprecation warnings\\n\\nare displayed in the browser. By default, this is set to True and', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='Streamlit displays app exceptions and associated tracebacks, and\\n\\ndeprecation warnings, in the browser.\\n\\nIf set to False, deprecation warnings and full exception messages\\n\\nwill print to the console only. Exceptions will still display in the\\n\\nbrowser with a generic error message. For now, the exception type and\\n\\ntraceback show in the browser also, but they will be removed in the\\n\\nfuture.\\n\\nDefault: true\\n\\nshowErrorDetails = true\\n\\nChange the visibility of items in the toolbar, options menu,\\n\\nand settings dialog (top right of the app).\\n\\nAllowed values:\\n\\n\"auto\"      : Show the developer options if the app is accessed through\\n\\nlocalhost or through Streamlit Community Cloud as a developer.\\n\\nHide them otherwise.\\n\\n\"developer\" : Show the developer options.\\n\\n\"viewer\"    : Hide the developer options.\\n\\n\"minimal\"   : Show only options set externally (e.g. through\\n\\nStreamlit Community Cloud) or through st.set_page_config.\\n\\nIf there are no options left, hide the menu.\\n\\nDefault: \"auto\"\\n\\ntoolbarMode = \"auto\"\\n```\\n\\nRunner\\n\\n```toml\\n[runner]\\n\\nAllows you to type a variable or string by itself in a single line of\\n\\nPython code to write it to the app.\\n\\nDefault: true\\n\\nmagicEnabled = true\\n\\nInstall a Python tracer to allow you to stop or pause your script at\\n\\nany point and introspect it. As a side-effect, this slows down your\\n\\nscript\\'s execution.\\n\\nDefault: false\\n\\ninstallTracer = false\\n\\nSets the MPLBACKEND environment variable to Agg inside Streamlit to\\n\\nprevent Python crashing.\\n\\nDefault: true\\n\\nfixMatplotlib = true\\n\\nRun the Python Garbage Collector after each script execution. This\\n\\ncan help avoid excess memory use in Streamlit apps, but could\\n\\nintroduce delay in rerunning the app script for high-memory-use\\n\\napplications.\\n\\nDefault: true\\n\\npostScriptGC = true\\n\\nHandle script rerun requests immediately, rather than waiting for script\\n\\nexecution to reach a yield point. This makes Streamlit much more\\n\\nresponsive to user interaction, but it can lead to race conditions in', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='apps that mutate session_state data outside of explicit session_state\\n\\nassignment statements.\\n\\nDefault: true\\n\\nfastReruns = true\\n\\nRaise an exception after adding unserializable data to Session State.\\n\\nSome execution environments may require serializing all data in Session\\n\\nState, so it may be useful to detect incompatibility during development,\\n\\nor when the execution environment will stop supporting it in the future.\\n\\nDefault: false\\n\\nenforceSerializableSessionState = false\\n```\\n\\nServer\\n\\n```toml\\n[server]\\n\\nList of folders that should not be watched for changes. This\\n\\nimpacts both \"Run on Save\" and @st.cache.\\n\\nRelative paths will be taken as relative to the current working directory.\\n\\nExample: [\\'/home/user1/env\\', \\'relative/path/to/folder\\']\\n\\nDefault: []\\n\\nfolderWatchBlacklist = []\\n\\nChange the type of file watcher used by Streamlit, or turn it off\\n\\ncompletely.\\n\\nAllowed values:\\n\\n\"auto\"     : Streamlit will attempt to use the watchdog module, and\\n\\nfalls back to polling if watchdog is not available.\\n\\n\"watchdog\" : Force Streamlit to use the watchdog module.\\n\\n\"poll\"     : Force Streamlit to always use polling.\\n\\n\"none\"     : Streamlit will not watch files.\\n\\nDefault: \"auto\"\\n\\nfileWatcherType = \"auto\"\\n\\nSymmetric key used to produce signed cookies. If deploying on multiple\\n\\nreplicas, this should be set to the same value across all replicas to ensure\\n\\nthey all share the same secret.\\n\\nDefault: randomly generated secret key.\\n\\ncookieSecret = \"a-random-key-appears-here\"\\n\\nIf false, will attempt to open a browser window on start.\\n\\nDefault: false unless (1) we are on a Linux box where DISPLAY is unset, or\\n\\n(2) we are running in the Streamlit Atom plugin.\\n\\nheadless = false\\n\\nAutomatically rerun script when the file is modified on disk.\\n\\nDefault: false\\n\\nrunOnSave = false\\n\\nThe address where the server will listen for client and browser\\n\\nconnections. Use this if you want to bind the server to a specific address.\\n\\nIf set, the server will only be accessible from this address, and not from', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='any aliases (like localhost).\\n\\nDefault: (unset)\\n\\naddress =\\n\\nThe port where the server will listen for browser connections.\\n\\nDefault: 8501\\n\\nport = 8501\\n\\nThe base path for the URL where Streamlit should be served from.\\n\\nDefault: \"\"\\n\\nbaseUrlPath = \"\"\\n\\nEnables support for Cross-Origin Resource Sharing (CORS) protection, for\\n\\nadded security.\\n\\nDue to conflicts between CORS and XSRF, if server.enableXsrfProtection is\\n\\non and server.enableCORS is off at the same time, we will prioritize\\n\\nserver.enableXsrfProtection.\\n\\nDefault: true\\n\\nenableCORS = true\\n\\nEnables support for Cross-Site Request Forgery (XSRF) protection, for added\\n\\nsecurity.\\n\\nDue to conflicts between CORS and XSRF, if server.enableXsrfProtection is\\n\\non and server.enableCORS is off at the same time, we will prioritize\\n\\nserver.enableXsrfProtection.\\n\\nDefault: true\\n\\nenableXsrfProtection = true\\n\\nMax size, in megabytes, for files uploaded with the file_uploader.\\n\\nDefault: 200\\n\\nmaxUploadSize = 200\\n\\nMax size, in megabytes, of messages that can be sent via the WebSocket\\n\\nconnection.\\n\\nDefault: 200\\n\\nmaxMessageSize = 200\\n\\nEnables support for websocket compression.\\n\\nDefault: false\\n\\nenableWebsocketCompression = false\\n\\nEnable serving files from a static directory in the running app\\'s\\n\\ndirectory.\\n\\nDefault: false\\n\\nenableStaticServing = false\\n\\nServer certificate file for connecting via HTTPS.\\n\\nMust be set at the same time as \"server.sslKeyFile\".\\n\\n[\\'DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through\\n\\nsecurity audits or performance tests. For the production environment, we\\n\\nrecommend performing SSL termination by the load balancer or the reverse\\n\\nproxy.\\']\\n\\nsslCertFile =\\n\\nCryptographic key file for connecting via HTTPS.\\n\\nMust be set at the same time as \"server.sslCertFile\".\\n\\n[\\'DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through\\n\\nsecurity audits or performance tests. For the production environment, we\\n\\nrecommend performing SSL termination by the load balancer or the reverse\\n\\nproxy.\\']', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='proxy.\\']\\n\\nsslKeyFile =\\n\\n```\\n\\nBrowser\\n\\n```toml\\n[browser]\\n\\nInternet address where users should point their browsers in order to\\n\\nconnect to the app. Can be IP address or DNS name and path.\\n\\nThis is used to:\\n\\nSet the correct URL for CORS and XSRF protection purposes.\\n\\nShow the URL on the terminal\\n\\nOpen the browser\\n\\nDefault: \"localhost\"\\n\\nserverAddress = \"localhost\"\\n\\nWhether to send usage statistics to Streamlit.\\n\\nDefault: true\\n\\ngatherUsageStats = true\\n\\nPort where users should point their browsers in order to connect to the\\n\\napp.\\n\\nThis is used to:\\n\\nSet the correct URL for CORS and XSRF protection purposes.\\n\\nShow the URL on the terminal\\n\\nOpen the browser\\n\\nDefault: whatever value is set in server.port.\\n\\nserverPort = 8501\\n```\\n\\nMapbox\\n\\n```toml\\n[mapbox]\\n\\nConfigure Streamlit to use a custom Mapbox\\n\\ntoken for elements like st.pydeck_chart and st.map.\\n\\nTo get a token for yourself, create an account at\\n\\nhttps://mapbox.com. It\\'s free (for moderate usage levels)!\\n\\nDefault: \"\"\\n\\ntoken = \"\"\\n```\\n\\nDeprecation\\n\\n```toml\\n[deprecation]\\n\\nSet to false to disable the deprecation warning for the file uploader\\n\\nencoding.\\n\\nDefault: true\\n\\nshowfileUploaderEncoding = true\\n\\nSet to false to disable the deprecation warning for using the global pyplot\\n\\ninstance.\\n\\nDefault: true\\n\\nshowPyplotGlobalUse = true\\n```\\n\\nTheme\\n\\n```toml\\n[theme]\\n\\nThe preset Streamlit theme that your custom theme inherits from.\\n\\nOne of \"light\" or \"dark\".\\n\\nbase =\\n\\nPrimary accent color for interactive elements.\\n\\nprimaryColor =\\n\\nBackground color for the main content area.\\n\\nbackgroundColor =\\n\\nBackground color used for the sidebar and most interactive widgets.\\n\\nsecondaryBackgroundColor =\\n\\nColor used for almost all text.\\n\\ntextColor =\\n\\nFont family for all text in the app, except code blocks. One of \"sans serif\",\\n\\n\"serif\", or \"monospace\".\\n\\nfont =\\n\\n```\\n\\n```toml\\n[global]\\n\\nBy default, Streamlit checks if the Python watchdog module is available\\n\\nand, if not, prints a warning asking for you to install it. The watchdog', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='module is not required, but highly recommended. It improves Streamlit\\'s\\n\\nability to detect changes to files in your filesystem.\\n\\nIf you\\'d like to turn off this warning, set this to True.\\n\\nDefault: false\\n\\ndisableWatchdogWarning = false\\n\\nBy default, Streamlit displays a warning when a user sets both a widget\\n\\ndefault value in the function defining the widget and a widget value via\\n\\nthe widget\\'s key in st.session_state.\\n\\nIf you\\'d like to turn off this warning, set this to True.\\n\\nDefault: false\\n\\ndisableWidgetStateDuplicationWarning = false\\n\\nIf True, will show a warning when you run a Streamlit-enabled script\\n\\nvia \"python my_script.py\".\\n\\nDefault: true\\n\\nshowWarningOnDirectExecution = true\\n\\nDataFrame serialization.\\n\\nAcceptable values:\\n\\n\\'legacy\\' : Serialize DataFrames using Streamlit\\'s custom format. Slow\\n\\nbut battle-tested.\\n\\n\\'arrow\\'  : Serialize DataFrames using Apache Arrow. Much faster and\\n\\nversatile.\\n\\nDefault: \"arrow\"\\n\\ndataFrameSerialization = \"arrow\"\\n\\n[logger]\\n\\nLevel of logging: \\'error\\', \\'warning\\', \\'info\\', or \\'debug\\'.\\n\\nDefault: \\'info\\'\\n\\nlevel = \"info\"\\n\\nString format for logging messages. If logger.datetimeFormat is set,\\n\\nlogger messages will default to %(asctime)s.%(msecs)03d %(message)s. See\\n\\nPython\\'s documentation for available attributes:\\n\\nhttps://docs.python.org/2.6/library/logging.html#formatter-objects\\n\\nDefault: \"%(asctime)s %(message)s\"\\n\\nmessageFormat = \"%(asctime)s %(message)s\"\\n\\n[client]\\n\\nWhether to enable st.cache. This does not affect st.cache_data or\\n\\nst.cache_resource.\\n\\nDefault: true\\n\\ncaching = true\\n\\nIf false, makes your Streamlit script not draw to a\\n\\nStreamlit app.\\n\\nDefault: true\\n\\ndisplayEnabled = true\\n\\nControls whether uncaught app exceptions and deprecation warnings\\n\\nare displayed in the browser. By default, this is set to True and\\n\\nStreamlit displays app exceptions and associated tracebacks, and\\n\\ndeprecation warnings, in the browser.\\n\\nIf set to False, deprecation warnings and full exception messages', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='will print to the console only. Exceptions will still display in the\\n\\nbrowser with a generic error message. For now, the exception type and\\n\\ntraceback show in the browser also, but they will be removed in the\\n\\nfuture.\\n\\nDefault: true\\n\\nshowErrorDetails = true\\n\\nChange the visibility of items in the toolbar, options menu,\\n\\nand settings dialog (top right of the app).\\n\\nAllowed values:\\n\\n\"auto\"      : Show the developer options if the app is accessed through\\n\\nlocalhost or through Streamlit Community Cloud as a developer.\\n\\nHide them otherwise.\\n\\n\"developer\" : Show the developer options.\\n\\n\"viewer\"    : Hide the developer options.\\n\\n\"minimal\"   : Show only options set externally (e.g. through\\n\\nStreamlit Community Cloud) or through st.set_page_config.\\n\\nIf there are no options left, hide the menu.\\n\\nDefault: \"auto\"\\n\\ntoolbarMode = \"auto\"\\n\\n[runner]\\n\\nAllows you to type a variable or string by itself in a single line of\\n\\nPython code to write it to the app.\\n\\nDefault: true\\n\\nmagicEnabled = true\\n\\nInstall a Python tracer to allow you to stop or pause your script at\\n\\nany point and introspect it. As a side-effect, this slows down your\\n\\nscript\\'s execution.\\n\\nDefault: false\\n\\ninstallTracer = false\\n\\nSets the MPLBACKEND environment variable to Agg inside Streamlit to\\n\\nprevent Python crashing.\\n\\nDefault: true\\n\\nfixMatplotlib = true\\n\\nRun the Python Garbage Collector after each script execution. This\\n\\ncan help avoid excess memory use in Streamlit apps, but could\\n\\nintroduce delay in rerunning the app script for high-memory-use\\n\\napplications.\\n\\nDefault: true\\n\\npostScriptGC = true\\n\\nHandle script rerun requests immediately, rather than waiting for script\\n\\nexecution to reach a yield point. This makes Streamlit much more\\n\\nresponsive to user interaction, but it can lead to race conditions in\\n\\napps that mutate session_state data outside of explicit session_state\\n\\nassignment statements.\\n\\nDefault: true\\n\\nfastReruns = true\\n\\nRaise an exception after adding unserializable data to Session State.', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='Some execution environments may require serializing all data in Session\\n\\nState, so it may be useful to detect incompatibility during development,\\n\\nor when the execution environment will stop supporting it in the future.\\n\\nDefault: false\\n\\nenforceSerializableSessionState = false\\n\\n[server]\\n\\nList of folders that should not be watched for changes. This\\n\\nimpacts both \"Run on Save\" and @st.cache.\\n\\nRelative paths will be taken as relative to the current working directory.\\n\\nExample: [\\'/home/user1/env\\', \\'relative/path/to/folder\\']\\n\\nDefault: []\\n\\nfolderWatchBlacklist = []\\n\\nChange the type of file watcher used by Streamlit, or turn it off\\n\\ncompletely.\\n\\nAllowed values:\\n\\n\"auto\"     : Streamlit will attempt to use the watchdog module, and\\n\\nfalls back to polling if watchdog is not available.\\n\\n\"watchdog\" : Force Streamlit to use the watchdog module.\\n\\n\"poll\"     : Force Streamlit to always use polling.\\n\\n\"none\"     : Streamlit will not watch files.\\n\\nDefault: \"auto\"\\n\\nfileWatcherType = \"auto\"\\n\\nSymmetric key used to produce signed cookies. If deploying on multiple\\n\\nreplicas, this should be set to the same value across all replicas to ensure\\n\\nthey all share the same secret.\\n\\nDefault: randomly generated secret key.\\n\\ncookieSecret = \"a-random-key-appears-here\"\\n\\nIf false, will attempt to open a browser window on start.\\n\\nDefault: false unless (1) we are on a Linux box where DISPLAY is unset, or\\n\\n(2) we are running in the Streamlit Atom plugin.\\n\\nheadless = false\\n\\nAutomatically rerun script when the file is modified on disk.\\n\\nDefault: false\\n\\nrunOnSave = false\\n\\nThe address where the server will listen for client and browser\\n\\nconnections. Use this if you want to bind the server to a specific address.\\n\\nIf set, the server will only be accessible from this address, and not from\\n\\nany aliases (like localhost).\\n\\nDefault: (unset)\\n\\naddress =\\n\\nThe port where the server will listen for browser connections.\\n\\nDefault: 8501\\n\\nport = 8501\\n\\nThe base path for the URL where Streamlit should be served from.', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='Default: \"\"\\n\\nbaseUrlPath = \"\"\\n\\nEnables support for Cross-Origin Resource Sharing (CORS) protection, for\\n\\nadded security.\\n\\nDue to conflicts between CORS and XSRF, if server.enableXsrfProtection is\\n\\non and server.enableCORS is off at the same time, we will prioritize\\n\\nserver.enableXsrfProtection.\\n\\nDefault: true\\n\\nenableCORS = true\\n\\nEnables support for Cross-Site Request Forgery (XSRF) protection, for added\\n\\nsecurity.\\n\\nDue to conflicts between CORS and XSRF, if server.enableXsrfProtection is\\n\\non and server.enableCORS is off at the same time, we will prioritize\\n\\nserver.enableXsrfProtection.\\n\\nDefault: true\\n\\nenableXsrfProtection = true\\n\\nMax size, in megabytes, for files uploaded with the file_uploader.\\n\\nDefault: 200\\n\\nmaxUploadSize = 200\\n\\nMax size, in megabytes, of messages that can be sent via the WebSocket\\n\\nconnection.\\n\\nDefault: 200\\n\\nmaxMessageSize = 200\\n\\nEnables support for websocket compression.\\n\\nDefault: false\\n\\nenableWebsocketCompression = false\\n\\nEnable serving files from a static directory in the running app\\'s\\n\\ndirectory.\\n\\nDefault: false\\n\\nenableStaticServing = false\\n\\nServer certificate file for connecting via HTTPS.\\n\\nMust be set at the same time as \"server.sslKeyFile\".\\n\\n[\\'DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through\\n\\nsecurity audits or performance tests. For the production environment, we\\n\\nrecommend performing SSL termination by the load balancer or the reverse\\n\\nproxy.\\']\\n\\nsslCertFile =\\n\\nCryptographic key file for connecting via HTTPS.\\n\\nMust be set at the same time as \"server.sslCertFile\".\\n\\n[\\'DO NOT USE THIS OPTION IN A PRODUCTION ENVIRONMENT. It has not gone through\\n\\nsecurity audits or performance tests. For the production environment, we\\n\\nrecommend performing SSL termination by the load balancer or the reverse\\n\\nproxy.\\']\\n\\nsslKeyFile =\\n\\n[browser]\\n\\nInternet address where users should point their browsers in order to\\n\\nconnect to the app. Can be IP address or DNS name and path.\\n\\nThis is used to:', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content='This is used to:\\n\\nSet the correct URL for CORS and XSRF protection purposes.\\n\\nShow the URL on the terminal\\n\\nOpen the browser\\n\\nDefault: \"localhost\"\\n\\nserverAddress = \"localhost\"\\n\\nWhether to send usage statistics to Streamlit.\\n\\nDefault: true\\n\\ngatherUsageStats = true\\n\\nPort where users should point their browsers in order to connect to the\\n\\napp.\\n\\nThis is used to:\\n\\nSet the correct URL for CORS and XSRF protection purposes.\\n\\nShow the URL on the terminal\\n\\nOpen the browser\\n\\nDefault: whatever value is set in server.port.\\n\\nserverPort = 8501\\n\\n[mapbox]\\n\\nConfigure Streamlit to use a custom Mapbox\\n\\ntoken for elements like st.pydeck_chart and st.map.\\n\\nTo get a token for yourself, create an account at\\n\\nhttps://mapbox.com. It\\'s free (for moderate usage levels)!\\n\\nDefault: \"\"\\n\\ntoken = \"\"\\n\\n[deprecation]\\n\\nSet to false to disable the deprecation warning for the file uploader\\n\\nencoding.\\n\\nDefault: true\\n\\nshowfileUploaderEncoding = true\\n\\nSet to false to disable the deprecation warning for using the global pyplot\\n\\ninstance.\\n\\nDefault: true\\n\\nshowPyplotGlobalUse = true\\n\\n[theme]\\n\\nThe preset Streamlit theme that your custom theme inherits from.\\n\\nOne of \"light\" or \"dark\".\\n\\nbase =\\n\\nPrimary accent color for interactive elements.\\n\\nprimaryColor =\\n\\nBackground color for the main content area.\\n\\nbackgroundColor =\\n\\nBackground color used for the sidebar and most interactive widgets.\\n\\nsecondaryBackgroundColor =\\n\\nColor used for almost all text.\\n\\ntextColor =\\n\\nFont family for all text in the app, except code blocks. One of \"sans serif\",\\n\\n\"serif\", or \"monospace\".\\n\\nfont =\\n\\n```', metadata={'source': 'docs/content/library/advanced-features/configuration.md'}),\n",
       " Document(page_content=\"title: Pre-release features\\nslug: /library/advanced-features/prerelease\\n\\nPre-release features\\n\\nAt Streamlit, we like to move quick while keeping things stable. In our latest effort to move even faster without sacrificing stability, we're offering our bold and fearless users two ways to try out Streamlit's bleeding-edge features:\\n\\nExperimental features\\n\\nNightly releases\\n\\nExperimental Features\\n\\nLess stable Streamlit features have one naming convention: st.experimental_. This distinction is a prefix we attach to our command names to make sure their status is clear to everyone.\\n\\nHere's a quick rundown of what you get from each naming convention:\\n\\nst: this is where our core features like st.write and st.dataframe live. If we ever make backward-incompatible changes to these, they will take place gradually and with months of announcements and warnings.\\n\\nexperimental: this is where we'll put all new features that may or may not ever make it into Streamlit core. This gives you a chance to try the next big thing we're cooking up weeks or months before we're ready to stabilize its API. We don't know whether these features have a future, but we want you to have access to everything we're trying, and work with us to figure them out.\\n\\nFeatures with the experimental_ naming convention are things that we're still working on or trying\\nto understand. If these features are successful, at some point they'll become part of Streamlit\\ncore. If unsuccessful, these features are removed without much notice. While in experimental, a feature's API and behaviors may not be stable, and it's possible they could change in ways that aren't backward-compatible.\\n\\nExperimental features and their APIs may change or be removed at any time.\\n\\nThe lifecycle of an experimental feature\\n\\nA feature is added with the experimental_ prefix.\\n\\nThe feature is potentially tweaked over time, with possible API/behavior breakages.\\n\\nIf successful, we promote the feature to Streamlit core and remove it from experimental_:\", metadata={'source': 'docs/content/library/advanced-features/prerelease-features.md'}),\n",
       " Document(page_content=\"a. The feature's API stabilizes and the feature is cloned without the experimental_ prefix, so it exists as both st and experimental_. At this point, users will see a warning when using the version of the feature with the experimental_ prefix -- but the feature will still work.\\n\\nb. At some point, the code of the experimental_-prefixed feature is removed, but there will still be a stub of the function prefixed with experimental_ that shows an error with appropriate instructions.\\n\\nc. Finally, at a later date the experimental_ version is removed.\\n\\nIf unsuccessful, the feature is removed without much notice and we leave a stub in experimental_ that shows an error with instructions.\\n\\nNightly releases\\n\\nIn addition to experimental features, we offer another way to try out Streamlit's newest features: nightly releases.\\n\\nAt the end of each day (at night 🌛), our bots run automated tests against the latest Streamlit code and, if everything looks good, it publishes them as the streamlit-nightly package. This means the nightly build includes all our latest features, bug fixes, and other enhancements on the same day they land on our codebase.\\n\\nHow does this differ from official releases?\\n\\nOfficial Streamlit releases go not only through both automated tests but also rigorous manual testing, while nightly releases only have automated tests. It's important to keep in mind that new features introduced in nightly releases often lack polish. In our official releases, we always make double-sure all new features are ready for prime time.\\n\\nHow do I use the nightly release?\\n\\nAll you need to do is install the streamlit-nightly package:\\n\\nbash\\npip uninstall streamlit\\npip install streamlit-nightly --upgrade\\n\\nYou should never have both streamlit and streamlit-nightly installed in the same environment!\\n\\nWhy should I use the nightly release?\\n\\nBecause you can't wait for official releases, and you want to help us find bugs early!\\n\\nWhy shouldn't I use the nightly release?\", metadata={'source': 'docs/content/library/advanced-features/prerelease-features.md'}),\n",
       " Document(page_content=\"Why shouldn't I use the nightly release?\\n\\nWhile our automated tests have high coverage, there's still a significant likelihood that there will be some bugs in the nightly code.\\n\\nCan I choose which nightly release I want to install?\\n\\nIf you'd like to use a specific version, you can find the version number in our Release history. Specify the desired version using pip as usual: pip install streamlit-nightly==x.yy.zz-123456.\\n\\nCan I compare changes between releases?\\n\\nIf you'd like to review the changes for a nightly release, you can use the comparison tool on GitHub.\", metadata={'source': 'docs/content/library/advanced-features/prerelease-features.md'}),\n",
       " Document(page_content=\"title: HTTPS support\\nslug: /library/advanced-features/https-support\\n\\nHTTPS support\\n\\nMany apps need to be accessed with SSL / TLS protocol or https://.\\n\\nWe recommend performing SSL termination in a reverse proxy or load balancer for self-hosted and production use cases, not directly in the app. Streamlit Community Cloud uses this approach, and every major cloud and app hosting platform should allow you to configure it and provide extensive documentation. You can find some of these platforms in our Deployment tutorials.\\n\\nTo terminate SSL in your Streamlit app, you must configure server.sslCertFile and server.sslKeyFile. Learn how to set config options in Configuration.\\n\\nDetails on usage\\n\\nThe configuration value should be a local file path to a cert file and key file. These must be available at the time the app starts.\\n\\nBoth server.sslCertFile and server.sslKeyFile must be specified. If only one is specified, your app will exit with an error.\\n\\nThis feature will not work in Community Cloud. Community Cloud already serves your app with TLS.\\n\\nIn a production environment, we recommend performing SSL termination by the load balancer or the reverse proxy, not using this option. The use of this option in Streamlit has not gone through extensive security audits or performance tests.\\n\\nExample usage\\n\\n```toml\\n\\n.streamlit/config.toml\\n\\n[server]\\nsslCertFile = '/path/to/certchain.pem'\\nsslKeyFile = '/path/to/private.key'\\n```\", metadata={'source': 'docs/content/library/advanced-features/https.md'}),\n",
       " Document(page_content='title: Button behavior and examples\\nslug: /library/advanced-features/button-behavior-and-examples\\n\\nButton behavior and examples\\n\\nSummary\\n\\nButtons created with st.button do not retain state. They return True on the script rerun resulting from their click and immediately return to False on the next script rerun. If a displayed element is nested inside if st.button(\\'Click me\\'):, the element will be visible when the button is clicked and disappear as soon as the user takes their next action. This is because the script reruns and the button return value becomes False.\\n\\nIn this guide, we will illustrate the use of buttons and explain common misconceptions. Read on to see a variety of examples that expand on st.button using st.session_state. Anti-patterns are included at the end. Go ahead and pull up your favorite code editor so you can streamlit run the examples as you read. Check out Streamlit\\'s Main concepts if you haven\\'t run your own Streamlit scripts yet.\\n\\nWhen to use if st.button()\\n\\nWhen code is conditioned on a button\\'s value, it will execute once in response to the button being clicked and not again (until the button is clicked again).\\n\\nGood to nest inside buttons:\\n\\nTransient messages that immediately disappear.\\n\\nOnce-per-click processes that saves data to session state, a file, or\\n  a database.\\n\\nBad to nest inside buttons:\\n\\nDisplayed items that should persist as the user continues.\\n\\nOther widgets which cause the script to rerun when used.\\n\\nProcesses that neither modify session state nor write to a file/database.*\\n\\nThis can be appropriate when disposable results are desired. If you\\nhave a \"Validate\" button, that could be a process conditioned directly on a\\nbutton. It could be used to create an alert to say \\'Valid\\' or \\'Invalid\\' with no\\nneed to keep that info.\\n\\nCommon logic with buttons\\n\\nShow a temporary message with a button\\n\\nIf you want to give the user a quick button to check if an entry is valid, but not keep that check displayed as the user continues.', metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md'}),\n",
       " Document(page_content='In this example, a user can click a button to check if their animal string is in the animal_shelter list. When the user clicks \"Check availability\" they will see \"We have that animal!\" or \"We don\\'t have that animal.\" If they change the animal in st.text_input, the script reruns and the message disappears until they click \"Check availability\" again.\\n\\n```python\\nimport streamlit as st\\n\\nanimal_shelter = [\\'cat\\', \\'dog\\', \\'rabbit\\', \\'bird\\']\\n\\nanimal = st.text_input(\\'Type an animal\\')\\n\\nif st.button(\\'Check availability\\'):\\n    have_it = animal.lower() in animal_shelter\\n    \\'We have that animal!\\' if have_it else \\'We don\\\\\\'t have that animal.\\'\\n```\\n\\nNote: The above example uses magic to render the message on the frontend.\\n\\nStateful button\\n\\nIf you want a clicked button to continue to be True, create a value in st.session_state and use the button to set that value to True in a callback.\\n\\n```python\\nimport streamlit as st\\n\\nif \\'clicked\\' not in st.session_state:\\n    st.session_state.clicked = False\\n\\ndef click_button():\\n    st.session_state.clicked = True\\n\\nst.button(\\'Click me\\', on_click=click_button)\\n\\nif st.session_state.clicked:\\n    # The message and nested widget will remain on the page\\n    st.write(\\'Button clicked!\\')\\n    st.slider(\\'Select a value\\')\\n```\\n\\nToggle button\\n\\nIf you want a button to work like a toggle switch, consider using st.checkbox. Otherwise, you can use a button with a callback function to reverse a boolean value saved in st.session_state.\\n\\nIn this example, we use st.button to toggle another widget on and off. By displaying st.slider conditionally on a value in st.session_state, the user can interact with the slider without it disappearing.\\n\\n```python\\nimport streamlit as st\\n\\nif \\'button\\' not in st.session_state:\\n    st.session_state.button = False\\n\\ndef click_button():\\n    st.session_state.button = not st.session_state.button\\n\\nst.button(\\'Click me\\', on_click=click_button)', metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md'}),\n",
       " Document(page_content='st.button(\\'Click me\\', on_click=click_button)\\n\\nif st.session_state.button:\\n    # The message and nested widget will remain on the page\\n    st.write(\\'Button is on!\\')\\n    st.slider(\\'Select a value\\')\\nelse:\\n    st.write(\\'Button is off!\\')\\n```\\n\\nAlternatively, you can use the value in st.session_state on the slider\\'s disabled parameter.\\n\\n```python\\nimport streamlit as st\\n\\nif \\'button\\' not in st.session_state:\\n    st.session_state.button = False\\n\\ndef click_button():\\n    st.session_state.button = not st.session_state.button\\n\\nst.button(\\'Click me\\', on_click=click_button)\\n\\nst.slider(\\'Select a value\\', disabled=st.session_state.button)\\n```\\n\\nButtons to continue or control stages of a process\\n\\nAnother alternative to nesting content inside a button is to use a value in st.session_state that designates the \"step\" or \"stage\" of a process. In this example, we have four stages in our script: 0. Before the user begins.\\n\\nUser enters their name.\\n\\nUser chooses a color.\\n\\nUser gets a thank-you message.\\n   A button at the beginning advances the stage from 0 to 1. A button at the end resets the stage from 3 to 0. The other widgets used in stage 1 and 2 have callbacks to set the stage. If you have a process with dependant steps and want to keep previous stages visible, such a callback forces a user to retrace subsequent stages if they change an earlier widget.\\n\\n```python\\nimport streamlit as st\\n\\nif \\'stage\\' not in st.session_state:\\n    st.session_state.stage = 0\\n\\ndef set_state(i):\\n    st.session_state.stage = i\\n\\nif st.session_state.stage == 0:\\n    st.button(\\'Begin\\', on_click=set_state, args=[1])\\n\\nif st.session_state.stage >= 1:\\n    name = st.text_input(\\'Name\\', on_change=set_state, args=[2])\\n\\nif st.session_state.stage >= 2:\\n    st.write(f\\'Hello {name}!\\')\\n    color = st.selectbox(\\n        \\'Pick a Color\\',\\n        [None, \\'red\\', \\'orange\\', \\'green\\', \\'blue\\', \\'violet\\'],\\n        on_change=set_state, args=[3]\\n    )\\n    if color is None:\\n        set_state(2)', metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md'}),\n",
       " Document(page_content='if st.session_state.stage >= 3:\\n    st.write(f\\':{color}[Thank you!]\\')\\n    st.button(\\'Start Over\\', on_click=set_state, args=[0])\\n```\\n\\nButtons to modify st.session_state\\n\\nIf you modify st.session_state inside of a button, you must consider where that button is within the script.\\n\\nA slight problem\\n\\nIn this example, we access st.session_state.name both before and after the buttons which modify it. When a button (\"Jane\" or \"John\") is clicked, the script reruns. The info displayed before the buttons lags behind the info written after the button. The data in st.session_state before the button is not updated. When the script executes the button function, that is when the conditional code to update st.session_state creates the change. Thus, this change is reflected after the button.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nif \\'name\\' not in st.session_state:\\n    st.session_state[\\'name\\'] = \\'John Doe\\'\\n\\nst.header(st.session_state[\\'name\\'])\\n\\nif st.button(\\'Jane\\'):\\n    st.session_state[\\'name\\'] = \\'Jane Doe\\'\\n\\nif st.button(\\'John\\'):\\n    st.session_state[\\'name\\'] = \\'John Doe\\'\\n\\nst.header(st.session_state[\\'name\\'])\\n```\\n\\nLogic used in a callback\\n\\nCallbacks are a clean way to modify st.session_state. Callbacks are executed as a prefix to the script rerunning, so the position of the button relative to accessing data is not important.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nif \\'name\\' not in st.session_state:\\n    st.session_state[\\'name\\'] = \\'John Doe\\'\\n\\ndef change_name(name):\\n    st.session_state[\\'name\\'] = name\\n\\nst.header(st.session_state[\\'name\\'])\\n\\nst.button(\\'Jane\\', on_click=change_name, args=[\\'Jane Doe\\'])\\nst.button(\\'John\\', on_click=change_name, args=[\\'John Doe\\'])\\n\\nst.header(st.session_state[\\'name\\'])\\n```\\n\\nLogic nested in a button with a rerun', metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md'}),\n",
       " Document(page_content=\"Logic nested in a button with a rerun\\n\\nAlthough callbacks are often preferred to avoid extra reruns, our first 'John Doe'/'Jane Doe' example can be modified by adding st.experimental_rerun instead. If you need to acces data in st.session_state before the button that modifies it, you can include st.experimental_rerun to rerun the script after the change has been committed. This means the script will rerun twice when a button is clicked.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nif 'name' not in st.session_state:\\n    st.session_state['name'] = 'John Doe'\\n\\nst.header(st.session_state['name'])\\n\\nif st.button('Jane'):\\n    st.session_state['name'] = 'Jane Doe'\\n    st.experimental_rerun()\\n\\nif st.button('John'):\\n    st.session_state['name'] = 'John Doe'\\n    st.experimental_rerun()\\n\\nst.header(st.session_state['name'])\\n```\\n\\nButtons to modify or reset other widgets\\n\\nWhen a button is used to modify or reset another widget, it is the same as the above examples to modify st.session_state. However, an extra consideration exists: you cannot modify a key-value pair in st.session_state if the widget with that key has already been rendered on the page for the current script run.\\n\\nDon't do this!\\n\\n```python\\nimport streamlit as st\\n\\nst.text_input('Name', key='name')\\n\\nThese buttons will error because their nested code changes\\n\\na widget's state after that widget within the script.\\n\\nif st.button('Clear name'):\\n    st.session_state.name = ''\\nif st.button('Streamlit!'):\\n    set_name('Streamlit')\\n```\\n\\nOption 1: Use a key for the button and put the logic before the widget\", metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md'}),\n",
       " Document(page_content=\"If you assign a key to a button, you can condition code on a button's state by using its value in st.session_state. This means that logic depending on your button can be in your script before that button. In the following example, we use the .get() method on st.session_state because the keys for the buttons will not exist when the script runs for the first time. The .get() method will return False if it can't find the key. Otherwise, it will return the value of the key.\\n\\n```python\\nimport streamlit as st\\n\\nUse the get method since the keys won't be in session_state\\n\\non the first script run\\n\\nif st.session_state.get('clear'):\\n    st.session_state['name'] = ''\\nif st.session_state.get('streamlit'):\\n    st.session_state['name'] = 'Streamlit'\\n\\nst.text_input('Name', key='name')\\n\\nst.button('Clear name', key='clear')\\nst.button('Streamlit!', key='streamlit')\\n```\\n\\nOption 2: Use a callback\\n\\n```python\\nimport streamlit as st\\n\\nst.text_input('Name', key='name')\\n\\ndef set_name(name):\\n    st.session_state.name = name\\n\\nst.button('Clear name', on_click=set_name, args=[''])\\nst.button('Streamlit!', on_click=set_name, args=['Streamlit'])\\n```\\n\\nOption 3: Use containers\\n\\nBy using st.container you can have widgets appear in different orders in your script and frontend view (webpage).\\n\\n```python\\nimport streamlit as st\\n\\nbegin = st.container()\\n\\nif st.button('Clear name'):\\n    st.session_state.name = ''\\nif st.button('Streamlit!'):\\n    st.session_state.name = ('Streamlit')\\n\\nThe widget is second in logic, but first in display\\n\\nbegin.text_input('Name', key='name')\\n```\\n\\nButtons to add other widgets dynamically\\n\\n```python\\nimport streamlit as st\\n\\ndef display_input_row(index):\\n    left, middle, right = st.columns(3)\\n    left.text_input('First', key=f'first_{index}')\\n    middle.text_input('Middle', key=f'middle_{index}')\\n    right.text_input('Last', key=f'last_{index}')\\n\\nif 'rows' not in st.session_state:\\n    st.session_state['rows'] = 0\\n\\ndef increase_rows():\\n    st.session_state['rows'] += 1\", metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md'}),\n",
       " Document(page_content='st.button(\\'Add person\\', on_click=increase_rows)\\n\\nfor i in range(st.session_state[\\'rows\\']):\\n    display_input_row(i)\\n\\nShow the results\\n\\nst.subheader(\\'People\\')\\nfor i in range(st.session_state[\\'rows\\']):\\n    st.write(\\n        f\\'Person {i+1}:\\',\\n        st.session_state[f\\'first_{i}\\'],\\n        st.session_state[f\\'middle_{i}\\'],\\n        st.session_state[f\\'last_{i}\\']\\n    )\\n```\\n\\nButtons to handle expensive or file-writing processes\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport time\\n\\ndef expensive_process(option, add):\\n    with st.spinner(\\'Processing...\\'):\\n        time.sleep(5)\\n    df = pd.DataFrame({\\'A\\': [1, 2, 3], \\'B\\': [4, 5, 6], \\'C\\':[7, 8, 9]}) + add\\n    return (df, add)\\n\\ncols = st.columns(2)\\noption = cols[0].selectbox(\\'Select a number\\', options=[\\'1\\', \\'2\\', \\'3\\'])\\nadd = cols[1].number_input(\\'Add a number\\', min_value=0, max_value=10)\\n\\nif \\'processed\\' not in st.session_state:\\n    st.session_state.processed = {}\\n\\nProcess and save results\\n\\nif st.button(\\'Process\\'):\\n    result = expensive_process(option, add)\\n    st.session_state.processed[option] = result\\n\\nif option in st.session_state.processed:\\n    st.write(f\\'Option {option} processed with add {add}\\')\\n    st.write(st.session_state.processed[option][0])\\n```\\n\\nAstute observers may think, \"This feels a little like caching.\" We are only saving results relative to one parameter, but the pattern could easily be expanded to save results relative to both parameters. In that sense, yes, it has some similarities to caching, but also some important differences. When you save results in st.session_state, the results are only available to the current user in their current session. If you use st.cache_data instead, the results are available to all users across all sessions. Furthermore, if you want to update a saved result, you have to clear all saved results for that function to do so.\\n\\nAnti-patterns\\n\\nHere are some simplified examples of how buttons can go wrong. Be on the lookout for these common mistakes.', metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md'}),\n",
       " Document(page_content='Buttons nested inside buttons\\n\\n```python\\nimport streamlit as st\\n\\nif st.button(\\'Button 1\\'):\\n    st.write(\\'Button 1 was clicked\\')\\n    if st.button(\\'Button 2\\'):\\n        # This will never be executed.\\n        st.write(\\'Button 2 was clicked\\')\\n```\\n\\nOther widgets nested inside buttons\\n\\n```python\\nimport streamlit as st\\n\\nif st.button(\\'Sign up\\'):\\n    name = st.text_input(\\'Name\\')\\n\\n```\\n\\nNesting a process inside a button without saving to session state\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nfile = st.file_uploader(\"Upload a file\", type=\"csv\")\\n\\nif st.button(\\'Get data\\'):\\n    df = pd.read_csv(file)\\n    # This display will go away with the user\\'s next action.\\n    st.write(df)\\n\\nif st.button(\\'Save\\'):\\n    # This will always error.\\n    df.to_csv(\\'data.csv\\')\\n```', metadata={'source': 'docs/content/library/advanced-features/button-behavior-and-examples.md'}),\n",
       " Document(page_content='title: ☰ App menu\\nslug: /library/advanced-features/app-menu\\n\\n☰ App menu\\n\\nStreamlit provides a configurable menu within your app to access convenient tools for developers and viewers. By default, you can access developer options from the app menu when viewing an app locally or on Streamlit Community Cloud while logged into an account with administrative access. While viewing an app, click the icon in the upper-right corner to access the menu.\\n\\nMenu options\\n\\nThe menu is split into two sections. The upper section contains options available to all viewers and the lower section contains options for developers. Read more about customizing this menu at the end of this page.\\n\\nRerun\\n\\nYou can manually trigger a rerun of your app by clicking \"Rerun\" from the app menu. This rerun will not reset your session. Your widget states and values stored in st.session_state will be preserved. As a shortcut, without opening the app menu, you can rerun your app by pressing \"R\" on your keyboard (if you aren\\'t currently focused on an input element).\\n\\nSettings\\n\\nWith the \"Settings\" option, you can control the appearance of your app while it is running. If viewing the app locally, you can set how your app responds to changes in your source code. See more about development flow in Main concepts. You can also force your app to appear in wide mode, even if not set within the script using st.set_page_config.\\n\\nTheme settings\\n\\nAfter clicking \"Settings\" from the app menu, you can choose between \"Light\", \"Dark\", or \"Use system setting\" for the app\\'s base theme. Click on \"Edit active theme\" to modify the theme, color-by-color.\\n\\nPrint\\n\\nClick \"Print\" to open a print dialog. This option uses your browser\\'s built-in print-to-pdf function.\\n\\nRecord a screencast', metadata={'source': 'docs/content/library/advanced-features/app-menu.md'}),\n",
       " Document(page_content='Record a screencast\\n\\nYou can easily make screen recordings right from your app! Screen recording is supported in the latest versions of Chrome, Edge, and Firefox. Ensure your browser is up-to-date for compatibility. Depending on your current settings, you may need to grant permission to your browser to record your screen or to use your microphone if recording a voiceover.\\n\\nWhile viewing your app, open the app menu from the upper-right corner.\\n\\nClick \"Record a screencast.\"\\n\\nIf you want to record audio through your microphone, check \"Also record audio.\"\\n\\nClick \"Start recording.\" (You may be prompted by your OS to permit your browser to record your screen or use your microphone.)\\n\\nSelect which tab, window, or monitor you want to record from the listed options. The interface will vary depending on your browser.\\n\\nClick \"Share.\"\\n\\nWhile recording, you will see a red circle on your app\\'s tab and on the app menu icon. If you want to cancel the recording, click \"Stop sharing\" at the bottom of your app.\\n\\nWhen you are done recording, press \"Esc\" on your keyboard or click \"Stop recording\" from your app\\'s menu.\\n\\nFollow your browser\\'s instructions to save your recording. Your saved recording will be available where your browser saves downloads.\\n\\nThe whole process looks like this:\\n\\nAbout\\n\\nYou can conveniently check what version of Streamlit is running from the \"About\" option. Developers also have the option to customize the message shown here using st.set_page_config.\\n\\nDeveloper options\\n\\nBy default, developer options only show when viewing an app locally or when viewing a Community Cloud app while logged in with administrative permission. You can customize the menu if you want to make these options available for all users.\\n\\nClear cache\\n\\nReset your app\\'s cache by clicking \"Clear cache\" from the app\\'s menu or by pressing \"C\" on your keyboard while not focused on an input element. This will remove all cached entries for @st.cache_data and @st.cache_resource.\\n\\nDeploy this app', metadata={'source': 'docs/content/library/advanced-features/app-menu.md'}),\n",
       " Document(page_content='Deploy this app\\n\\nIf you are running an app locally from within a git repo, you can deploy your app to Streamlit Community Cloud in a few easy clicks! Make sure your work has been pushed to your online GitHub repository before beginning. For the greatest convenience, make sure you have already created your Community Cloud account and are signed in. Click \"Deploy this app\" to be taken directly to Community Cloud\\'s \"Deploy an app\" page. Your app\\'s repository, branch, and file name will be prefilled to match your current app! Learn more about deploying an app on Streamlit Community Cloud.\\n\\nCustomize the menu\\n\\nUsing client.toobarMode in your app\\'s configuration, you can make the app menu appear in the following ways:\\n\\n\"developer\" — Show the developer options to all viewers.\\n\\n\"viewer\" — Hide the developer options from all viewers.\\n\\n\"minimal\" — Show only those options set externally. These can be options declared through st.set_page_config or options populated through Streamlit Community Cloud.\\n\\n\"auto\" — This is the default and will show the developer options when accessed through localhost or through Streamlit Community Cloud when logged into an administrative account for the app. Otherwise, the developer options will not show.', metadata={'source': 'docs/content/library/advanced-features/app-menu.md'}),\n",
       " Document(page_content=\"title: Components\\nslug: /library/components\\n\\nCustom Components\\n\\nComponents are third-party Python modules that extend what's possible with Streamlit.\\n\\nHow to use a Component\\n\\nComponents are super easy to use:\\n\\nStart by finding the Component you'd like to use. Two great resources for this are:\\n\\nThe Component gallery\\n\\nThis thread,\\n     by Fanilo A. from our forums.\\n\\nInstall the Component using your favorite Python package manager. This step and all following\\n   steps are described in your component's instructions.\\n\\nFor example, to use the fantastic AgGrid\\n   Component, you first install it with:\\n\\npython\\n   pip install streamlit-aggrid\\n\\nIn your Python code, import the Component as described in its instructions. For AgGrid, this step\\n   is:\\n\\npython\\n   from st_aggrid import AgGrid\\n\\n...now you're ready to use it! For AgGrid, that's:\\n\\npython\\n   AgGrid(my_dataframe)\\n\\nMaking your own Component\\n\\nIf you're interested in making your own component, check out the following resources:\\n\\nCreate a Component\\n\\nPublish a Component\\n\\nComponents API\\n\\nBlog post for when we launched Components!\\n\\nAlternatively, if you prefer to learn using videos, our engineer Tim Conkling has put together some\\namazing tutorials:\\n\\nVideo tutorial, part 1\\n\\nVideo tutorial, part 2\", metadata={'source': 'docs/content/library/components/components.md'}),\n",
       " Document(page_content=\"title: Publish a Component\\nslug: /library/components/publish\\n\\nPublish a Component\\n\\nPublish to PyPI\\n\\nPublishing your Streamlit Component to PyPI makes it easily accessible to Python users around the world. This step is completely optional, so if you won’t be releasing your component publicly, you can skip this section!\\n\\nFor static Streamlit Components, publishing a Python package to PyPI follows the same steps as the\\ncore PyPI packaging instructions. A static Component likely contains only Python code, so once you have your\\nsetup.py file correct and\\ngenerate your distribution files, you're ready to\\nupload to PyPI.\\n\\nBi-directional Streamlit Components at minimum include both Python and JavaScript code, and as such, need a bit more preparation before they can be published on PyPI. The remainder of this page focuses on the bi-directional Component preparation process.\\n\\nPrepare your Component\\n\\nA bi-directional Streamlit Component varies slightly from a pure Python library in that it must contain pre-compiled frontend code. This is how base Streamlit works as well; when you pip install streamlit, you are getting a Python library where the HTML and frontend code contained within it have been compiled into static assets.\\n\\nThe component-template GitHub repo provides the folder structure necessary for PyPI publishing. But before you can publish, you'll need to do a bit of housekeeping:\\n\\nGive your Component a name, if you haven't already\\n\\nRename the template/my_component/ folder to template/<component name>/\\n\\nPass your component's name as the the first argument to declare_component()\\n\\nEdit MANIFEST.in, change the path for recursive-include from package/frontend/build * to <component name>/frontend/build *\\n\\nEdit setup.py, adding your component's name and other relevant info\\n\\nCreate a release build of your frontend code. This will add a new directory, frontend/build/, with your compiled frontend in it:\\n\\nbash\\n   cd frontend\\n   npm run build\", metadata={'source': 'docs/content/library/components/publish-component.md'}),\n",
       " Document(page_content=\"bash\\n   cd frontend\\n   npm run build\\n\\nPass the build folder's path as the path parameter to declare_component. (If you're using the template Python file, you can set _RELEASE = True at the top of the file):\\n\\n```python\\n      import streamlit.components.v1 as components\\n\\n```\\n\\nBuild a Python wheel\\n\\nOnce you've changed the default my_component references, compiled the HTML and JavaScript code and set your new component name in components.declare_component(), you're ready to build a Python wheel:\\n\\nMake sure you have the latest versions of setuptools, wheel, and twine\\n\\nCreate a wheel from the source code:\\n\\nbash\\n    # Run this from your component's top-level directory; that is,\\n    # the directory that contains `setup.py`\\n    python setup.py sdist bdist_wheel\\n\\nUpload your wheel to PyPI\\n\\nWith your wheel created, the final step is to upload to PyPI. The instructions here highlight how to upload to Test PyPI, so that you can learn the mechanics of the process without worrying about messing anything up. Uploading to PyPI follows the same basic procedure.\\n\\nCreate an account on Test PyPI if you don't already have one\\n\\nVisit https://test.pypi.org/account/register/ and complete the steps\\n\\nVisit https://test.pypi.org/manage/account/#api-tokens and create a new API token. Don’t limit the token scope to a particular project, since you are creating a new project. Copy your token before closing the page, as you won’t be able to retrieve it again.\\n\\nUpload your wheel to Test PyPI. twine will prompt you for a username and password. For the username, use __token__. For the password, use your token value from the previous step, including the pypi- prefix:\\n\\nbash\\n   python3 -m twine upload --repository testpypi dist/*\\n\\nInstall your newly-uploaded package in a new Python project to make sure it works:\\n\\nbash\\n    python -m pip install --index-url https://test.pypi.org/simple/ --no-deps example-pkg-YOUR-USERNAME-HERE\", metadata={'source': 'docs/content/library/components/publish-component.md'}),\n",
       " Document(page_content='If all goes well, you\\'re ready to upload your library to PyPI by following the instructions at https://packaging.python.org/tutorials/packaging-projects/#next-steps.\\n\\nCongratulations, you\\'ve created a publicly-available Streamlit Component!\\n\\nPromote your Component!\\n\\nWe\\'d love to help you share your Component with the Streamlit Community! To share it, please post on the Streamlit \\'Show the Community!\\' Forum category with the title similar to \"New Component: <your component name>, a new way to do X\".\\n\\nYou can also Tweet at us @streamlit so that we can retweet your announcement for you.\\n\\nIf you host your code on GitHub, add the tag streamlit-component, so that it\\'s listed in the GitHub streamlit-component topic:', metadata={'source': 'docs/content/library/components/publish-component.md'}),\n",
       " Document(page_content='title: Create a Component\\nslug: /library/components/create\\n\\nCreate a Component\\n\\nIf you are only interested in using Streamlit Components, then you can skip this section and\\nhead over to the Streamlit Components Gallery to find and install\\ncomponents created by the community!\\n\\nDevelopers can write JavaScript and HTML \"components\" that can be rendered in Streamlit apps. Streamlit Components can receive data from, and also send data to, Streamlit Python scripts.\\n\\nStreamlit Components let you expand the functionality provided in the base Streamlit package. Use Streamlit Components to create the needed functionality for your use-case, then wrap it up in a Python package and share with the broader Streamlit community!\\n\\nTypes of Streamlit Components you could create include:\\n\\nCustom versions of existing Streamlit elements and widgets, such as st.slider or st.file_uploader.\\n\\nCompletely new Streamlit elements and widgets by wrapping existing React.js, Vue.js, or other JavaScript widget toolkits.\\n\\nRendering Python objects having methods that output HTML, such as IPython __repr_html__.\\n\\nConvenience functions for commonly-used web features like GitHub gists and Pastebin.\\n\\nCheck out these Streamlit Components tutorial videos by Streamlit engineer Tim Conkling to get started:\\n\\nPart 1: Setup and Architecture\\n\\nPart 2: Make a Slider Widget', metadata={'source': 'docs/content/library/components/create-component.md'}),\n",
       " Document(page_content=\"title: Optimize performance with st.cache\\nslug: /library/advanced-features/st.cache\\n\\nst.cache was deprecated in version 1.18.0. Use st.cache_data or st.cache_resource instead. Learn more in Caching.\\n\\nOptimize performance with st.cache\\n\\nStreamlit provides a caching mechanism that allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations. This is done with the @st.cache decorator.\\n\\nWhen you mark a function with the @st.cache decorator, it tells Streamlit that whenever the function is called it needs to check a few things:\\n\\nThe input parameters that you called the function with\\n\\nThe value of any external variable used in the function\\n\\nThe body of the function\\n\\nThe body of any function used inside the cached function\\n\\nIf this is the first time Streamlit has seen these four components with these exact values and in this exact combination and order, it runs the function and stores the result in a local cache. Then, next time the cached function is called, if none of these components changed, Streamlit will just skip executing the function altogether and, instead, return the output previously stored in the cache.\\n\\nThe way Streamlit keeps track of changes in these components is through hashing. Think of the cache as an in-memory key-value store, where the key is a hash of all of the above and the value is the actual output object passed by reference.\\n\\nFinally, @st.cache supports arguments to configure the cache's behavior. You can find more information on those in our API reference.\\n\\nLet's take a look at a few examples that illustrate how caching works in a Streamlit app.\\n\\nExample 1: Basic usage\\n\\nFor starters, let's take a look at a sample app that has a function that performs an expensive, long-running computation. Without caching, this function is rerun each time the app is refreshed, leading to a poor user experience. Copy this code into a new app and try it out yourself:\", metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content='```python\\nimport streamlit as st\\nimport time\\n\\ndef expensive_computation(a, b):\\n    time.sleep(2)  # 👈 This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nTry pressing R to rerun the app, and notice how long it takes for the result to show up. This is because expensive_computation(a, b) is being re-executed every time the app runs. This isn\\'t a great experience.\\n\\nLet\\'s add the @st.cache decorator:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache  # 👈 Added this\\ndef expensive_computation(a, b):\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nNow run the app again and you\\'ll notice that it is much faster every time you press R to rerun. To understand what is happening, let\\'s add an st.write inside the function:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)  # 👈 Changed this\\ndef expensive_computation(a, b):\\n    # 👇 Added this\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nNow when you rerun the app the text \"Cache miss\" appears on the first run, but not on any subsequent runs. That\\'s because the cached function is only being executed once, and every time after that you\\'re actually hitting the cache.', metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content='You may have noticed that we\\'ve added the suppress_st_warning keyword to the @st.cache decorators. That\\'s because the cached function above uses a Streamlit command itself (st.write in this case), and when Streamlit sees that, it shows a warning that your command will only execute when you get a cache miss. More often than not, when you see that warning it\\'s because there\\'s a bug in your code. However, in our case we\\'re using the st.write command to demonstrate when the cache is being missed, so the behavior Streamlit is warning us about is exactly what we want. As a result, we are passing in suppress_st_warning=True to turn that warning off.\\n\\nExample 2: When the function arguments change\\n\\nWithout stopping the previous app server, let\\'s change one of the arguments to our cached function:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = 210  # 👈 Changed this\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nNow the first time you rerun the app it\\'s a cache miss. This is evidenced by the \"Cache miss\" text showing up and the app taking 2s to finish running. After that, if you press R to rerun, it\\'s always a cache hit. That is, no such text shows up and the app is fast again.\\n\\nThis is because Streamlit notices whenever the arguments a and b change and determines whether the function should be re-executed and re-cached.\\n\\nExample 3: When the function body changes\\n\\nWithout stopping and restarting your Streamlit server, let\\'s remove the widget from our app and modify the function\\'s code by adding a + 1 to the return value.\\n\\n```python\\nimport streamlit as st\\nimport time', metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content='```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b + 1  # 👈 Added a +1 at the end here\\n\\na = 2\\nb = 210\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nThe first run is a \"Cache miss\", but when you press R each subsequent run is a cache hit. This is because on first run, Streamlit detected that the function body changed, reran the function, and put the result in the cache.\\n\\nIf you change the function back the result will already be in the Streamlit cache from a previous run. Try it out!\\n\\nExample 4: When an inner function changes\\n\\nLet\\'s make our cached function depend on another function internally:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\ndef inner_func(a, b):\\n    st.write(\"inner_func(\", a, \",\", b, \") ran\")\\n    return a * b\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return inner_func(a, b) + 1\\n\\na = 2\\nb = 210\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nWhat you see is the usual:\\n\\nThe first run results in a cache miss.\\n\\nEvery subsequent rerun results in a cache hit.\\n\\nBut now let\\'s try modifying the inner_func():\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\ndef inner_func(a, b):\\n    st.write(\"inner_func(\", a, \",\", b, \") ran\")\\n    return a ** b  # 👈 Changed the * to ** here\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return inner_func(a, b) + 1\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```', metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content='st.write(\"Result:\", res)\\n```\\n\\nEven though inner_func() is not annotated with @st.cache, when we edit its body we cause a \"Cache miss\" in the outer expensive_computation().\\n\\nThat\\'s because Streamlit always traverses your code and its dependencies to verify that the cached values are still valid. This means that while developing your app you can edit your code freely without worrying about the cache. Any change you make to your app, Streamlit should do the right thing!\\n\\nStreamlit is also smart enough to only traverse dependencies that belong to your app, and skip over any dependency that comes from an installed Python library.\\n\\nExample 5: Use caching to speed up your app across users\\n\\nGoing back to our original function, let\\'s add a widget to control the value of b:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return a * b\\n\\na = 2\\nb = st.slider(\"Pick a number\", 0, 10)  # 👈 Changed this\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n```\\n\\nWhat you\\'ll see:\\n\\nIf you move the slider to a number Streamlit hasn\\'t seen before, you\\'ll have a cache miss again. And every subsequent rerun with the same number will be a cache hit, of course.\\n\\nIf you move the slider back to a number Streamlit has seen before, the cache is hit and the app is fast as expected.\\n\\nIn computer science terms, what is happening here is that @st.cache is memoizing expensive_computation(a, b).\\n\\nBut now let\\'s go one step further! Try the following:\\n\\nMove the slider to a number you haven\\'t tried before, such as 9.\\n\\nPretend you\\'re another user by opening another browser tab pointing to your Streamlit app (usually at http://localhost:8501)\\n\\nIn the new tab, move the slider to 9.', metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content='In the new tab, move the slider to 9.\\n\\nNotice how this is actually a cache hit! That is, you don\\'t actually see the \"Cache miss\" text on the second tab even though that second user never moved the slider to 9 at any point prior to this.\\n\\nThis happens because the Streamlit cache is global to all users. So everyone contributes to everyone else\\'s performance.\\n\\nExample 6: Mutating cached values\\n\\nAs mentioned in the overview section, the Streamlit cache stores items by reference. This allows the Streamlit cache to support structures that aren\\'t memory-managed by Python, such as TensorFlow objects. However, it can also lead to unexpected behavior — which is why Streamlit has a few checks to guide developers in the right direction. Let\\'s look into those checks now.\\n\\nLet\\'s write an app that has a cached function which returns a mutable object, and then let\\'s follow up by mutating that object:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n@st.cache(suppress_st_warning=True)\\ndef expensive_computation(a, b):\\n    st.write(\"Cache miss: expensive_computation(\", a, \",\", b, \") ran\")\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return {\"output\": a * b}  # 👈 Mutable object\\n\\na = 2\\nb = 21\\nres = expensive_computation(a, b)\\n\\nst.write(\"Result:\", res)\\n\\nres[\"output\"] = \"result was manually mutated\"  # 👈 Mutated cached value\\n\\nst.write(\"Mutated result:\", res)\\n```\\n\\nWhen you run this app for the first time, you should see three messages on the screen:\\n\\nCache miss (...)\\n\\nResult: {output: 42}\\n\\nMutated result: {output: \"result was manually mutated\"}\\n\\nNo surprises here. But now notice what happens when you rerun you app (i.e. press R):\\n\\nResult: {output: \"result was manually mutated\"}\\n\\nMutated result: {output: \"result was manually mutated\"}\\n\\nCached object mutated. (...)\\n\\nSo what\\'s up?', metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content='Cached object mutated. (...)\\n\\nSo what\\'s up?\\n\\nWhat\\'s going on here is that Streamlit caches the output res by reference. When you mutated res[\"output\"] outside the cached function you ended up inadvertently modifying the cache. This means every subsequent call to expensive_computation(2, 21) will return the wrong value!\\n\\nSince this behavior is usually not what you\\'d expect, Streamlit tries to be helpful and show you a warning, along with some ideas about how to fix your code.\\n\\nIn this specific case, the fix is just to not mutate res[\"output\"] outside the cached function. There was no good reason for us to do that anyway! Another solution would be to clone the result value with res = deepcopy(expensive_computation(2, 21)). Check out the section entitled Fixing caching issues for more information on these approaches and more.\\n\\nAdvanced caching\\n\\nIn caching, you learned about the Streamlit cache, which is accessed with the @st.cache decorator. In this article you\\'ll see how Streamlit\\'s caching functionality is implemented, so that you can use it to improve the performance of your Streamlit apps.\\n\\nThe cache is a key-value store, where the key is a hash of:\\n\\nThe input parameters that you called the function with\\n\\nThe value of any external variable used in the function\\n\\nThe body of the function\\n\\nThe body of any function used inside the cached function\\n\\nAnd the value is a tuple of:\\n\\nThe cached output\\n\\nA hash of the cached output (you\\'ll see why soon)\\n\\nFor both the key and the output hash, Streamlit uses a specialized hash function that knows how to traverse code, hash special objects, and can have its behavior customized by the user.\\n\\nFor example, when the function expensive_computation(a, b), decorated with @st.cache, is executed with a=2 and b=21, Streamlit does the following:\\n\\nComputes the cache key\\n\\nIf the key is found in the cache, then:\\n\\nExtracts the previously-cached (output, output_hash) tuple.', metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content=\"Performs an Output Mutation Check, where a fresh hash of the output is computed and compared to the stored output_hash.\\nIf the two hashes are different, shows a Cached Object Mutated warning. (Note: Setting allow_output_mutation=True disables this step).\\n\\nIf the input key is not found in the cache, then:\\n\\nExecutes the cached function (i.e. output = expensive_computation(2, 21)).\\n\\nCalculates the output_hash from the function's output.\\n\\nStores key → (output, output_hash) in the cache.\\n\\nReturns the output.\\n\\nIf an error is encountered an exception is raised. If the error occurs while hashing either the key or the output an UnhashableTypeError error is thrown. If you run into any issues, see fixing caching issues.\\n\\nThe hash_funcs parameter\\n\\nAs described above, Streamlit's caching functionality relies on hashing to calculate the key for cached objects, and to detect unexpected mutations in the cached result.\\n\\nFor added expressive power, Streamlit lets you override this hashing process using the hash_funcs argument. Suppose you define a type called FileReference which points to a file in the filesystem:\\n\\n```python\\nclass FileReference:\\n    def init(self, filename):\\n        self.filename = filename\\n\\n@st.cache\\ndef func(file_reference):\\n    ...\\n```\\n\\nBy default, Streamlit hashes custom classes like FileReference by recursively navigating their structure. In this case, its hash is the hash of the filename property. As long as the file name doesn't change, the hash will remain constant.\\n\\nHowever, what if you wanted to have the hasher check for changes to the file's modification time, not just its name? This is possible with @st.cache's hash_funcs parameter:\\n\\n```python\\nclass FileReference:\\n    def init(self, filename):\\n        self.filename = filename\\n\\ndef hash_file_reference(file_reference):\\n    filename = file_reference.filename\\n    return (filename, os.path.getmtime(filename))\\n\\n@st.cache(hash_funcs={FileReference: hash_file_reference})\\ndef func(file_reference):\\n    ...\\n```\", metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content=\"Additionally, you can hash FileReference objects by the file's contents:\\n\\n```python\\nclass FileReference:\\n    def init(self, filename):\\n        self.filename = filename\\n\\ndef hash_file_reference(file_reference):\\n    with open(file_reference.filename) as f:\\n      return f.read()\\n\\n@st.cache(hash_funcs={FileReference: hash_file_reference})\\ndef func(file_reference):\\n    ...\\n```\\n\\nBecause Streamlit's hash function works recursively, you don't have to hash the contents inside hash_file_reference Instead, you can return a primitive type, in this case the contents of the file, and Streamlit's internal hasher will compute the actual hash from it.\\n\\nTypical hash functions\\n\\nWhile it's possible to write custom hash functions, let's take a look at some of the tools that Python provides out of the box. Here's a list of some hash functions and when it makes sense to use them.\\n\\nPython's id function | Example\\n\\nSpeed: Fast\\n\\nUse case: If you're hashing a singleton object, like an open database connection or a TensorFlow session. These are objects that will only be instantiated once, no matter how many times your script reruns.\\n\\nlambda _: None | Example\\n\\nSpeed: Fast\\n\\nUse case: If you want to turn off hashing of this type. This is useful if you know the object is not going to change.\\n\\nPython's hash() function | Example\\n\\nSpeed: Can be slow based the size of the object being cached\\n\\nUse case: If Python already knows how to hash this type correctly.\\n\\nCustom hash function | Example\\n\\nSpeed: N/a\\n\\nUse case: If you'd like to override how Streamlit hashes a particular type.\\n\\nExample 1: Pass a database connection around\\n\\nSuppose we want to open a database connection that can be reused across multiple runs of a Streamlit app. For this you can make use of the fact that cached objects are stored by reference to automatically initialize and reuse the connection:\\n\\npython\\n@st.cache(allow_output_mutation=True)\\ndef get_database_connection():\\n    return db.get_connection()\", metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content=\"With just 3 lines of code, the database connection is created once and stored in the cache. Then, every subsequent time get_database_connection is called, the already-created connection object is reused automatically. In other words, it becomes a singleton.\\n\\nUse the allow_output_mutation=True flag to suppress the immutability check. This prevents Streamlit from trying to hash the output connection, and also turns off Streamlit's mutation warning in the process.\\n\\nWhat if you want to write a function that receives a database connection as input? For that, you'll use hash_funcs:\\n\\npython\\n@st.cache(hash_funcs={DBConnection: id})\\ndef get_users(connection):\\n    # Note: We assume that connection is of type DBConnection.\\n    return connection.execute_sql('SELECT * from Users')\\n\\nHere, we use Python's built-in id function, because the connection object is coming from the Streamlit cache via the get_database_connection function. This means that the same connection instance is passed around every time, and therefore it always has the same id. However, if you happened to have a second connection object around that pointed to an entirely different database, it would still be safe to pass it to get_users because its id is guaranteed to be different than the first id.\\n\\nThese design patterns apply any time you have an object that points to an external resource, such as a database connection or Tensorflow session.\\n\\nExample 2: Turn off hashing for a specific type\\n\\nYou can turn off hashing entirely for a particular type by giving it a custom hash function that returns a constant. One reason that you might do this is to avoid hashing large, slow-to-hash objects that you know are not going to change. For example:\\n\\npython\\n@st.cache(hash_funcs={pd.DataFrame: lambda _: None})\\ndef func(huge_constant_dataframe):\\n    ...\", metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content=\"When Streamlit encounters an object of this type, it always converts the object into None, no matter which instance of FooType its looking at. This means all instances are hash to the same value, which effectively cancels out the hashing mechanism.\\n\\nExample 3: Use Python's hash() function\\n\\nSometimes, you might want to use Python’s default hashing instead of Streamlit's. For example, maybe you've encountered a type that Streamlit is unable to hash, but it's hashable with Python's built-in hash() function:\\n\\npython\\n@st.cache(hash_funcs={FooType: hash})\\ndef func(...):\\n    ...\", metadata={'source': 'docs/content/library/advanced-features/st.cache.md'}),\n",
       " Document(page_content='title: Components API\\nslug: /library/components/components-api\\n\\nComponents API Reference\\n\\nThe first step in developing a Streamlit Component is deciding whether to create a static component (i.e. rendered once, controlled by Python) or to create a bi-directional component that can communicate from Python to JavaScript and back.\\n\\nCreate a static component\\n\\nIf your goal in creating a Streamlit Component is solely to display HTML code or render a chart from a Python visualization library, Streamlit provides two methods that greatly simplify the process: components.html() and components.iframe().\\n\\nIf you are unsure whether you need bi-directional communication, start here first!\\n\\nRender an HTML string\\n\\nWhile st.text, st.markdown and st.write make it easy to write text to a Streamlit app, sometimes you\\'d rather implement a custom piece of HTML. Similarly, while Streamlit natively supports many charting libraries, you may want to implement a specific HTML/JavaScript template for a new charting library. components.html works by giving you the ability to embed an iframe inside of a Streamlit app that contains your desired output.\\n\\nExample\\n\\n```python\\nimport streamlit as st\\nimport streamlit.components.v1 as components\\n\\nbootstrap 4 collapse example\\n\\ncomponents.html(\\n    \"\"\"\\n\\nCollapsible Group Item #1\\n\\nCollapsible Group Item #1 content\\n\\nCollapsible Group Item #2\\n\\nCollapsible Group Item #2 content\\n\\nRender an iframe URL\\n\\ncomponents.iframe is similar in features to components.html, with the difference being that components.iframe takes a URL as its input. This is used for situations where you want to include an entire page within a Streamlit app.\\n\\nExample\\n\\n```python\\nimport streamlit as st\\nimport streamlit.components.v1 as components\\n\\nembed streamlit docs in a streamlit app\\n\\ncomponents.iframe(\"https://docs.streamlit.io/en/latest\")\\n```\\n\\nCreate a bi-directional component\\n\\nA bi-directional Streamlit Component has two parts:', metadata={'source': 'docs/content/library/components/components-api.md'}),\n",
       " Document(page_content='A frontend, which is built out of HTML and any other web tech you like (JavaScript, React, Vue, etc.), and gets rendered in Streamlit apps via an iframe tag.\\n\\nA Python API, which Streamlit apps use to instantiate and talk to that frontend\\n\\nTo make the process of creating bi-directional Streamlit Components easier, we\\'ve created a React template and a TypeScript-only template in the Streamlit Component-template GitHub repo. We also provide some example Components in the same repo.\\n\\nDevelopment Environment Setup\\n\\nTo build a Streamlit Component, you need the following installed in your development environment:\\n\\nPython 3.8 - Python 3.11\\n\\nStreamlit 1.11.1 or higher\\n\\nnodejs\\n\\nnpm or yarn\\n\\nClone the component-template GitHub repo, then decide whether you want to use the React.js (\"template\") or plain TypeScript (\"template-reactless\") template.\\n\\nInitialize and build the component template frontend from the terminal:\\n\\n```bash\\n   # React template\\n   template/my_component/frontend\\n   npm install    # Initialize the project and install npm dependencies\\n   npm run start  # Start the Webpack dev server\\n\\n# or\\n\\n# TypeScript-only template\\n   template-reactless/my_component/frontend\\n   npm install    # Initialize the project and install npm dependencies\\n   npm run start  # Start the Webpack dev server\\n   ```\\n\\nFrom a separate terminal, run the Streamlit app (Python) that declares and uses the component:\\n\\n```bash\\n   # React template\\n   cd template\\n   . venv/bin/activate # or similar to activate the venv/conda environment where Streamlit is installed\\n   streamlit run my_component/init.py # run the example\\n\\n# or\\n\\n# TypeScript-only template\\n   cd template-reactless\\n   . venv/bin/activate # or similar to activate the venv/conda environment where Streamlit is installed\\n   streamlit run my_component/init.py # run the example\\n   ```\\n\\nAfter running the steps above, you should see a Streamlit app in your browser that looks like this:', metadata={'source': 'docs/content/library/components/components-api.md'}),\n",
       " Document(page_content='The example app from the template shows how bi-directional communication is implemented. The Streamlit Component displays a button (Python → JavaScript), and the end-user can click the button. Each time the button is clicked, the JavaScript front-end increments the counter value and passes it back to Python (JavaScript → Python), which is then displayed by Streamlit (Python → JavaScript).\\n\\nFrontend\\n\\nBecause each Streamlit Component is its own webpage that gets rendered into an iframe, you can use just about any web tech you\\'d like to create that web page. We provide two templates to get started with in the Streamlit Components-template GitHub repo; one of those templates uses React and the other does not.\\n\\nEven if you\\'re not already familiar with React, you may still want to check out the React-based\\ntemplate. It handles most of the boilerplate required to send and receive data from Streamlit, and\\nyou can learn the bits of React you need as you go.\\n\\nIf you\\'d rather not use React, please read this section anyway! It explains the fundamentals of\\nStreamlit ↔ Component communication.\\n\\nReact\\n\\nThe React-based template is in template/my_component/frontend/src/MyComponent.tsx.\\n\\nMyComponent.render() is called automatically when the component needs to be re-rendered (just like in any React app)\\n\\nArguments passed from the Python script are available via the this.props.args dictionary:\\n\\n```python\\n\\nSend arguments in Python:\\n\\nresult = my_component(greeting=\"Hello\", name=\"Streamlit\")\\n```\\n\\njavascript\\n// Receive arguments in frontend:\\nlet greeting = this.props.args[\"greeting\"]; // greeting = \"Hello\"\\nlet name = this.props.args[\"name\"]; // name = \"Streamlit\"\\n\\nUse Streamlit.setComponentValue() to return data from the component to the Python script:\\n\\njavascript\\n// Set value in frontend:\\nStreamlit.setComponentValue(3.14);\\n\\n```python\\n\\nAccess value in Python:\\n\\nresult = my_component(greeting=\"Hello\", name=\"Streamlit\")\\nst.write(\"result = \", result) # result = 3.14\\n```', metadata={'source': 'docs/content/library/components/components-api.md'}),\n",
       " Document(page_content=\"When you call Streamlit.setComponentValue(new_value), that new value is sent to Streamlit, which then re-executes the Python script from top to bottom. When the script is re-executed, the call to my_component(...) will return the new value.\\n\\nFrom a code flow perspective, it appears that you're transmitting data synchronously with the frontend: Python sends the arguments to JavaScript, and JavaScript returns a value to Python, all in a single function call! But in reality this is all happening asynchronously, and it's the re-execution of the Python script that achieves the sleight of hand.\\n\\nUse Streamlit.setFrameHeight() to control the height of your component. By default, the React template calls this automatically (see StreamlitComponentBase.componentDidUpdate()). You can override this behavior if you need more control.\\n\\nThere's a tiny bit of magic in the last line of the file: export default withStreamlitConnection(MyComponent) - this does some handshaking with Streamlit, and sets up the mechanisms for bi-directional data communication.\\n\\nTypeScript-only\\n\\nThe TypeScript-only template is in template-reactless/my_component/frontend/src/MyComponent.tsx.\\n\\nThis template has much more code than its React sibling, in that all the mechanics of handshaking, setting up event listeners, and updating the component's frame height are done manually. The React version of the template handles most of these details automatically.\\n\\nTowards the bottom of the source file, the template calls Streamlit.setComponentReady() to tell Streamlit it's ready to start receiving data. (You'll generally want to do this after creating and loading everything that the Component relies on.)\\n\\nIt subscribes to Streamlit.RENDER_EVENT to be notified of when to redraw. (This event won't be fired until setComponentReady is called)\\n\\nWithin its onRender event handler, it accesses the arguments passed in the Python script via event.detail.args\", metadata={'source': 'docs/content/library/components/components-api.md'}),\n",
       " Document(page_content='It sends data back to the Python script in the same way that the React template does—clicking on the \"Click Me!\" button calls Streamlit.setComponentValue()\\n\\nIt informs Streamlit when its height may have changed via Streamlit.setFrameHeight()\\n\\nWorking with Themes\\n\\nCustom component theme support requires streamlit-component-lib version 1.2.0 or higher.\\n\\nAlong with sending an args object to your component, Streamlit also sends\\na theme object defining the active theme so that your component can adjust\\nits styling in a compatible way. This object is sent in the same message as\\nargs, so it can be accessed via this.props.theme (when using the React\\ntemplate) or event.detail.theme (when using the plain TypeScript template).\\n\\nThe theme object has the following shape:\\n\\njson\\n{\\n  \"base\": \"lightORdark\",\\n  \"primaryColor\": \"someColor1\",\\n  \"backgroundColor\": \"someColor2\",\\n  \"secondaryBackgroundColor\": \"someColor3\",\\n  \"textColor\": \"someColor4\",\\n  \"font\": \"someFont\"\\n}\\n\\nThe base option allows you to specify a preset Streamlit theme that your custom theme inherits from. Any theme config options not defined in your theme settings have their values set to those of the base theme. Valid values for base are \"light\" and \"dark\".\\n\\nNote that the theme object has fields with the same names and semantics as the\\noptions in the \"theme\" section of the config options printed with the command\\nstreamlit config show.\\n\\nWhen using the React template, the following CSS variables are also set\\nautomatically.\\n\\ncss\\n--base\\n--primary-color\\n--background-color\\n--secondary-background-color\\n--text-color\\n--font\\n\\nIf you\\'re not familiar with\\nCSS variables,\\nthe TLDR version is that you can use them like this:\\n\\ncss\\n.mySelector {\\n  color: var(--text-color);\\n}\\n\\nThese variables match the fields defined in the theme object above, and\\nwhether to use CSS variables or the theme object in your component is a matter\\nof personal preference.\\n\\nOther frontend details', metadata={'source': 'docs/content/library/components/components-api.md'}),\n",
       " Document(page_content='Other frontend details\\n\\nBecause you\\'re hosting your component from a dev server (via npm run start), any changes you make should be automatically reflected in the Streamlit app when you save.\\n\\nIf you want to add more packages to your component, run npm add to add them from within your component\\'s frontend/ directory.\\n\\nbash\\nnpm add baseui\\n\\nTo build a static version of your component, run npm run build. See Prepare your Component for more information\\n\\nPython API\\n\\ncomponents.declare_component() is all that\\'s required to create your Component\\'s Python API:\\n\\npython\\n  import streamlit.components.v1 as components\\n  my_component = components.declare_component(\\n    \"my_component\",\\n    url=\"http://localhost:3001\"\\n  )\\n\\nYou can then use the returned my_component function to send and receive data with your frontend code:\\n\\n```python\\n\\nSend data to the frontend using named arguments.\\n\\nreturn_value = my_component(name=\"Blackbeard\", ship=\"Queen Anne\\'s Revenge\")\\n\\nmy_component\\'s return value is the data returned from the frontend.\\n\\nst.write(\"Value = \", return_value)\\n```\\n\\nWhile the above is all you need to define from the Python side to have a working Component, we recommend creating a \"wrapper\" function with named arguments and default values, input validation and so on. This will make it easier for end-users to understand what data values your function accepts and allows for defining helpful docstrings.\\n\\nPlease see this example from the Components-template for an example of creating a wrapper function.\\n\\nData serialization\\n\\nPython → Frontend\\n\\nYou send data from Python to the frontend by passing keyword args to your Component\\'s invoke function (that is, the function returned from declare_component). You can send the following types of data from Python to the frontend:\\n\\nAny JSON-serializable data\\n\\nnumpy.array\\n\\npandas.DataFrame', metadata={'source': 'docs/content/library/components/components-api.md'}),\n",
       " Document(page_content=\"numpy.array\\n\\npandas.DataFrame\\n\\nAny JSON-serializable data gets serialized to a JSON string, and deserialized to its JavaScript equivalent. numpy.array and pandas.DataFrame get serialized using Apache Arrow and are deserialized as instances of ArrowTable, which is a custom type that wraps Arrow structures and provides a convenient API on top of them.\\n\\nCheck out the CustomDataframe and SelectableDataTable Component example code for more context on how to use ArrowTable.\\n\\nFrontend → Python\\n\\nYou send data from the frontend to Python via the Streamlit.setComponentValue() API (which is part of the template code). Unlike arg-passing from Python → frontend, this API takes a single value. If you want to return multiple values, you'll need to wrap them in an Array or Object.\\n\\nCustom Components can send JSON-serializable data from the frontend to Python, as well as Apache Arrow ArrowTables to represent dataframes.\", metadata={'source': 'docs/content/library/components/components-api.md'}),\n",
       " Document(page_content='title: st.sidebar\\nslug: /library/api-reference/layout/st.sidebar\\ndescription: st.sidebar displays items in a sidebar.\\n\\nst.sidebar\\n\\nAdd widgets to sidebar\\n\\nNot only can you add interactivity to your app with widgets, you can organize them into a sidebar. Elements can be passed to st.sidebar using object notation and with notation.\\n\\nThe following two snippets are equivalent:\\n\\n```python\\n\\nObject notation\\n\\nst.sidebar.[element_name]\\n```\\n\\n```python\\n\\n\"with\" notation\\n\\nwith st.sidebar:\\n    st.[element_name]\\n```\\n\\nEach element that\\'s passed to st.sidebar is pinned to the left, allowing users to focus on the content in your app.\\n\\nThe sidebar is resizable! Drag and drop the right border of the sidebar to resize it! ↔️\\n\\nHere\\'s an example of how you\\'d add a selectbox and a radio button to your sidebar:\\n\\n```python\\nimport streamlit as st\\n\\nUsing object notation\\n\\nadd_selectbox = st.sidebar.selectbox(\\n    \"How would you like to be contacted?\",\\n    (\"Email\", \"Home phone\", \"Mobile phone\")\\n)\\n\\nUsing \"with\" notation\\n\\nwith st.sidebar:\\n    add_radio = st.radio(\\n        \"Choose a shipping method\",\\n        (\"Standard (5-15 days)\", \"Express (2-5 days)\")\\n    )\\n```\\n\\nThe only elements that aren\\'t supported using object notation are st.echo and st.spinner. To use these elements, you must use with notation.\\n\\nHere\\'s an example of how you\\'d add st.echo and st.spinner to your sidebar:\\n\\n```python\\nimport streamlit as st\\n\\nwith st.sidebar:\\n    with st.echo():\\n        st.write(\"This code will be printed to the sidebar.\")\\n\\n```', metadata={'source': 'docs/content/library/api/layout/sidebar.md'}),\n",
       " Document(page_content='title: st.container\\nslug: /library/api-reference/layout/st.container\\ndescription: st.container inserts a multi-element container.', metadata={'source': 'docs/content/library/api/layout/container.md'}),\n",
       " Document(page_content='title: st.empty\\nslug: /library/api-reference/layout/st.empty\\ndescription: st.empty inserts a single-element container.', metadata={'source': 'docs/content/library/api/layout/empty.md'}),\n",
       " Document(page_content='title: Layouts and Containers\\nslug: /library/api-reference/layout\\n\\nLayouts and Containers\\n\\nComplex layouts\\n\\nStreamlit provides several options for controlling how different elements are laid out on the screen.\\n\\nSidebar\\n\\nDisplay items in a sidebar.\\n\\npython\\nst.sidebar.write(\"This lives in the sidebar\")\\nst.sidebar.button(\"Click me!\")\\n\\nColumns\\n\\nInsert containers laid out as side-by-side columns.\\n\\npython\\ncol1, col2 = st.columns(2)\\ncol1.write(\"this is column 1\")\\ncol2.write(\"this is column 2\")\\n\\nTabs\\n\\nInsert containers separated into tabs.\\n\\npython\\ntab1, tab2 = st.tabs([\"Tab 1\", \"Tab2\"])\\ntab1.write(\"this is tab 1\")\\ntab2.write(\"this is tab 2\")\\n\\nExpander\\n\\nInsert a multi-element container that can be expanded/collapsed.\\n\\npython\\nwith st.expander(\"Open to see more\"):\\n  st.write(\"This is more content\")\\n\\nContainer\\n\\nInsert a multi-element container.\\n\\npython\\nc = st.container()\\nst.write(\"This will show last\")\\nc.write(\"This will show first\")\\nc.write(\"This will show second\")\\n\\nEmpty\\n\\nInsert a single-element container.\\n\\npython\\nc = st.empty()\\nst.write(\"This will show last\")\\nc.write(\"This will be replaced\")\\nc.write(\"This will show first\")\\n\\nStreamlit Elements\\n\\nCreate a draggable and resizable dashboard in Streamlit. Created by @okls.\\n\\n```python\\nfrom streamlit_elements import elements, mui, html\\n\\nwith elements(\"new_element\"):\\n  mui.Typography(\"Hello world\")\\n```\\n\\nPydantic\\n\\nAuto-generate Streamlit UI from Pydantic Models and Dataclasses. Created by @lukasmasuch.\\n\\n```python\\nimport streamlit_pydantic as sp\\n\\nsp.pydantic_form(key=\"my_form\",\\n  model=ExampleModel)\\n```\\n\\nStreamlit Pages\\n\\nAn experimental version of Streamlit Multi-Page Apps. Created by @blackary.\\n\\n```python\\nfrom st_pages import Page, show_pages, add_page_title\\n\\nshow_pages([ Page(\"streamlit_app.py\", \"Home\", \"🏠\"),\\n  Page(\"other_pages/page2.py\", \"Page 2\", \":books:\"), ])\\n```', metadata={'source': 'docs/content/library/api/layout/layout.md'}),\n",
       " Document(page_content='title: st.expander\\nslug: /library/api-reference/layout/st.expander\\ndescription: st.expander inserts a multi-element container that can be expanded/collapsed.', metadata={'source': 'docs/content/library/api/layout/expander.md'}),\n",
       " Document(page_content='title: st.columns\\nslug: /library/api-reference/layout/st.columns\\ndescription: st.columns inserts containers laid out as side-by-side columns.', metadata={'source': 'docs/content/library/api/layout/columns.md'}),\n",
       " Document(page_content='title: st.tabs\\nslug: /library/api-reference/layout/st.tabs\\ndescription: st.tabs inserts containers separated into tabs.', metadata={'source': 'docs/content/library/api/layout/tabs.md'}),\n",
       " Document(page_content='title: st.form\\nslug: /library/api-reference/control-flow/st.form\\ndescription: st.form creates a form that batches elements together with a “Submit\" button.', metadata={'source': 'docs/content/library/api/control-flow/form.md'}),\n",
       " Document(page_content='title: st.stop\\nslug: /library/api-reference/control-flow/st.stop\\ndescription: st.stop stops the execution immediately.', metadata={'source': 'docs/content/library/api/control-flow/stop.md'}),\n",
       " Document(page_content='title: st.experimental_rerun\\nslug: /library/api-reference/control-flow/st.experimental_rerun\\ndescription: st.experimental_rerun will rerun the script immediately.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.', metadata={'source': 'docs/content/library/api/control-flow/experimental_rerun.md'}),\n",
       " Document(page_content='title: st.form_submit_button\\nslug: /library/api-reference/control-flow/st.form_submit_button\\ndescription: st.form_submit_button displays a form submit button.', metadata={'source': 'docs/content/library/api/control-flow/form_submit_button.md'}),\n",
       " Document(page_content='title: Control flow\\nslug: /library/api-reference/control-flow\\n\\nControl flow\\n\\nChange execution\\n\\nBy default, Streamlit apps execute the script entirely, but we allow some functionality to handle control flow in your applications.\\n\\nStop execution\\n\\nStops execution immediately.\\n\\npython\\nst.stop()\\n\\nRerun script\\n\\nRerun the script immediately.\\n\\npython\\nst.experimental_rerun()\\n\\nGroup multiple widgets\\n\\nBy default, Streamlit reruns your script everytime a user interacts with your app.\\nHowever, sometimes it\\'s a better user experience to wait until a group of related\\nwidgets is filled before actually rerunning the script. That\\'s what st.form is for!\\n\\nForms\\n\\nCreate a form that batches elements together with a “Submit\" button.\\n\\npython\\nwith st.form(key=\"my_form\"):\\n    username = st.text_input(\"Username\")\\n    password = st.text_input(\"Password\")\\n    st.form_submit_button(\"Login\")\\n\\nForm submit button\\n\\nDisplay a form submit button.\\n\\npython\\nwith st.form(key=\"my_form\"):\\n    username = st.text_input(\"Username\")\\n    password = st.text_input(\"Password\")\\n    st.form_submit_button(\"Login\")\\n\\nAutorefresh\\n\\nForce a refresh without tying up a script. Created by @kmcgrady.\\n\\n```python\\nfrom streamlit_autorefresh import st_autorefresh\\n\\nst_autorefresh(interval=2000, limit=100,\\n  key=\"fizzbuzzcounter\")\\n```\\n\\nPydantic\\n\\nAuto-generate Streamlit UI from Pydantic Models and Dataclasses. Created by @lukasmasuch.\\n\\n```python\\nimport streamlit_pydantic as sp\\n\\nsp.pydantic_form(key=\"my_form\",\\n  model=ExampleModel)\\n```\\n\\nStreamlit Pages\\n\\nAn experimental version of Streamlit Multi-Page Apps. Created by @blackary.\\n\\n```python\\nfrom st_pages import Page, show_pages, add_page_title\\n\\nshow_pages([ Page(\"streamlit_app.py\", \"Home\", \"🏠\"),\\n  Page(\"other_pages/page2.py\", \"Page 2\", \":books:\"), ])\\n```', metadata={'source': 'docs/content/library/api/control-flow/control-flow.md'}),\n",
       " Document(page_content=\"title: Caching\\nslug: /library/advanced-features/caching\\n\\nDocumentation for the deprecated @st.cache decorator can be found in Optimize performance with st.cache.\\n\\nCaching\\n\\nStreamlit runs your script from top to bottom at every user interaction or code change. This execution model makes development super easy. But it comes with two major challenges:\\n\\nLong-running functions run again and again, which slows down your app.\\n\\nObjects get recreated again and again, which makes it hard to persist them across reruns or sessions.\\n\\nBut don't worry! Streamlit lets you tackle both issues with its built-in caching mechanism. Caching stores the results of slow function calls, so they only need to run once. This makes your app much faster and helps with persisting objects across reruns.\\n\\nMinimal example\\n\\nBasic usage\\n\\nAdvanced usage\\n\\nMigrating from st.cache\\n\\nMinimal example\\n\\nTo cache a function in Streamlit, you must decorate it with one of two decorators (st.cache_data or st.cache_resource):\\n\\npython\\n@st.cache_data\\ndef long_running_function(param1, param2):\\n    return …\\n\\nIn this example, decorating long_running_function with @st.cache_data tells Streamlit that whenever the function is called, it checks two things:\\n\\nThe values of the input parameters (in this case, param1 and param2).\\n\\nThe code inside the function.\\n\\nIf this is the first time Streamlit sees these parameter values and function code, it runs the function and stores the return value in a cache. The next time the function is called with the same parameters and code (e.g., when a user interacts with the app), Streamlit will skip executing the function altogether and return the cached value instead. During development, the cache updates automatically as the function code changes, ensuring that the latest changes are reflected in the cache.\\n\\nAs mentioned, there are two caching decorators:\", metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='As mentioned, there are two caching decorators:\\n\\nst.cache_data\\xa0is the recommended way to cache computations that return data: loading a DataFrame from CSV, transforming a NumPy array, querying an API, or any other function that returns a serializable data object (str, int, float, DataFrame, array, list, …). It creates a new copy of the data at each function call, making it safe against mutations and race conditions. The behavior of st.cache_data is what you want in most cases – so if you\\'re unsure, start with\\xa0st.cache_data\\xa0and see if it works!\\n\\nst.cache_resource\\xa0is the recommended way to cache global resources like ML models or database connections – unserializable objects that you don\\'t want to load multiple times. Using it, you can share these resources across all reruns and sessions of an app without copying or duplication. Note that any mutations to the cached return value directly mutate the object in the cache (more details below).\\n\\nBasic usage\\n\\nst.cache_data\\n\\nst.cache_data is your go-to command for all functions that return data – whether DataFrames, NumPy arrays, str, int, float, or other serializable types. It\\'s the right command for almost all use cases!\\n\\nUsage\\n\\nLet\\'s look at an example of using\\xa0st.cache_data. Suppose your app loads the Uber ride-sharing dataset – a CSV file of 50 MB – from the internet into a DataFrame:\\n\\n```python\\ndef load_data(url):\\n    df = pd.read_csv(url)  # 👈 Download the data\\n    return df\\n\\ndf = load_data(\"https://github.com/plotly/datasets/raw/master/uber-rides-data1.csv\")\\nst.dataframe(df)\\n\\nst.button(\"Rerun\")\\n```\\n\\nRunning the load_data function takes 2 to 30 seconds, depending on your internet connection. (Tip: if you are on a slow connection, use this 5 MB dataset instead). Without caching, the download is rerun each time the app is loaded or with user interaction. Try it yourself by clicking the button we added! Not a great experience… 😕\\n\\nNow let\\'s add the\\xa0@st.cache_data\\xa0decorator on load_data:', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='```python\\n@st.cache_data  # 👈 Add the caching decorator\\ndef load_data(url):\\n    df = pd.read_csv(url)\\n    return df\\n\\ndf = load_data(\"https://github.com/plotly/datasets/raw/master/uber-rides-data1.csv\")\\nst.dataframe(df)\\n\\nst.button(\"Rerun\")\\n```\\n\\nRun the app again. You\\'ll notice that the slow download only happens on the first run. Every subsequent rerun should be almost instant! 💨\\n\\nBehavior\\n\\nHow does this work? Let\\'s go through the behavior of st.cache_data step by step:\\n\\nOn the first run, Streamlit recognizes that it has never called the load_data function with the specified parameter value (the URL of the CSV file) So it runs the function and downloads the data.\\n\\nNow our caching mechanism becomes active: the returned DataFrame is serialized (converted to bytes) via\\xa0pickle\\xa0and stored in the cache (together with the value of the url parameter).\\n\\nOn the next run, Streamlit checks the cache for an entry of load_data with the specific url. There is one! So it retrieves the cached object, deserializes it to a DataFrame, and returns it instead of re-running the function and downloading the data again.\\n\\nThis process of serializing and deserializing the cached object creates a copy of our original DataFrame. While this copying behavior may seem unnecessary, it\\'s what we want when caching data objects since it effectively prevents mutation and concurrency issues. Read the section “Mutation and concurrency issues\" below to understand this in more detail.\\n\\nExamples\\n\\nDataFrame transformations\\n\\nIn the example above, we already showed how to cache loading a DataFrame. It can also be useful to cache DataFrame transformations such as df.filter, df.apply, or df.sort_values. Especially with large DataFrames, these operations can be slow.\\n\\npython\\n@st.cache_data\\ndef transform(df):\\n    df = df.filter(items=[\\'one\\', \\'three\\'])\\n    df = df.apply(np.sum, axis=0)\\n    return df\\n\\nArray computations\\n\\nSimilarly, it can make sense to cache computations on NumPy arrays:', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='python\\n@st.cache_data\\ndef add(arr1, arr2):\\n    return arr1 + arr2\\n\\nDatabase queries\\n\\nYou usually make SQL queries to load data into your app when working with databases. Repeatedly running these queries can be slow, cost money, and degrade the performance of your database. We strongly recommend caching any database queries in your app. See also our guides on connecting Streamlit to different databases for in-depth examples.\\n\\n```python\\nconnection = database.connect()\\n\\n@st.cache_data\\ndef query():\\n    return pd.read_sql_query(\"SELECT * from table\", connection)\\n```\\n\\nYou should set a ttl (time to live) to get new results from your database. If you set st.cache_data(ttl=3600), Streamlit invalidates any cached values after 1 hour (3600 seconds) and runs the cached function again. See details in Controlling cache size and duration.\\n\\nAPI calls\\n\\nSimilarly, it makes sense to cache API calls. Doing so also avoids rate limits.\\n\\npython\\n@st.cache_data\\ndef api_call():\\n    response = requests.get(\\'https://jsonplaceholder.typicode.com/posts/1\\')\\n    return response.json()\\n\\nRunning ML models (inference)\\n\\nRunning complex machine learning models can use significant time and memory. To avoid rerunning the same computations over and over, use caching.\\n\\npython\\n@st.cache_data\\ndef run_model(inputs):\\n    return model(inputs)\\n\\nst.cache_resource\\n\\nst.cache_resource is the right command to cache “resources\" that should be available globally across all users, sessions, and reruns. It has more limited use cases than st.cache_data, especially for caching database connections and ML models.\\n\\nUsage\\n\\nAs an example for st.cache_resource, let\\'s look at a typical machine learning app. As a first step, we need to load an ML model. We do this with Hugging Face\\'s transformers library:\\n\\npython\\nfrom transformers import pipeline\\nmodel = pipeline(\"sentiment-analysis\")  # 👈 Load the model', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='If we put this code into a Streamlit app directly, the app will load the model at each rerun or user interaction. Repeatedly loading the model poses two problems:\\n\\nLoading the model takes time and slows down the app.\\n\\nEach session loads the model from scratch, which takes up a huge amount of memory.\\n\\nInstead, it would make much more sense to load the model once and use that same object across all users and sessions. That\\'s exactly the use case for st.cache_resource! Let\\'s add it to our app and process some text the user entered:\\n\\n```python\\nfrom transformers import pipeline\\n\\n@st.cache_resource  # 👈 Add the caching decorator\\ndef load_model():\\n    return pipeline(\"sentiment-analysis\")\\n\\nmodel = load_model()\\n\\nquery = st.text_input(\"Your query\", value=\"I love Streamlit! 🎈\")\\nif query:\\n    result = model(query)[0]  # 👈 Classify the query text\\n    st.write(result)\\n```\\n\\nIf you run this app, you\\'ll see that the app calls load_model only once – right when the app starts. Subsequent runs will reuse that same model stored in the cache, saving time and memory!\\n\\nBehavior\\n\\nUsing st.cache_resource is very similar to using st.cache_data. But there are a few important differences in behavior:\\n\\nst.cache_resource does not create a copy of the cached return value but instead stores the object itself in the cache. All mutations on the function\\'s return value directly affect the object in the cache, so you must ensure that mutations from multiple sessions do not cause problems. In short, the return value must be thread-safe.\\n\\nUsing st.cache_resource on objects that are not thread-safe might lead to crashes or corrupted data. Learn more below under Mutation and concurrency issues.\\n\\nNot creating a copy means there\\'s just one global instance of the cached return object, which saves memory, e.g. when using a large ML model. In computer science terms, we create a singleton.', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='Return values of functions do not need to be serializable. This behavior is great for types not serializable by nature, e.g., database connections, file handles, or threads. Caching these objects with st.cache_data is not possible.\\n\\nExamples\\n\\nDatabase connections\\n\\nst.cache_resource is useful for connecting to databases. Usually, you\\'re creating a connection object that you want to reuse globally for every query. Creating a new connection object at each run would be inefficient and might lead to connection errors. That\\'s exactly what st.cache_resource can do, e.g., for a Postgres database:\\n\\n```python\\n@st.cache_resource\\ndef init_connection():\\n    host = \"hh-pgsql-public.ebi.ac.uk\"\\n    database = \"pfmegrnargs\"\\n    user = \"reader\"\\n    password = \"NWDMCE5xdipIjRrp\"\\n    return psycopg2.connect(host=host, database=database, user=user, password=password)\\n\\nconn = init_connection()\\n```\\n\\nOf course, you can do the same for any other database. Have a look at our guides on how to connect Streamlit to databases for in-depth examples.\\n\\nLoading ML models\\n\\nYour app should always cache ML models, so they are not loaded into memory again for every new session. See the example above for how this works with 🤗\\xa0Hugging Face models. You can do the same thing for PyTorch, TensorFlow, etc. Here\\'s an example for PyTorch:\\n\\n```python\\n@st.cache_resource\\ndef load_model():\\n    model = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\\n    model.eval()\\n    return model\\n\\nmodel = load_model()\\n```\\n\\nDeciding which caching decorator to use\\n\\nThe sections above showed many common examples for each caching decorator. But there are edge cases for which it\\'s less trivial to decide which caching decorator to use. Eventually, it all comes down to the difference between “data\" and “resource\":', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='Data are serializable objects (objects that can be converted to bytes via\\xa0pickle) that you could easily save to disk. Imagine all the types you would usually store in a database or on a file system – basic types like str, int, and float, but also arrays, DataFrames, images, or combinations of these types (lists, tuples, dicts, and so on).\\n\\nResources are unserializable objects that you usually would not save to disk or a database. They are often more complex, non-permanent objects like database connections, ML models, file handles, threads, etc.\\n\\nFrom the types listed above, it should be obvious that most objects in Python are “data.\" That\\'s also why st.cache_data is the correct command for almost all use cases. st.cache_resource is a more exotic command that you should only use in specific situations.\\n\\nOr if you\\'re lazy and don\\'t want to think too much, look up your use case or return type in the table below 😉:', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='| Use case                             |                                                                                                       Typical return types |                                                                                                                                            Caching decorator |\\n| :----------------------------------- | -------------------------------------------------------------------------------------------------------------------------: | -----------------------------------------------------------------------------------------------------------------------------------------------------------: |\\n| Reading a CSV file with pd.read_csv  |                                                                                                           pandas.DataFrame |                                                                                                                                                st.cache_data |\\n| Reading a text file                  |                                                                                                           str, list of str |                                                                                                                                                st.cache_data |\\n| Transforming pandas dataframes       |                                                                                            pandas.DataFrame, pandas.Series |                                                                                                                                                st.cache_data |\\n| Computing with numpy arrays          |                                                                                                              numpy.ndarray |                                                                                                                                                st.cache_data |', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='| Simple computations with basic types |                                                                                                         str, int, float, … |                                                                                                                                                st.cache_data |\\n| Querying a database                  |                                                                                                           pandas.DataFrame |                                                                                                                                                st.cache_data |\\n| Querying an API                      |                                                                                                pandas.DataFrame, str, dict |                                                                                                                                                st.cache_data |\\n| Running an ML model (inference)      |                                                                                     pandas.DataFrame, str, int, dict, list |                                                                                                                                                st.cache_data |\\n| Creating or processing images        |                                                                                             PIL.Image.Image, numpy.ndarray |                                                                                                                                                st.cache_data |\\n| Creating charts                      |                                                        matplotlib.figure.Figure, plotly.graph_objects.Figure, altair.Chart | st.cache_data (but some libraries require st.cache_resource, since the chart object is not serializable – make sure not to mutate the chart after creation!) |', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='| Loading ML models                    |                                                             transformers.Pipeline, torch.nn.Module, tensorflow.keras.Model |                                                                                                                                            st.cache_resource |\\n| Initializing database connections    | pyodbc.Connection, sqlalchemy.engine.base.Engine, psycopg2.connection, mysql.connector.MySQLConnection, sqlite3.Connection |                                                                                                                                            st.cache_resource |\\n| Opening persistent file handles      |                                                                                                         _io.TextIOWrapper |                                                                                                                                            st.cache_resource |\\n| Opening persistent threads           |                                                                                                           threading.thread |                                                                                                                                            st.cache_resource |', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content=\"Advanced usage\\n\\nControlling cache size and duration\\n\\nIf your app runs for a long time and constantly caches functions, you might run into two problems:\\n\\nThe app runs out of memory because the cache is too large.\\n\\nObjects in the cache become stale, e.g. because you cached old data from a database.\\n\\nYou can combat these problems with the ttl and max_entries parameters, which are available for both caching decorators.\\n\\nThe ttl (time-to-live) parameter\\n\\nttl sets a time to live on a cached function. If that time is up and you call the function again, the app will discard any old, cached values, and the function will be rerun. The newly computed value will then be stored in the cache. This behavior is useful for preventing stale data (problem 2) and the cache from growing too large (problem 1). Especially when pulling data from a database or API, you should always set a ttl so you are not using old data. Here's an example:\\n\\npython\\n@st.cache_data(ttl=3600)  # 👈 Cache data for 1 hour (=3600 seconds)\\ndef get_api_data():\\n    data = api.get(...)\\n    return data\\n\\nYou can also set ttl values using timedelta, e.g., ttl=datetime.timedelta(hours=1).\\n\\nThe max_entries parameter\\n\\nmax_entries sets the maximum number of entries in the cache. An upper bound on the number of cache entries is useful for limiting memory (problem 1), especially when caching large objects. The oldest entry will be removed when a new entry is added to a full cache. Here's an example:\\n\\npython\\n@st.cache_data(max_entries=1000)  # 👈 Maximum 1000 entries in the cache\\ndef get_large_array(seed):\\n    np.random.seed(seed)\\n    arr = np.random.rand(100000)\\n    return arr\\n\\nCustomizing the spinner\\n\\nBy default, Streamlit shows a small loading spinner in the app when a cached function is running. You can modify it easily with the show_spinner parameter, which is available for both caching decorators:\", metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='```python\\n@st.cache_data(show_spinner=False)  # 👈 Disable the spinner\\ndef get_api_data():\\n    data = api.get(...)\\n    return data\\n\\n@st.cache_data(show_spinner=\"Fetching data from API...\")  # 👈 Use custom text for spinner\\ndef get_api_data():\\n    data = api.get(...)\\n    return data\\n```\\n\\nExcluding input parameters\\n\\nIn a cached function, all input parameters must be hashable. Let\\'s quickly explain why and what it means. When the function is called, Streamlit looks at its parameter values to determine if it was cached before. Therefore, it needs a reliable way to compare the parameter values across function calls. Trivial for a string or int – but complex for arbitrary objects! Streamlit uses hashing to solve that. It converts the parameter to a stable key and stores that key. At the next function call, it hashes the parameter again and compares it with the stored hash key.\\n\\nUnfortunately, not all parameters are hashable! E.g., you might pass an unhashable database connection or ML model to your cached function. In this case, you can exclude input parameters from caching. Simply prepend the parameter name with an underscore (e.g., _param1), and it will not be used for caching. Even if it changes, Streamlit will return a cached result if all the other parameters match up.\\n\\nHere\\'s an example:\\n\\n```python\\n@st.cache_data\\ndef fetch_data(_db_connection, num_rows):  # 👈 Don\\'t hash _db_connection\\n    data = _db_connection.fetch(num_rows)\\n    return data\\n\\nconnection = init_connection()\\nfetch_data(connection, 10)\\n```\\n\\nBut what if you want to cache a function that takes an unhashable parameter? For example, you might want to cache a function that takes an ML model as input and returns the layer names of that model. Since the model is the only input parameter, you cannot exclude it from caching. In this case you can use the hash_funcs parameter to specify a custom hashing function for the model.\\n\\nThe hash_funcs parameter', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='The hash_funcs parameter\\n\\nAs described above, Streamlit\\'s caching decorators hash the input parameters and cached function\\'s signature to determine whether the function has been run before and has a return value stored (\"cache hit\") or needs to be run (\"cache miss\"). Input parameters that are not hashable by Streamlit\\'s hashing implementation can be ignored by prepending an underscore to their name. But there two rare cases where this is undesirable. i.e. where you want to hash the parameter that Streamlit is unable to hash:\\n\\nWhen Streamlit\\'s hashing mechanism fails to hash a parameter, resulting in a UnhashableParamError being raised.\\n\\nWhen you want to override Streamlit\\'s default hashing mechanism for a parameter.\\n\\nLet\\'s discuss each of these cases in turn with examples.\\n\\nExample 1: Hashing a custom class\\n\\nStreamlit does not know how to hash custom classes. If you pass a custom class to a cached function, Streamlit will raise a UnhashableParamError. For example, let\\'s define a custom class MyCustomClass that accepts an initial integer score. Let\\'s also define a cached function multiply_score that multiplies the score by a multiplier:\\n\\n```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\n@st.cache_data\\ndef multiply_score(obj: MyCustomClass, multiplier: int) -> int:\\n    return obj.my_score * multiplier\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(multiply_score(score, multiplier))\\n```\\n\\nIf you run this app, you\\'ll see that Streamlit raises a UnhashableParamError since it does not know how to hash MyCustomClass:\\n\\npython\\nUnhashableParamError: Cannot hash argument \\'obj\\' (of type __main__.MyCustomClass) in \\'multiply_score\\'.', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='To fix this, we can use the hash_funcs parameter to tell Streamlit how to hash MyCustomClass. We do this by passing a dictionary to hash_funcs that maps the name of the parameter to a hash function. The choice of hash function is up to the developer. In this case, let\\'s define a custom hash function hash_func that takes the custom class as input and returns the score. We want the score to be the unique identifier of the object, so we can use it to deterministically hash the object:\\n\\n```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\ndef hash_func(obj: MyCustomClass) -> int:\\n    return obj.my_score  # or any other value that uniquely identifies the object\\n\\n@st.cache_data(hash_funcs={MyCustomClass: hash_func})\\ndef multiply_score(obj: MyCustomClass, multiplier: int) -> int:\\n    return obj.my_score * multiplier\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(multiply_score(score, multiplier))\\n```\\n\\nNow if you run the app, you\\'ll see that Streamlit no longer raises a UnhashableParamError and the app runs as expected.\\n\\nLet\\'s now consider the case where multiply_score is an attribute of MyCustomClass and we want to hash the entire object:\\n\\n```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(score.multiply_score(multiplier))\\n```\\n\\nIf you run this app, you\\'ll see that Streamlit raises a UnhashableParamError since it cannot hash the argument \\'self\\' (of type __main__.MyCustomClass) in \\'multiply_score\\'. A simple fix here could be to use Python\\'s hash() function to hash the object:\\n\\n```python\\nimport streamlit as st', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(score.multiply_score(multiplier))\\n```\\n\\nAbove, the hash function is defined as lambda x: hash(x.my_score). This creates a hash based on the my_score attribute of the MyCustomClass instance. As long as my_score remains the same, the hash remains the same. Thus, the result of multiply_score can be retrieved from the cache without recomputation.\\n\\nAs an astute Pythonista, you may have been tempted to use Python\\'s id() function to hash the object like so:\\n\\n```python\\nimport streamlit as st\\n\\nclass MyCustomClass:\\n    def init(self, initial_score: int):\\n        self.my_score = initial_score\\n\\ninitial_score = st.number_input(\"Enter initial score\", value=15)\\n\\nscore = MyCustomClass(initial_score)\\nmultiplier = 2\\n\\nst.write(score.multiply_score(multiplier))\\n```\\n\\nThis is why we discourage using it as hash func, and instead encourage functions that return deterministic, true hash values. That said, if you know what you\\'re doing, you can use id() as a hash function. Just be aware of the consequences. For example, id is often the correct hash func when you\\'re passing the result of an @st.cache_resource function as the input param to another cached function. There\\'s a whole class of object types that aren’t otherwise hashable.\\n\\nExample 2: Hashing a Pydantic model\\n\\nLet\\'s consider another example where we want to hash a Pydantic model:\\n\\n```python\\nimport streamlit as st\\nfrom pydantic import BaseModel\\n\\nclass Person(BaseModel):\\n    name: str\\n\\n@st.cache_data\\ndef identity(person: Person):\\n    return person\\n\\nperson = identity(Person(name=\"Lee\"))\\nst.write(f\"The person is {person.name}\")\\n```', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='Above, we define a custom class Person using Pydantic\\'s BaseModel with a single attribute name. We also define an identity function which accepts an instance of Person as an arg and returns it without modification. This function is intended to cache the result, therefore, if called multiple times with the same Person instance, it won\\'t recompute but return the cached instance.\\n\\nIf you run the app, however, you\\'ll run into a UnhashableParamError: Cannot hash argument \\'person\\' (of type __main__.Person) in \\'identity\\'. error. This is because Streamlit does not know how to hash the Person class. To fix this, we can use the hash_funcs kwarg to tell Streamlit how to hash Person.\\n\\nIn the version below, we define a custom hash function hash_func that takes the Person instance as input and returns the name attribute. We want the name to be the unique identifier of the object, so we can use it to deterministically hash the object:\\n\\n```python\\nimport streamlit as st\\nfrom pydantic import BaseModel\\n\\nclass Person(BaseModel):\\n    name: str\\n\\n@st.cache_data(hash_funcs={Person: lambda p: p.name})\\ndef identity(person: Person):\\n    return person\\n\\nperson = identity(Person(name=\"Lee\"))\\nst.write(f\"The person is {person.name}\")\\n```\\n\\nExample 3: Hashing a ML model\\n\\nThere may be cases where you want to pass your favorite machine learning model to a cached function. For example, let\\'s say you want to pass a TensorFlow model to a cached function, based on what model the user selects in the app. You might try something like this:\\n\\n```python\\nimport streamlit as st\\nimport tensorflow as tf\\n\\n@st.cache_resource\\ndef load_base_model(option):\\n    if option == 1:\\n        return tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\")\\n    else:\\n        return tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\")\\n\\n@st.cache_resource\\ndef load_layers(base_model):\\n    return [layer.name for layer in base_model.layers]\\n\\noption = st.radio(\"Model 1 or 2\", [1, 2])', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='option = st.radio(\"Model 1 or 2\", [1, 2])\\n\\nbase_model = load_base_model(option)\\n\\nlayers = load_layers(base_model)\\n\\nst.write(layers)\\n```\\n\\nIn the above app, the user can select one of two models. Based on the selection, the app loads the corresponding model and passes it to load_layers. This function then returns the names of the layers in the model. If you run the app, you\\'ll see that Streamlit raises a UnhashableParamError since it cannot hash the argument \\'base_model\\' (of type keras.engine.functional.Functional) in \\'load_layers\\'.\\n\\nIf you disable hashing for base_model by prepending an underscore to its name, you\\'ll observe that regardless of which base model is chosen, the layers displayed are same. This subtle bug is due to the fact that the load_layers function is not re-run when the base model changes. This is because Streamlit does not hash the base_model argument, so it does not know that the function needs to be re-run when the base model changes.\\n\\n```python\\nimport streamlit as st\\nimport tensorflow as tf\\nfrom keras.engine.functional import Functional\\n\\n@st.cache_resource\\ndef load_base_model(option):\\n    if option == 1:\\n        return tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\")\\n    else:\\n        return tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\")\\n\\n@st.cache_resource(hash_funcs={Functional: lambda x: x.name})\\ndef load_layers(base_model):\\n    return [layer.name for layer in base_model.layers]\\n\\noption = st.radio(\"Model 1 or 2\", [1, 2])\\n\\nbase_model = load_base_model(option)\\n\\nlayers = load_layers(base_model)\\n\\nst.write(layers)\\n```\\n\\nIn the above case, we could also have used hash_funcs={Functional: id} as the hash function. This is because id is often the correct hash func when you\\'re passing the result of an @st.cache_resource function as the input param to another cached function.\\n\\nExample 4: Overriding Streamlit\\'s default hashing mechanism', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='Let\\'s consider another example where we want to override Streamlit\\'s default hashing mechanism for a pytz-localized datetime object:\\n\\n```python\\nfrom datetime import datetime\\nimport pytz\\nimport streamlit as st\\n\\ntz = pytz.timezone(\"Europe/Berlin\")\\n\\n@st.cache_data\\ndef load_data(dt):\\n    return dt\\n\\nnow = datetime.now()\\nst.text(load_data(dt=now))\\n\\nnow_tz = tz.localize(datetime.now())\\nst.text(load_data(dt=now_tz))\\n```\\n\\n```python\\nfrom datetime import datetime\\n\\nimport pytz\\nimport streamlit as st\\n\\ntz = pytz.timezone(\"Europe/Berlin\")\\n\\n@st.cache_data(hash_funcs={datetime: lambda x: x.strftime(\"%a %d %b %Y, %I:%M%p\")})\\ndef load_data(dt):\\n    return dt\\n\\nnow = datetime.now()\\nst.text(load_data(dt=now))\\n\\nnow_tz = tz.localize(datetime.now())\\nst.text(load_data(dt=now_tz))\\n```\\n\\nLet\\'s now consider a case where we want to override Streamlit\\'s default hashing mechanism for NumPy arrays. While Streamlit natively hashes Pandas and NumPy objects, there may be cases where you want to override Streamlit\\'s default hashing mechanism for these objects.\\n\\nFor example, let\\'s say we create a cache-decorated show_data function that accepts a NumPy array and returns it without modification. In the bellow app, data = df[\"str\"].unique() (which is a NumPy array) is passed to the show_data function.\\n\\n```python\\nimport time\\nimport numpy as np\\nimport pandas as pd\\nimport streamlit as st\\n\\n@st.cache_data\\ndef get_data():\\n    df = pd.DataFrame({\"num\": [112, 112, 2, 3], \"str\": [\"be\", \"a\", \"be\", \"c\"]})\\n    return df\\n\\n@st.cache_data\\ndef show_data(data):\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return data\\n\\ndf = get_data()\\ndata = df[\"str\"].unique()\\n\\nst.dataframe(show_data(data))\\nst.button(\"Re-run\")\\n```', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='Since data is always the same, we expect the show_data function to return the cached value. However, if you run the app, and click the Re-run button, you\\'ll notice that the show_data function is re-run each time. We can assume this behavior is a consequence of Streamlit\\'s default hashing mechanism for NumPy arrays.\\n\\nTo work around this, let\\'s define a custom hash function hash_func that takes a NumPy array as input and returns a string representation of the array:\\n\\n```python\\nimport time\\nimport numpy as np\\nimport pandas as pd\\nimport streamlit as st\\n\\n@st.cache_data\\ndef get_data():\\n    df = pd.DataFrame({\"num\": [112, 112, 2, 3], \"str\": [\"be\", \"a\", \"be\", \"c\"]})\\n    return df\\n\\n@st.cache_data(hash_funcs={np.ndarray: str})\\ndef show_data(data):\\n    time.sleep(2)  # This makes the function take 2s to run\\n    return data\\n\\ndf = get_data()\\ndata = df[\"str\"].unique()\\n\\nst.dataframe(show_data(data))\\nst.button(\"Re-run\")\\n```\\n\\nNow if you run the app, and click the Re-run button, you\\'ll notice that the show_data function is no longer re-run each time. It\\'s important to note here that our choice of hash function was very naive and not necessarily the best choice. For example, if the NumPy array is large, converting it to a string representation may be expensive. In such cases, it is up to you as the developer to define what a good hash function is for your use case.\\n\\nStatic elements\\n\\nSince version 1.16.0, cached functions can contain Streamlit commands! For example, you can do this:\\n\\npython\\n@st.cache_data\\ndef get_api_data():\\n    data = api.get(...)\\n    st.success(\"Fetched data from API!\")  # 👈 Show a success message\\n    return data', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='As we know, Streamlit only runs this function if it hasn\\'t been cached before. On this first run, the st.success message will appear in the app. But what happens on subsequent runs? It still shows up! Streamlit realizes that there is an st. command inside the cached function, saves it during the first run, and replays it on subsequent runs. Replaying static elements works for both caching decorators.\\n\\nYou can also use this functionality to cache entire parts of your UI:\\n\\npython\\n@st.cache_data\\ndef show_data():\\n    st.header(\"Data analysis\")\\n    data = api.get(...)\\n    st.success(\"Fetched data from API!\")\\n    st.write(\"Here is a plot of the data:\")\\n    st.line_chart(data)\\n    st.write(\"And here is the raw data:\")\\n    st.dataframe(data)\\n\\nInput widgets\\n\\nYou can also use interactive input widgets like st.slider or st.text_input in cached functions. Widget replay is an experimental feature at the moment. To enable it, you need to set the experimental_allow_widgets parameter:\\n\\npython\\n@st.cache_data(experimental_allow_widgets=True)  # 👈 Set the parameter\\ndef get_data():\\n    num_rows = st.slider(\"Number of rows to get\")  # 👈 Add a slider\\n    data = api.get(..., num_rows)\\n    return data\\n\\nStreamlit treats the slider like an additional input parameter to the cached function. If you change the slider position, Streamlit will see if it has already cached the function for this slider value. If yes, it will return the cached value. If not, it will rerun the function using the new slider value.', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content=\"Using widgets in cached functions is extremely powerful because it lets you cache entire parts of your app. But it can be dangerous! Since Streamlit treats the widget value as an additional input parameter, it can easily lead to excessive memory usage. Imagine your cached function has five sliders and returns a 100 MB DataFrame. Then we'll add 100 MB to the cache for every permutation of these five slider values – even if the sliders do not influence the returned data! These additions can make your cache explode very quickly. Please be aware of this limitation if you use widgets in cached functions. We recommend using this feature only for isolated parts of your UI where the widgets directly influence the cached return value.\\n\\nSupport for widgets in cached functions is experimental. We may change or remove it anytime without warning. Please use it with care!\\n\\nTwo widgets are currently not supported in cached functions: st.file_uploader and st.camera_input. We may support them in the future. Feel free to open a GitHub issue if you need them!\\n\\nDealing with large data\\n\\nAs we explained, you should cache data objects with st.cache_data. But this can be slow for extremely large data, e.g., DataFrames or arrays with >100 million rows. That's because of the copying behavior of st.cache_data: on the first run, it serializes the return value to bytes and deserializes it on subsequent runs. Both operations take time.\\n\\nIf you're dealing with extremely large data, it can make sense to use st.cache_resource instead. It does not create a copy of the return value via serialization/deserialization and is almost instant. But watch out: any mutation to the function's return value (such as dropping a column from a DataFrame or setting a value in an array) directly manipulates the object in the cache. You must ensure this doesn't corrupt your data or lead to crashes. See the section on Mutation and concurrency issues below.\", metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content=\"When benchmarking st.cache_data on pandas DataFrames with four columns, we found that it becomes slow when going beyond 100 million rows. The table shows runtimes for both caching decorators at different numbers of rows (all with four columns):\\n\\n|                   |                 | 10M rows | 50M rows | 100M rows | 200M rows |\\n| ----------------- | --------------- | :------: | :------: | :-------: | :-------: |\\n| st.cache_data     | First run*     |  0.4 s   |   3 s    |   14 s    |   28 s    |\\n|                   | Subsequent runs |  0.2 s   |   1 s    |    2 s    |    7 s    |\\n| st.cache_resource | First run*     |  0.01 s  |  0.1 s   |   0.2 s   |    1 s    |\\n|                   | Subsequent runs |   0 s    |   0 s    |    0 s    |    0 s    |\\n\\n|                                                                                                                                                              |\\n| :----------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| *For the first run, the table only shows the overhead time of using the caching decorator. It does not include the runtime of the cached function itself. |\\n\\nMutation and concurrency issues\\n\\nIn the sections above, we talked a lot about issues when mutating return objects of cached functions. This topic is complicated! But it's central to understanding the behavior differences between st.cache_data and st.cache_resource. So let's dive in a bit deeper.\\n\\nFirst, we should clearly define what we mean by mutations and concurrency:\\n\\nBy mutations, we mean any changes made to a cached function's return value after that function has been called. I.e. something like this:\\n\\n```python\\n  @st.cache_data\\n  def create_list():\\n      l = [1, 2, 3]\\n\\nl = create_list()  # 👈 Call the function\\n  l[0] = 2  # 👈 Mutate its return value\\n  ```\", metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='By concurrency, we mean that multiple sessions can cause these mutations at the same time. Streamlit is a web framework that needs to handle many users and sessions connecting to an app. If two people view an app at the same time, they will both cause the Python script to rerun, which may manipulate cached return objects at the same time – concurrently.\\n\\nMutating cached return objects can be dangerous. It can lead to exceptions in your app and even corrupt your data (which can be worse than a crashed app!). Below, we\\'ll first explain the copying behavior of st.cache_data and show how it can avoid mutation issues. Then, we\\'ll show how concurrent mutations can lead to data corruption and how to prevent it.\\n\\nCopying behavior\\n\\nst.cache_data creates a copy of the cached return value each time the function is called. This avoids most mutations and concurrency issues. To understand it in detail, let\\'s go back to the Uber ridesharing example from the section on st.cache_data above. We are making two modifications to it:\\n\\nWe are using st.cache_resource instead of st.cache_data. st.cache_resource does not create a copy of the cached object, so we can see what happens without the copying behavior.\\n\\nAfter loading the data, we manipulate the returned DataFrame (in place!) by dropping the column \"Lat\".\\n\\nHere\\'s the code:\\n\\n```python\\n@st.cache_resource   # 👈 Turn off copying behavior\\ndef load_data(url):\\n    df = pd.read_csv(url)\\n    return df\\n\\ndf = load_data(\"https://raw.githubusercontent.com/plotly/datasets/master/uber-rides-data1.csv\")\\nst.dataframe(df)\\n\\ndf.drop(columns=[\\'Lat\\'], inplace=True)  # 👈 Mutate the dataframe inplace\\n\\nst.button(\"Rerun\")\\n```\\n\\nLet\\'s run it and see what happens! The first run should work fine. But in the second run, you see an exception: KeyError: \"[\\'Lat\\'] not found in axis\". Why is that happening? Let\\'s go step by step:', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='On the first run, Streamlit runs load_data and stores the resulting DataFrame in the cache. Since we\\'re using st.cache_resource, it does not create a copy but stores the original DataFrame.\\n\\nThen we drop the column \"Lat\" from the DataFrame. Note that this is dropping the column from the original DataFrame stored in the cache. We are manipulating it!\\n\\nOn the second run, Streamlit returns that exact same manipulated DataFrame from the cache. It does not have the column \"Lat\" anymore! So our call to df.drop results in an exception. Pandas cannot drop a column that doesn\\'t exist.\\n\\nThe copying behavior of st.cache_data prevents this kind of mutation error. Mutations can only affect a specific copy and not the underlying object in the cache. The next rerun will get its own, unmutated copy of the DataFrame. You can try it yourself, just replace st.cache_resource with st.cache_data above, and you\\'ll see that everything works.\\n\\nBecause of this copying behavior,\\xa0st.cache_data\\xa0is the recommended way to cache data transforms and computations – anything that returns a serializable object.\\n\\nConcurrency issues\\n\\nNow let\\'s look at what can happen when multiple users concurrently mutate an object in the cache. Let\\'s say you have a function that returns a list. Again, we are using st.cache_resource to cache it so that we are not creating a copy:\\n\\n```python\\n@st.cache_resource\\ndef create_list():\\n    l = [1, 2, 3]\\n    return l\\n\\nl = create_list()\\nfirst_list_value = l[0]\\nl[0] = first_list_value + 1\\n\\nst.write(\"l[0] is:\", l[0])\\n```\\n\\nLet\\'s say user A runs the app. They will see the following output:\\n\\npython\\nl[0] is: 2\\n\\nLet\\'s say another user, B, visits the app right after. In contrast to user A, they will see the following output:\\n\\npython\\nl[0] is: 3\\n\\nNow, user A reruns the app immediately after user B. They will see the following output:\\n\\npython\\nl[0] is: 4\\n\\nWhat is happening here? Why are all outputs different?', metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content=\"When user A visits the app,\\xa0create_list()\\xa0is called, and the list\\xa0[1, 2, 3]\\xa0is stored in the cache. This list is then returned to user A. The first value of the list, 1, is assigned to first_list_value , and l[0]\\xa0is changed to 2.\\n\\nWhen user B visits the app,\\xa0create_list()\\xa0returns the mutated list from the cache:\\xa0[2, 2, 3]. The first value of the list, 2, is assigned to first_list_value and l[0]\\xa0is changed to 3.\\n\\nWhen user A reruns the app,\\xa0create_list()\\xa0returns the mutated list again:\\xa0[3, 2, 3]. The first value of the list, 3, is assigned to first_list_value, and l[0]\\xa0is changed to 4.\\n\\nIf you think about it, this makes sense. Users A and B use the same list object (the one stored in the cache). And since the list object is mutated, user A's change to the list object is also reflected in user B's app.\\n\\nThis is why you must be careful about mutating objects cached with st.cache_resource, especially when multiple users access the app concurrently. If we had used\\xa0st.cache_data\\xa0instead of\\xa0st.cache_resource, the app would have copied the list object for each user, and the above example would have worked as expected – users A and B would have both seen:\\n\\npython\\nl[0] is: 2\\n\\nThis toy example might seem benign. But data corruption can be extremely dangerous! Imagine we had worked with the financial records of a large bank here. You surely don't want to wake up with less money on your account just because someone used the wrong caching decorator 😉\\n\\nMigrating from st.cache\\n\\nWe introduced the caching commands described above in Streamlit 1.18.0. Before that, we had one catch-all command st.cache. Using it was often confusing, resulted in weird exceptions, and was slow. That's why we replaced st.cache with the new commands in 1.18.0 (read more in this blog post). The new commands provide a more intuitive and efficient way to cache your data and resources and are intended to replace st.cache in all new development.\", metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content=\"If your app is still using st.cache, don't despair! Here are a few notes on migrating:\\n\\nst.cache is deprecated. • New versions of Streamlit will show a deprecation warning if your app uses it.\\n\\nWe will not remove st.cache soon, so you don't need to worry about your 2-year-old app breaking. But we encourage you to try the new commands going forward – they will be way less annoying!\\n\\nSwitching code to the new commands should be easy in most cases. To decide whether to use st.cache_data or st.cache_resource, read Deciding which caching decorator to use. Streamlit will also recognize common use cases and show hints right in the deprecation warnings.\\n\\nMost parameters from st.cache are also present in the new commands, with a few exceptions:\\n\\nallow_output_mutation does not exist anymore. You can safely delete it. Just make sure you use the right caching command for your use case.\\n\\nsuppress_st_warning does not exist anymore. You can safely delete it. Cached functions can now contain Streamlit commands and will replay them. If you want to use widgets inside cached functions, set experimental_allow_widgets=True. See here.\\n\\nhash_funcs does not exist anymore. You can exclude parameters from caching (and being hashed) by prepending them with an underscore: _excluded_param. See here.\\n\\nIf you have any questions or issues during the migration process, please contact us on the forum, and we will be happy to assist you. 🎈\", metadata={'source': 'docs/content/library/advanced-features/caching.md'}),\n",
       " Document(page_content='title: st.write\\nslug: /library/api-reference/write-magic/st.write\\ndescription: st.write writes arguments to the app.\\n\\nFeatured video\\n\\nLearn what the st.write and magic commands are and how to use them.', metadata={'source': 'docs/content/library/api/write-magic/write.md'}),\n",
       " Document(page_content='title: st.multiselect\\nslug: /library/api-reference/widgets/st.multiselect\\ndescription: st.multiselect displays a multiselect widget. The multiselect widget starts as empty.', metadata={'source': 'docs/content/library/api/widgets/multiselect.md'}),\n",
       " Document(page_content=\"title: Magic\\nslug: /library/api-reference/write-magic/magic\\n\\nMagic\\n\\nMagic commands are a feature in Streamlit that allows you to write almost anything (markdown, data,\\ncharts) without having to type an explicit command at all. Just put the thing you want to show on\\nits own line of code, and it will appear in your app. Here's an example:\\n\\n```python\\n\\nDraw a title and some text to the app:\\n\\n'''\\n\\nThis is the document title\\n\\nThis is some markdown.\\n'''\\n\\nimport pandas as pd\\ndf = pd.DataFrame({'col1': [1,2,3]})\\ndf  # 👈 Draw the dataframe\\n\\nx = 10\\n'x', x  # 👈 Draw the string 'x' and then the value of x\\n\\nAlso works with most supported chart types\\n\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\narr = np.random.normal(1, 1, size=100)\\nfig, ax = plt.subplots()\\nax.hist(arr, bins=20)\\n\\nfig  # 👈 Draw a Matplotlib chart\\n```\\n\\nHow Magic works\\n\\nAny time Streamlit sees either a variable or literal\\nvalue on its own line, it automatically writes that to your app using\\nst.write (which you'll learn about later).\\n\\nAlso, magic is smart enough to ignore docstrings. That is, it ignores the\\nstrings at the top of files and functions.\\n\\nIf you prefer to call Streamlit commands more explicitly, you can always turn\\nmagic off in your ~/.streamlit/config.toml with the following setting:\\n\\ntoml\\n[runner]\\nmagicEnabled = false\\n\\nRight now, Magic only works in the main Python app file, not in imported files. See GitHub issue #288 for a discussion of the issues.\\n\\nFeatured video\\n\\nLearn what the st.write and magic commands are and how to use them.\", metadata={'source': 'docs/content/library/api/write-magic/magic.md'}),\n",
       " Document(page_content='title: st.write and magic commands\\nslug: /library/api-reference/write-magic\\n\\nst.write and magic commands\\n\\nStreamlit has two easy ways to display information into your app, which should typically be the\\nfirst thing you try: st.write and magic.\\n\\nst.write\\n\\nWrite arguments to the app.\\n\\npython\\nst.write(\"Hello **world**!\")\\nst.write(my_data_frame)\\nst.write(my_mpl_figure)\\n\\nMagic\\n\\nAny time Streamlit sees either a variable or literal value on its own line, it automatically writes that to your app using st.write\\n\\npython\\n\"Hello **world**!\"\\nmy_data_frame\\nmy_mpl_figure', metadata={'source': 'docs/content/library/api/write-magic/write-magic.md'}),\n",
       " Document(page_content='title: st.radio\\nslug: /library/api-reference/widgets/st.radio\\ndescription: st.radio displays a radio button widget.\\n\\nWidgets can customize how to hide their labels with the label_visibility parameter. If \"hidden\", the label doesn’t show but there is still empty space for it above the widget (equivalent to label=\"\"). If \"collapsed\", both the label and the space are removed. Default is \"visible\". Radio buttons can also be disabled with the disabled parameter, and oriented horizontally with the horizontal parameter:\\n\\n```python\\nimport streamlit as st\\n\\nStore the initial value of widgets in session state\\n\\nif \"visibility\" not in st.session_state:\\n    st.session_state.visibility = \"visible\"\\n    st.session_state.disabled = False\\n    st.session_state.horizontal = False\\n\\ncol1, col2 = st.columns(2)\\n\\nwith col1:\\n    st.checkbox(\"Disable radio widget\", key=\"disabled\")\\n    st.checkbox(\"Orient radio options horizontally\", key=\"horizontal\")\\n\\nwith col2:\\n    st.radio(\\n        \"Set label visibility 👇\",\\n        [\"visible\", \"hidden\", \"collapsed\"],\\n        key=\"visibility\",\\n        label_visibility=st.session_state.visibility,\\n        disabled=st.session_state.disabled,\\n        horizontal=st.session_state.horizontal,\\n    )\\n```\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit\\'s core functions, the radio button! 🔘\\n\\nIn the video below, we\\'ll take it a step further and learn how to combine a button, checkbox and radio button!', metadata={'source': 'docs/content/library/api/widgets/radio.md'}),\n",
       " Document(page_content='title: st.date_input\\nslug: /library/api-reference/widgets/st.date_input\\ndescription: st.date_input displays a date input widget.', metadata={'source': 'docs/content/library/api/widgets/date_input.md'}),\n",
       " Document(page_content='title: st.text_input\\nslug: /library/api-reference/widgets/st.text_input\\ndescription: st.text_input displays a single-line text input widget.\\n\\nText input widgets can customize how to hide their labels with the label_visibility parameter. If \"hidden\", the label doesn’t show but there is still empty space for it above the widget (equivalent to label=\"\"). If \"collapsed\", both the label and the space are removed. Default is \"visible\". Text input widgets can also be disabled with the disabled parameter, and can display an optional placeholder text when the text input is empty using the placeholder parameter:\\n\\n```python\\nimport streamlit as st\\n\\nStore the initial value of widgets in session state\\n\\nif \"visibility\" not in st.session_state:\\n    st.session_state.visibility = \"visible\"\\n    st.session_state.disabled = False\\n\\ncol1, col2 = st.columns(2)\\n\\nwith col1:\\n    st.checkbox(\"Disable text input widget\", key=\"disabled\")\\n    st.radio(\\n        \"Set text input label visibility 👉\",\\n        key=\"visibility\",\\n        options=[\"visible\", \"hidden\", \"collapsed\"],\\n    )\\n    st.text_input(\\n        \"Placeholder for the other text input widget\",\\n        \"This is a placeholder\",\\n        key=\"placeholder\",\\n    )\\n\\nwith col2:\\n    text_input = st.text_input(\\n        \"Enter some text 👇\",\\n        label_visibility=st.session_state.visibility,\\n        disabled=st.session_state.disabled,\\n        placeholder=st.session_state.placeholder,\\n    )\\n\\n```', metadata={'source': 'docs/content/library/api/widgets/text_input.md'}),\n",
       " Document(page_content='title: st.color_picker\\nslug: /library/api-reference/widgets/st.color_picker\\ndescription: st.color_picker displays a color picker widget.', metadata={'source': 'docs/content/library/api/widgets/color_picker.md'}),\n",
       " Document(page_content='title: st.file_uploader\\nslug: /library/api-reference/widgets/st.file_uploader\\ndescription: st.file_uploader displays a file uploader widget.', metadata={'source': 'docs/content/library/api/widgets/file_uploader.md'}),\n",
       " Document(page_content=\"title: st.select_slider\\nslug: /library/api-reference/widgets/st.select_slider\\ndescription: st.select_slider displays a slider widget to select items from a list.\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit's core functions, the select slider! 🎈\\n\\nIn the video below, we'll take it a step further and make a double-ended slider.\", metadata={'source': 'docs/content/library/api/widgets/select_slider.md'}),\n",
       " Document(page_content=\"title: st.button\\nslug: /library/api-reference/widgets/st.button\\ndescription: st.button displays a button widget.\\nkeywords: button\\n\\nAdvanced functionality\\n\\nAlthough a button is the simplest of input widgets, it's very common for buttons to be deeply tied to the use of st.session_state. Check out our advanced guide on Button behavior and examples.\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit's core functions, the button!\\n\\nIn the video below, we'll take it a step further and learn how to combine a button, checkbox and radio button!\", metadata={'source': 'docs/content/library/api/widgets/button.md'}),\n",
       " Document(page_content='title: st.camera_input\\nslug: /library/api-reference/widgets/st.camera_input\\ndescription: st.camera_input displays a widget to upload images from a camera\\n\\nTo read the image file buffer as bytes, you can use getvalue() on the UploadedFile object.\\n\\n```python\\nimport streamlit as st\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as bytes:\\n    bytes_data = img_file_buffer.getvalue()\\n    # Check the type of bytes_data:\\n    # Should output: \\n    st.write(type(bytes_data))\\n```\\n\\nst.camera_input returns an object of the UploadedFile class, which a subclass of BytesIO. Therefore it is a \"file-like\" object. This means you can pass it anywhere where a file is expected, similar to st.file_uploader.\\n\\nImage processing examples\\n\\nPillow,\\n\\nNumPy,\\n\\nOpenCV,\\n\\nTensorFlow,\\n\\ntorchvision, and\\n\\nPyTorch.\\n\\nWhile we provide examples for the most popular use-cases and libraries, you are welcome to adapt these examples to your own needs and favorite libraries.\\n\\nPillow (PIL) and NumPy\\n\\nEnsure you have installed Pillow and NumPy.\\n\\nTo read the image file buffer as a PIL Image and convert it to a NumPy array:\\n\\n```python\\nimport streamlit as st\\nfrom PIL import Image\\nimport numpy as np\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a PIL Image:\\n    img = Image.open(img_file_buffer)\\n\\n```\\n\\nOpenCV (cv2)\\n\\nEnsure you have installed OpenCV and NumPy.\\n\\nTo read the image file buffer with OpenCV:\\n\\n```python\\nimport streamlit as st\\nimport cv2\\nimport numpy as np\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer with OpenCV:\\n    bytes_data = img_file_buffer.getvalue()\\n    cv2_img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)\\n\\n```\\n\\nTensorFlow\\n\\nEnsure you have installed TensorFlow.\\n\\nTo read the image file buffer as a 3 dimensional uint8 tensor with TensorFlow:', metadata={'source': 'docs/content/library/api/widgets/camera_input.md'}),\n",
       " Document(page_content='```python\\nimport streamlit as st\\nimport tensorflow as tf\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a 3D uint8 tensor with TensorFlow:\\n    bytes_data = img_file_buffer.getvalue()\\n    img_tensor = tf.io.decode_image(bytes_data, channels=3)\\n\\n```\\n\\nTorchvision\\n\\nEnsure you have installed Torchvision (it is not bundled with PyTorch) and PyTorch.\\n\\nTo read the image file buffer as a 3 dimensional uint8 tensor with torchvision.io:\\n\\n```python\\nimport streamlit as st\\nimport torch\\nimport torchvision\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a 3D uint8 tensor with torchvision.io:\\n    bytes_data = img_file_buffer.getvalue()\\n    torch_img = torchvision.io.decode_image(\\n        torch.frombuffer(bytes_data, dtype=torch.uint8)\\n    )\\n\\n```\\n\\nPyTorch\\n\\nEnsure you have installed PyTorch and NumPy.\\n\\nTo read the image file buffer as a 3 dimensional uint8 tensor with PyTorch:\\n\\n```python\\nimport streamlit as st\\nimport torch\\nimport numpy as np\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a 3D uint8 tensor with PyTorch:\\n    bytes_data = img_file_buffer.getvalue()\\n    torch_img = torch.ops.image.decode_image(\\n        torch.from_numpy(np.frombuffer(bytes_data, np.uint8)), 3\\n    )\\n\\n```', metadata={'source': 'docs/content/library/api/widgets/camera_input.md'}),\n",
       " Document(page_content='title: st.download_button\\nslug: /library/api-reference/widgets/st.download_button\\ndescription: st.download_button displays a download button widget.', metadata={'source': 'docs/content/library/api/widgets/download_button.md'}),\n",
       " Document(page_content='title: st.number_input\\nslug: /library/api-reference/widgets/st.number_input\\ndescription: st.number_input displays a numeric input widget.', metadata={'source': 'docs/content/library/api/widgets/number_input.md'}),\n",
       " Document(page_content=\"title: st.slider\\nslug: /library/api-reference/widgets/st.slider\\ndescription: st.slider displays a slider widget.\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit's core functions, the slider!\\n\\nIn the video below, we'll take it a step further and make a double-ended slider.\", metadata={'source': 'docs/content/library/api/widgets/slider.md'}),\n",
       " Document(page_content='title: st.text_area\\nslug: /library/api-reference/widgets/st.text_area\\ndescription: st.text_area displays a multi-line text input widget.', metadata={'source': 'docs/content/library/api/widgets/text_area.md'}),\n",
       " Document(page_content='title: st.time_input\\nslug: /library/api-reference/widgets/st.time_input\\ndescription: st.time_input displays a time input widget.', metadata={'source': 'docs/content/library/api/widgets/time_input.md'}),\n",
       " Document(page_content=\"title: st.checkbox\\nslug: /library/api-reference/widgets/st.checkbox\\ndescription: st.checkbox displays a checkbox widget.\\n\\nFeatured videos\\n\\nCheck out our video on how to use one of Streamlit's core functions, the checkbox! ☑\\n\\nIn the video below, we'll take it a step further and learn how to combine a button, checkbox and radio button!\", metadata={'source': 'docs/content/library/api/widgets/checkbox.md'}),\n",
       " Document(page_content='title: st.selectbox\\nslug: /library/api-reference/widgets/st.selectbox\\ndescription: st.selectbox displays a select widget.\\n\\nSelect widgets can customize how to hide their labels with the label_visibility parameter. If \"hidden\", the label doesn’t show but there is still empty space for it above the widget (equivalent to label=\"\"). If \"collapsed\", both the label and the space are removed. Default is \"visible\". Select widgets can also be disabled with the disabled parameter:\\n\\n```python\\nimport streamlit as st\\n\\nStore the initial value of widgets in session state\\n\\nif \"visibility\" not in st.session_state:\\n    st.session_state.visibility = \"visible\"\\n    st.session_state.disabled = False\\n\\ncol1, col2 = st.columns(2)\\n\\nwith col1:\\n    st.checkbox(\"Disable selectbox widget\", key=\"disabled\")\\n    st.radio(\\n        \"Set selectbox label visibility 👉\",\\n        key=\"visibility\",\\n        options=[\"visible\", \"hidden\", \"collapsed\"],\\n    )\\n\\nwith col2:\\n    option = st.selectbox(\\n        \"How would you like to be contacted?\",\\n        (\"Email\", \"Home phone\", \"Mobile phone\"),\\n        label_visibility=st.session_state.visibility,\\n        disabled=st.session_state.disabled,\\n    )\\n```', metadata={'source': 'docs/content/library/api/widgets/selectbox.md'}),\n",
       " Document(page_content=\"title: Connections and databases\\nslug: /library/api-reference/connections\\n\\nConnections and databases\\n\\nSetup your connection\\n\\nCreate a connection\\n\\nConnect to a data source or API\\n\\npython\\nconn = st.experimental_connection('pets_db', type='sql')\\npet_owners = conn.query('select * from pet_owners')\\nst.dataframe(pet_owners)\\n\\nBuilt-in connections\\n\\nSQLConnection\\n\\nA connection to a SQL database using SQLAlchemy.\\n\\npython\\nconn = st.experimental_connection('sql')\\n\\nSnowparkConnection\\n\\nA connection to Snowflake Snowpark.\\n\\npython\\nconn = st.experimental_connection('snowpark')\\n\\nThird-party connections\\n\\nConnection base class\\n\\nBuild your own connection with ExperimentalBaseConnection.\\n\\npython\\nclass MyConnection(ExperimentalBaseConnection[myconn.MyConnection]):\\n    def _connect(self, **kwargs) -> MyConnection:\\n        return myconn.connect(**self._secrets, **kwargs)\\n    def query(self, query):\\n        return self._instance.query(query)\", metadata={'source': 'docs/content/library/api/connections/connections.md'}),\n",
       " Document(page_content='title: st.connections.SnowparkConnection\\nslug: /library/api-reference/connections/st.connections.snowparkconnection\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nThis page only contains on the st.connections.SnowparkConnection API. For a deeper dive into creating and managing data connections within Streamlit apps, read Connecting to data.', metadata={'source': 'docs/content/library/api/connections/connections-snowpark.md'}),\n",
       " Document(page_content=\"title: Session State\\nslug: /library/api-reference/session-state\\ndescription: st.session_state is a way to share variables between reruns, for each user session.\\n\\nSession State\\n\\nSession State is a way to share variables between reruns, for each user session. In addition to the ability to store and persist state, Streamlit also exposes the ability to manipulate state using Callbacks. Session state also persists across apps inside a multipage app.\\n\\nCheck out this Session State basics tutorial video by Streamlit Developer Advocate Dr. Marisa Smith to get started:\\n\\nInitialize values in Session State\\n\\nThe Session State API follows a field-based API, which is very similar to Python dictionaries:\\n\\n```python\\n\\nInitialization\\n\\nif 'key' not in st.session_state:\\n    st.session_state['key'] = 'value'\\n\\nSession State also supports attribute based syntax\\n\\nif 'key' not in st.session_state:\\n    st.session_state.key = 'value'\\n```\\n\\nReads and updates\\n\\nRead the value of an item in Session State and display it by passing to st.write :\\n\\n```python\\n\\nRead\\n\\nst.write(st.session_state.key)\\n\\nOutputs: value\\n\\n```\\n\\nUpdate an item in Session State by assigning it a value:\\n\\npython\\nst.session_state.key = 'value2'     # Attribute API\\nst.session_state['key'] = 'value2'  # Dictionary like API\\n\\nCurious about what is in Session State? Use st.write or magic:\\n\\n```python\\nst.write(st.session_state)\\n\\nWith magic:\\n\\nst.session_state\\n```\\n\\nStreamlit throws a handy exception if an uninitialized variable is accessed:\\n\\n```python\\nst.write(st.session_state['value'])\\n\\nThrows an exception!\\n\\n```\\n\\nDelete items\\n\\nDelete items in Session State using the syntax to delete items in any Python dictionary:\\n\\n```python\\n\\nDelete a single key-value pair\\n\\ndel st.session_state[key]\\n\\nDelete all the items in Session state\\n\\nfor key in st.session_state.keys():\\n    del st.session_state[key]\\n```\\n\\nSession State can also be cleared by going to Settings → Clear Cache, followed by Rerunning the app.\\n\\nSession State and Widget State association\", metadata={'source': 'docs/content/library/api/state/state.md'}),\n",
       " Document(page_content='Session State and Widget State association\\n\\nEvery widget with a key is automatically added to Session State:\\n\\n```python\\nst.text_input(\"Your name\", key=\"name\")\\n\\nThis exists now:\\n\\nst.session_state.name\\n```\\n\\nUse Callbacks to update Session State\\n\\nA callback is a python function which gets called when an input widget changes.\\n\\nOrder of execution: When updating Session state in response to events, a callback function gets executed first, and then the app is executed from top to bottom.\\n\\nCallbacks can be used with widgets using the parameters on_change (or on_click), args, and kwargs:\\n\\nParameters\\n\\non_change or on_click - The function name to be used as a callback\\n\\nargs (tuple) - List of arguments to be passed to the callback function\\n\\nkwargs (dict) - Named arguments to be passed to the callback function\\n\\nWidgets which support the on_change event:\\n\\nst.checkbox\\n\\nst.color_picker\\n\\nst.date_input\\n\\nst.multiselect\\n\\nst.number_input\\n\\nst.radio\\n\\nst.select_slider\\n\\nst.selectbox\\n\\nst.slider\\n\\nst.text_area\\n\\nst.text_input\\n\\nst.time_input\\n\\nst.file_uploader\\n\\nWidgets which support the on_click event:\\n\\nst.button\\n\\nst.download_button\\n\\nst.form_submit_button\\n\\nTo add a callback, define a callback function above the widget declaration and pass it to the widget via the on_change (or on_click ) parameter.\\n\\nForms and Callbacks\\n\\nWidgets inside a form can have their values be accessed and set via the Session State API. st.form_submit_button can have a callback associated with it. The callback gets executed upon clicking on the submit button. For example:\\n\\n```python\\ndef form_callback():\\n    st.write(st.session_state.my_slider)\\n    st.write(st.session_state.my_checkbox)\\n\\nwith st.form(key=\\'my_form\\'):\\n    slider_input = st.slider(\\'My slider\\', 0, 10, 5, key=\\'my_slider\\')\\n    checkbox_input = st.checkbox(\\'Yes or No\\', key=\\'my_checkbox\\')\\n    submit_button = st.form_submit_button(label=\\'Submit\\', on_click=form_callback)\\n```\\n\\nCaveats and limitations', metadata={'source': 'docs/content/library/api/state/state.md'}),\n",
       " Document(page_content=\"Caveats and limitations\\n\\nOnly the st.form_submit_button has a callback in forms. Other widgets inside a form are not allowed to have callbacks.\\n\\non_change and on_click events are only supported on input type widgets.\\n\\nModifying the value of a widget via the Session state API, after instantiating it, is not allowed and will raise a StreamlitAPIException. For example:\\n\\n```python\\n  slider = st.slider(\\n      label='My Slider', min_value=1,\\n      max_value=10, value=5, key='my_slider')\\n\\nst.session_state.my_slider = 7\\n\\n# Throws an exception!\\n  ```\\n\\nSetting the widget state via the Session State API and using the value parameter in the widget declaration is not recommended, and will throw a warning on the first run. For example:\\n\\n```python\\n  st.session_state.my_slider = 7\\n\\nslider = st.slider(\\n      label='Choose a Value', min_value=1,\\n      max_value=10, value=5, key='my_slider')\\n  ```\\n\\nSetting the state of button-like widgets: st.button, st.download_button, and st.file_uploader via the Session State API is not allowed. Such type of widgets are by default False and have ephemeral True states which are only valid for a single run. For example:\\n\\n```python\\n  if 'my_button' not in st.session_state:\\n      st.session_state.my_button = True\\n\\nst.button('My button', key='my_button')\\n\\n# Throws an exception!\\n  ```\", metadata={'source': 'docs/content/library/api/state/state.md'}),\n",
       " Document(page_content=\"title: st.experimental_connection\\nslug: /library/api-reference/connections/st.experimental_connection\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nThis page only contains on the st.experimental_connection API. For a deeper dive into creating and managing data connections within Streamlit apps, read Connecting to data.\\n\\nFor a comprehensive overview of this feature, check out this video tutorial by Joshua Carroll, Streamlit's Product Manager for Developer Experience. You'll learn about the feature's utility in creating and managing data connections within your apps by using real-world examples.\", metadata={'source': 'docs/content/library/api/connections/experimental-connection.md'}),\n",
       " Document(page_content='title: st.connections.SQLConnection\\nslug: /library/api-reference/connections/st.connections.sqlconnection\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nThis page only contains on the st.connections.SQLConnection API. For a deeper dive into creating and managing data connections within Streamlit apps, read Connecting to data.\\n\\nBasic usage:\\n\\n```python\\nimport streamlit as st\\n\\nconn = st.experimental_connection(\"sql\")\\ndf = conn.query(\"select * from pet_owners\")\\nst.dataframe(df)\\n```\\n\\nIn case you want to pass a connection URL (or other parameters) directly, it also works:\\n\\npython\\nconn = st.experimental_connection(\\n    \"local_db\",\\n    type=\"sql\",\\n    url=\"mysql://user:pass@localhost:3306/mydb\"\\n)\\n\\nOr specify parameters in secrets:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.mydb]\\ndialect = \"mysql\"\\nusername = \"myuser\"\\npassword = \"password\"\\nhost = \"localhost\"\\ndatabase = \"mydb\"\\n```\\n\\n```python\\n\\nstreamlit_app.py\\n\\nconn = st.experimental_connection(\"mydb\", type=\"sql\", autocommit=True)\\n```\\n\\nAs described above, some cloud databases use extra **kwargs to specify credentials. These can be passed via secrets using the create_engine_kwargs section:\\n\\n```toml\\n\\n.streamlit/secrets.toml\\n\\n[connections.snowflake]\\nurl = \"snowflake://@/\"\\n\\n[connections.snowflake.create_engine_kwargs.connect_args]\\nauthenticator = \"externalbrowser\"\\nrole = \"...\"\\n\\n...\\n\\n```', metadata={'source': 'docs/content/library/api/connections/connections-sql.md'}),\n",
       " Document(page_content='title: Input widgets\\nslug: /library/api-reference/widgets\\n\\nInput widgets\\n\\nWith widgets, Streamlit allows you to bake interactivity directly into your apps with buttons, sliders, text inputs, and more.\\n\\nButton\\n\\nDisplay a button widget.\\n\\npython\\nclicked = st.button(\"Click me\")\\n\\nDownload button\\n\\nDisplay a download button widget.\\n\\npython\\nst.download_button(\"Download file\", file)\\n\\nCheckbox\\n\\nDisplay a checkbox widget.\\n\\npython\\nselected = st.checkbox(\"I agree\")\\n\\nRadio\\n\\nDisplay a radio button widget.\\n\\npython\\nchoice = st.radio(\"Pick one\", [\"cats\", \"dogs\"])\\n\\nSelectbox\\n\\nDisplay a select widget.\\n\\npython\\nchoice = st.selectbox(\"Pick one\", [\"cats\", \"dogs\"])\\n\\nMultiselect\\n\\nDisplay a multiselect widget. The multiselect widget starts as empty.\\n\\npython\\nchoices = st.multiselect(\"Buy\", [\"milk\", \"apples\", \"potatoes\"])\\n\\nSlider\\n\\nDisplay a slider widget.\\n\\npython\\nnumber = st.slider(\"Pick a number\", 0, 100)\\n\\nSelect slider\\n\\nDisplay a slider widget to select items from a list.\\n\\npython\\nsize = st.select_slider(\"Pick a size\", [\"S\", \"M\", \"L\"])\\n\\nText input\\n\\nDisplay a single-line text input widget.\\n\\npython\\nname = st.text_input(\"First name\")\\n\\nNumber input\\n\\nDisplay a numeric input widget.\\n\\npython\\nchoice = st.number_input(\"Pick a number\", 0, 10)\\n\\nText area\\n\\nDisplay a multi-line text input widget.\\n\\npython\\ntext = st.text_area(\"Text to translate\")\\n\\nDate input\\n\\nDisplay a date input widget.\\n\\npython\\ndate = st.date_input(\"Your birthday\")\\n\\nTime input\\n\\nDisplay a time input widget.\\n\\npython\\ntime = st.time_input(\"Meeting time\")\\n\\nFile uploader\\n\\nDisplay a file uploader widget.\\n\\npython\\ndata = st.file_uploader(\"Upload a CSV\")\\n\\nCamera input\\n\\nDisplay a widget that allows users to upload images directly from a camera.\\n\\npython\\nimage = st.camera_input(\"Take a picture\")\\n\\nColor picker\\n\\nDisplay a color picker widget.\\n\\npython\\ncolor = st.color_picker(\"Pick a color\")\\n\\nStreamlit Elements\\n\\nCreate a draggable and resizable dashboard in Streamlit. Created by @okls.\\n\\n```python\\nfrom streamlit_elements import elements, mui, html', metadata={'source': 'docs/content/library/api/widgets/widgets.md'}),\n",
       " Document(page_content='with elements(\"new_element\"):\\n  mui.Typography(\"Hello world\")\\n```\\n\\nTags\\n\\nAdd tags to your Streamlit apps. Created by @gagan3012.\\n\\n```python\\nfrom streamlit_tags import st_tags\\n\\nst_tags(label=\\'# Enter Keywords:\\', text=\\'Press enter to add more\\', value=[\\'Zero\\', \\'One\\', \\'Two\\'],\\nsuggestions=[\\'five\\', \\'six\\', \\'seven\\', \\'eight\\', \\'nine\\', \\'three\\', \\'eleven\\', \\'ten\\', \\'four\\'], maxtags = 4, key=\\'1\\')\\n```\\n\\nStqdm\\n\\nThe simplest way to handle a progress bar in streamlit app. Created by @Wirg.\\n\\n```python\\nfrom stqdm import stqdm\\n\\nfor _ in stqdm(range(50)):\\n    sleep(0.5)\\n```\\n\\nTimeline\\n\\nDisplay a Timeline in Streamlit apps using TimelineJS. Created by @innerdoc.\\n\\n```python\\nfrom streamlit_timeline import timeline\\n\\nwith open(\\'example.json\\', \"r\") as f:\\n  timeline(f.read(), height=800)\\n```\\n\\nCamera input live\\n\\nAlternative for st.camera_input which returns the webcam images live. Created by @blackary.\\n\\n```python\\nfrom camera_input_live import camera_input_live\\n\\nimage = camera_input_live()\\nst.image(value)\\n```\\n\\nStreamlit Ace\\n\\nAce editor component for Streamlit. Created by @okld.\\n\\n```python\\nfrom streamlit_ace import st_ace\\n\\ncontent = st_ace()\\ncontent\\n```\\n\\nStreamlit Chat\\n\\nStreamlit Component for a Chatbot UI. Created by @AI-Yash.\\n\\n```python\\nfrom streamlit_chat import message\\n\\nmessage(\"My message\")\\nmessage(\"Hello bot!\", is_user=True)  # align\\'s the message to the right\\n```\\n\\nStreamlit Option Menu\\n\\nSelect a single item from a list of options in a menu. Created by @victoryhb.\\n\\n```python\\nfrom streamlit_option_menu import option_menu\\n\\noption_menu(\"Main Menu\", [\"Home\", \\'Settings\\'],\\n  icons=[\\'house\\', \\'gear\\'], menu_icon=\"cast\", default_index=1)\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.stoggle import stoggle\\n\\nstoggle(\\n    \"Click me!\", \"\"\"🥷 Surprise! Here\\'s some additional content\"\"\",)\\n```', metadata={'source': 'docs/content/library/api/widgets/widgets.md'}),\n",
       " Document(page_content='title: st.connections.ExperimentalBaseConnection\\nslug: /library/api-reference/connections/st.connections.experimentalbaseconnection\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nThis page only contains on the st.connections.ExperimentalBaseConnection API. For a deeper dive into creating and managing data connections within Streamlit apps, read Connecting to data.', metadata={'source': 'docs/content/library/api/connections/connections-experimentalbaseconnection.md'}),\n",
       " Document(page_content=\"title: st.experimental_get_query_params\\nslug: /library/api-reference/utilities/st.experimental_get_query_params\\ndescription: st.experimental_get_query_params returns query parameters currently showing in the browser's URL bar.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\", metadata={'source': 'docs/content/library/api/utilities/experimental_get_query_params.md'}),\n",
       " Document(page_content=\"title: st.experimental_set_query_params\\nslug: /library/api-reference/utilities/st.experimental_set_query_params\\ndescription: st.experimental_set_query_params sets query parameters shown in the browser's URL bar.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\", metadata={'source': 'docs/content/library/api/utilities/experimental_set_query_params.md'}),\n",
       " Document(page_content='title: st.set_page_config\\nslug: /library/api-reference/utilities/st.set_page_config\\ndescription: st.set_page_config configures the default settings of the page.', metadata={'source': 'docs/content/library/api/utilities/set_page_config.md'}),\n",
       " Document(page_content=\"title: st.help\\nslug: /library/api-reference/utilities/st.help\\ndescription: st.help displays object's doc string, nicely formatted.\", metadata={'source': 'docs/content/library/api/utilities/help.md'}),\n",
       " Document(page_content='title: st.echo\\nslug: /library/api-reference/utilities/st.echo\\ndescription: st.echo displays some code on the app, then execute it. Useful for tutorials.\\n\\nDisplay code\\n\\nSometimes you want your Streamlit app to contain both your usual\\nStreamlit graphic elements and the code that generated those elements.\\nThat\\'s where st.echo() comes in.\\n\\nOk so let\\'s say you have the following file, and you want to make its\\napp a little bit more self-explanatory by making that middle section\\nvisible in the Streamlit app:\\n\\n```python\\nimport streamlit as st\\n\\ndef get_user_name():\\n    return \\'John\\'\\n\\n------------------------------------------------\\n\\nWant people to see this part of the code...\\n\\ndef get_punctuation():\\n    return \\'!!!\\'\\n\\ngreeting = \"Hi there, \"\\nuser_name = get_user_name()\\npunctuation = get_punctuation()\\n\\nst.write(greeting, user_name, punctuation)\\n\\n...up to here\\n\\n------------------------------------------------\\n\\nfoo = \\'bar\\'\\nst.write(\\'Done!\\')\\n```\\n\\nThe file above creates a Streamlit app containing the words \"Hi there,\\nJohn\", and then \"Done!\".\\n\\nNow let\\'s use st.echo() to make that middle section of the code visible\\nin the app:\\n\\n```python\\nimport streamlit as st\\n\\ndef get_user_name():\\n    return \\'John\\'\\n\\nwith st.echo():\\n    # Everything inside this block will be both printed to the screen\\n    # and executed.\\n\\nAnd now we\\'re back to not printing to the screen\\n\\nfoo = \\'bar\\'\\nst.write(\\'Done!\\')\\n```\\n\\nIt\\'s that simple!\\n\\nYou can have multiple st.echo() blocks in the same file.\\nUse it as often as you wish!', metadata={'source': 'docs/content/library/api/utilities/echo.md'}),\n",
       " Document(page_content='title: Placeholders, help, and options\\nslug: /library/api-reference/utilities\\n\\nPlaceholders, help, and options\\n\\nThere are a handful of methods that allow you to create placeholders in your\\napp, provide help using doc strings, get and modify configuration options and query parameters.\\n\\nSet page title, favicon, and more\\n\\nConfigures the default settings of the page.\\n\\npython\\nst.set_page_config(\\n  title=\"My app\",\\n  favicon=\":shark:\",\\n)\\n\\nEcho\\n\\nDisplay some code on the app, then execute it. Useful for tutorials.\\n\\npython\\nwith st.echo():\\n  st.write(\\'This code will be printed\\')\\n\\nGet help\\n\\nDisplay object’s doc string, nicely formatted.\\n\\npython\\nst.help(st.write)\\nst.help(pd.DataFrame)\\n\\nGet query parameters\\n\\nReturn the query parameters that are currently showing in the browser\\'s URL bar.\\n\\npython\\nst.experimental_get_query_params()\\n\\nSet query parameters\\n\\nSet the query parameters that are shown in the browser\\'s URL bar.\\n\\npython\\nst.experimental_set_query_params(\\n  show_map=True,\\n  selected=[\"asia\"]\\n)', metadata={'source': 'docs/content/library/api/utilities/utilities.md'}),\n",
       " Document(page_content='title: st.chat_message\\nslug: /library/api-reference/chat/st.chat_message\\ndescription: st.chat_message inserts a chat message container into the app.\\n\\nRead the Build conversational apps tutorial to learn how to use st.chat_message and st.chat_input to build chat-based apps.', metadata={'source': 'docs/content/library/api/chat/chat-message.md'}),\n",
       " Document(page_content='title: st.cache_resource.clear\\nslug: /library/api-reference/performance/st.cache_resource.clear\\ndescription: st.cache_resource.clear clears all cache_resource caches.\\n\\nExample\\n\\nIn the example below, pressing the \"Clear All\" button will clear all cache_resource caches. i.e. Clears cached global resources from all functions decorated with @st.cache_resource.\\n\\n```python\\nimport streamlit as st\\nfrom transformers import BertModel\\n\\n@st.cache_resource\\n def get_database_session(url):\\n     # Create a database session object that points to the URL.\\n     return session\\n\\n@st.cache_resource\\ndef get_model(model_type):\\n    # Create a model of the specified type.\\n    return BertModel.from_pretrained(model_type)\\n\\nif st.button(\"Clear All\"):\\n    # Clears all st.cache_resource caches:\\n    st.cache_resource.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/cache-resource-clear.md'}),\n",
       " Document(page_content='title: st.chat_input\\nslug: /library/api-reference/chat/st.chat_input\\ndescription: st.chat_input displays a chat input widget.\\n\\nRead the Build conversational apps tutorial to learn how to use st.chat_message and st.chat_input to build chat-based apps.', metadata={'source': 'docs/content/library/api/chat/chat-input.md'}),\n",
       " Document(page_content='title: Chat elements\\nslug: /library/api-reference/chat\\n\\nChat elements\\n\\nStreamlit provides a few commands to help you build conversational apps. These chat elements are designed to be used in conjunction with each other, but you can also use them separately.\\n\\nst.chat_message lets you insert a chat message container into the app so you can display messages from the user or the app. Chat containers can contain other Streamlit elements, including charts, tables, text, and more. st.chat_input lets you display a chat input widget so the user can type in a message.\\n\\nChat message\\n\\nInsert a chat message container.\\n\\npython\\nimport numpy as np\\nwith st.chat_message(\"user\"):\\n    st.write(\"Hello 👋\")\\n    st.line_chart(np.random.randn(30, 3))\\n\\nChat input\\n\\nDisplay a chat input widget.\\n\\npython\\nprompt = st.chat_input(\"Say something\")\\nif prompt:\\n    st.write(f\"The user has sent: {prompt}\")', metadata={'source': 'docs/content/library/api/chat/chat.md'}),\n",
       " Document(page_content='title: st.experimental_singleton.clear\\nslug: /library/api-reference/performance/st.experimental_singleton.clear\\ndescription: st.experimental_singleton.clear clears all singleton caches.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nExample\\n\\nIn the example below, pressing the \"Clear All\" button will clear all singleton caches. i.e. Clears cached singleton objects from all functions decorated with @st.experimental_singleton.\\n\\n```python\\nimport streamlit as st\\nfrom transformers import BertModel\\n\\n@st.experimental_singleton\\n def get_database_session(url):\\n     # Create a database session object that points to the URL.\\n     return session\\n\\n@st.experimental_singleton\\ndef get_model(model_type):\\n    # Create a model of the specified type.\\n    return BertModel.from_pretrained(model_type)\\n\\nif st.button(\"Clear All\"):\\n    # Clears all singleton caches:\\n    st.experimental_singleton.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/experimental-singleton-clear.md'}),\n",
       " Document(page_content='title: st.experimental_memo.clear\\nslug: /library/api-reference/performance/st.experimental_memo.clear\\ndescription: st.experimental_memo.clear clears all in-memory and on-disk memo caches.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nExample\\n\\nIn the example below, pressing the \"Clear All\" button will clear memoized values from all functions decorated with @st.experimental_memo.\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_memo\\ndef square(x):\\n    return x**2\\n\\n@st.experimental_memo\\ndef cube(x):\\n    return x**3\\n\\nif st.button(\"Clear All\"):\\n    # Clear values from all memoized functions:\\n    # i.e. clear values from both square and cube\\n    st.experimental_memo.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/experimental-memo-clear.md'}),\n",
       " Document(page_content='title: st.cache_data\\nslug: /library/api-reference/performance/st.cache_data\\ndescription: st.cache_data is used to cache functions that return data (e.g. dataframe transforms, database queries, ML inference).\\n\\nThis page only contains information on the st.cache_data API. For a deeper dive into caching and how to use it, check out Caching.\\n\\nUsing Streamlit commands in cached functions\\n\\nStatic elements\\n\\nSince version 1.16.0, cached functions can contain Streamlit commands! For example, you can do this:\\n\\npython\\n@st.cache_data\\ndef get_api_data():\\n    data = api.get(...)\\n    st.success(\"Fetched data from API!\")  # 👈 Show a success message\\n    return data\\n\\nAs we know, Streamlit only runs this function if it hasn’t been cached before. On this first run, the st.success message will appear in the app. But what happens on subsequent runs? It still shows up! Streamlit realizes that there is an st. command inside the cached function, saves it during the first run, and replays it on subsequent runs. Replaying static elements works for both caching decorators.\\n\\nYou can also use this functionality to cache entire parts of your UI:\\n\\npython\\n@st.cache_data\\ndef show_data():\\n    st.header(\"Data analysis\")\\n    data = api.get(...)\\n    st.success(\"Fetched data from API!\")\\n    st.write(\"Here is a plot of the data:\")\\n    st.line_chart(data)\\n    st.write(\"And here is the raw data:\")\\n    st.dataframe(data)\\n\\nInput widgets\\n\\nYou can also use interactive input widgets like st.slider or st.text_input in cached functions. Widget replay is an experimental feature at the moment. To enable it, you need to set the experimental_allow_widgets parameter:\\n\\npython\\n@st.cache_data(experimental_allow_widgets=True)  # 👈 Set the parameter\\ndef get_data():\\n    num_rows = st.slider(\"Number of rows to get\")  # 👈 Add a slider\\n    data = api.get(..., num_rows)\\n    return data', metadata={'source': 'docs/content/library/api/performance/cache-data.md'}),\n",
       " Document(page_content='Streamlit treats the slider like an additional input parameter to the cached function. If you change the slider position, Streamlit will see if it has already cached the function for this slider value. If yes, it will return the cached value. If not, it will rerun the function using the new slider value.\\n\\nUsing widgets in cached functions is extremely powerful because it lets you cache entire parts of your app. But it can be dangerous! Since Streamlit treats the widget value as an additional input parameter, it can easily lead to excessive memory usage. Imagine your cached function has five sliders and returns a 100 MB DataFrame. Then we’ll add 100 MB to the cache for every permutation of these five slider values – even if the sliders do not influence the returned data! These additions can make your cache explode very quickly. Please be aware of this limitation if you use widgets in cached functions. We recommend using this feature only for isolated parts of your UI where the widgets directly influence the cached return value.\\n\\nSupport for widgets in cached functions is currently experimental. We may change or remove it anytime without warning. Please use it with care!\\n\\nTwo widgets are currently not supported in cached functions: st.file_uploader and st.camera_input. We may support them in the future. Feel free to open a GitHub issue if you need them!', metadata={'source': 'docs/content/library/api/performance/cache-data.md'}),\n",
       " Document(page_content='title: st.cache_resource\\nslug: /library/api-reference/performance/st.cache_resource\\ndescription: st.cache_resource is used to cache functions that return shared global resources (e.g. database connections, ML models).\\n\\nThis page only contains information on the st.cache_resource API. For a deeper dive into caching and how to use it, check out Caching.\\n\\nUsing Streamlit commands in cached functions\\n\\nStatic elements\\n\\nSince version 1.16.0, cached functions can contain Streamlit commands! For example, you can do this:\\n\\n```python\\nfrom transformers import pipeline\\n\\n@st.cache_resource\\ndef load_model():\\n    model = pipeline(\"sentiment-analysis\")\\n    st.success(\"Loaded NLP model from Hugging Face!\")  # 👈 Show a success message\\n    return model\\n```\\n\\nAs we know, Streamlit only runs this function if it hasn’t been cached before. On this first run, the st.success message will appear in the app. But what happens on subsequent runs? It still shows up! Streamlit realizes that there is an st. command inside the cached function, saves it during the first run, and replays it on subsequent runs. Replaying static elements works for both caching decorators.\\n\\nYou can also use this functionality to cache entire parts of your UI:\\n\\npython\\n@st.cache_resource\\ndef load_model():\\n    st.header(\"Data analysis\")\\n    model = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\\n    st.success(\"Loaded model!\")\\n    st.write(\"Turning on evaluation mode...\")\\n    model.eval()\\n    st.write(\"Here\\'s the model:\")\\n    return model\\n\\nInput widgets\\n\\nYou can also use interactive input widgets like st.slider or st.text_input in cached functions. Widget replay is an experimental feature at the moment. To enable it, you need to set the experimental_allow_widgets parameter:', metadata={'source': 'docs/content/library/api/performance/cache-resource.md'}),\n",
       " Document(page_content='python\\n@st.cache_data(experimental_allow_widgets=True)  # 👈 Set the parameter\\ndef load_model():\\n    pretrained = st.checkbox(\"Use pre-trained model:\")  # 👈 Add a checkbox\\n    model = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT, pretrained=pretrained)\\n    return model\\n\\nStreamlit treats the checkbox like an additional input parameter to the cached function. If you uncheck it, Streamlit will see if it has already cached the function for this checkbox state. If yes, it will return the cached value. If not, it will rerun the function using the new slider value.\\n\\nUsing widgets in cached functions is extremely powerful because it lets you cache entire parts of your app. But it can be dangerous! Since Streamlit treats the widget value as an additional input parameter, it can easily lead to excessive memory usage. Imagine your cached function has five sliders and returns a 100 MB DataFrame. Then we’ll add 100 MB to the cache for every permutation of these five slider values – even if the sliders do not influence the returned data! These additions can make your cache explode very quickly. Please be aware of this limitation if you use widgets in cached functions. We recommend using this feature only for isolated parts of your UI where the widgets directly influence the cached return value.\\n\\nSupport for widgets in cached functions is currently experimental. We may change or remove it anytime without warning. Please use it with care!\\n\\nTwo widgets are currently not supported in cached functions: st.file_uploader and st.camera_input. We may support them in the future. Feel free to open a GitHub issue if you need them!', metadata={'source': 'docs/content/library/api/performance/cache-resource.md'}),\n",
       " Document(page_content='title: st.cache_data.clear\\nslug: /library/api-reference/performance/st.cache_data.clear\\ndescription: st.cache_data.clear clears all in-memory and on-disk data caches.\\n\\nExample\\n\\nIn the example below, pressing the \"Clear All\" button will clear memoized values from all functions decorated with @st.cache_data.\\n\\n```python\\nimport streamlit as st\\n\\n@st.cache_data\\ndef square(x):\\n    return x**2\\n\\n@st.cache_data\\ndef cube(x):\\n    return x**3\\n\\nif st.button(\"Clear All\"):\\n    # Clear values from all all in-memory and on-disk data caches:\\n    # i.e. clear values from both square and cube\\n    st.cache_data.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/cache-data-clear.md'}),\n",
       " Document(page_content='title: st.cache\\nslug: /library/api-reference/performance/st.cache\\ndescription: st.cache is used to memoize function executions.\\n\\nst.cache\\n\\nWhen you mark a function with Streamlit’s cache annotation, it tells Streamlit\\nthat whenever the function is called it should check three things:\\n\\nThe name of the function\\n\\nThe actual code that makes up the body of the function\\n\\nThe input parameters that you called the function with\\n\\nIf this is the first time Streamlit has seen those three items, with those exact\\nvalues, and in that exact combination, it runs the function and stores the\\nresult in a local cache.\\n\\nThen, next time the function is called, if those three values have not changed\\nStreamlit knows it can skip executing the function altogether. Instead, it just\\nreads the output from the local cache and passes it on to the caller.\\n\\nThe main limitation is that Streamlit’s cache feature doesn’t know about\\nchanges that take place outside the body of the annotated function.\\n\\nFor more information about the Streamlit cache, its configuration parameters,\\nand its limitations, see Caching.', metadata={'source': 'docs/content/library/api/performance/cache.md'}),\n",
       " Document(page_content='title: st.experimental_memo\\nslug: /library/api-reference/performance/st.experimental_memo\\ndescription: st.experimental_memo is used to memoize function executions.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nPersistent memo caches currently don\\'t support TTL. ttl will be ignored if persist is specified:\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_memo(ttl=60, persist=\"disk\")\\ndef load_data():\\n    return 42\\n\\nst.write(load_data())\\n```\\n\\nAnd a warning will be logged to your terminal:\\n\\n```bash\\nstreamlit run app.py\\n\\nYou can now view your Streamlit app in your browser.\\n  Local URL: http://localhost:8501\\n  Network URL: http://192.168.1.1:8501\\n\\n2022-09-22 13:35:41.587 The memoized function \\'load_data\\' has a TTL that will be ignored. Persistent memo caches currently don\\'t support TTL.\\n```\\n\\nReplay static st elements in cache-decorated functions\\n\\nFunctions decorated with @st.experimental_memo can contain static st elements. When a cache-decorated function is executed, we record the element and block messages produced, so the elements will appear in the app even when execution of the function is skipped because the result was cached.\\n\\nIn the example below, the @st.experimental_memo decorator is used to cache the execution of the load_data function, that returns a pandas DataFrame. Notice the cached function also contains a st.area_chart command, which will be replayed when the function is skipped because the result was cached.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport streamlit as st\\n\\n@st.experimental_memo\\ndef load_data(rows):\\n    chart_data = pd.DataFrame(\\n        np.random.randn(rows, 10),\\n        columns=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"],\\n    )\\n    # Contains a static element st.area_chart\\n    st.area_chart(chart_data) # This will be recorded and displayed even when the function is skipped\\n    return chart_data', metadata={'source': 'docs/content/library/api/performance/experimental-memo.md'}),\n",
       " Document(page_content='df = load_data(20)\\nst.dataframe(df)\\n```\\n\\nSupported static st elements in cache-decorated functions include:\\n\\nst.alert\\n\\nst.altair_chart\\n\\nst.area_chart\\n\\nst.audio\\n\\nst.bar_chart\\n\\nst.ballons\\n\\nst.bokeh_chart\\n\\nst.caption\\n\\nst.code\\n\\nst.components.v1.html\\n\\nst.components.v1.iframe\\n\\nst.container\\n\\nst.dataframe\\n\\nst.echo\\n\\nst.empty\\n\\nst.error\\n\\nst.exception\\n\\nst.expander\\n\\nst.experimental_get_query_params\\n\\nst.experimental_set_query_params\\n\\nst.form\\n\\nst.form_submit_button\\n\\nst.graphviz_chart\\n\\nst.help\\n\\nst.image\\n\\nst.info\\n\\nst.json\\n\\nst.latex\\n\\nst.line_chart\\n\\nst.markdown\\n\\nst.metric\\n\\nst.plotly_chart\\n\\nst.progress\\n\\nst.pydeck_chart\\n\\nst.snow\\n\\nst.spinner\\n\\nst.success\\n\\nst.table\\n\\nst.text\\n\\nst.vega_lite_chart\\n\\nst.video\\n\\nst.warning\\n\\nReplay input widgets in cache-decorated functions\\n\\nIn addition to static elements, functions decorated with @st.experimental_memo can also contain input widgets! Replaying input widgets is disabled by default. To enable it, you can set the experimental_allow_widgets parameter for @st.experimental_memo to True. The example below enables widget replaying, and shows the use of a checkbox widget within a cache-decorated function.\\n\\n```python\\nimport streamlit as st\\n\\nEnable widget replay\\n\\n@st.experimental_memo(experimental_allow_widgets=True)\\ndef func():\\n    # Contains an input widget\\n    st.checkbox(\"Works!\")\\n\\nfunc()\\n```\\n\\nIf the cache decorated function contains input widgets, but experimental_allow_widgets is set to False or unset, Streamlit will throw a CachedStFunctionWarning, like the one below:\\n\\n```python\\nimport streamlit as st\\n\\nWidget replay is disabled by default\\n\\n@st.experimental_memo\\ndef func():\\n    # Streamlit will throw a CachedStFunctionWarning\\n    st.checkbox(\"Doesn\\'t work\")\\n\\nfunc()\\n```\\n\\nHow widget replay works', metadata={'source': 'docs/content/library/api/performance/experimental-memo.md'}),\n",
       " Document(page_content='func()\\n```\\n\\nHow widget replay works\\n\\nLet\\'s demystify how widget replay in cache-decorated functions works and gain a conceptual understanding. Widget values are treated as additional inputs to the function, and are used to determine whether the function should be executed or not. Consider the following example:\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_memo(experimental_allow_widgets=True)\\ndef plus_one(x):\\n    y = x + 1\\n    if st.checkbox(\"Nuke the value 💥\"):\\n        st.write(\"Value was nuked, returning 0\")\\n        y = 0\\n    return y\\n\\nst.write(plus_one(2))\\n```\\n\\nIn order to know which value the cache should return (in case of a cache hit), Streamlit treats the checkbox state (checked / unchecked) as an additional input to the function plus_one (just like x). If the user checks the checkbox (thereby changing its state), we look up the cache for the same value of x (2) and the same checkbox state (checked). If the cache contains a value for this combination of inputs, we return it. Otherwise, we execute the function and store the result in the cache.\\n\\nLet\\'s now understand how enabling and disabling widget replay changes the behavior of the function.\\n\\nWidget replay disabled\\n\\nWidgets in cached functions throw a CachedStFunctionWarning and are ignored.\\n\\nOther static elements in cached functions replay as expected.\\n\\nWidget replay enabled\\n\\nWidgets in cached functions don\\'t lead to a warning, and are replayed as expected.\\n\\nInteracting with a widget in a cached function will cause the function to be executed again, and the cache to be updated.\\n\\nWidgets in cached functions retain their state across reruns.\\n\\nEach unique combination of widget values is treated as a separate input to the function, and is used to determine whether the function should be executed or not. i.e. Each unique combination of widget values has its own cache entry; the cached function runs the first time and the saved value is used afterwards.', metadata={'source': 'docs/content/library/api/performance/experimental-memo.md'}),\n",
       " Document(page_content='Calling a cached function multiple times in one script run with the same arguments triggers a DuplicateWidgetID error.\\n\\nIf the arguments to a cached function change, widgets from that function that render again retain their state.\\n\\nChanging the source code of a cached function invalidates the cache.\\n\\nBoth st.experimental_memo and st.experimental_singleton support widget replay.\\n\\nFundamentally, the behavior of a function with (supported) widgets in it doesn\\'t change when it is decorated with @st.experimental_memo or @st.experimental_singleton. The only difference is that the function is only executed when we detect a cache \"miss\".\\n\\nSupported widgets\\n\\nAll input widgets are supported in cache-decorated functions. The following is an exhaustive list of supported widgets:\\n\\nst.button\\n\\nst.camera_input\\n\\nst.checkbox\\n\\nst.color_picker\\n\\nst.date_input\\n\\nst.download_button\\n\\nst.file_uploader\\n\\nst.multiselect\\n\\nst.number_input\\n\\nst.radio\\n\\nst.selectbox\\n\\nst.select_slider\\n\\nst.slider\\n\\nst.text_area\\n\\nst.text_input\\n\\nst.time_input', metadata={'source': 'docs/content/library/api/performance/experimental-memo.md'}),\n",
       " Document(page_content=\"title: Mutate charts\\nslug: /library/api-reference/mutate\\ndescription: st.add_rows appends a dataframe to the bottom of the current one in certain elements, for optimized data updates.\\n\\nMutate charts\\n\\nSometimes you display a chart or dataframe and want to modify live as the app\\nruns (for example, in a loop). Depending on what you're looking for, there are 3 different ways to\\ndo this:\\n\\nUsing st.empty to replace a single element.\\n\\nUsing st.container or\\n   st.columns to replace multiple elements.\\n\\nUsing add_rows to append data to specific types of elements.\\n\\nHere we discuss that last case.\", metadata={'source': 'docs/content/library/api/mutate/mutate.md'}),\n",
       " Document(page_content='title: st.audio\\nslug: /library/api-reference/media/st.audio\\ndescription: st.audio displays an audio player.', metadata={'source': 'docs/content/library/api/media/audio.md'}),\n",
       " Document(page_content='title: Optimize performance\\nslug: /library/api-reference/performance\\n\\nOptimize performance\\n\\nStreamlit provides powerful cache primitives for data and global resources. They allow your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations.\\n\\nCache data\\n\\nFunction decorator to cache functions that return data (e.g. dataframe transforms, database queries, ML inference).\\n\\npython\\n@st.cache_data\\ndef long_function(param1, param2):\\n  # Perform expensive computation here or\\n  # fetch data from the web here\\n  return data\\n\\nCache resource\\n\\nFunction decorator to cache functions that return global resources (e.g. database connections, ML models).\\n\\npython\\n@st.cache_resource\\ndef init_model():\\n  # Return a global resource here\\n  return pipeline(\\n    \"sentiment-analysis\",\\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\\n  )\\n\\nClear cached data\\n\\nClear all in-memory and on-disk data caches.\\n\\n```python\\n@st.cache_data\\ndef long_function(param1, param2):\\n  # Perform expensive computation here or\\n  # fetch data from the web here\\n  return data\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all cache_data functions\\n  st.cache_data.clear()\\n```\\n\\nClear cached resources\\n\\nClear all st.cache_resource caches.\\n\\n```python\\n@st.cache_resource\\ndef init_model():\\n  # Return a global resource here\\n  return pipeline(\\n    \"sentiment-analysis\",\\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\\n  )\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all cache_resource functions\\n  st.cache_data.clear()\\n```\\n\\nAll the below commands were deprecated in version 1.18.0. Use the new commands above instead. Learn more in Caching.\\n\\nDeprecated commands\\n\\nThis command was deprecated in version 1.18.0. Use st.cache_data or st.cache_resource instead.\\n\\nCaching\\n\\nFunction decorator to memoize function executions.\\n\\npython\\n@st.cache(ttl=3600)\\ndef run_long_computation(arg1, arg2):\\n  # Do stuff here\\n  return computation_output', metadata={'source': 'docs/content/library/api/performance/performance.md'}),\n",
       " Document(page_content='This command was deprecated in version 1.18.0. Use st.cache_data instead.\\n\\nMemo\\n\\nExperimental function decorator to memoize function executions.\\n\\npython\\n@st.experimental_memo\\ndef fetch_and_clean_data(url):\\n  # Fetch data from URL here, and then clean it up.\\n  return data\\n\\nThis command was deprecated in version 1.18.0. Use st.cache_resource instead.\\n\\nSingleton\\n\\nExperimental function decorator to store singleton objects.\\n\\npython\\n@st.experimental_singleton\\ndef get_database_session(url):\\n  # Create a database session object that points to the URL.\\n  return session\\n\\nThis command was deprecated in version 1.18.0. Use st.cache_data.clear instead.\\n\\nClear memo\\n\\nClear all in-memory and on-disk memo caches.\\n\\n```python\\n@st.experimental_memo\\ndef fetch_and_clean_data(url):\\n  # Fetch data from URL here, and then clean it up.\\n  return data\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all memoized functions\\n  st.experimental_memo.clear()\\n```\\n\\nThis command was deprecated in version 1.18.0. Use st.cache_resource.clearinstead.\\n\\nClear singleton\\n\\nClear all singleton caches.\\n\\n```python\\n@st.experimental_singleton\\ndef get_database_session(url):\\n  # Create a database session object that points to the URL.\\n  return session\\n\\nif st.button(\"Clear All\"):\\n  # Clears all singleton caches:\\n  st.experimental_singleton.clear()\\n```', metadata={'source': 'docs/content/library/api/performance/performance.md'}),\n",
       " Document(page_content='title: st.image\\nslug: /library/api-reference/media/st.image\\ndescription: st.image displays an image or list of images.', metadata={'source': 'docs/content/library/api/media/image.md'}),\n",
       " Document(page_content='title: st.video\\nslug: /library/api-reference/media/st.video\\ndescription: st.video displays a video player.', metadata={'source': 'docs/content/library/api/media/video.md'}),\n",
       " Document(page_content='title: st.experimental_singleton\\nslug: /library/api-reference/performance/st.experimental_singleton\\ndescription: st.experimental_singleton is a function decorator used to store singleton objects.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nValidating the cache\\n\\nThe @st.experimental_singleton decorator is used to cache the output of a function, so that it only needs to be executed once. This can improve performance in certain situations, such as when a function takes a long time to execute or makes a network request.\\n\\nHowever, in some cases, the cached output may become invalid over time, such as when a database connection times out. To handle this, the @st.experimental_singleton decorator supports an optional validate parameter, which accepts a validation function that is called each time the cached output is accessed. If the validation function returns False, the cached output is discarded and the decorated function is executed again.\\n\\nBest Practices\\n\\nUse the validate parameter when the cached output may become invalid over time, such as when a database connection or an API key expires.\\n\\nUse the validate parameter judiciously, as it will add an additional overhead of calling the validation function each time the cached output is accessed.\\n\\nMake sure that the validation function is as fast as possible, as it will be called each time the cached output is accessed.\\n\\nConsider to validate cached data periodically, instead of each time it is accessed, to mitigate the performance impact.\\n\\nHandle errors that may occur during validation and provide a fallback mechanism if the validation fails.\\n\\nReplay static st elements in cache-decorated functions', metadata={'source': 'docs/content/library/api/performance/experimental-singleton.md'}),\n",
       " Document(page_content='Functions decorated with @st.experimental_singleton can contain static st elements. When a cache-decorated function is executed, we record the element and block messages produced, so the elements will appear in the app even when execution of the function is skipped because the result was cached.\\n\\nIn the example below, the @st.experimental_singleton decorator is used to cache the execution of the get_model function, that returns a 🤗 Hugging Face Transformers model. Notice the cached function also contains a st.bar_chart command, which will be replayed when the function is skipped because the result was cached.\\n\\n```python\\nimport numpy as np\\nimport pandas as pd\\nimport streamlit as st\\nfrom transformers import AutoModel\\n\\n@st.experimental_singleton\\ndef get_model(model_type):\\n    # Contains a static element st.bar_chart\\n    st.bar_chart(\\n        np.random.rand(10, 1)\\n    )  # This will be recorded and displayed even when the function is skipped\\n\\nbert_model = get_model(\"distilbert-base-uncased\")\\nst.help(bert_model) # Display the model\\'s docstring\\n```\\n\\nSupported static st elements in cache-decorated functions include:\\n\\nst.alert\\n\\nst.altair_chart\\n\\nst.area_chart\\n\\nst.audio\\n\\nst.bar_chart\\n\\nst.ballons\\n\\nst.bokeh_chart\\n\\nst.caption\\n\\nst.code\\n\\nst.components.v1.html\\n\\nst.components.v1.iframe\\n\\nst.container\\n\\nst.dataframe\\n\\nst.echo\\n\\nst.empty\\n\\nst.error\\n\\nst.exception\\n\\nst.expander\\n\\nst.experimental_get_query_params\\n\\nst.experimental_set_query_params\\n\\nst.form\\n\\nst.form_submit_button\\n\\nst.graphviz_chart\\n\\nst.help\\n\\nst.image\\n\\nst.info\\n\\nst.json\\n\\nst.latex\\n\\nst.line_chart\\n\\nst.markdown\\n\\nst.metric\\n\\nst.plotly_chart\\n\\nst.progress\\n\\nst.pydeck_chart\\n\\nst.snow\\n\\nst.spinner\\n\\nst.success\\n\\nst.table\\n\\nst.text\\n\\nst.vega_lite_chart\\n\\nst.video\\n\\nst.warning\\n\\nReplay input widgets in cache-decorated functions', metadata={'source': 'docs/content/library/api/performance/experimental-singleton.md'}),\n",
       " Document(page_content='In addition to static elements, functions decorated with @st.experimental_singleton can also contain input widgets! Replaying input widgets is disabled by default. To enable it, you can set the experimental_allow_widgets parameter for @st.experimental_singleton to True. The example below enables widget replaying, and shows the use of a checkbox widget within a cache-decorated function.\\n\\n```python\\nimport streamlit as st\\n\\nEnable widget replay\\n\\n@st.experimental_singleton(experimental_allow_widgets=True)\\ndef func():\\n    # Contains an input widget\\n    st.checkbox(\"Works!\")\\n\\nfunc()\\n```\\n\\nIf the cache decorated function contains input widgets, but experimental_allow_widgets is set to False or unset, Streamlit will throw a CachedStFunctionWarning, like the one below:\\n\\n```python\\nimport streamlit as st\\n\\nWidget replay is disabled by default\\n\\n@st.experimental_singleton\\ndef func():\\n    # Streamlit will throw a CachedStFunctionWarning\\n    st.checkbox(\"Doesn\\'t work\")\\n\\nfunc()\\n```\\n\\nHow widget replay works\\n\\nLet\\'s demystify how widget replay in cache-decorated functions works and gain a conceptual understanding. Widget values are treated as additional inputs to the function, and are used to determine whether the function should be executed or not. Consider the following example:\\n\\n```python\\nimport streamlit as st\\n\\n@st.experimental_singleton(experimental_allow_widgets=True)\\ndef plus_one(x):\\n    y = x + 1\\n    if st.checkbox(\"Nuke the value 💥\"):\\n        st.write(\"Value was nuked, returning 0\")\\n        y = 0\\n    return y\\n\\nst.write(plus_one(2))\\n```', metadata={'source': 'docs/content/library/api/performance/experimental-singleton.md'}),\n",
       " Document(page_content=\"st.write(plus_one(2))\\n```\\n\\nIn order to know which value the cache should return (in case of a cache hit), Streamlit treats the checkbox state (checked / unchecked) as an additional input to the function plus_one (just like x). If the user checks the checkbox (thereby changing its state), we look up the cache for the same value of x (2) and the same checkbox state (checked). If the cache contains a value for this combination of inputs, we return it. Otherwise, we execute the function and store the result in the cache.\\n\\nLet's now understand how enabling and disabling widget replay changes the behavior of the function.\\n\\nWidget replay disabled\\n\\nWidgets in cached functions throw a CachedStFunctionWarning and are ignored.\\n\\nOther static elements in cached functions replay as expected.\\n\\nWidget replay enabled\\n\\nWidgets in cached functions don't lead to a warning, and are replayed as expected.\\n\\nInteracting with a widget in a cached function will cause the function to be executed again, and the cache to be updated.\\n\\nWidgets in cached functions retain their state across reruns.\\n\\nEach unique combination of widget values is treated as a separate input to the function, and is used to determine whether the function should be executed or not. i.e. Each unique combination of widget values has its own cache entry; the cached function runs the first time and the saved value is used afterwards.\\n\\nCalling a cached function multiple times in one script run with the same arguments triggers a DuplicateWidgetID error.\\n\\nIf the arguments to a cached function change, widgets from that function that render again retain their state.\\n\\nChanging the source code of a cached function invalidates the cache.\\n\\nBoth st.experimental_singleton and st.experimental_memo support widget replay.\", metadata={'source': 'docs/content/library/api/performance/experimental-singleton.md'}),\n",
       " Document(page_content='Fundamentally, the behavior of a function with (supported) widgets in it doesn\\'t change when it is decorated with @st.experimental_singleton or @st.experimental_memo. The only difference is that the function is only executed when we detect a cache \"miss\".\\n\\nSupported widgets\\n\\nAll input widgets are supported in cache-decorated functions. The following is an exhaustive list of supported widgets:\\n\\nst.button\\n\\nst.camera_input\\n\\nst.checkbox\\n\\nst.color_picker\\n\\nst.date_input\\n\\nst.download_button\\n\\nst.file_uploader\\n\\nst.multiselect\\n\\nst.number_input\\n\\nst.radio\\n\\nst.selectbox\\n\\nst.select_slider\\n\\nst.slider\\n\\nst.text_area\\n\\nst.text_input\\n\\nst.time_input', metadata={'source': 'docs/content/library/api/performance/experimental-singleton.md'}),\n",
       " Document(page_content='title: st.graphviz_chart\\nslug: /library/api-reference/charts/st.graphviz_chart\\ndescription: st.graphviz_chart displays a graph using the dagre-d3 library.', metadata={'source': 'docs/content/library/api/charts/graphviz_chart.md'}),\n",
       " Document(page_content='title: st.bar_chart\\nslug: /library/api-reference/charts/st.bar_chart\\ndescription: st.bar_chart displays a bar chart.', metadata={'source': 'docs/content/library/api/charts/bar_chart.md'}),\n",
       " Document(page_content='title: st.plotly_chart\\nslug: /library/api-reference/charts/st.plotly_chart\\ndescription: st.plotly_chart displays an interactive Plotly chart.\\n\\nTheming\\n\\nPlotly charts are displayed using the Streamlit theme by default. This theme is sleek, user-friendly, and incorporates Streamlit\\'s color palette. The added benefit is that your charts better integrate with the rest of your app\\'s design.\\n\\nThe Streamlit theme is available from Streamlit 1.16.0 through the theme=\"streamlit\" keyword argument. To disable it, and use Plotly\\'s native theme, use theme=None instead.\\n\\nLet\\'s look at an example of charts with the Streamlit theme and the native Plotly theme:\\n\\n```python\\nimport plotly.express as px\\nimport streamlit as st\\n\\ndf = px.data.gapminder()\\n\\nfig = px.scatter(\\n    df.query(\"year==2007\"),\\n    x=\"gdpPercap\",\\n    y=\"lifeExp\",\\n    size=\"pop\",\\n    color=\"continent\",\\n    hover_name=\"country\",\\n    log_x=True,\\n    size_max=60,\\n)\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Plotly native theme\"])\\nwith tab1:\\n    # Use the Streamlit theme.\\n    # This is the default. So you can also omit the theme argument.\\n    st.plotly_chart(fig, theme=\"streamlit\", use_container_width=True)\\nwith tab2:\\n    # Use the native Plotly theme.\\n    st.plotly_chart(fig, theme=None, use_container_width=True)\\n```\\n\\nClick the tabs in the interactive app below to see the charts with the Streamlit theme enabled and disabled.\\n\\nIf you\\'re wondering if your own customizations will still be taken into account, don\\'t worry! You can still make changes to your chart configurations. In other words, although we now enable the Streamlit theme by default, you can overwrite it with custom colors or fonts. For example, if you want a chart line to be green instead of the default red, you can do it!\\n\\nHere\\'s an example of an Plotly chart where a custom color scale is defined and reflected:\\n\\n```python\\nimport plotly.express as px\\nimport streamlit as st', metadata={'source': 'docs/content/library/api/charts/plotly_chart.md'}),\n",
       " Document(page_content='st.subheader(\"Define a custom colorscale\")\\ndf = px.data.iris()\\nfig = px.scatter(\\n    df,\\n    x=\"sepal_width\",\\n    y=\"sepal_length\",\\n    color=\"sepal_length\",\\n    color_continuous_scale=\"reds\",\\n)\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Plotly native theme\"])\\nwith tab1:\\n    st.plotly_chart(fig, theme=\"streamlit\", use_container_width=True)\\nwith tab2:\\n    st.plotly_chart(fig, theme=None, use_container_width=True)\\n```\\n\\nNotice how the custom color scale is still reflected in the chart, even when the Streamlit theme is enabled 👇\\n\\nFor many more examples of Plotly charts with and without the Streamlit theme, check out the plotly.streamlit.app.', metadata={'source': 'docs/content/library/api/charts/plotly_chart.md'}),\n",
       " Document(page_content='title: st.bokeh_chart\\nslug: /library/api-reference/charts/st.bokeh_chart\\ndescription: st.bokeh_chart displays an interactive Bokeh chart.', metadata={'source': 'docs/content/library/api/charts/bokeh_chart.md'}),\n",
       " Document(page_content='title: st.map\\nslug: /library/api-reference/charts/st.map\\ndescription: st.map displays a map with points on it.', metadata={'source': 'docs/content/library/api/charts/map.md'}),\n",
       " Document(page_content='title: st.pyplot\\nslug: /library/api-reference/charts/st.pyplot\\ndescription: st.pyplot displays a matplotlib.pyplot figure.', metadata={'source': 'docs/content/library/api/charts/pyplot.md'}),\n",
       " Document(page_content='title: Media elements\\nslug: /library/api-reference/media\\n\\nMedia elements\\n\\nIt\\'s easy to embed images, videos, and audio files directly into your Streamlit apps.\\n\\nImage\\n\\nDisplay an image or list of images.\\n\\npython\\nst.image(numpy_array)\\nst.image(image_bytes)\\nst.image(file)\\nst.image(\"https://example.com/myimage.jpg\")\\n\\nAudio\\n\\nDisplay an audio player.\\n\\npython\\nst.audio(numpy_array)\\nst.audio(audio_bytes)\\nst.audio(file)\\nst.audio(\"https://example.com/myaudio.mp3\", format=\"audio/mp3\")\\n\\nVideo\\n\\nDisplay a video player.\\n\\npython\\nst.video(numpy_array)\\nst.video(video_bytes)\\nst.video(file)\\nst.video(\"https://example.com/myvideo.mp4\", format=\"video/mp4\")\\n\\nStreamlit Webrtc\\n\\nHandling and transmitting real-time video/audio streams with Streamlit. Created by @whitphx.\\n\\n```python\\nfrom streamlit_webrtc import webrtc_streamer\\n\\nwebrtc_streamer(key=\"sample\")\\n```\\n\\nDrawable Canvas\\n\\nProvides a sketching canvas using Fabric.js. Created by @andfanilo.\\n\\n```python\\nfrom streamlit_drawable_canvas import st_canvas\\n\\nst_canvas(fill_color=\"rgba(255, 165, 0, 0.3)\", stroke_width=stroke_width, stroke_color=stroke_color, background_color=bg_color, background_image=Image.open(bg_image) if bg_image else None, update_streamlit=realtime_update, height=150, drawing_mode=drawing_mode, point_display_radius=point_display_radius if drawing_mode == \\'point\\' else 0, key=\"canvas\",)\\n```\\n\\nImage Comparison\\n\\nCompare images with a slider using JuxtaposeJS. Created by @fcakyon.\\n\\n```python\\nfrom streamlit_image_comparison import image_comparison\\n\\nimage_comparison(img1=\"image1.jpg\", img2=\"image2.jpg\",)\\n```\\n\\nStreamlit Cropper\\n\\nA simple image cropper for Streamlit. Created by @turner-anderson.\\n\\n```python\\nfrom streamlit_cropper import st_cropper\\n\\nst_cropper(img, realtime_update=realtime_update, box_color=box_color, aspect_ratio=aspect_ratio)\\n```\\n\\nImage Coordinates\\n\\nGet the coordinates of clicks on an image. Created by @blackary.\\n\\n```python\\nfrom streamlit_image_coordinates import streamlit_image_coordinates', metadata={'source': 'docs/content/library/api/media/media.md'}),\n",
       " Document(page_content='streamlit_image_coordinates(\"https://placekitten.com/200/300\")\\n```\\n\\nStreamlit Lottie\\n\\nIntegrate Lottie animations inside your Streamlit app. Created by @andfanilo.\\n\\n```python\\nlottie_hello = load_lottieurl(\"https://assets5.lottiefiles.com/packages/lf20_V9t630.json\")\\n\\nst_lottie(lottie_hello, key=\"hello\")\\n```', metadata={'source': 'docs/content/library/api/media/media.md'}),\n",
       " Document(page_content='title: st.vega_lite_chart\\nslug: /library/api-reference/charts/st.vega_lite_chart\\ndescription: st.vega_lite_chart displays a chart using the Vega-Lite library.\\n\\nTheming\\n\\nVega-Lite charts are displayed using the Streamlit theme by default. This theme is sleek, user-friendly, and incorporates Streamlit\\'s color palette. The added benefit is that your charts better integrate with the rest of your app\\'s design.\\n\\nThe Streamlit theme is available from Streamlit 1.16.0 through the theme=\"streamlit\" keyword argument. To disable it, and use Vega-Lite\\'s native theme, use theme=None instead.\\n\\nLet\\'s look at an example of charts with the Streamlit theme and the native Vega-Lite theme:\\n\\n```python\\nimport streamlit as st\\nfrom vega_datasets import data\\n\\nsource = data.cars()\\n\\nchart = {\\n    \"mark\": \"point\",\\n    \"encoding\": {\\n        \"x\": {\\n            \"field\": \"Horsepower\",\\n            \"type\": \"quantitative\",\\n        },\\n        \"y\": {\\n            \"field\": \"Miles_per_Gallon\",\\n            \"type\": \"quantitative\",\\n        },\\n        \"color\": {\"field\": \"Origin\", \"type\": \"nominal\"},\\n        \"shape\": {\"field\": \"Origin\", \"type\": \"nominal\"},\\n    },\\n}\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Vega-Lite native theme\"])\\n\\nwith tab1:\\n    # Use the Streamlit theme.\\n    # This is the default. So you can also omit the theme argument.\\n    st.vega_lite_chart(\\n        source, chart, theme=\"streamlit\", use_container_width=True\\n    )\\nwith tab2:\\n    st.vega_lite_chart(\\n        source, chart, theme=None, use_container_width=True\\n    )\\n```\\n\\nClick the tabs in the interactive app below to see the charts with the Streamlit theme enabled and disabled.', metadata={'source': 'docs/content/library/api/charts/vega_lite_chart.md'}),\n",
       " Document(page_content=\"If you're wondering if your own customizations will still be taken into account, don't worry! You can still make changes to your chart configurations. In other words, although we now enable the Streamlit theme by default, you can overwrite it with custom colors or fonts. For example, if you want a chart line to be green instead of the default red, you can do it!\", metadata={'source': 'docs/content/library/api/charts/vega_lite_chart.md'}),\n",
       " Document(page_content='title: st.pydeck_chart\\nslug: /library/api-reference/charts/st.pydeck_chart\\ndescription: st.pydeck_chart displays a chart using the PyDeck library.', metadata={'source': 'docs/content/library/api/charts/pydeck_chart.md'}),\n",
       " Document(page_content='title: st.line_chart\\nslug: /library/api-reference/charts/st.line_chart\\ndescription: st.line_chart displays a line chart.', metadata={'source': 'docs/content/library/api/charts/line_chart.md'}),\n",
       " Document(page_content='title: st.area_chart\\nslug: /library/api-reference/charts/st.area_chart\\ndescription: st.area_chart displays an area chart.', metadata={'source': 'docs/content/library/api/charts/area_chart.md'}),\n",
       " Document(page_content='title: st.altair_chart\\nslug: /library/api-reference/charts/st.altair_chart\\ndescription: st.altair_chart displays a chart using the Altair library.\\n\\nTheming\\n\\nAltair charts are displayed using the Streamlit theme by default. This theme is sleek, user-friendly, and incorporates Streamlit\\'s color palette. The added benefit is that your charts better integrate with the rest of your app\\'s design.\\n\\nThe Streamlit theme is available from Streamlit 1.16.0 through the theme=\"streamlit\" keyword argument. To disable it, and use Altair\\'s native theme, use theme=None instead.\\n\\nLet\\'s look at an example of charts with the Streamlit theme and the native Altair theme:\\n\\n```python\\nimport altair as alt\\nfrom vega_datasets import data\\n\\nsource = data.cars()\\n\\nchart = alt.Chart(source).mark_circle().encode(\\n    x=\\'Horsepower\\',\\n    y=\\'Miles_per_Gallon\\',\\n    color=\\'Origin\\',\\n).interactive()\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Altair native theme\"])\\n\\nwith tab1:\\n    # Use the Streamlit theme.\\n    # This is the default. So you can also omit the theme argument.\\n    st.altair_chart(chart, theme=\"streamlit\", use_container_width=True)\\nwith tab2:\\n    # Use the native Altair theme.\\n    st.altair_chart(chart, theme=None, use_container_width=True)\\n```\\n\\nClick the tabs in the interactive app below to see the charts with the Streamlit theme enabled and disabled.\\n\\nIf you\\'re wondering if your own customizations will still be taken into account, don\\'t worry! You can still make changes to your chart configurations. In other words, although we now enable the Streamlit theme by default, you can overwrite it with custom colors or fonts. For example, if you want a chart line to be green instead of the default red, you can do it!\\n\\nHere\\'s an example of an Altair chart where manual color passing is done and reflected:\\n\\n```python\\nimport altair as alt\\nimport streamlit as st\\nfrom vega_datasets import data\\n\\nsource = data.seattle_weather()', metadata={'source': 'docs/content/library/api/charts/altair_chart.md'}),\n",
       " Document(page_content='source = data.seattle_weather()\\n\\nscale = alt.Scale(\\n    domain=[\"sun\", \"fog\", \"drizzle\", \"rain\", \"snow\"],\\n    range=[\"#e7ba52\", \"#a7a7a7\", \"#aec7e8\", \"#1f77b4\", \"#9467bd\"],\\n)\\ncolor = alt.Color(\"weather:N\", scale=scale)\\n\\nWe create two selections:\\n\\na brush that is active on the top panel\\n\\na multi-click that is active on the bottom panel\\n\\nbrush = alt.selection_interval(encodings=[\"x\"])\\nclick = alt.selection_multi(encodings=[\"color\"])\\n\\nTop panel is scatter plot of temperature vs time\\n\\npoints = (\\n    alt.Chart()\\n    .mark_point()\\n    .encode(\\n        alt.X(\"monthdate(date):T\", title=\"Date\"),\\n        alt.Y(\\n            \"temp_max:Q\",\\n            title=\"Maximum Daily Temperature (C)\",\\n            scale=alt.Scale(domain=[-5, 40]),\\n        ),\\n        color=alt.condition(brush, color, alt.value(\"lightgray\")),\\n        size=alt.Size(\"precipitation:Q\", scale=alt.Scale(range=[5, 200])),\\n    )\\n    .properties(width=550, height=300)\\n    .add_selection(brush)\\n    .transform_filter(click)\\n)\\n\\nBottom panel is a bar chart of weather type\\n\\nbars = (\\n    alt.Chart()\\n    .mark_bar()\\n    .encode(\\n        x=\"count()\",\\n        y=\"weather:N\",\\n        color=alt.condition(click, color, alt.value(\"lightgray\")),\\n    )\\n    .transform_filter(brush)\\n    .properties(\\n        width=550,\\n    )\\n    .add_selection(click)\\n)\\n\\nchart = alt.vconcat(points, bars, data=source, title=\"Seattle Weather: 2012-2015\")\\n\\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Altair native theme\"])\\n\\nwith tab1:\\n    st.altair_chart(chart, theme=\"streamlit\", use_container_width=True)\\nwith tab2:\\n    st.altair_chart(chart, theme=None, use_container_width=True)\\n```\\n\\nNotice how the custom colors are still reflected in the chart, even when the Streamlit theme is enabled 👇\\n\\nFor many more examples of Altair charts with and without the Streamlit theme, check out the altair.streamlit.app.\\n\\nAnnotating charts', metadata={'source': 'docs/content/library/api/charts/altair_chart.md'}),\n",
       " Document(page_content='Annotating charts\\n\\nAltair also allows you to annotate your charts with text, images, and emojis. You can do this by creating layered charts, which let you overlay two different charts on top of each other. The idea is to use the first chart to show the data, and the second chart to show the annotations. The second chart can then be overlaid on top of the first chart using the + operator to create a layered chart.\\n\\nLet\\'s walk through an example of annotating a time-series chart with text and an emoji.\\n\\nStep 1: Create the base chart\\n\\nIn this example, we create a time-series chart to track the evolution of stock prices. The chart is interactive and contains a multi-line tooltip. Click here to learn more about multi-line tooltips in Altair.\\n\\nFirst, we import the required libraries and load the example stocks dataset using the vega_datasets package:\\n\\n```python\\nimport altair as alt\\nimport pandas as pd\\nimport streamlit as st\\nfrom vega_datasets import data\\n\\nWe use @st.cache_data to keep the dataset in cache\\n\\n@st.cache_data\\ndef get_data():\\n    source = data.stocks()\\n    source = source[source.date.gt(\"2004-01-01\")]\\n    return source\\n\\nsource = get_data()\\n```\\n\\nNext, we define a function get_chart() to create the interactive time-series chart of the stock prices with a multi-line tooltip. The x-axis represents the date, and the y-axis represents the stock price.\\n\\nWe then invoke get_chart() that takes the stock prices dataframe as an input and returns a chart object. This is going to be our base chart on which we will overlay the annotations in Step 2.\\n\\n```python\\n\\nDefine the base time-series chart.\\n\\ndef get_chart(data):\\n    hover = alt.selection_single(\\n        fields=[\"date\"],\\n        nearest=True,\\n        on=\"mouseover\",\\n        empty=\"none\",\\n    )\\n\\nchart = get_chart(source)\\n```\\n\\nStep 2: Annotate the chart', metadata={'source': 'docs/content/library/api/charts/altair_chart.md'}),\n",
       " Document(page_content='Step 2: Annotate the chart\\n\\nNow that we have our first chart that shows the data, we can annotate it with text and an emoji. Let\\'s overlay the ⬇ emoji on top of the time-series chart at specifc points of interest. We want users to hover over the ⬇ emoji to see the associated annotation text.\\n\\nFor simplicity, let\\'s annotate four specific dates and set the height of the annotations at constant value of 10.\\n\\nYou can vary the horizontal and vertical postions of the annotations by replacing the hard-coded values with the output of Streamlit widgets! Click here to jump to a live example below, and develop an intuition for the ideal horizontal and vertical positions of the annotations by playing with Streamlit widgets.\\n\\nTo do so, we create a dataframe annotations_df containing the dates, annotation text, and the height of the annotations:\\n\\n```python\\n\\nAdd annotations\\n\\nANNOTATIONS = [\\n    (\"Mar 01, 2008\", \"Pretty good day for GOOG\"),\\n    (\"Dec 01, 2007\", \"Something\\'s going wrong for GOOG & AAPL\"),\\n    (\"Nov 01, 2008\", \"Market starts again thanks to...\"),\\n    (\"Dec 01, 2009\", \"Small crash for GOOG after...\"),\\n]\\nannotations_df = pd.DataFrame(ANNOTATIONS, columns=[\"date\", \"event\"])\\nannotations_df.date = pd.to_datetime(annotations_df.date)\\nannotations_df[\"y\"] = 10\\n```\\n\\nUsing this dataframe, we create a scatter plot with the x-axis representing the date, and the y-axis representing the height of the annotation. The data point at the specific date and height is represented by the ⬇ emoji, using Altair\\'s mark_text() mark.\\n\\nThe annotation text is displayed as a tooltip when users hover over the ⬇ emoji. This is achieved using Altair\\'s encode() method to map the event column containing the annotation text to the visual attribute ⬇ of the plot.\\n\\npython\\nannotation_layer = (\\n    alt.Chart(annotations_df)\\n    .mark_text(size=20, text=\"⬇\", dx=-8, dy=-10, align=\"left\")\\n    .encode(\\n        x=\"date:T\",\\n        y=alt.Y(\"y:Q\"),\\n        tooltip=[\"event\"],\\n    )\\n    .interactive()\\n)', metadata={'source': 'docs/content/library/api/charts/altair_chart.md'}),\n",
       " Document(page_content='Finally, we overlay the annotations on top of the base chart using the + operator to create the final layered chart! 🎈\\n\\npython\\nst.altair_chart(\\n    (chart + annotation_layer).interactive(),\\n    use_container_width=True\\n)\\n\\nTo use images instead of emojis, replace the line containing .mark_text() with .mark_image(), and replace image_url below with the URL of the image:\\n\\npython\\n.mark_image(\\n    width=12,\\n    height=12,\\n    url=\"image_url\",\\n)\\n\\nInteractive example\\n\\nNow that you\\'ve learned how to annotate charts, the sky\\'s the limit! We\\'ve extended the above example to let you interactively paste your favorite emoji and set its position on the chart with Streamlit widgets. 👇', metadata={'source': 'docs/content/library/api/charts/altair_chart.md'}),\n",
       " Document(page_content='title: st.error\\nslug: /library/api-reference/status/st.error\\ndescription: st.error displays error message.', metadata={'source': 'docs/content/library/api/status/error.md'}),\n",
       " Document(page_content='title: Chart elements\\nslug: /library/api-reference/charts\\n\\nChart elements\\n\\nStreamlit supports several different charting libraries, and our goal is to\\ncontinually add support for more. Right now, the most basic library in our\\narsenal is Matplotlib. Then there are also\\ninteractive charting libraries like Vega\\nLite (2D charts) and\\ndeck.gl (maps and 3D charts). And\\nfinally we also provide a few chart types that are \"native\" to Streamlit,\\nlike st.line_chart and st.area_chart.\\n\\nSimple line charts\\n\\nDisplay a line chart.\\n\\npython\\nst.line_chart(my_data_frame)\\n\\nSimple area charts\\n\\nDisplay an area chart.\\n\\npython\\nst.area_chart(my_data_frame)\\n\\nSimple bar charts\\n\\nDisplay a bar chart.\\n\\npython\\nst.bar_chart(my_data_frame)\\n\\nScatterplots on maps\\n\\nDisplay a map with points on it.\\n\\npython\\nst.map(my_data_frame)\\n\\nMatplotlib\\n\\nDisplay a matplotlib.pyplot figure.\\n\\npython\\nst.pyplot(my_mpl_figure)\\n\\nAltair\\n\\nDisplay a chart using the Altair library.\\n\\npython\\nst.altair_chart(my_altair_chart)\\n\\nVega-Lite\\n\\nDisplay a chart using the Vega-Lite library.\\n\\npython\\nst.vega_lite_chart(my_vega_lite_chart)\\n\\nPlotly\\n\\nDisplay an interactive Plotly chart.\\n\\npython\\nst.plotly_chart(my_plotly_chart)\\n\\nBokeh\\n\\nDisplay an interactive Bokeh chart.\\n\\npython\\nst.bokeh_chart(my_bokeh_chart)\\n\\nPyDeck\\n\\nDisplay a chart using the PyDeck library.\\n\\npython\\nst.pydeck_chart(my_pydeck_chart)\\n\\nGraphViz\\n\\nDisplay a graph using the dagre-d3 library.\\n\\npython\\nst.graphviz_chart(my_graphviz_spec)\\n\\nPlost\\n\\nA deceptively simple plotting library for Streamlit. Created by @tvst.\\n\\npython\\nimport plost\\nplost.line_chart(my_dataframe, x=\\'time\\', y=\\'stock_value\\', color=\\'stock_name\\',)\\n\\nHiPlot\\n\\nHigh dimensional Interactive Plotting. Created by @facebookresearch.\\n\\npython\\ndata = [{\\'dropout\\':0.1, \\'lr\\': 0.001, \\'loss\\': 10.0, \\'optimizer\\': \\'SGD\\'}, {\\'dropout\\':0.15, \\'lr\\': 0.01, \\'loss\\': 3.5, \\'optimizer\\': \\'Adam\\'}, {\\'dropout\\':0.3, \\'lr\\': 0.1, \\'loss\\': 4.5, \\'optimizer\\': \\'Adam\\'}]\\nhip.Experiment.from_iterable(data).display()\\n\\nECharts', metadata={'source': 'docs/content/library/api/charts/charts.md'}),\n",
       " Document(page_content='ECharts\\n\\nHigh dimensional Interactive Plotting. Created by @andfanilo.\\n\\npython\\nfrom streamlit_echarts import st_echarts\\nst_echarts(options=options)\\n\\nStreamlit Folium\\n\\nStreamlit Component for rendering Folium maps. Created by @randyzwitch.\\n\\npython\\nm = folium.Map(location=[39.949610, -75.150282], zoom_start=16)\\nst_data = st_folium(m, width=725)\\n\\nSpacy-Streamlit\\n\\nspaCy building blocks and visualizers for Streamlit apps. Created by @explosion.\\n\\npython\\nmodels = [\"en_core_web_sm\", \"en_core_web_md\"]\\nspacy_streamlit.visualize(models, \"Sundar Pichai is the CEO of Google.\")\\n\\nStreamlit Agraph\\n\\nA Streamlit Graph Vis, based on react-grah-vis. Created by @ChrisDelClea.\\n\\npython\\nfrom streamlit_agraph import agraph, Node, Edge, Config\\nagraph(nodes=nodes, edges=edges, config=config)\\n\\nStreamlit Lottie\\n\\nIntegrate Lottie animations inside your Streamlit app. Created by @andfanilo.\\n\\npython\\nlottie_hello = load_lottieurl(\"https://assets5.lottiefiles.com/packages/lf20_V9t630.json\")\\nst_lottie(lottie_hello, key=\"hello\")\\n\\nPlotly Events\\n\\nMake Plotly charts interactive!. Created by @null-jones.\\n\\npython\\nfig = px.line(x=[1], y=[1])\\nselected_points = plotly_events(fig)\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\npython\\nchart += get_annotations_chart(annotations=[(\"Mar 01, 2008\", \"Pretty good day for GOOG\"), (\"Dec 01, 2007\", \"Something\\'s going wrong for GOOG & AAPL\"), (\"Nov 01, 2008\", \"Market starts again thanks to...\"), (\"Dec 01, 2009\", \"Small crash for GOOG after...\"),],)\\nst.altair_chart(chart, use_container_width=True)', metadata={'source': 'docs/content/library/api/charts/charts.md'}),\n",
       " Document(page_content='title: st.progress\\nslug: /library/api-reference/status/st.progress\\ndescription: st.progress displays a progress bar.', metadata={'source': 'docs/content/library/api/status/progress.md'}),\n",
       " Document(page_content='title: st.spinner\\nslug: /library/api-reference/status/st.spinner\\ndescription: st.spinner temporarily displays a message while executing a block of code.', metadata={'source': 'docs/content/library/api/status/spinner.md'}),\n",
       " Document(page_content='title: st.success\\nslug: /library/api-reference/status/st.success\\ndescription: st.success displays a success message.', metadata={'source': 'docs/content/library/api/status/success.md'}),\n",
       " Document(page_content='title: st.snow\\nslug: /library/api-reference/status/st.snow\\ndescription: st.snow displays celebratory snowflakes!', metadata={'source': 'docs/content/library/api/status/snow.md'}),\n",
       " Document(page_content='title: st.info\\nslug: /library/api-reference/status/st.info\\ndescription: st.info displays an informational message.', metadata={'source': 'docs/content/library/api/status/info.md'}),\n",
       " Document(page_content='title: st.balloons\\nslug: /library/api-reference/status/st.balloons\\ndescription: st.balloons displays celebratory balloons!', metadata={'source': 'docs/content/library/api/status/balloons.md'}),\n",
       " Document(page_content='title: st.exception\\nslug: /library/api-reference/status/st.exception\\ndescription: st.exception displays an exception.', metadata={'source': 'docs/content/library/api/status/exception.md'}),\n",
       " Document(page_content='title: Display progress and status\\nslug: /library/api-reference/status\\n\\nDisplay progress and status\\n\\nStreamlit provides a few methods that allow you to add animation to your\\napps. These animations include progress bars, status messages (like\\nwarnings), and celebratory balloons.\\n\\nProgress bar\\n\\nDisplay a progress bar.\\n\\npython\\nfor i in range(101):\\n  st.progress(i)\\n  do_something_slow()\\n\\nSpinner\\n\\nTemporarily displays a message while executing a block of code.\\n\\npython\\nwith st.spinner(\"Please wait...\"):\\n  do_something_slow()\\n\\nBalloons\\n\\nDisplay celebratory balloons!\\n\\n```python\\ndo_something()\\n\\nCelebrate when all done!\\n\\nst.balloons()\\n```\\n\\nSnowflakes\\n\\nDisplay celebratory snowflakes!\\n\\n```python\\ndo_something()\\n\\nCelebrate when all done!\\n\\nst.snow()\\n```\\n\\nError box\\n\\nDisplay error message.\\n\\npython\\nst.error(\"We encountered an error\")\\n\\nWarning box\\n\\nDisplay warning message.\\n\\npython\\nst.warning(\"Unable to fetch image. Skipping...\")\\n\\nInfo box\\n\\nDisplay an informational message.\\n\\npython\\nst.info(\"Dataset is updated every day at midnight.\")\\n\\nSuccess box\\n\\nDisplay a success message.\\n\\npython\\nst.success(\"Match found!\")\\n\\nException output\\n\\nDisplay an exception.\\n\\npython\\ne = RuntimeError(\"This is an exception of type RuntimeError\")\\nst.exception(e)\\n\\nStqdm\\n\\nThe simplest way to handle a progress bar in streamlit app. Created by @Wirg.\\n\\n```python\\nfrom stqdm import stqdm\\n\\nfor _ in stqdm(range(50)):\\n    sleep(0.5)\\n```\\n\\nCustom notification box\\n\\nA custom notification box with the ability to close it out. Created by @Socvest.\\n\\n```python\\nfrom streamlit_custom_notification_box import custom_notification_box\\n\\nstyles = {\\'material-icons\\':{\\'color\\': \\'red\\'}, \\'text-icon-link-close-container\\': {\\'box-shadow\\': \\'#3896de 0px 4px\\'}, \\'notification-text\\': {\\'\\':\\'\\'}, \\'close-button\\':{\\'\\':\\'\\'}, \\'link\\':{\\'\\':\\'\\'}}\\ncustom_notification_box(icon=\\'info\\', textDisplay=\\'We are almost done with your registration...\\', externalLink=\\'more info\\', url=\\'#\\', styles=styles, key=\"foo\")\\n```\\n\\nStreamlit Extras', metadata={'source': 'docs/content/library/api/status/status.md'}),\n",
       " Document(page_content='Streamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.let_it_rain import rain\\n\\nrain(emoji=\"🎈\", font_size=54,\\n  falling_speed=5, animation_length=\"infinite\",)\\n```', metadata={'source': 'docs/content/library/api/status/status.md'}),\n",
       " Document(page_content='title: st.warning\\nslug: /library/api-reference/status/st.warning\\ndescription: st.warning displays warning message.', metadata={'source': 'docs/content/library/api/status/warning.md'}),\n",
       " Document(page_content='title: st.metric\\nslug: /library/api-reference/data/st.metric\\ndescription: st.metric displays a metric in big bold font, with an optional indicator of how the metric changed.', metadata={'source': 'docs/content/library/api/data/metric.md'}),\n",
       " Document(page_content='title: API Reference\\nslug: /library/api-reference\\nnext: caching\\nprevious: index.md\\n\\nAPI reference\\n\\nStreamlit makes it easy for you to visualize, mutate, and share data. The API\\nreference is organized by activity type, like displaying data or optimizing\\nperformance. Each section includes methods associated with the activity type,\\nincluding examples.\\n\\nBrowse our API below and click to learn more about any of our available commands! 🎈\\n\\nDisplay almost anything\\n\\nst.write\\n\\nWrite arguments to the app.\\n\\npython\\nst.write(\"Hello **world**!\")\\nst.write(my_data_frame)\\nst.write(my_mpl_figure)\\n\\nMagic\\n\\nAny time Streamlit sees either a variable or literal value on its own line, it automatically writes that to your app using st.write\\n\\npython\\n\"Hello **world**!\"\\nmy_data_frame\\nmy_mpl_figure\\n\\nText elements\\n\\nMarkdown\\n\\nDisplay string formatted as Markdown.\\n\\npython\\nst.markdown(\"Hello **world**!\")\\n\\nTitle\\n\\nDisplay text in title formatting.\\n\\npython\\nst.title(\"The app title\")\\n\\nHeader\\n\\nDisplay text in header formatting.\\n\\npython\\nst.header(\"This is a header\")\\n\\nSubheader\\n\\nDisplay text in subheader formatting.\\n\\npython\\nst.subheader(\"This is a subheader\")\\n\\nCaption\\n\\nDisplay text in small font.\\n\\npython\\nst.caption(\"This is written small caption text\")\\n\\nCode block\\n\\nDisplay a code block with optional syntax highlighting.\\n\\npython\\nst.code(\"a = 1234\")\\n\\nPreformatted text\\n\\nWrite fixed-width and preformatted text.\\n\\npython\\nst.text(\"Hello world\")\\n\\nLaTeX\\n\\nDisplay mathematical expressions formatted as LaTeX.\\n\\npython\\nst.latex(\"\\\\int a x^2 \\\\,dx\")\\n\\nDivider\\n\\nDisplay a horizontal rule.\\n\\npython\\nst.divider()\\n\\nAnnotated text\\n\\nDisplay annotated text in Streamlit apps. Created by @tvst.\\n\\npython\\nannotated_text(\"This \", (\"is\", \"verb\"), \" some \", (\"annotated\", \"adj\"), (\"text\", \"noun\"), \" for those of \", (\"you\", \"pronoun\"), \" who \", (\"like\", \"verb\"), \" this sort of \", (\"thing\", \"noun\"), \".\")\\n\\nDrawable Canvas\\n\\nProvides a sketching canvas using Fabric.js. Created by @andfanilo.', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='python\\nst_canvas(fill_color=\"rgba(255, 165, 0, 0.3)\", stroke_width=stroke_width, stroke_color=stroke_color, background_color=bg_color, background_image=Image.open(bg_image) if bg_image else None, update_streamlit=realtime_update, height=150, drawing_mode=drawing_mode, point_display_radius=point_display_radius if drawing_mode == \\'point\\' else 0, key=\"canvas\",)\\n\\nTags\\n\\nAdd tags to your Streamlit apps. Created by @gagan3012.\\n\\npython\\nst_tags(label=\\'# Enter Keywords:\\', text=\\'Press enter to add more\\', value=[\\'Zero\\', \\'One\\', \\'Two\\'], suggestions=[\\'five\\', \\'six\\', \\'seven\\', \\'eight\\', \\'nine\\', \\'three\\', \\'eleven\\', \\'ten\\', \\'four\\'], maxtags = 4, key=\\'1\\')\\n\\nNLU\\n\\nApply text mining on a dataframe. Created by @JohnSnowLabs.\\n\\npython\\nnlu.load(\\'sentiment\\').predict(\\'I love NLU! <3\\')\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\npython\\nmention(label=\"An awesome Streamlit App\", icon=\"streamlit\",  url=\"https://extras.streamlit.app\",)\\n\\nData elements\\n\\nDataframes\\n\\nDisplay a dataframe as an interactive table.\\n\\npython\\nst.dataframe(my_data_frame)\\n\\nData editor\\n\\nDisplay a data editor widget.\\n\\npython\\nedited = st.data_editor(df, num_rows=\"dynamic\")\\n\\nColumn configuration\\n\\nConfigure the display and editing behavior of dataframes and data editors.\\n\\npython\\nst.column_config.NumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nStatic tables\\n\\nDisplay a static table.\\n\\npython\\nst.table(my_data_frame)\\n\\nMetrics\\n\\nDisplay a metric in big bold font, with an optional indicator of how the metric changed.\\n\\npython\\nst.metric(\"My metric\", 42, 2)\\n\\nDicts and JSON\\n\\nDisplay object or string as a pretty-printed JSON string.\\n\\npython\\nst.json(my_dict)\\n\\nStreamlit Aggrid\\n\\nImplementation of Ag-Grid component for Streamlit. Created by @PablocFonseca.\\n\\n```python\\ndf = pd.DataFrame({\\'col1\\': [1, 2, 3], \\'col2\\': [4, 5, 6]})\\ngrid_return = AgGrid(df, editable=True)\\n\\nnew_df = grid_return[\\'data\\']\\n```\\n\\nStreamlit Folium\\n\\nStreamlit Component for rendering Folium maps. Created by @randyzwitch.', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='```python\\nm = folium.Map(location=[39.949610, -75.150282], zoom_start=16)\\nfolium.Marker([39.949610, -75.150282], popup=\"Liberty Bell\", tooltip=\"Liberty Bell\").add_to(m)\\n\\nst_data = st_folium(m, width=725)\\n```\\n\\nPandas Profiling\\n\\nPandas profiling component for Streamlit. Created by @okld.\\n\\n```python\\ndf = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\\npr = df.profile_report()\\n\\nst_profile_report(pr)\\n```\\n\\nImage Coordinates\\n\\nGet the coordinates of clicks on an image. Created by @blackary.\\n\\n```python\\nfrom streamlit_image_coordinates import streamlit_image_coordinates\\nvalue = streamlit_image_coordinates(\"https://placekitten.com/200/300\")\\n\\nst.write(value)\\n```\\n\\nPlotly Events\\n\\nMake Plotly charts interactive!. Created by @null-jones.\\n\\n```python\\nfrom streamlit_plotly_events import plotly_events\\nfig = px.line(x=[1], y=[1])\\n\\nselected_points = plotly_events(fig)\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.metric_cards import style_metric_cards\\ncol3.metric(label=\"No Change\", value=5000, delta=0)\\n\\nstyle_metric_cards()\\n```\\n\\nChart elements\\n\\nSimple line charts\\n\\nDisplay a line chart.\\n\\npython\\nst.line_chart(my_data_frame)\\n\\nSimple area charts\\n\\nDisplay an area chart.\\n\\npython\\nst.area_chart(my_data_frame)\\n\\nSimple bar charts\\n\\nDisplay a bar chart.\\n\\npython\\nst.bar_chart(my_data_frame)\\n\\nScatterplots on maps\\n\\nDisplay a map with points on it.\\n\\npython\\nst.map(my_data_frame)\\n\\nMatplotlib\\n\\nDisplay a matplotlib.pyplot figure.\\n\\npython\\nst.pyplot(my_mpl_figure)\\n\\nAltair\\n\\nDisplay a chart using the Altair library.\\n\\npython\\nst.altair_chart(my_altair_chart)\\n\\nVega-Lite\\n\\nDisplay a chart using the Vega-Lite library.\\n\\npython\\nst.vega_lite_chart(my_vega_lite_chart)\\n\\nPlotly\\n\\nDisplay an interactive Plotly chart.\\n\\npython\\nst.plotly_chart(my_plotly_chart)\\n\\nBokeh\\n\\nDisplay an interactive Bokeh chart.\\n\\npython\\nst.bokeh_chart(my_bokeh_chart)\\n\\nPyDeck\\n\\nDisplay a chart using the PyDeck library.', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='Display a chart using the PyDeck library.\\n\\npython\\nst.pydeck_chart(my_pydeck_chart)\\n\\nGraphViz\\n\\nDisplay a graph using the dagre-d3 library.\\n\\npython\\nst.graphviz_chart(my_graphviz_spec)\\n\\nPlost\\n\\nA deceptively simple plotting library for Streamlit. Created by @tvst.\\n\\npython\\nimport plost\\nplost.line_chart(my_dataframe, x=\\'time\\', y=\\'stock_value\\', color=\\'stock_name\\',)\\n\\nHiPlot\\n\\nHigh dimensional Interactive Plotting. Created by @facebookresearch.\\n\\npython\\ndata = [{\\'dropout\\':0.1, \\'lr\\': 0.001, \\'loss\\': 10.0, \\'optimizer\\': \\'SGD\\'}, {\\'dropout\\':0.15, \\'lr\\': 0.01, \\'loss\\': 3.5, \\'optimizer\\': \\'Adam\\'}, {\\'dropout\\':0.3, \\'lr\\': 0.1, \\'loss\\': 4.5, \\'optimizer\\': \\'Adam\\'}]\\nhip.Experiment.from_iterable(data).display()\\n\\nECharts\\n\\nHigh dimensional Interactive Plotting. Created by @andfanilo.\\n\\npython\\nfrom streamlit_echarts import st_echarts\\nst_echarts(options=options)\\n\\nStreamlit Folium\\n\\nStreamlit Component for rendering Folium maps. Created by @randyzwitch.\\n\\npython\\nm = folium.Map(location=[39.949610, -75.150282], zoom_start=16)\\nst_data = st_folium(m, width=725)\\n\\nSpacy-Streamlit\\n\\nspaCy building blocks and visualizers for Streamlit apps. Created by @explosion.\\n\\npython\\nmodels = [\"en_core_web_sm\", \"en_core_web_md\"]\\nspacy_streamlit.visualize(models, \"Sundar Pichai is the CEO of Google.\")\\n\\nStreamlit Agraph\\n\\nA Streamlit Graph Vis, based on react-grah-vis. Created by @ChrisDelClea.\\n\\npython\\nfrom streamlit_agraph import agraph, Node, Edge, Config\\nagraph(nodes=nodes, edges=edges, config=config)\\n\\nStreamlit Lottie\\n\\nIntegrate Lottie animations inside your Streamlit app. Created by @andfanilo.\\n\\npython\\nlottie_hello = load_lottieurl(\"https://assets5.lottiefiles.com/packages/lf20_V9t630.json\")\\nst_lottie(lottie_hello, key=\"hello\")\\n\\nPlotly Events\\n\\nMake Plotly charts interactive!. Created by @null-jones.\\n\\npython\\nfig = px.line(x=[1], y=[1])\\nselected_points = plotly_events(fig)\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='python\\nchart += get_annotations_chart(annotations=[(\"Mar 01, 2008\", \"Pretty good day for GOOG\"), (\"Dec 01, 2007\", \"Something\\'s going wrong for GOOG & AAPL\"), (\"Nov 01, 2008\", \"Market starts again thanks to...\"), (\"Dec 01, 2009\", \"Small crash for GOOG after...\"),],)\\nst.altair_chart(chart, use_container_width=True)\\n\\nInput widgets\\n\\nButton\\n\\nDisplay a button widget.\\n\\npython\\nclicked = st.button(\"Click me\")\\n\\nData editor\\n\\nDisplay a data editor widget.\\n\\npython\\nedited = st.experimental_data_editor(df, num_rows=\"dynamic\")\\n\\nDownload button\\n\\nDisplay a download button widget.\\n\\npython\\nst.download_button(\"Download file\", file)\\n\\nCheckbox\\n\\nDisplay a checkbox widget.\\n\\npython\\nselected = st.checkbox(\"I agree\")\\n\\nRadio\\n\\nDisplay a radio button widget.\\n\\npython\\nchoice = st.radio(\"Pick one\", [\"cats\", \"dogs\"])\\n\\nSelectbox\\n\\nDisplay a select widget.\\n\\npython\\nchoice = st.selectbox(\"Pick one\", [\"cats\", \"dogs\"])\\n\\nMultiselect\\n\\nDisplay a multiselect widget. The multiselect widget starts as empty.\\n\\npython\\nchoices = st.multiselect(\"Buy\", [\"milk\", \"apples\", \"potatoes\"])\\n\\nSlider\\n\\nDisplay a slider widget.\\n\\npython\\nnumber = st.slider(\"Pick a number\", 0, 100)\\n\\nSelect-slider\\n\\nDisplay a slider widget to select items from a list.\\n\\npython\\nsize = st.select_slider(\"Pick a size\", [\"S\", \"M\", \"L\"])\\n\\nText input\\n\\nDisplay a single-line text input widget.\\n\\npython\\nname = st.text_input(\"First name\")\\n\\nNumber input\\n\\nDisplay a numeric input widget.\\n\\npython\\nchoice = st.number_input(\"Pick a number\", 0, 10)\\n\\nText-area\\n\\nDisplay a multi-line text input widget.\\n\\npython\\ntext = st.text_area(\"Text to translate\")\\n\\nDate input\\n\\nDisplay a date input widget.\\n\\npython\\ndate = st.date_input(\"Your birthday\")\\n\\nTime input\\n\\nDisplay a time input widget.\\n\\npython\\ntime = st.time_input(\"Meeting time\")\\n\\nFile Uploader\\n\\nDisplay a file uploader widget.\\n\\npython\\ndata = st.file_uploader(\"Upload a CSV\")\\n\\nCamera input\\n\\nDisplay a widget that allows users to upload images directly from a camera.\\n\\npython\\nimage = st.camera_input(\"Take a picture\")\\n\\nColor picker', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='Color picker\\n\\nDisplay a color picker widget.\\n\\npython\\ncolor = st.color_picker(\"Pick a color\")\\n\\nStreamlit Elements\\n\\nCreate a draggable and resizable dashboard in Streamlit. Created by @okls.\\n\\n```python\\nfrom streamlit_elements import elements, mui, html\\n\\nwith elements(\"new_element\"):\\n  mui.Typography(\"Hello world\")\\n```\\n\\nTags\\n\\nAdd tags to your Streamlit apps. Created by @gagan3012.\\n\\n```python\\nfrom streamlit_tags import st_tags\\n\\nst_tags(label=\\'# Enter Keywords:\\', text=\\'Press enter to add more\\', value=[\\'Zero\\', \\'One\\', \\'Two\\'],\\nsuggestions=[\\'five\\', \\'six\\', \\'seven\\', \\'eight\\', \\'nine\\', \\'three\\', \\'eleven\\', \\'ten\\', \\'four\\'], maxtags = 4, key=\\'1\\')\\n```\\n\\nStqdm\\n\\nThe simplest way to handle a progress bar in streamlit app. Created by @Wirg.\\n\\n```python\\nfrom stqdm import stqdm\\n\\nfor _ in stqdm(range(50)):\\n    sleep(0.5)\\n```\\n\\nTimeline\\n\\nDisplay a Timeline in Streamlit apps using TimelineJS. Created by @innerdoc.\\n\\n```python\\nfrom streamlit_timeline import timeline\\n\\nwith open(\\'example.json\\', \"r\") as f:\\n  timeline(f.read(), height=800)\\n```\\n\\nCamera input live\\n\\nAlternative for st.camera_input which returns the webcam images live. Created by @blackary.\\n\\n```python\\nfrom camera_input_live import camera_input_live\\n\\nimage = camera_input_live()\\nst.image(value)\\n```\\n\\nStreamlit Ace\\n\\nAce editor component for Streamlit. Created by @okld.\\n\\n```python\\nfrom streamlit_ace import st_ace\\n\\ncontent = st_ace()\\ncontent\\n```\\n\\nStreamlit Chat\\n\\nStreamlit Component for a Chatbot UI. Created by @AI-Yash.\\n\\n```python\\nfrom streamlit_chat import message\\n\\nmessage(\"My message\")\\nmessage(\"Hello bot!\", is_user=True)  # align\\'s the message to the right\\n```\\n\\nStreamlit Option Menu\\n\\nSelect a single item from a list of options in a menu. Created by @victoryhb.\\n\\n```python\\nfrom streamlit_option_menu import option_menu\\n\\noption_menu(\"Main Menu\", [\"Home\", \\'Settings\\'],\\n  icons=[\\'house\\', \\'gear\\'], menu_icon=\"cast\", default_index=1)\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='```python\\nfrom streamlit_extras.stoggle import stoggle\\n\\nstoggle(\\n    \"Click me!\", \"\"\"🥷 Surprise! Here\\'s some additional content\"\"\",)\\n```\\n\\nMedia elements\\n\\nImage\\n\\nDisplay an image or list of images.\\n\\npython\\nst.image(numpy_array)\\nst.image(image_bytes)\\nst.image(file)\\nst.image(\"https://example.com/myimage.jpg\")\\n\\nAudio\\n\\nDisplay an audio player.\\n\\npython\\nst.audio(numpy_array)\\nst.audio(audio_bytes)\\nst.audio(file)\\nst.audio(\"https://example.com/myaudio.mp3\", format=\"audio/mp3\")\\n\\nVideo\\n\\nDisplay a video player.\\n\\npython\\nst.video(numpy_array)\\nst.video(video_bytes)\\nst.video(file)\\nst.video(\"https://example.com/myvideo.mp4\", format=\"video/mp4\")\\n\\nStreamlit Webrtc\\n\\nHandling and transmitting real-time video/audio streams with Streamlit. Created by @whitphx.\\n\\n```python\\nfrom streamlit_webrtc import webrtc_streamer\\n\\nwebrtc_streamer(key=\"sample\")\\n```\\n\\nDrawable Canvas\\n\\nProvides a sketching canvas using Fabric.js. Created by @andfanilo.\\n\\n```python\\nfrom streamlit_drawable_canvas import st_canvas\\n\\nst_canvas(fill_color=\"rgba(255, 165, 0, 0.3)\", stroke_width=stroke_width, stroke_color=stroke_color, background_color=bg_color, background_image=Image.open(bg_image) if bg_image else None, update_streamlit=realtime_update, height=150, drawing_mode=drawing_mode, point_display_radius=point_display_radius if drawing_mode == \\'point\\' else 0, key=\"canvas\",)\\n```\\n\\nImage Comparison\\n\\nCompare images with a slider using JuxtaposeJS. Created by @fcakyon.\\n\\n```python\\nfrom streamlit_image_comparison import image_comparison\\n\\nimage_comparison(img1=\"image1.jpg\", img2=\"image2.jpg\",)\\n```\\n\\nStreamlit Cropper\\n\\nA simple image cropper for Streamlit. Created by @turner-anderson.\\n\\n```python\\nfrom streamlit_cropper import st_cropper\\n\\nst_cropper(img, realtime_update=realtime_update, box_color=box_color, aspect_ratio=aspect_ratio)\\n```\\n\\nImage Coordinates\\n\\nGet the coordinates of clicks on an image. Created by @blackary.\\n\\n```python\\nfrom streamlit_image_coordinates import streamlit_image_coordinates', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='streamlit_image_coordinates(\"https://placekitten.com/200/300\")\\n```\\n\\nStreamlit Lottie\\n\\nIntegrate Lottie animations inside your Streamlit app. Created by @andfanilo.\\n\\n```python\\nlottie_hello = load_lottieurl(\"https://assets5.lottiefiles.com/packages/lf20_V9t630.json\")\\n\\nst_lottie(lottie_hello, key=\"hello\")\\n```\\n\\nLayouts and containers\\n\\nSidebar\\n\\nDisplay items in a sidebar.\\n\\npython\\nst.sidebar.write(\"This lives in the sidebar\")\\nst.sidebar.button(\"Click me!\")\\n\\nColumns\\n\\nInsert containers laid out as side-by-side columns.\\n\\npython\\ncol1, col2 = st.columns(2)\\ncol1.write(\"this is column 1\")\\ncol2.write(\"this is column 2\")\\n\\nTabs\\n\\nInsert containers separated into tabs.\\n\\npython\\ntab1, tab2 = st.tabs([\"Tab 1\", \"Tab2\"])\\ntab1.write(\"this is tab 1\")\\ntab2.write(\"this is tab 2\")\\n\\nExpander\\n\\nInsert a multi-element container that can be expanded/collapsed.\\n\\npython\\nwith st.expander(\"Open to see more\"):\\n  st.write(\"This is more content\")\\n\\nContainer\\n\\nInsert a multi-element container.\\n\\npython\\nc = st.container()\\nst.write(\"This will show last\")\\nc.write(\"This will show first\")\\nc.write(\"This will show second\")\\n\\nEmpty\\n\\nInsert a single-element container.\\n\\npython\\nc = st.empty()\\nst.write(\"This will show last\")\\nc.write(\"This will be replaced\")\\nc.write(\"This will show first\")\\n\\nStreamlit Elements\\n\\nCreate a draggable and resizable dashboard in Streamlit. Created by @okls.\\n\\n```python\\nfrom streamlit_elements import elements, mui, html\\n\\nwith elements(\"new_element\"):\\n  mui.Typography(\"Hello world\")\\n```\\n\\nPydantic\\n\\nAuto-generate Streamlit UI from Pydantic Models and Dataclasses. Created by @lukasmasuch.\\n\\n```python\\nimport streamlit_pydantic as sp\\n\\nsp.pydantic_form(key=\"my_form\",\\n  model=ExampleModel)\\n```\\n\\nStreamlit Pages\\n\\nAn experimental version of Streamlit Multi-Page Apps. Created by @blackary.\\n\\n```python\\nfrom st_pages import Page, show_pages, add_page_title\\n\\nshow_pages([ Page(\"streamlit_app.py\", \"Home\", \"🏠\"),\\n  Page(\"other_pages/page2.py\", \"Page 2\", \":books:\"), ])\\n```\\n\\nChat elements', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='Chat elements\\n\\nStreamlit provides a few commands to help you build conversational apps. These chat elements are designed to be used in conjunction with each other, but you can also use them separately.\\n\\nst.chat_message lets you insert a chat message container into the app so you can display messages from the user or the app. Chat containers can contain other Streamlit elements, including charts, tables, text, and more. st.chat_input lets you display a chat input widget so the user can type in a message.\\n\\nChat message\\n\\nInsert a chat message container.\\n\\npython\\nimport numpy as np\\nwith st.chat_message(\"user\"):\\n    st.write(\"Hello 👋\")\\n    st.line_chart(np.random.randn(30, 3))\\n\\nChat input\\n\\nDisplay a chat input widget.\\n\\npython\\nprompt = st.chat_input(\"Say something\")\\nif prompt:\\n    st.write(f\"The user has sent: {prompt}\")\\n\\nDisplay progress and status\\n\\nProgress bar\\n\\nDisplay a progress bar.\\n\\npython\\nfor i in range(101):\\n  st.progress(i)\\n  do_something_slow()\\n\\nSpinner\\n\\nTemporarily displays a message while executing a block of code.\\n\\npython\\nwith st.spinner(\"Please wait...\"):\\n  do_something_slow()\\n\\nBalloons\\n\\nDisplay celebratory balloons!\\n\\n```python\\ndo_something()\\n\\nCelebrate when all done!\\n\\nst.balloons()\\n```\\n\\nSnowflakes\\n\\nDisplay celebratory snowflakes!\\n\\n```python\\ndo_something()\\n\\nCelebrate when all done!\\n\\nst.snow()\\n```\\n\\nError box\\n\\nDisplay error message.\\n\\npython\\nst.error(\"We encountered an error\")\\n\\nWarning box\\n\\nDisplay warning message.\\n\\npython\\nst.warning(\"Unable to fetch image. Skipping...\")\\n\\nInfo box\\n\\nDisplay an informational message.\\n\\npython\\nst.info(\"Dataset is updated every day at midnight.\")\\n\\nSuccess box\\n\\nDisplay a success message.\\n\\npython\\nst.success(\"Match found!\")\\n\\nException output\\n\\nDisplay an exception.\\n\\npython\\ne = RuntimeError(\"This is an exception of type RuntimeError\")\\nst.exception(e)\\n\\nStqdm\\n\\nThe simplest way to handle a progress bar in streamlit app. Created by @Wirg.\\n\\n```python\\nfrom stqdm import stqdm\\n\\nfor _ in stqdm(range(50)):\\n    sleep(0.5)\\n```', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='for _ in stqdm(range(50)):\\n    sleep(0.5)\\n```\\n\\nCustom notification box\\n\\nA custom notification box with the ability to close it out. Created by @Socvest.\\n\\n```python\\nfrom streamlit_custom_notification_box import custom_notification_box\\n\\nstyles = {\\'material-icons\\':{\\'color\\': \\'red\\'}, \\'text-icon-link-close-container\\': {\\'box-shadow\\': \\'#3896de 0px 4px\\'}, \\'notification-text\\': {\\'\\':\\'\\'}, \\'close-button\\':{\\'\\':\\'\\'}, \\'link\\':{\\'\\':\\'\\'}}\\ncustom_notification_box(icon=\\'info\\', textDisplay=\\'We are almost done with your registration...\\', externalLink=\\'more info\\', url=\\'#\\', styles=styles, key=\"foo\")\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.let_it_rain import rain\\n\\nrain(emoji=\"🎈\", font_size=54,\\n  falling_speed=5, animation_length=\"infinite\",)\\n```\\n\\nControl flow\\n\\nForms\\n\\nCreate a form that batches elements together with a “Submit\" button.\\n\\npython\\nwith st.form(key=\\'my_form\\'):\\n    username = st.text_input(\"Username\")\\n    password = st.text_input(\"Password\")\\n    st.form_submit_button(\"Login\")\\n\\nStop execution\\n\\nStops execution immediately.\\n\\npython\\nst.stop()\\n\\nRerun script\\n\\nRerun the script immediately.\\n\\npython\\nst.experimental_rerun()\\n\\nAutorefresh\\n\\nForce a refresh without tying up a script. Created by @kmcgrady.\\n\\n```python\\nfrom streamlit_autorefresh import st_autorefresh\\n\\nst_autorefresh(interval=2000, limit=100,\\n  key=\"fizzbuzzcounter\")\\n```\\n\\nPydantic\\n\\nAuto-generate Streamlit UI from Pydantic Models and Dataclasses. Created by @lukasmasuch.\\n\\n```python\\nimport streamlit_pydantic as sp\\n\\nsp.pydantic_form(key=\"my_form\",\\n  model=ExampleModel)\\n```\\n\\nStreamlit Pages\\n\\nAn experimental version of Streamlit Multi-Page Apps. Created by @blackary.\\n\\n```python\\nfrom st_pages import Page, show_pages, add_page_title\\n\\nshow_pages([ Page(\"streamlit_app.py\", \"Home\", \"🏠\"),\\n  Page(\"other_pages/page2.py\", \"Page 2\", \":books:\"), ])\\n```\\n\\nDeveloper tools\\n\\nPandas Profiling\\n\\nPandas profiling component for Streamlit. Created by @okld.', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='```python\\ndf = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\\npr = df.profile_report()\\n\\nst_profile_report(pr)\\n```\\n\\nStreamlit Ace\\n\\nAce editor component for Streamlit. Created by @okld.\\n\\n```python\\nfrom streamlit_ace import st_ace\\n\\ncontent = st_ace()\\ncontent\\n```\\n\\nStreamlit Anaylitics\\n\\nTrack & visualize user interactions with your streamlit app. Created by @jrieke.\\n\\n```python\\nimport streamlit_analytics\\n\\nwith streamlit_analytics.track():\\n    st.text_input(\"Write something\")\\n```\\n\\nUtilities\\n\\nSet page title, favicon, and more\\n\\nConfigures the default settings of the page.\\n\\npython\\nst.set_page_config(\\n  page_title=\"My app\",\\n  page_icon=\":shark:\",\\n)\\n\\nEcho\\n\\nDisplay some code on the app, then execute it. Useful for tutorials.\\n\\npython\\nwith st.echo():\\n  st.write(\\'This code will be printed\\')\\n\\nGet help\\n\\nDisplay object’s doc string, nicely formatted.\\n\\npython\\nst.help(st.write)\\nst.help(pd.DataFrame)\\n\\nGet query parameters\\n\\nReturn the query parameters that are currently showing in the browser\\'s URL bar.\\n\\npython\\nst.experimental_get_query_params()\\n\\nSet query parameters\\n\\nSet the query parameters that are shown in the browser\\'s URL bar.\\n\\npython\\nst.experimental_set_query_params(\\n  show_map=True,\\n  selected=[\"asia\"]\\n)\\n\\nMutate charts\\n\\nAdd rows\\n\\nAppend a dataframe to the bottom of the current one in certain elements, for optimized data updates.\\n\\npython\\nelement = st.line_chart(df)\\nelement.add_rows(df_with_extra_rows)\\n\\nState management\\n\\nSession state\\n\\nSession state is a way to share variables between reruns, for each user session.\\n\\npython\\nst.session_state[\\'key\\'] = value\\n\\nConnections and databases\\n\\nAuthenticator\\n\\nA secure authentication module to validate user credentials. Created by @mkhorasani.\\n\\n```python\\nimport streamlit_authenticator as stauth\\n\\nauthenticator = stauth.Authenticate( config[\\'credentials\\'], config[\\'cookie\\'][\\'name\\'],\\nconfig[\\'cookie\\'][\\'key\\'], config[\\'cookie\\'][\\'expiry_days\\'], config[\\'preauthorized\\'])\\n```\\n\\nWS localStorage', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='WS localStorage\\n\\nA simple synchronous way of accessing localStorage from your app. Created by @gagangoku.\\n\\n```python\\nfrom streamlit_ws_localstorage import injectWebsocketCode\\n\\nret = conn.setLocalStorageVal(key=\\'k1\\', val=\\'v1\\')\\nst.write(\\'ret: \\' + ret)\\n```\\n\\nStreamlit Auth0\\n\\nThe fastest way to provide comprehensive login inside Streamlit. Created by @conradbez.\\n\\n```python\\nfrom auth0_component import login_button\\n\\nuser_info = login_button(clientId, domain = domain)\\nst.write(user_info)\\n```\\n\\nPerformance\\n\\nCache data\\n\\nFunction decorator to cache functions that return data (e.g. dataframe transforms, database queries, ML inference).\\n\\npython\\n@st.cache_data\\ndef long_function(param1, param2):\\n  # Perform expensive computation here or\\n  # fetch data from the web here\\n  return data\\n\\nCache resource\\n\\nFunction decorator to cache functions that return global resources (e.g. database connections, ML models).\\n\\npython\\n@st.cache_resource\\ndef init_model():\\n  # Return a global resource here\\n  return pipeline(\\n    \"sentiment-analysis\",\\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\\n  )\\n\\nClear cached data\\n\\nClear all in-memory and on-disk data caches.\\n\\n```python\\n@st.cache_data\\ndef long_function(param1, param2):\\n  # Perform expensive computation here or\\n  # fetch data from the web here\\n  return data\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all cache_data functions\\n  st.cache_data.clear()\\n```\\n\\nClear cached resources\\n\\nClear all st.cache_resource caches.\\n\\n```python\\n@st.cache_resource\\ndef init_model():\\n  # Return a global resource here\\n  return pipeline(\\n    \"sentiment-analysis\",\\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\\n  )\\n\\nif st.checkbox(\"Clear All\"):\\n  # Clear values from all cache_resource functions\\n  st.cache_data.clear()\\n```\\n\\nConnections and databases\\n\\nSetup your connection\\n\\nCreate a connection\\n\\nConnect to a data source or API', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='Connect to a data source or API\\n\\npython\\nconn = st.experimental_connection(\\'pets_db\\', type=\\'sql\\')\\npet_owners = conn.query(\\'select * from pet_owners\\')\\nst.dataframe(pet_owners)\\n\\nBuilt-in connections\\n\\nSQLConnection\\n\\nA connection to a SQL database using SQLAlchemy.\\n\\npython\\nconn = st.experimental_connection(\\'sql\\')\\n\\nSnowparkConnection\\n\\nA connection to Snowflake Snowpark.\\n\\npython\\nconn = st.experimental_connection(\\'snowpark\\')\\n\\nThird-party connections\\n\\nConnection base class\\n\\nBuild your own connection with ExperimentalBaseConnection.\\n\\npython\\nclass MyConnection(ExperimentalBaseConnection[myconn.MyConnection]):\\n    def _connect(self, **kwargs) -> MyConnection:\\n        return myconn.connect(**self._secrets, **kwargs)\\n    def query(self, query):\\n        return self._instance.query(query)\\n\\nPersonalization\\n\\nUser info\\n\\nst.experimental_user returns information about the logged-in user of private apps on Streamlit Community Cloud.\\n\\npython\\nif st.experimental_user.email == \"foo@corp.com\":\\n  st.write(\"Welcome back, \", st.experimental_user.email)\\nelse:\\n  st.write(\"You are not authorized to view this page.\")', metadata={'source': 'docs/content/library/api/api-reference.md'}),\n",
       " Document(page_content='title: st.table\\nslug: /library/api-reference/data/st.table\\ndescription: st.table displays a static table.\\n\\nStatic tables with st.table are the most basic way to display dataframes. For the majority of cases, we recommend using st.dataframe to display interactive dataframes, and st.data_editor to let users edit dataframes.', metadata={'source': 'docs/content/library/api/data/table.md'}),\n",
       " Document(page_content='title: st.json\\nslug: /library/api-reference/data/st.json\\ndescription: st.json displays object or string as a pretty-printed JSON string.', metadata={'source': 'docs/content/library/api/data/json.md'}),\n",
       " Document(page_content='title: Data elements\\nslug: /library/api-reference/data\\n\\nData elements\\n\\nWhen you\\'re working with data, it is extremely valuable to visualize that\\ndata quickly, interactively, and from multiple different angles. That\\'s what\\nStreamlit is actually built and optimized for.\\n\\nYou can display data via charts, and you can display it in\\nraw form. These are the Streamlit commands you can use to display and interact with raw data.\\n\\nDataframes\\n\\nDisplay a dataframe as an interactive table.\\n\\npython\\nst.dataframe(my_data_frame)\\n\\nData editor\\n\\nDisplay a data editor widget.\\n\\npython\\nedited = st.data_editor(df, num_rows=\"dynamic\")\\n\\nColumn configuration\\n\\nConfigure the display and editing behavior of dataframes and data editors.\\n\\npython\\nst.column_config.NumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nStatic tables\\n\\nDisplay a static table.\\n\\npython\\nst.table(my_data_frame)\\n\\nMetrics\\n\\nDisplay a metric in big bold font, with an optional indicator of how the metric changed.\\n\\npython\\nst.metric(\"My metric\", 42, 2)\\n\\nDicts and JSON\\n\\nDisplay object or string as a pretty-printed JSON string.\\n\\npython\\nst.json(my_dict)\\n\\nStreamlit Aggrid\\n\\nImplementation of Ag-Grid component for Streamlit. Created by @PablocFonseca.\\n\\n```python\\ndf = pd.DataFrame({\\'col1\\': [1, 2, 3], \\'col2\\': [4, 5, 6]})\\ngrid_return = AgGrid(df, editable=True)\\n\\nnew_df = grid_return[\\'data\\']\\n```\\n\\nStreamlit Folium\\n\\nStreamlit Component for rendering Folium maps. Created by @randyzwitch.\\n\\n```python\\nm = folium.Map(location=[39.949610, -75.150282], zoom_start=16)\\nfolium.Marker([39.949610, -75.150282], popup=\"Liberty Bell\", tooltip=\"Liberty Bell\").add_to(m)\\n\\nst_data = st_folium(m, width=725)\\n```\\n\\nPandas Profiling\\n\\nPandas profiling component for Streamlit. Created by @okld.\\n\\n```python\\ndf = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\\npr = df.profile_report()\\n\\nst_profile_report(pr)\\n```\\n\\nImage Coordinates\\n\\nGet the coordinates of clicks on an image. Created by @blackary.', metadata={'source': 'docs/content/library/api/data/data.md'}),\n",
       " Document(page_content='```python\\nfrom streamlit_image_coordinates import streamlit_image_coordinates\\nvalue = streamlit_image_coordinates(\"https://placekitten.com/200/300\")\\n\\nst.write(value)\\n```\\n\\nPlotly Events\\n\\nMake Plotly charts interactive!. Created by @null-jones.\\n\\n```python\\nfrom streamlit_plotly_events import plotly_events\\nfig = px.line(x=[1], y=[1])\\n\\nselected_points = plotly_events(fig)\\n```\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\n```python\\nfrom streamlit_extras.metric_cards import style_metric_cards\\ncol3.metric(label=\"No Change\", value=5000, delta=0)\\n\\nstyle_metric_cards()\\n```', metadata={'source': 'docs/content/library/api/data/data.md'}),\n",
       " Document(page_content='title: st.column_config.LineChartColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.linechartcolumn', metadata={'source': 'docs/content/library/api/data/column_config/linechartcolumn.md'}),\n",
       " Document(page_content='title: st.dataframe\\nslug: /library/api-reference/data/st.dataframe\\ndescription: st.dataframe displays a dataframe as an interactive table.\\n\\nThis page only contains information on the st.dataframe API. For a deeper dive into working with dataframes read Dataframes. If you want to let users interactively edit dataframes, check out st.data_editor.\\n\\nst.dataframe supports the use_container_width parameter to stretch across the full container width:\\n\\n```python\\nimport pandas as pd\\nimport streamlit as st\\n\\nCache the dataframe so it\\'s only loaded once\\n\\n@st.cache_data\\ndef load_data():\\n    return pd.DataFrame(\\n        {\\n            \"first column\": [1, 2, 3, 4],\\n            \"second column\": [10, 20, 30, 40],\\n        }\\n    )\\n\\nBoolean to resize the dataframe, stored as a session state variable\\n\\nst.checkbox(\"Use container width\", value=False, key=\"use_container_width\")\\n\\ndf = load_data()\\n\\nDisplay the dataframe and allow the user to stretch the dataframe\\n\\nacross the full width of the container, based on the checkbox value\\n\\nst.dataframe(df, use_container_width=st.session_state.use_container_width)\\n```\\n\\nColumn configuration\\n\\nWhen working with data in Streamlit, the st.column_config class is a powerful tool for configuring data display and interaction. Specifically designed for the column_config parameter in st.dataframe and st.data_editor, it provides a suite of methods to tailor your columns to various data types - from simple text and numbers to lists, URLs, images, and more.\\n\\nWhether it\\'s translating temporal data into user-friendly formats or utilizing charts and progress bars for clearer data visualization, column configuration not only provides the user with an enriched data viewing experience but also ensures that you\\'re equipped with the tools to present and interact with your data, just the way you want it.\\n\\nColumn\\n\\nConfigure a generic column.\\n\\npython\\nColumn(\"Streamlit Widgets\", width=\"medium\", help=\"Streamlit **widget** commands 🎈\")\\n\\nText column\\n\\nConfigure a text column.', metadata={'source': 'docs/content/library/api/data/dataframe.md'}),\n",
       " Document(page_content='Text column\\n\\nConfigure a text column.\\n\\npython\\nTextColumn(\"Widgets\", max_chars=50, validate=\"^st\\\\.[a-z_]+$\")\\n\\nNumber column\\n\\nConfigure a number column.\\n\\npython\\nNumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nCheckbox column\\n\\nConfigure a checkbox column.\\n\\npython\\nCheckboxColumn(\"Your favorite?\", help=\"Select your **favorite** widgets\")\\n\\nSelectbox column\\n\\nConfigure a selectbox column.\\n\\npython\\nSelectboxColumn(\"App Category\", options=[\"🤖 LLM\", \"📈 Data Viz\"])\\n\\nDatetime column\\n\\nConfigure a datetime column.\\n\\npython\\nDatetimeColumn(\"Appointment\", min_value=datetime(2023, 6, 1), format=\"D MMM YYYY, h:mm a\")\\n\\nDate column\\n\\nConfigure a date column.\\n\\npython\\nDateColumn(\"Birthday\", max_value=date(2005, 1, 1), format=\"DD.MM.YYYY\")\\n\\nTime column\\n\\nConfigure a time column.\\n\\npython\\nTimeColumn(\"Appointment\", min_value=time(8, 0, 0), format=\"hh:mm a\")\\n\\nList column\\n\\nConfigure a list column.\\n\\npython\\nListColumn(\"Sales (last 6 months)\", width=\"medium\")\\n\\nLink column\\n\\nConfigure a link column.\\n\\npython\\nLinkColumn(\"Trending apps\", max_chars=100, validate=\"^https://.*$\")\\n\\nImage column\\n\\nConfigure an image column.\\n\\npython\\nImageColumn(\"Preview Image\", help=\"The preview screenshots\")\\n\\nLine chart column\\n\\nConfigure a line chart column.\\n\\npython\\nLineChartColumn(\"Sales (last 6 months)\" y_min=0, y_max=100)\\n\\nBar chart column\\n\\nConfigure a bar chart column.\\n\\npython\\nBarChartColumn(\"Marketing spend\" y_min=0, y_max=100)\\n\\nProgress column\\n\\nConfigure a progress column.\\n\\npython\\nProgressColumn(\"Sales volume\", min_value=0, max_value=1000, format=\"$%f\")\\n\\nInteractivity\\n\\nDataframes displayed as interactive tables with st.dataframe have the following interactive features:\\n\\nColumn sorting: sort columns by clicking on their headers.\\n\\nColumn resizing: resize columns by dragging and dropping column header borders.\\n\\nTable (height, width) resizing: resize tables by dragging and dropping the bottom right corner of tables.', metadata={'source': 'docs/content/library/api/data/dataframe.md'}),\n",
       " Document(page_content='Search: search through data by clicking a table, using hotkeys (⌘ Cmd + F or Ctrl + F) to bring up the search bar, and using the search bar to filter data.\\n\\nCopy to clipboard: select one or multiple cells, copy them to clipboard, and paste them into your favorite spreadsheet software.', metadata={'source': 'docs/content/library/api/data/dataframe.md'}),\n",
       " Document(page_content='title: st.column_config.DateColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.datecolumn', metadata={'source': 'docs/content/library/api/data/column_config/datecolumn.md'}),\n",
       " Document(page_content='title: st.data_editor\\nslug: /library/api-reference/data/st.data_editor\\ndescription: st.data_editor display a data editor widget that allows you to edit dataframes and many other data structures in a table-like UI.\\n\\nThis page only contains information on the st.data_editor API. For a deeper dive into working with dataframes and the data editor\\'s capabilities and limitations, read Dataframes.\\n\\nColumn configuration\\n\\nWhen working with data in Streamlit, the st.column_config class is a powerful tool for configuring data display and interaction. Specifically designed for the column_config parameter in st.dataframe and st.data_editor, it provides a suite of methods to tailor your columns to various data types - from simple text and numbers to lists, URLs, images, and more.\\n\\nWhether it\\'s translating temporal data into user-friendly formats or utilizing charts and progress bars for clearer data visualization, column configuration not only provides the user with an enriched data viewing experience but also ensures that you\\'re equipped with the tools to present and interact with your data, just the way you want it.\\n\\nColumn\\n\\nConfigure a generic column.\\n\\npython\\nColumn(\"Streamlit Widgets\", width=\"medium\", help=\"Streamlit **widget** commands 🎈\")\\n\\nText column\\n\\nConfigure a text column.\\n\\npython\\nTextColumn(\"Widgets\", max_chars=50, validate=\"^st\\\\.[a-z_]+$\")\\n\\nNumber column\\n\\nConfigure a number column.\\n\\npython\\nNumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nCheckbox column\\n\\nConfigure a checkbox column.\\n\\npython\\nCheckboxColumn(\"Your favorite?\", help=\"Select your **favorite** widgets\")\\n\\nSelectbox column\\n\\nConfigure a selectbox column.\\n\\npython\\nSelectboxColumn(\"App Category\", options=[\"🤖 LLM\", \"📈 Data Viz\"])\\n\\nDatetime column\\n\\nConfigure a datetime column.\\n\\npython\\nDatetimeColumn(\"Appointment\", min_value=datetime(2023, 6, 1), format=\"D MMM YYYY, h:mm a\")\\n\\nDate column\\n\\nConfigure a date column.\\n\\npython\\nDateColumn(\"Birthday\", max_value=date(2005, 1, 1), format=\"DD.MM.YYYY\")\\n\\nTime column', metadata={'source': 'docs/content/library/api/data/data_editor.md'}),\n",
       " Document(page_content='Time column\\n\\nConfigure a time column.\\n\\npython\\nTimeColumn(\"Appointment\", min_value=time(8, 0, 0), format=\"hh:mm a\")\\n\\nList column\\n\\nConfigure a list column.\\n\\npython\\nListColumn(\"Sales (last 6 months)\", width=\"medium\")\\n\\nLink column\\n\\nConfigure a link column.\\n\\npython\\nLinkColumn(\"Trending apps\", max_chars=100, validate=\"^https://.*$\")\\n\\nImage column\\n\\nConfigure an image column.\\n\\npython\\nImageColumn(\"Preview Image\", help=\"The preview screenshots\")\\n\\nLine chart column\\n\\nConfigure a line chart column.\\n\\npython\\nLineChartColumn(\"Sales (last 6 months)\" y_min=0, y_max=100)\\n\\nBar chart column\\n\\nConfigure a bar chart column.\\n\\npython\\nBarChartColumn(\"Marketing spend\" y_min=0, y_max=100)\\n\\nProgress column\\n\\nConfigure a progress column.\\n\\npython\\nProgressColumn(\"Sales volume\", min_value=0, max_value=1000, format=\"$%f\")', metadata={'source': 'docs/content/library/api/data/data_editor.md'}),\n",
       " Document(page_content='title: st.column_config.BarChartColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.barchartcolumn', metadata={'source': 'docs/content/library/api/data/column_config/barchartcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.TimeColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.timecolumn', metadata={'source': 'docs/content/library/api/data/column_config/timecolumn.md'}),\n",
       " Document(page_content='title: st.column_config.NumberColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.numbercolumn', metadata={'source': 'docs/content/library/api/data/column_config/numbercolumn.md'}),\n",
       " Document(page_content='title: st.column_config.DatetimeColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.datetimecolumn', metadata={'source': 'docs/content/library/api/data/column_config/datetimecolumn.md'}),\n",
       " Document(page_content='title: st.column_config.CheckboxColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.checkboxcolumn', metadata={'source': 'docs/content/library/api/data/column_config/checkboxcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.TextColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.textcolumn', metadata={'source': 'docs/content/library/api/data/column_config/textcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.ImageColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.imagecolumn', metadata={'source': 'docs/content/library/api/data/column_config/imagecolumn.md'}),\n",
       " Document(page_content='title: st.column_config.SelectboxColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.selectboxcolumn', metadata={'source': 'docs/content/library/api/data/column_config/selectboxcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.Column\\nslug: /library/api-reference/data/st.column_config/st.column_config.column', metadata={'source': 'docs/content/library/api/data/column_config/column.md'}),\n",
       " Document(page_content='title: st.column_config.ListColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.listcolumn', metadata={'source': 'docs/content/library/api/data/column_config/listcolumn.md'}),\n",
       " Document(page_content='title: st.column_config.ProgressColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.progresscolumn', metadata={'source': 'docs/content/library/api/data/column_config/progresscolumn.md'}),\n",
       " Document(page_content='title: st.column_config.LinkColumn\\nslug: /library/api-reference/data/st.column_config/st.column_config.linkcolumn', metadata={'source': 'docs/content/library/api/data/column_config/linkcolumn.md'}),\n",
       " Document(page_content='title: st.markdown\\nslug: /library/api-reference/text/st.markdown\\ndescription: st.markdown displays string formatted as Markdown.', metadata={'source': 'docs/content/library/api/text/markdown.md'}),\n",
       " Document(page_content='title: Personalize apps for the user\\nslug: /library/api-reference/personalization\\n\\nPersonalize apps for the user\\n\\nUser info\\n\\nst.experimental_user returns information about the logged-in user of private apps on Streamlit Community Cloud.\\n\\npython\\nif st.experimental_user.email == \"foo@corp.com\":\\n  st.write(\"Welcome back, \", st.experimental_user.email)\\nelse:\\n  st.write(\"You are not authorized to view this page.\")', metadata={'source': 'docs/content/library/api/personalization/personalization.md'}),\n",
       " Document(page_content='title: st.caption\\nslug: /library/api-reference/text/st.caption\\ndescription: st.caption displays text in small font.', metadata={'source': 'docs/content/library/api/text/caption.md'}),\n",
       " Document(page_content='title: st.code\\nslug: /library/api-reference/text/st.code\\ndescription: st.code displays a code block with optional syntax highlighting.', metadata={'source': 'docs/content/library/api/text/code.md'}),\n",
       " Document(page_content='title: st.column_config\\nslug: /library/api-reference/data/st.column_config\\n\\nColumn configuration\\n\\nWhen working with data in Streamlit, the st.column_config class is a powerful tool for configuring data display and interaction. Specifically designed for the column_config parameter in st.dataframe and st.data_editor, it provides a suite of methods to tailor your columns to various data types - from simple text and numbers to lists, URLs, images, and more.\\n\\nWhether it\\'s translating temporal data into user-friendly formats or utilizing charts and progress bars for clearer data visualization, column configuration not only provides the user with an enriched data viewing experience but also ensures that you\\'re equipped with the tools to present and interact with your data, just the way you want it.\\n\\nColumn\\n\\nConfigure a generic column.\\n\\npython\\nColumn(\"Streamlit Widgets\", width=\"medium\", help=\"Streamlit **widget** commands 🎈\")\\n\\nText column\\n\\nConfigure a text column.\\n\\npython\\nTextColumn(\"Widgets\", max_chars=50, validate=\"^st\\\\.[a-z_]+$\")\\n\\nNumber column\\n\\nConfigure a number column.\\n\\npython\\nNumberColumn(\"Price (in USD)\", min_value=0, format=\"$%d\")\\n\\nCheckbox column\\n\\nConfigure a checkbox column.\\n\\npython\\nCheckboxColumn(\"Your favorite?\", help=\"Select your **favorite** widgets\")\\n\\nSelectbox column\\n\\nConfigure a selectbox column.\\n\\npython\\nSelectboxColumn(\"App Category\", options=[\"🤖 LLM\", \"📈 Data Viz\"])\\n\\nDatetime column\\n\\nConfigure a datetime column.\\n\\npython\\nDatetimeColumn(\"Appointment\", min_value=datetime(2023, 6, 1), format=\"D MMM YYYY, h:mm a\")\\n\\nDate column\\n\\nConfigure a date column.\\n\\npython\\nDateColumn(\"Birthday\", max_value=date(2005, 1, 1), format=\"DD.MM.YYYY\")\\n\\nTime column\\n\\nConfigure a time column.\\n\\npython\\nTimeColumn(\"Appointment\", min_value=time(8, 0, 0), format=\"hh:mm a\")\\n\\nList column\\n\\nConfigure a list column.\\n\\npython\\nListColumn(\"Sales (last 6 months)\", width=\"medium\")\\n\\nLink column\\n\\nConfigure a link column.\\n\\npython\\nLinkColumn(\"Trending apps\", max_chars=100, validate=\"^https://.*$\")', metadata={'source': 'docs/content/library/api/data/column_config/index.md'}),\n",
       " Document(page_content='Image column\\n\\nConfigure an image column.\\n\\npython\\nImageColumn(\"Preview Image\", help=\"The preview screenshots\")\\n\\nLine chart column\\n\\nConfigure a line chart column.\\n\\npython\\nLineChartColumn(\"Sales (last 6 months)\" y_min=0, y_max=100)\\n\\nBar chart column\\n\\nConfigure a bar chart column.\\n\\npython\\nBarChartColumn(\"Marketing spend\" y_min=0, y_max=100)\\n\\nProgress column\\n\\nConfigure a progress column.\\n\\npython\\nProgressColumn(\"Sales volume\", min_value=0, max_value=1000, format=\"$%f\")', metadata={'source': 'docs/content/library/api/data/column_config/index.md'}),\n",
       " Document(page_content='title: st.subheader\\nslug: /library/api-reference/text/st.subheader\\ndescription: st.subheader displays text in subheader formatting.', metadata={'source': 'docs/content/library/api/text/subheader.md'}),\n",
       " Document(page_content='title: st.header\\nslug: /library/api-reference/text/st.header\\ndescription: st.header displays text in header formatting.', metadata={'source': 'docs/content/library/api/text/header.md'}),\n",
       " Document(page_content='title: st.title\\nslug: /library/api-reference/text/st.title\\ndescription: st.title displays text in title formatting.', metadata={'source': 'docs/content/library/api/text/title.md'}),\n",
       " Document(page_content='title: st.latex\\nslug: /library/api-reference/text/st.latex\\ndescription: st.latex displays mathematical expressions formatted as LaTeX.', metadata={'source': 'docs/content/library/api/text/latex.md'}),\n",
       " Document(page_content='title: st.text\\nslug: /library/api-reference/text/st.text\\ndescription: st.text writes fixed-width and preformatted text.', metadata={'source': 'docs/content/library/api/text/text.md'}),\n",
       " Document(page_content='title: st.divider\\nslug: /library/api-reference/text/st.divider\\ndescription: st.divider displays a horizontal rule in your app.\\n\\nHere\\'s what it looks like in action when you have multiple elements in the app:\\n\\n```python\\nimport streamlit as st\\n\\nst.write(\"This is some text.\")\\n\\nst.slider(\"This is a slider\", 0, 100, (25, 75))\\n\\nst.divider()  # 👈 Draws a horizontal rule\\n\\nst.write(\"This text is between the horizontal rules.\")\\n\\nst.divider()  # 👈 Another horizontal rule\\n```', metadata={'source': 'docs/content/library/api/text/divider.md'}),\n",
       " Document(page_content='title: Text elements\\nslug: /library/api-reference/text\\n\\nText elements\\n\\nStreamlit apps usually start with a call to st.title to set the\\napp\\'s title. After that, there are 2 heading levels you can use:\\nst.header and st.subheader.\\n\\nPure text is entered with st.text, and Markdown with\\nst.markdown.\\n\\nWe also offer a \"swiss-army knife\" command called st.write, which accepts\\nmultiple arguments, and multiple data types. And as described above, you can\\nalso use magic commands in place of st.write.\\n\\nMarkdown\\n\\nDisplay string formatted as Markdown.\\n\\npython\\nst.markdown(\"Hello **world**!\")\\n\\nTitle\\n\\nDisplay text in title formatting.\\n\\npython\\nst.title(\"The app title\")\\n\\nHeader\\n\\nDisplay text in header formatting.\\n\\npython\\nst.header(\"This is a header\")\\n\\nSubheader\\n\\nDisplay text in subheader formatting.\\n\\npython\\nst.subheader(\"This is a subheader\")\\n\\nCaption\\n\\nDisplay text in small font.\\n\\npython\\nst.caption(\"This is written small caption text\")\\n\\nCode block\\n\\nDisplay a code block with optional syntax highlighting.\\n\\npython\\nst.code(\"a = 1234\")\\n\\nPreformatted text\\n\\nWrite fixed-width and preformatted text.\\n\\npython\\nst.text(\"Hello world\")\\n\\nLaTeX\\n\\nDisplay mathematical expressions formatted as LaTeX.\\n\\npython\\nst.latex(\"\\\\int a x^2 \\\\,dx\")\\n\\nDivider\\n\\nDisplay a horizontal rule.\\n\\npython\\nst.divider()\\n\\nAnnotated text\\n\\nDisplay annotated text in Streamlit apps. Created by @tvst.\\n\\npython\\nannotated_text(\"This \", (\"is\", \"verb\"), \" some \", (\"annotated\", \"adj\"), (\"text\", \"noun\"), \" for those of \", (\"you\", \"pronoun\"), \" who \", (\"like\", \"verb\"), \" this sort of \", (\"thing\", \"noun\"), \".\")\\n\\nDrawable Canvas\\n\\nProvides a sketching canvas using Fabric.js. Created by @andfanilo.', metadata={'source': 'docs/content/library/api/text/text-elements.md'}),\n",
       " Document(page_content='python\\nst_canvas(fill_color=\"rgba(255, 165, 0, 0.3)\", stroke_width=stroke_width, stroke_color=stroke_color, background_color=bg_color, background_image=Image.open(bg_image) if bg_image else None, update_streamlit=realtime_update, height=150, drawing_mode=drawing_mode, point_display_radius=point_display_radius if drawing_mode == \\'point\\' else 0, key=\"canvas\",)\\n\\nTags\\n\\nAdd tags to your Streamlit apps. Created by @gagan3012.\\n\\npython\\nst_tags(label=\\'# Enter Keywords:\\', text=\\'Press enter to add more\\', value=[\\'Zero\\', \\'One\\', \\'Two\\'], suggestions=[\\'five\\', \\'six\\', \\'seven\\', \\'eight\\', \\'nine\\', \\'three\\', \\'eleven\\', \\'ten\\', \\'four\\'], maxtags = 4, key=\\'1\\')\\n\\nNLU\\n\\nApply text mining on a dataframe. Created by @JohnSnowLabs.\\n\\npython\\nnlu.load(\\'sentiment\\').predict(\\'I love NLU! <3\\')\\n\\nStreamlit Extras\\n\\nA library with useful Streamlit extras. Created by @arnaudmiribel.\\n\\npython\\nmention(label=\"An awesome Streamlit App\", icon=\"streamlit\",  url=\"https://extras.streamlit.app\",)', metadata={'source': 'docs/content/library/api/text/text-elements.md'}),\n",
       " Document(page_content='title: st.experimental_user\\nslug: /library/api-reference/personalization/st.experimental_user\\ndescription: st.experimental_user returns information about the logged-in user of private apps on Streamlit Community Cloud.\\n\\nst.experimental_user\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.\\n\\nst.experimental_user is a Streamlit command that returns information about the logged-in user on Streamlit Community Cloud. It allows developers to personalize apps for the user viewing the app. In a private Streamlit Community Cloud app, it returns a dictionary with the viewer\\'s email. This value of this field is empty in a public Streamlit Community Cloud app to prevent leaking user emails to developers.\\n\\nThe API closely resembles that of st.session_state and st.secrets. It follows a field-based API, which is very similar to Python dictionaries.\\n\\nAllowed fields\\n\\nThe st.experimental_user command returns a dictionary with only one field: email.\\n\\nDisplay the allowed field by passing the command to st.write:\\n\\n```python\\n\\nDisplay the contents of the dictionary\\n\\nst.write(st.experimental_user)\\n```\\n\\nThe above displays a dict with one field and value. The field is always email:\\n\\njson\\n{\\n  \"email\": \"value\"\\n}\\n\\nYou can check if a field exists in st.experimental_user:\\n\\n```python\\n\\nReturns True if the field exists\\n\\n\"email\" in st.experimental_user\\n\\nReturns False if the field does not exist\\n\\n\"name\" in st.experimental_user\\n```\\n\\nRead values\\n\\nRead the value of the email field and display it by passing to st.write:\\n\\n```python\\n\\nDictionary like API\\n\\nst.write(st.experimental_user[\\'email\\'])\\n\\nAttribute API\\n\\nst.write(st.experimental_user.email)\\n```\\n\\nThe above outputs either None or the logged-in user\\'s email or test@localhost.com, depending on where the app is running. Read further to learn about st.experimental_user\\'s context-dependent behavior.\\n\\nUpdates and modifications', metadata={'source': 'docs/content/library/api/personalization/experimental-user.md'}),\n",
       " Document(page_content='Updates and modifications\\n\\nKeys and values for st.experimental_user cannot be updated or modified. Streamlit throws a handy StreamlitAPIException exception if you try to update them:\\n\\n```python\\nst.experimental_user.name = None\\n\\nThrows an exception!\\n\\nst.experimental_user.email = \"hello\"\\n\\nThrows an exception!\\n\\n```\\n\\nContext-dependent behavior\\n\\nThe value of st.experimental_user.email is context-dependent. It returns a value depending on where the app is running. The private or public app can be running on Streamlit Community Cloud, locally, or on a 3rd party cloud provider. Let\\'s look at the different scenarios.\\n\\nPrivate app on Streamlit Community Cloud\\n\\nUsers need to be logged in to Streamlit Community Cloud to view private apps. If a user is not logged, they see:\\n\\nIf a user is logged in, st.experimental_user.email returns the user\\'s email. Suppose a user logged in to Streamlit Community Cloud using jane@email.com:\\n\\n```python\\nst.experimental_user.email\\n\\nReturns: jane@email.com\\n\\n```\\n\\nPublic app on Streamlit Community Cloud\\n\\nCurrently, st.experimental_user.email returns information about the logged-in user of private apps on Streamlit Community Cloud. If used in a public app, it returns None. For example:\\n\\n```python\\nst.experimental_user.email\\n\\nReturns: None\\n\\n```\\n\\nThis value of this field is empty in a public Streamlit Community Cloud app to prevent leaking user emails to developers.\\n\\nLocal development\\n\\nWhen developing locally, st.experimental_user.email returns test@localhost.com. We don\\'t return None to make it easier to locally test this functionality. For example:\\n\\n```python\\nst.experimental_user.email\\n\\nReturns: test@localhost.com\\n\\n```\\n\\nApp deployed on a 3rd party cloud provider\\n\\nWhen deploying an app on a 3rd party cloud provider (e.g. Amazon EC2, Heroku, etc), st.experimental_user.email behaves the same as during local development. For example:\\n\\n```python\\nst.experimental_user.email # On a 3rd party cloud provider\\n\\nReturns: test@localhost.com\\n\\n```\\n\\nExamples', metadata={'source': 'docs/content/library/api/personalization/experimental-user.md'}),\n",
       " Document(page_content='Returns: test@localhost.com\\n\\n```\\n\\nExamples\\n\\nThe ability to personalize apps for the user viewing the app is a great way to make your app more engaging.\\n\\nIt unlocks a plethora of use-cases for developers, some of which could include: showing additional controls for admins, visualizing a user\\'s Streamlit history, a personalized stock ticker, a chatbot app, and much more. We\\'re excited to see what you build with this feature!\\n\\nHere\\'s a code snippet that shows extra buttons for admins:\\n\\n```python\\n\\nShow extra buttons for admin users.\\n\\nADMIN_USERS = {\\n    \\'person1@email.com\\',\\n    \\'person2@email.com\\',\\n    \\'person3@email.com\\'\\n}\\nif st.experimental_user.email in ADMIN_USERS:\\n    display_the_extra_admin_buttons()\\ndisplay_the_interface_everyone_sees()\\n```\\n\\nShow different content to users based on their email address:\\n\\n```python\\n\\nShow different content based on the user\\'s email address.\\n\\nif st.experimental_user.email == \\'jane@email.com\\':\\n    display_jane_content()\\nelif st.experimental_user.email == \\'adam@foocorp.io\\':\\n    display_adam_content()\\nelse:\\n    st.write(\"Please contact us to get access!\")\\n```\\n\\nGreet users with their name that\\'s stored in a database:\\n\\n```python\\n\\nGreet the user by their name.\\n\\nif st.experimental_user.email:\\n    # Get the user\\'s name from the database.\\n    name = get_name_from_db(st.experimental_user.email)\\n    st.write(\\'Hello, %s!\\' % name)\\n```\\n\\nCaveats and limitations\\n\\nst.experimental_user is read-only. You cannot update or modify its value. Doing so will throw a StreamlitAPIException.\\n\\nA valid email is returned only if the user is logged in to Streamlit Community Cloud and the app is private. Else, None or test@localhost.com is returned.\\n\\nThis is an experimental feature. Experimental features and their APIs may change or be removed at any time. To learn more, click here.', metadata={'source': 'docs/content/library/api/personalization/experimental-user.md'}),\n",
       " Document(page_content=\"title: Get started\\nslug: /library/get-started\\n\\nGet started\\n\\nThis Get Started guide explains how Streamlit works, how to install Streamlit on your preferred\\noperating system, and how to create your first Streamlit app!\\n\\nhelps you set up your virtual environment and walks you through installing Streamlit on Windows, macOS, and Linux. Regardless of which package management tool and OS you're using, we recommend running the commands on this page in a virtual environment.\\n  \\n  \\n    introduces you to Streamlit's data model and development flow. You'll learn what makes Streamlit the most powerful way to build data apps, including the ability to display and style data, draw charts and maps, add interactive widgets, customize app layouts, cache computation, and define themes.\\n  \\n  \\n    using Streamlit's core features to fetch and cache data, draw charts, plot information on a map, and use interactive widgets, like a slider, to filter results.\\n  \\n  \\n    teaches you how to add pages to your app, including how to define pages, structure and run multipage apps, and navigate between pages. Once you understand the basics, create your first multipage app based on the familiar streamlit hello command!\\n  \\n  {/\\n    to Streamlit Community Cloud. With Streamlit Community Cloud your data team can directly serve the needs of the rest of the company. Quickly go from data to app, from prototype to production. Share apps in one click and collaborate instantly with live code updates.\\n/}\", metadata={'source': 'docs/content/library/get-started/index.md'}),\n",
       " Document(page_content='title: Multipage apps\\nslug: /library/get-started/multipage-apps\\n\\nMultipage apps\\n\\nAs apps grow large, it becomes useful to organize them into multiple pages. This makes the app easier to manage as a developer and easier to navigate as a user. Streamlit provides a frictionless way to create multipage apps. Pages are automatically shown in a nice navigation widget inside the app sidebar, and clicking on a page will navigate to the page without reloading the frontend — making app browsing incredibly fast!\\n\\nWe created a \"single-page app\" to explore a public Uber dataset for pickups and drop-offs in New York City on the previous page. In this guide, let’s learn how to create multipage apps. Once we have a solid foundation on what it takes to create multipage apps, let’s build one for ourselves in the next section!\\n\\nStructuring multipage apps\\n\\nLet\\'s understand what it takes to create multipage apps — including how to define pages, structure and run multipage apps, and navigate between pages in the user interface. Once you\\'ve understood the basics, you can jump right into the next section to convert the familiar streamlit hello command into a multipage app!\\n\\nRun a multipage app\\n\\nRunning a multipage app is identical to running a single-page app. The command to run a multipage app is:\\n\\npython\\nstreamlit run [entrypoint file]\\n\\nThe \"entrypoint file\" is the first page the app will show to the user. Once you have added pages to your app, the entrypoint file appears as the top-most page in the sidebar. You can think of the entrypoint file as your app\\'s \"main page\". For example, say your entrypoint file is Home.py. Then, to run your app, you can run streamlit run Home.py. This will start your app and execute the code in Home.py.\\n\\nAdding pages', metadata={'source': 'docs/content/library/get-started/multipage-apps/index.md'}),\n",
       " Document(page_content='Adding pages\\n\\nOnce you\\'ve created your entrypoint file, you can add pages by creating .py files in a pages/ directory relative to your entrypoint file. For example, if your entrypoint file is Home.py, then you can create a pages/About.py file to define the \"About\" page. Here\\'s a valid directory structure for a multipage app:\\n\\nHome.py # This is the file you run with \"streamlit run\"\\n└─── pages/\\n  └─── About.py # This is a page\\n  └─── 2_Page_two.py # This is another page\\n  └─── 3_😎_three.py # So is this\\n\\nWhen adding emojis to filenames, it’s best practice to include a numbered-prefix to make autocompletion in your terminal easier. Terminal-autocomplete can get confused by unicode (which is how emojis are represented).\\n\\nsection below. For example, the\\n\\nOnly .py files in the pages/ directory will be loaded as pages. Streamlit ignores all other files in the pages/ directory and subdirectories.\\n\\nHow pages are labeled and sorted in the UI\\n\\nPage labels in the sidebar UI are generated from filenames. They may differ from the page title set in st.set_page_config. Let\\'s learn what constitutes a valid filename for a page, how pages are displayed in the sidebar, and how pages are sorted.\\n\\nValid filenames for pages\\n\\nFilenames are composed of four different parts:\\n\\nA number — if the file is prefixed with a number.\\n\\nA separator — could be _, -, space, or any combination thereof.\\n\\nA label — which is everything up to, but not including, .py.\\n\\nThe extension — which is always .py.\\n\\nHow pages are displayed in the sidebar\\n\\nWhat is displayed in the sidebar is the label part of the filename:\\n\\nIf there\\'s no label, Streamlit uses the number as the label.\\n\\nIn the UI, Streamlit beautifies the label by replacing _ with space.\\n\\nHow pages are sorted in the sidebar\\n\\nSorting considers numbers in the filename to be actual numbers (integers):\\n\\nFiles that have a number appear before files without a number.\\n\\nFiles are sorted based on the number (if any), followed by the title (if any).', metadata={'source': 'docs/content/library/get-started/multipage-apps/index.md'}),\n",
       " Document(page_content='When files are sorted, Streamlit treats the number as an actual number rather than a string. So 03 is the same as 3.\\n\\nThis table shows examples of filenames and their corresponding labels, sorted by the order in which they appear in the sidebar.\\n\\nExamples:\\n\\nEmojis can be used to make your page names more fun! For example, a file named 🏠_Home.py will create a page titled \"🏠 Home\" in the sidebar.\\n\\nNavigating between pages\\n\\nPages are automatically shown in a nice navigation UI inside the app\\'s sidebar. When you click on a page in the sidebar UI, Streamlit navigates to that page without reloading the entire frontend — making app browsing incredibly fast!\\n\\nYou can also navigate between pages using URLs. Pages have their own URLs, defined by the file\\'s label. When multiple files have the same label, Streamlit picks the first one (based on the ordering described above). Users can view a specific page by visiting the page\\'s URL.\\n\\nIf a user tries to access a URL for a page that does not exist, they will see a modal like the one below, saying the user has requested a page that was not found in the app’s pages/ directory.\\n\\nNotes\\n\\nPages support magic commands.\\n\\nPages support run-on-save. Additionally, when you save a page, this causes a rerun for users currently viewing that exact page.\\n\\nAdding or deleting a page causes the UI to update immediately.\\n\\nUpdating pages in the sidebar does not rerun the script.\\n\\nst.set_page_config works at the page level. When you set a title or favicon using st.set_page_config, this applies to the current page only.\\n\\nPages share the same Python modules globally:\\n\\n```python\\n  # page1.py\\n  import foo\\n  foo.hello = 123\\n\\n# page2.py\\n  import foo\\n  st.write(foo.hello)  # If page1 already executed, this should write 123\\n  ```\\n\\nPages share the same st.session_state:\\n\\n```python\\n  # page1.py\\n  import streamlit as st\\n  if \"shared\" not in st.session_state:\\n     st.session_state[\"shared\"] = True', metadata={'source': 'docs/content/library/get-started/multipage-apps/index.md'}),\n",
       " Document(page_content='# page2.py\\n  import streamlit as st\\n  st.write(st.session_state[\"shared\"])\\n  # If page1 already executed, this should write True\\n  ```\\n\\nYou now have a solid understanding of multipage apps. You\\'ve learned how to structure apps, define pages, and navigate between pages in the user interface. It\\'s time to create your first multipage app! 🥳', metadata={'source': 'docs/content/library/get-started/multipage-apps/index.md'}),\n",
       " Document(page_content='title: Installation\\nslug: /library/get-started/installation\\n\\nInstall Streamlit\\n\\nTable of contents\\n\\nPrerequisites\\n\\nInstall Streamlit on Windows\\n\\nInstall Streamlit on macOS/Linux\\n\\nPrerequisites\\n\\nBefore you get started, you\\'re going to need a few things:\\n\\nYour favorite IDE or text editor\\n\\nPython 3.8 - Python 3.11\\n\\nPIP\\n\\nSet up your virtual environment\\n\\nRegardless of which package management tool you\\'re using, we recommend running\\nthe commands on this page in a virtual environment. This ensures that the dependencies\\npulled in for Streamlit don\\'t impact any other Python projects\\nyou\\'re working on.\\n\\nBelow are a few tools you can use for environment management:\\n\\nvenv\\n\\npipenv\\n\\npoetry\\n\\nvirtualenv\\n\\nconda\\n\\nInstall Streamlit on Windows\\n\\nStreamlit\\'s officially-supported environment manager on Windows is Anaconda Navigator.\\n\\nInstall Anaconda\\n\\nIf you don\\'t have Anaconda install yet, follow the steps provided on the Anaconda installation page.\\n\\nCreate a new environment with Streamlit\\n\\nNext you\\'ll need to set up your environment.\\n\\nFollow the steps provided by Anaconda to set up and manage your environment using the Anaconda Navigator.\\n\\nSelect the \"▶\" icon next to your new environment. Then select \"Open terminal\":\\n\\nIn the terminal that appears, type:\\n\\nbash\\n   pip install streamlit\\n\\nTest that the installation worked:\\n\\nbash\\n   streamlit hello\\n\\nStreamlit\\'s Hello app should appear in a new tab in your web browser!\\n\\nUse your new environment\\n\\nIn Anaconda Navigator, open a terminal in your environment (see step 2 above).\\n\\nIn the terminal that appears, use Streamlit as usual:\\n\\nbash\\n   streamlit run myfile.py\\n\\nInstall Streamlit on macOS/Linux\\n\\nStreamlit\\'s officially-supported package manager and environment manager for macOS and Linux are pip and venv, respectively. venv is a part of The Python Standard Library and comes bundled with your installation of Python. See instructions on how to install and use pip below.\\n\\nInstall pip', metadata={'source': 'docs/content/library/get-started/installation.md'}),\n",
       " Document(page_content=\"Install pip\\n\\nInstall pip. More details about installing pip can be found in pip's documentation.\\n\\nOn a macOS:\\n\\nbash\\npython -m ensurepip --upgrade\\n\\nOn Ubuntu with Python 3:\\n\\nbash\\nsudo apt-get install python3-pip\\n\\nFor other Linux distributions, see How to install PIP for Python.\\n\\nInstall Xcode command line tools on macOS\\n\\nOn macOS, you'll need to install Xcode command line tools. They are required to compile some of Streamlit's Python dependencies during installation. To install Xcode command line tools, run:\\n\\nbash\\nxcode-select --install\\n\\nCreate a new environment with Streamlit\\n\\nNavigate to your project folder:\\n\\nbash\\n   cd myproject\\n\\nCreate a new virtual environment in that folder and activate that environment:\\n\\nbash\\n   python -m venv .venv\\n\\nWhen you run the command above, a directory called .venv will appear in myproject/. This directory is where your virtual environment and its dependencies are installed.\\n\\nInstall Streamlit in your environment:\\n\\nbash\\n   pip install streamlit\\n\\nTest that the installation worked:\\n\\nbash\\n   streamlit hello\\n\\nStreamlit's Hello app should appear in a new tab in your web browser!\\n\\nUse your new environment\\n\\nAny time you want to use the new environment, you first need to go to your project folder (where the .venv directory lives) and run:\\n\\nbash\\n   source .venv/bin/activate\\n\\nNow you can use Python and Streamlit as usual:\\n\\nbash\\n   streamlit run myfile.py\\n\\nTo stop the Streamlit server, press ctrl-C.\\n\\nWhen you're done using this environment, type deactivate to return to your normal shell.\\n\\nNow that you've installed Streamlit, take a few minutes to read through Main concepts to understand Streamlit's data flow model.\", metadata={'source': 'docs/content/library/get-started/installation.md'}),\n",
       " Document(page_content='title: Create a multipage app\\nslug: /library/get-started/multipage-apps/create-a-multipage-app\\n\\nCreate a multipage app\\n\\nIn the last section, we learned what it takes to create multipage apps, including how to define pages, structure and run multipage apps, and navigate between pages in the user interface. If you need a refresher, now is a good time to take a look.\\n\\nIn this guide, let’s put our understanding of multipage apps to use by converting the familiar streamlit hello command to a multipage app!\\n\\nMotivation\\n\\nBefore Streamlit 1.10.0, the streamlit hello command was a large single-page app. As there was no support for multiple pages, we resorted to splitting the app\\'s content using st.selectbox in the sidebar to choose what content to run. The content is comprised of three demos for plotting, mapping, and dataframes.\\n\\nHere\\'s what the code and single-page app looked like:\\n\\nNotice how large the file is! Each app “page\" is written as a function, and the selectbox is used to pick which page to display. As our app grows, maintaining the code requires a lot of additional overhead. Moreover, we’re limited by the st.selectbox UI to choose which “page\" to run, we cannot customize individual page titles with st.set_page_config, and we’re unable to navigate between pages using URLs.\\n\\nConvert an existing app into a multipage app\\n\\nNow that we\\'ve identified the limitations of a single-page app, what can we do about it? Armed with our knowledge from the previous section, we can convert the existing app to be a multipage app, of course! At a high level, we need to perform the following steps:\\n\\nCreate a new pages folder in the same folder where the “entrypoint file\" (hello.py) lives\\n\\nRename our entrypoint file to Hello.py , so that the title in the sidebar is capitalized\\n\\nCreate three new files inside of pages:\\n\\npages/1_📈_Plotting_Demo.py\\n\\npages/2_🌍_Mapping_Demo.py\\n\\npages/3_📊_DataFrame_Demo.py', metadata={'source': 'docs/content/library/get-started/multipage-apps/create-a-multi-page-app.md'}),\n",
       " Document(page_content='pages/3_📊_DataFrame_Demo.py\\n\\nMove the contents of the plotting_demo, mapping_demo, and data_frame_demo functions into their corresponding new files from Step 3\\n\\nRun streamlit run Hello.py to view your newly converted multipage app!\\n\\nNow, let’s walk through each step of the process and view the corresponding changes in code.\\n\\nCreate the entrypoint file\\n\\nWe rename our entrypoint file to Hello.py , so that the title in the sidebar is capitalized and only the code for the intro page is included. Additionally, we’re able to customize the page title and favicon — as it appears in the browser tab with st.set_page_config. We can do so for each of our pages too!\\n\\nNotice how the sidebar does not contain page labels as we haven’t created any pages yet.\\n\\nCreate multiple pages\\n\\nA few things to remember here:\\n\\nWe can change the ordering of pages in our MPA by adding numbers to the beginning of each Python file. If we add a 1 to the front of our file name, Streamlit will put that file first in the list.\\n\\nThe name of each Streamlit app is determined by the file name, so to change the app name you need to change the file name!\\n\\nWe can add some fun to our app by adding emojis to our file names that will render in our Streamlit app.\\n\\nEach page will have its own URL, defined by the name of the file.\\n\\nCheck out how we do all this below! For each new page, we create a new file inside the pages folder, and add the appropriate demo code into it.\\n\\nWith our additional pages created, we can now put it all together in the final step below.\\n\\nRun the multipage app\\n\\nTo run your newly converted multipage app, run:\\n\\nbash\\nstreamlit run Hello.py\\n\\nThat’s it! The Hello.py script now corresponds to the main page of your app, and other scripts that Streamlit finds in the pages folder will also be present in the new page selector that appears in the sidebar.\\n\\nNext steps', metadata={'source': 'docs/content/library/get-started/multipage-apps/create-a-multi-page-app.md'}),\n",
       " Document(page_content=\"Next steps\\n\\nCongratulations! 🎉 If you've read this far, chances are you've learned to create both single-page and multipage apps. Where you go from here is entirely up to your creativity! We’re excited to see what you’ll build now that adding additional pages to your apps is easier than ever. Try adding more pages to the app we've just built as an exercise. Also, stop by the forum to show off your multipage apps with the Streamlit community! 🎈\\n\\nHere are a few resources to help you get started:\\n\\nDeploy your app for free on Streamlit's Community Cloud.\\n\\nPost a question or share your multipage app on our community forum.\\n\\nCheck out our documentation on multipage apps.\\n\\nRead through Advanced features for things like caching, theming, and adding statefulness to apps.\\n\\nBrowse our API reference for examples of every Streamlit command.\", metadata={'source': 'docs/content/library/get-started/multipage-apps/create-a-multi-page-app.md'}),\n",
       " Document(page_content=\"title: Create an app\\nslug: /library/get-started/create-an-app\\n\\nCreate an app\\n\\nIf you've made it this far, chances are you've\\ninstalled Streamlit and\\nrun through the basics in our Main concepts guide. If\\nnot, now is a good time to take a look.\\n\\nThe easiest way to learn how to use Streamlit is to try things out yourself. As you read through this guide, test each method. As long as your app is running, every time you add a new element to your script and save, Streamlit's UI will ask if you'd like to rerun the app and view the changes. This allows you to work in a fast interactive loop: you write some code, save it, review the output, write some more, and so on, until you're happy with the results. The goal is to use Streamlit to create an interactive app for your data or model and along the way to use Streamlit to review, debug, perfect, and share your code.\\n\\nIn this guide, you're going to use Streamlit's core features to\\ncreate an interactive app; exploring a public Uber dataset for pickups and\\ndrop-offs in New York City. When you're finished, you'll know how to fetch\\nand cache data, draw charts, plot information on a map, and use interactive\\nwidgets, like a slider, to filter results.\\n\\nIf you'd like to skip ahead and see everything at once, the complete script\\nis available below.\\n\\nCreate your first app\\n\\nStreamlit is more than just a way to make data apps, it’s also a community of creators that share their apps and ideas and help each other make their work better. Please come join us on the community forum. We love to hear your questions, ideas, and help you work through your bugs — stop by today!\\n\\nThe first step is to create a new Python script. Let's call it\\n   uber_pickups.py.\\n\\nOpen uber_pickups.py in your favorite IDE or text editor, then add these\\n   lines:\\n\\npython\\n   import streamlit as st\\n   import pandas as pd\\n   import numpy as np\\n\\nEvery good app has a title, so let's add one:\\n\\npython\\n   st.title('Uber pickups in NYC')\", metadata={'source': 'docs/content/library/get-started/create-an-app.md'}),\n",
       " Document(page_content=\"python\\n   st.title('Uber pickups in NYC')\\n\\nNow it's time to run Streamlit from the command line:\\n\\nbash\\n   streamlit run uber_pickups.py\\n\\nRunning a Streamlit app is no different than any other Python script. Whenever you need to view the app, you can use this command.\\n\\nDid you know you can also pass a URL to streamlit run? This is great when combined with GitHub Gists. For example:\\n\\nbash\\n   streamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n\\nAs usual, the app should automatically open in a new tab in your\\n   browser.\\n\\nFetch some data\\n\\nNow that you have an app, the next thing you'll need to do is fetch the Uber\\ndataset for pickups and drop-offs in New York City.\\n\\nLet's start by writing a function to load the data. Add this code to your\\n   script:\\n\\n```python\\n   DATE_COLUMN = 'date/time'\\n   DATA_URL = ('https://s3-us-west-2.amazonaws.com/'\\n            'streamlit-demo-data/uber-raw-data-sep14.csv.gz')\\n\\ndef load_data(nrows):\\n       data = pd.read_csv(DATA_URL, nrows=nrows)\\n       lowercase = lambda x: str(x).lower()\\n       data.rename(lowercase, axis='columns', inplace=True)\\n       data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])\\n       return data\\n   ```\\n\\nYou'll notice that load_data is a plain old function that downloads some\\n   data, puts it in a Pandas dataframe, and converts the date column from text\\n   to datetime. The function accepts a single parameter (nrows), which\\n   specifies the number of rows that you want to load into the dataframe.\\n\\nNow let's test the function and review the output. Below your function, add\\n   these lines:\\n\\npython\\n   # Create a text element and let the reader know the data is loading.\\n   data_load_state = st.text('Loading data...')\\n   # Load 10,000 rows of data into the dataframe.\\n   data = load_data(10000)\\n   # Notify the reader that the data was successfully loaded.\\n   data_load_state.text('Loading data...done!')\", metadata={'source': 'docs/content/library/get-started/create-an-app.md'}),\n",
       " Document(page_content='You\\'ll see a few buttons in the upper-right corner of your app asking if\\n   you\\'d like to rerun the app. Choose Always rerun, and you\\'ll see your\\n   changes automatically each time you save.\\n\\nOk, that\\'s underwhelming...\\n\\nIt turns out that it takes a long time to download data, and load 10,000 lines\\ninto a dataframe. Converting the date column into datetime isn’t a quick job\\neither. You don’t want to reload the data each time the app is updated –\\nluckily Streamlit allows you to cache the data.\\n\\nEffortless caching\\n\\nTry adding @st.cache_data before the load_data declaration:\\n\\npython\\n   @st.cache_data\\n   def load_data(nrows):\\n\\nThen save the script, and Streamlit will automatically rerun your app. Since\\n   this is the first time you’re running the script with @st.cache_data, you won\\'t\\n   see anything change. Let’s tweak your file a little bit more so that you can\\n   see the power of caching.\\n\\nReplace the line data_load_state.text(\\'Loading data...done!\\') with this:\\n\\npython\\n   data_load_state.text(\"Done! (using st.cache_data)\")\\n\\nNow save. See how the line you added appeared immediately? If you take a\\n   step back for a second, this is actually quite amazing. Something magical is\\n   happening behind the scenes, and it only takes one line of code to activate\\n   it.\\n\\nHow\\'s it work?\\n\\nLet\\'s take a few minutes to discuss how @st.cache_data actually works.\\n\\nWhen you mark a function with Streamlit’s cache annotation, it tells Streamlit\\nthat whenever the function is called that it should check two things:\\n\\nThe input parameters you used for the function call.\\n\\nThe code inside the function.', metadata={'source': 'docs/content/library/get-started/create-an-app.md'}),\n",
       " Document(page_content='The code inside the function.\\n\\nIf this is the first time Streamlit has seen both these items, with these exact\\nvalues, and in this exact combination, it runs the function and stores the\\nresult in a local cache. The next time the function is called, if the two\\nvalues haven\\'t changed, then Streamlit knows it can skip executing the function\\naltogether. Instead, it reads the output from the local cache and passes it on\\nto the caller -- like magic.\\n\\n\"But, wait a second,\" you’re saying to yourself, \"this sounds too good to be\\ntrue. What are the limitations of all this awesomesauce?\"\\n\\nWell, there are a few:\\n\\nStreamlit will only check for changes within the current working directory.\\n   If you upgrade a Python library, Streamlit\\'s cache will only notice this if\\n   that library is installed inside your working directory.\\n\\nIf your function is not deterministic (that is, its output depends on random\\n   numbers), or if it pulls data from an external time-varying source (for\\n   example, a live stock market ticker service) the cached value will be\\n   none-the-wiser.\\n\\nLastly, you should avoid mutating the output of a function cached with st.cache_data since cached\\n   values are stored by reference.\\n\\nWhile these limitations are important to keep in mind, they tend not to be an\\nissue a surprising amount of the time. Those times, this cache is really\\ntransformational.\\n\\nWhenever you have a long-running computation in your code, consider\\nrefactoring it so you can use @st.cache_data, if possible. Please read Caching for more details.\\n\\nNow that you know how caching with Streamlit works, let’s get back to the Uber\\npickup data.\\n\\nInspect the raw data\\n\\nIt\\'s always a good idea to take a look at the raw data you\\'re working with\\nbefore you start working with it. Let\\'s add a subheader and a printout of the\\nraw data to the app:\\n\\npython\\nst.subheader(\\'Raw data\\')\\nst.write(data)', metadata={'source': 'docs/content/library/get-started/create-an-app.md'}),\n",
       " Document(page_content=\"python\\nst.subheader('Raw data')\\nst.write(data)\\n\\nIn the Main concepts guide you learned that\\nst.write will render almost anything you pass\\nto it. In this case, you're passing in a dataframe and it's rendering as an\\ninteractive table.\\n\\nst.write tries to do the right thing based on\\nthe data type of the input. If it isn't doing what you expect you can use a\\nspecialized command like st.dataframe\\ninstead. For a full list, see API reference.\\n\\nDraw a histogram\\n\\nNow that you've had a chance to take a look at the dataset and observe what's\\navailable, let's take things a step further and draw a histogram to see what\\nUber's busiest hours are in New York City.\\n\\nTo start, let's add a subheader just below the raw data section:\\n\\npython\\n   st.subheader('Number of pickups by hour')\\n\\nUse NumPy to generate a histogram that breaks down pickup times binned by\\n   hour:\\n\\npython\\n   hist_values = np.histogram(\\n       data[DATE_COLUMN].dt.hour, bins=24, range=(0,24))[0]\\n\\nNow, let's use Streamlit's\\n   st.bar_chart() method to draw this\\n   histogram.\\n\\npython\\n   st.bar_chart(hist_values)\\n\\nSave your script. This histogram should show up in your app right away.\\n   After a quick review, it looks like the busiest time is 17:00 (5 P.M.).\\n\\nTo draw this diagram we used Streamlit's native bar_chart() method, but it's\\nimportant to know that Streamlit supports more complex charting libraries like\\nAltair, Bokeh, Plotly, Matplotlib and more. For a full list, see\\nsupported charting libraries.\\n\\nPlot data on a map\\n\\nUsing a histogram with Uber's dataset helped us determine what the busiest\\ntimes are for pickups, but what if we wanted to figure out where pickups were\\nconcentrated throughout the city. While you could use a bar chart to show this\\ndata, it wouldn't be easy to interpret unless you were intimately familiar with\\nlatitudinal and longitudinal coordinates in the city. To show pickup\\nconcentration, let's use Streamlit st.map()\\nfunction to overlay the data on a map of New York City.\", metadata={'source': 'docs/content/library/get-started/create-an-app.md'}),\n",
       " Document(page_content=\"Add a subheader for the section:\\n\\npython\\n   st.subheader('Map of all pickups')\\n\\nUse the st.map() function to plot the data:\\n\\npython\\n   st.map(data)\\n\\nSave your script. The map is fully interactive. Give it a try by panning or\\n   zooming in a bit.\\n\\nAfter drawing your histogram, you determined that the busiest hour for Uber\\npickups was 17:00. Let's redraw the map to show the concentration of pickups\\nat 17:00.\\n\\nLocate the following code snippet:\\n\\npython\\n   st.subheader('Map of all pickups')\\n   st.map(data)\\n\\nReplace it with:\\n\\npython\\n   hour_to_filter = 17\\n   filtered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]\\n   st.subheader(f'Map of all pickups at {hour_to_filter}:00')\\n   st.map(filtered_data)\\n\\nYou should see the data update instantly.\\n\\nTo draw this map we used the st.map function that's built into Streamlit, but\\nif you'd like to visualize complex map data, we encourage you to take a look at\\nthe st.pydeck_chart.\\n\\nFilter results with a slider\\n\\nIn the last section, when you drew the map, the time used to filter results was\\nhardcoded into the script, but what if we wanted to let a reader dynamically\\nfilter the data in real time? Using Streamlit's widgets you can. Let's add a\\nslider to the app with the st.slider() method.\\n\\nLocate hour_to_filter and replace it with this code snippet:\\n\\npython\\n   hour_to_filter = st.slider('hour', 0, 23, 17)  # min: 0h, max: 23h, default: 17h\\n\\nUse the slider and watch the map update in real time.\\n\\nUse a button to toggle data\\n\\nSliders are just one way to dynamically change the composition of your app.\\nLet's use the st.checkbox function to add a\\ncheckbox to your app. We'll use this checkbox to show/hide the raw data\\ntable at the top of your app.\\n\\nLocate these lines:\\n\\npython\\n   st.subheader('Raw data')\\n   st.write(data)\\n\\nReplace these lines with the following code:\\n\\npython\\n   if st.checkbox('Show raw data'):\\n       st.subheader('Raw data')\\n       st.write(data)\", metadata={'source': 'docs/content/library/get-started/create-an-app.md'}),\n",
       " Document(page_content='We\\'re sure you\\'ve got your own ideas. When you\\'re done with this tutorial, check out all the widgets that Streamlit exposes in our API Reference.\\n\\nLet\\'s put it all together\\n\\nThat\\'s it, you\\'ve made it to the end. Here\\'s the complete script for our interactive app.\\n\\nIf you\\'ve skipped ahead, after you\\'ve created your script, the command to run\\nStreamlit is streamlit run [app name].\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\nimport numpy as np\\n\\nst.title(\\'Uber pickups in NYC\\')\\n\\nDATE_COLUMN = \\'date/time\\'\\nDATA_URL = (\\'https://s3-us-west-2.amazonaws.com/\\'\\n            \\'streamlit-demo-data/uber-raw-data-sep14.csv.gz\\')\\n\\n@st.cache_data\\ndef load_data(nrows):\\n    data = pd.read_csv(DATA_URL, nrows=nrows)\\n    lowercase = lambda x: str(x).lower()\\n    data.rename(lowercase, axis=\\'columns\\', inplace=True)\\n    data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])\\n    return data\\n\\ndata_load_state = st.text(\\'Loading data...\\')\\ndata = load_data(10000)\\ndata_load_state.text(\"Done! (using st.cache_data)\")\\n\\nif st.checkbox(\\'Show raw data\\'):\\n    st.subheader(\\'Raw data\\')\\n    st.write(data)\\n\\nst.subheader(\\'Number of pickups by hour\\')\\nhist_values = np.histogram(data[DATE_COLUMN].dt.hour, bins=24, range=(0,24))[0]\\nst.bar_chart(hist_values)\\n\\nSome number in the range 0-23\\n\\nhour_to_filter = st.slider(\\'hour\\', 0, 23, 17)\\nfiltered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]\\n\\nst.subheader(\\'Map of all pickups at %s:00\\' % hour_to_filter)\\nst.map(filtered_data)\\n```\\n\\nShare your app\\n\\nAfter you’ve built a Streamlit app, it\\'s time to share it! To show it off to the world you can use Streamlit Community Cloud to deploy, manage, and share your app for free.\\n\\nIt works in 3 simple steps:\\n\\nPut your app in a public GitHub repo (and make sure it has a requirements.txt!)\\n\\nSign into share.streamlit.io\\n\\nClick \\'Deploy an app\\' and then paste in your GitHub URL', metadata={'source': 'docs/content/library/get-started/create-an-app.md'}),\n",
       " Document(page_content=\"That's it! 🎈 You now have a publicly deployed app that you can share with the world. Click to learn more about how to use Streamlit Community Cloud.\\n\\nGet help\\n\\nThat's it for getting started, now you can go and build your own apps! If you\\nrun into difficulties here are a few things you can do.\\n\\nCheck out our community forum and post a question\\n\\nQuick help from command line with streamlit help\\n\\nGo through our Knowledge Base for tips, step-by-step tutorials, and articles that answer your questions about creating and deploying Streamlit apps.\\n\\nRead more documentation! Check out:\\n\\nAdvanced features for things like caching, theming, and adding statefulness to apps.\\n\\nAPI reference for examples of every Streamlit command.\", metadata={'source': 'docs/content/library/get-started/create-an-app.md'}),\n",
       " Document(page_content='title: Main concepts\\nslug: /library/get-started/main-concepts\\n\\nMain concepts\\n\\nWorking with Streamlit is simple. First you sprinkle a few Streamlit commands\\ninto a normal Python script, then you run it with streamlit run:\\n\\nbash\\nstreamlit run your_script.py [-- script args]\\n\\nAs soon as you run the script as shown above, a local Streamlit server will\\nspin up and your app will open in a new tab in your default web browser. The app\\nis your canvas, where you\\'ll draw charts, text, widgets, tables, and more.\\n\\nWhat gets drawn in the app is up to you. For example\\nst.text writes raw text to your app, and\\nst.line_chart draws — you guessed it — a\\nline chart. Refer to our API documentation to see all commands that\\nare available to you.\\n\\nWhen passing your script some custom arguments, they must be passed after two dashes. Otherwise the\\narguments get interpreted as arguments to Streamlit itself.\\n\\nAnother way of running Streamlit is to run it as a Python module. This can be\\nuseful when configuring an IDE like PyCharm to work with Streamlit:\\n\\n```bash\\n\\nRunning\\n\\npython -m streamlit run your_script.py\\n\\nis equivalent to:\\n\\nstreamlit run your_script.py\\n```\\n\\nYou can also pass a URL to streamlit run! This is great when combined with\\nGitHub Gists. For example:\\n\\nbash\\nstreamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py\\n\\nDevelopment flow\\n\\nEvery time you want to update your app, save the source file. When you do\\nthat, Streamlit detects if there is a change and asks you whether you want to\\nrerun your app. Choose \"Always rerun\" at the top-right of your screen to\\nautomatically update your app every time you change its source code.\\n\\nThis allows you to work in a fast interactive loop: you type some code, save\\nit, try it out live, then type some more code, save it, try it out, and so on\\nuntil you\\'re happy with the results. This tight loop between coding and viewing\\nresults live is one of the ways Streamlit makes your life easier.', metadata={'source': 'docs/content/library/get-started/main-concepts.md'}),\n",
       " Document(page_content=\"While developing a Streamlit app, it's recommended to lay out your editor and\\nbrowser windows side by side, so the code and the app can be seen at the same\\ntime. Give it a try!\\n\\nAs of Streamlit version 1.10.0 and higher, Streamlit apps cannot be run from the root directory of Linux distributions. If you try to run a Streamlit app from the root directory, Streamlit will throw a FileNotFoundError: [Errno 2] No such file or directory error. For more information, see GitHub issue #5239.\\n\\nIf you are using Streamlit version 1.10.0 or higher, your main script should live in a directory other than the root directory. When using Docker, you can use the WORKDIR command to specify the directory where your main script lives. For an example of how to do this, read Create a Dockerfile.\\n\\nData flow\\n\\nStreamlit's architecture allows you to write apps the same way you write plain\\nPython scripts. To unlock this, Streamlit apps have a unique data flow: any\\ntime something must be updated on the screen, Streamlit reruns your entire\\nPython script from top to bottom.\\n\\nThis can happen in two situations:\\n\\nWhenever you modify your app's source code.\\n\\nWhenever a user interacts with widgets in the app. For example, when dragging\\n  a slider, entering text in an input box, or clicking a button.\\n\\nWhenever a callback is passed to a widget via the on_change (or on_click) parameter, the callback will always run before the rest of your script. For details on the Callbacks API, please refer to our Session State API Reference Guide.\\n\\nAnd to make all of this fast and seamless, Streamlit does some heavy lifting\\nfor you behind the scenes. A big player in this story is the\\n@st.cache_data decorator, which allows developers to skip certain\\ncostly computations when their apps rerun. We'll cover caching later in this\\npage.\\n\\nDisplay and style data\", metadata={'source': 'docs/content/library/get-started/main-concepts.md'}),\n",
       " Document(page_content='Display and style data\\n\\nThere are a few ways to display data (tables, arrays, data frames) in Streamlit\\napps. Below, you will be introduced to magic\\nand st.write(), which can be used to write\\nanything from text to tables. After that, let\\'s take a look at methods designed\\nspecifically for visualizing data.\\n\\nUse magic\\n\\nYou can also write to your app without calling any Streamlit methods.\\nStreamlit supports \"magic commands,\" which means you don\\'t have to use\\nst.write() at all! To see this in action try this snippet:\\n\\n```python\\n\"\"\"\\n\\nMy first app\\n\\nHere\\'s our first attempt at using data to create a table:\\n\"\"\"\\n\\nimport streamlit as st\\nimport pandas as pd\\ndf = pd.DataFrame({\\n  \\'first column\\': [1, 2, 3, 4],\\n  \\'second column\\': [10, 20, 30, 40]\\n})\\n\\ndf\\n```\\n\\nAny time that Streamlit sees a variable or a literal\\nvalue on its own line, it automatically writes that to your app using\\nst.write(). For more information, refer to the\\ndocumentation on magic commands.\\n\\nWrite a data frame\\n\\nAlong with magic commands,\\nst.write() is Streamlit\\'s \"Swiss Army knife\". You\\ncan pass almost anything to st.write():\\ntext, data, Matplotlib figures, Altair charts, and more. Don\\'t worry, Streamlit\\nwill figure it out and render things the right way.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\nst.write(\"Here\\'s our first attempt at using data to create a table:\")\\nst.write(pd.DataFrame({\\n    \\'first column\\': [1, 2, 3, 4],\\n    \\'second column\\': [10, 20, 30, 40]\\n}))\\n```\\n\\nThere are other data specific functions like\\nst.dataframe() and\\nst.table() that you can also use for displaying\\ndata. Let\\'s understand when to use these features and how to add colors and styling to your data frames.\\n\\nYou might be asking yourself, \"why wouldn\\'t I always use st.write()?\" There are\\na few reasons:', metadata={'source': 'docs/content/library/get-started/main-concepts.md'}),\n",
       " Document(page_content=\"Magic and st.write() inspect the type of\\n   data that you've passed in, and then decide how to best render it in the\\n   app. Sometimes you want to draw it another way. For example, instead of\\n   drawing a dataframe as an interactive table, you may want to draw it as a\\n   static table by using st.table(df).\\n\\nThe second reason is that other methods return an object that can be used\\n   and modified, either by adding data to it or replacing it.\\n\\nFinally, if you use a more specific Streamlit method you can pass additional\\n   arguments to customize its behavior.\\n\\nFor example, let's create a data frame and change its formatting with a Pandas\\nStyler object. In this example, you'll use Numpy to generate a random sample,\\nand the st.dataframe() method to draw an\\ninteractive table.\\n\\nThis example uses Numpy to generate a random sample, but you can use Pandas\\nDataFrames, Numpy arrays, or plain Python arrays.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\n\\ndataframe = np.random.randn(10, 20)\\nst.dataframe(dataframe)\\n```\\n\\nLet's expand on the first example using the Pandas Styler object to highlight\\nsome elements in the interactive table.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\ndataframe = pd.DataFrame(\\n    np.random.randn(10, 20),\\n    columns=('col %d' % i for i in range(20)))\\n\\nst.dataframe(dataframe.style.highlight_max(axis=0))\\n```\\n\\nStreamlit also has a method for static table generation:\\nst.table().\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\ndataframe = pd.DataFrame(\\n    np.random.randn(10, 20),\\n    columns=('col %d' % i for i in range(20)))\\nst.table(dataframe)\\n```\\n\\nDraw charts and maps\\n\\nStreamlit supports several popular data charting libraries like Matplotlib,\\nAltair, deck.gl, and more. In this section, you'll\\nadd a bar chart, line chart, and a map to your app.\\n\\nDraw a line chart\\n\\nYou can easily add a line chart to your app with\\nst.line_chart(). We'll generate a random\\nsample using Numpy and then chart it.\", metadata={'source': 'docs/content/library/get-started/main-concepts.md'}),\n",
       " Document(page_content='```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\nchart_data = pd.DataFrame(\\n     np.random.randn(20, 3),\\n     columns=[\\'a\\', \\'b\\', \\'c\\'])\\n\\nst.line_chart(chart_data)\\n```\\n\\nPlot a map\\n\\nWith st.map() you can display data points on a map.\\nLet\\'s use Numpy to generate some sample data and plot it on a map of\\nSan Francisco.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\nmap_data = pd.DataFrame(\\n    np.random.randn(1000, 2) / [50, 50] + [37.76, -122.4],\\n    columns=[\\'lat\\', \\'lon\\'])\\n\\nst.map(map_data)\\n```\\n\\nWidgets\\n\\nWhen you\\'ve got the data or model into the state that you want to explore, you\\ncan add in widgets like st.slider(),\\nst.button() or\\nst.selectbox(). It\\'s really straightforward\\n— treat widgets as variables:\\n\\npython\\nimport streamlit as st\\nx = st.slider(\\'x\\')  # 👈 this is a widget\\nst.write(x, \\'squared is\\', x * x)\\n\\nOn first run, the app above should output the text \"0 squared is 0\". Then\\nevery time a user interacts with a widget, Streamlit simply reruns your script\\nfrom top to bottom, assigning the current state of the widget to your variable\\nin the process.\\n\\nFor example, if the user moves the slider to position 10, Streamlit will\\nrerun the code above and set x to 10 accordingly. So now you should see the\\ntext \"10 squared is 100\".\\n\\nWidgets can also be accessed by key, if you choose to specify a string to use as the unique key for the widget:\\n\\n```python\\nimport streamlit as st\\nst.text_input(\"Your name\", key=\"name\")\\n\\nYou can access the value at any point with:\\n\\nst.session_state.name\\n```\\n\\nEvery widget with a key is automatically added to Session State. For more information about Session State, its association with widget state, and its limitations, see Session State API Reference Guide.\\n\\nUse checkboxes to show/hide data', metadata={'source': 'docs/content/library/get-started/main-concepts.md'}),\n",
       " Document(page_content=\"Use checkboxes to show/hide data\\n\\nOne use case for checkboxes is to hide or show a specific chart or section in\\nan app. st.checkbox() takes a single argument,\\nwhich is the widget label. In this sample, the checkbox is used to toggle a\\nconditional statement.\\n\\n```python\\nimport streamlit as st\\nimport numpy as np\\nimport pandas as pd\\n\\nif st.checkbox('Show dataframe'):\\n    chart_data = pd.DataFrame(\\n       np.random.randn(20, 3),\\n       columns=['a', 'b', 'c'])\\n\\n```\\n\\nUse a selectbox for options\\n\\nUse st.selectbox to choose from a series. You\\ncan write in the options you want, or pass through an array or data frame\\ncolumn.\\n\\nLet's use the df data frame we created earlier.\\n\\n```python\\nimport streamlit as st\\nimport pandas as pd\\n\\ndf = pd.DataFrame({\\n    'first column': [1, 2, 3, 4],\\n    'second column': [10, 20, 30, 40]\\n    })\\n\\noption = st.selectbox(\\n    'Which number do you like best?',\\n     df['first column'])\\n\\n'You selected: ', option\\n```\\n\\nLayout\\n\\nStreamlit makes it easy to organize your widgets in a left panel sidebar with\\nst.sidebar. Each element that's passed to\\nst.sidebar is pinned to the left, allowing\\nusers to focus on the content in your app while still having access to UI\\ncontrols.\\n\\nFor example, if you want to add a selectbox and a slider to a sidebar,\\nuse st.sidebar.slider and st.sidebar.selectbox instead of st.slider and\\nst.selectbox:\\n\\n```python\\nimport streamlit as st\\n\\nAdd a selectbox to the sidebar:\\n\\nadd_selectbox = st.sidebar.selectbox(\\n    'How would you like to be contacted?',\\n    ('Email', 'Home phone', 'Mobile phone')\\n)\\n\\nAdd a slider to the sidebar:\\n\\nadd_slider = st.sidebar.slider(\\n    'Select a range of values',\\n    0.0, 100.0, (25.0, 75.0)\\n)\\n```\\n\\nBeyond the sidebar, Streamlit offers several other ways to control the layout\\nof your app. st.columns lets you place widgets side-by-side, and\\nst.expander lets you conserve space by hiding away large content.\\n\\n```python\\nimport streamlit as st\\n\\nleft_column, right_column = st.columns(2)\", metadata={'source': 'docs/content/library/get-started/main-concepts.md'}),\n",
       " Document(page_content='left_column, right_column = st.columns(2)\\n\\nYou can use a column just like st.sidebar:\\n\\nleft_column.button(\\'Press me!\\')\\n\\nOr even better, call Streamlit functions inside a \"with\" block:\\n\\nwith right_column:\\n    chosen = st.radio(\\n        \\'Sorting hat\\',\\n        (\"Gryffindor\", \"Ravenclaw\", \"Hufflepuff\", \"Slytherin\"))\\n    st.write(f\"You are in {chosen} house!\")\\n```\\n\\nst.echo and st.spinner are not currently supported inside the sidebar\\nor layout options. Rest assured, though, we\\'re currently working on adding support for those too!\\n\\nShow progress\\n\\nWhen adding long running computations to an app, you can use\\nst.progress() to display status in real time.\\n\\nFirst, let\\'s import time. We\\'re going to use the time.sleep() method to\\nsimulate a long running computation:\\n\\npython\\nimport time\\n\\nNow, let\\'s create a progress bar:\\n\\n```python\\nimport streamlit as st\\nimport time\\n\\n\\'Starting a long computation...\\'\\n\\nAdd a placeholder\\n\\nlatest_iteration = st.empty()\\nbar = st.progress(0)\\n\\nfor i in range(100):\\n  # Update the progress bar with each iteration.\\n  latest_iteration.text(f\\'Iteration {i+1}\\')\\n  bar.progress(i + 1)\\n  time.sleep(0.1)\\n\\n\\'...and now we\\\\\\'re done!\\'\\n```\\n\\nThemes\\n\\nStreamlit supports Light and Dark themes out of the box. Streamlit will first\\ncheck if the user viewing an app has a Light or Dark mode preference set by\\ntheir operating system and browser. If so, then that preference will be used.\\nOtherwise, the Light theme is applied by default.\\n\\nYou can also change the active theme from \"☰\" → \"Settings\".\\n\\nWant to add your own theme to an app? The \"Settings\" menu has a theme editor\\naccessible by clicking on \"Edit active theme\". You can use this editor to try\\nout different colors and see your app update live.', metadata={'source': 'docs/content/library/get-started/main-concepts.md'}),\n",
       " Document(page_content='When you\\'re happy with your work, themes can be saved by\\nsetting config options\\nin the [theme] config section. After you\\'ve defined a theme for your app, it\\nwill appear as \"Custom Theme\" in the theme selector and will be applied by\\ndefault instead of the included Light and Dark themes.\\n\\nMore information about the options available when defining a theme can be found\\nin the theme option documentation.\\n\\nThe theme editor menu is available only in local development. If you\\'ve deployed your app using\\nStreamlit Community Cloud, the \"Edit active theme\" button will no longer be displayed in the \"Settings\"\\nmenu.\\n\\nAnother way to experiment with different theme colors is to turn on the \"Run on save\" option, edit\\nyour config.toml file, and watch as your app reruns with the new theme colors applied.\\n\\nCaching\\n\\nThe Streamlit cache allows your app to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations.\\n\\nThe basic idea behind caching is to store the results of expensive function calls and return the cached result when the same inputs occur again rather than calling the function on subsequent runs.\\n\\nTo cache a function in Streamlit, you need to decorate it with one of two decorators (st.cache_data and st.cache_resource):\\n\\npython\\n@st.cache_data\\ndef long_running_function(param1, param2):\\n    return …\\n\\nIn this example, decorating long_running_function with @st.cache_data tells Streamlit that whenever the function is called, it checks two things:\\n\\nThe values of the input parameters (in this case, param1 and param2).\\n\\nThe code inside the function.', metadata={'source': 'docs/content/library/get-started/main-concepts.md'}),\n",
       " Document(page_content=\"The code inside the function.\\n\\nIf this is the first time Streamlit sees these parameter values and function code, it runs the function and stores the return value in a cache. The next time the function is called with the same parameters and code (e.g., when a user interacts with the app), Streamlit will skip executing the function altogether and return the cached value instead. During development, the cache updates automatically as the function code changes, ensuring that the latest changes are reflected in the cache.\\n\\nAs mentioned, there are two caching decorators:\\n\\nst.cache_data\\xa0is the recommended way to cache computations that return data: loading a DataFrame from CSV, transforming a NumPy array, querying an API, or any other function that returns a serializable data object (str, int, float, DataFrame, array, list, …). It creates a new copy of the data at each function call, making it safe against mutations and race conditions. The behavior of st.cache_data is what you want in most cases – so if you're unsure, start with\\xa0st.cache_data\\xa0and see if it works!\\n\\nst.cache_resource\\xa0is the recommended way to cache global resources like ML models or database connections – unserializable objects that you don’t want to load multiple times. Using it, you can share these resources across all reruns and sessions of an app without copying or duplication. Note that any mutations to the cached return value directly mutate the object in the cache (more details below).\\n\\nFor more information about the Streamlit caching decorators, their configuration parameters, and their limitations, see Caching.\\n\\nPages\\n\\nAs apps grow large, it becomes useful to organize them into multiple pages. This makes the app easier to manage as a developer and easier to navigate as a user. Streamlit provides a frictionless way to create multipage apps.\\n\\nWe designed this feature so that building a multipage app is as easy as building a single-page app! Just add more pages to an existing app as follows:\", metadata={'source': 'docs/content/library/get-started/main-concepts.md'}),\n",
       " Document(page_content=\"In the folder containing your main script, create a new pages folder. Let’s say your main script is named main_page.py.\\n\\nAdd new .py files in the pages folder to add more pages to your app.\\n\\nRun streamlit run main_page.py as usual.\\n\\nThat’s it! The main_page.py script will now correspond to the main page of your app. And you’ll see the other scripts from the pages folder in the sidebar page selector. For example:\\n\\nNow run streamlit run main_page.py and view your shiny new multipage app!\\n\\nOur documentation on Multipage apps teaches you how to add pages to your app, including how to define pages, structure and run multipage apps, and navigate between pages. Once you understand the basics, create your first multipage app!\\n\\nApp model\\n\\nNow that you know a little more about all the individual pieces, let's close\\nthe loop and review how it works together:\\n\\nStreamlit apps are Python scripts that run from top to bottom\\n\\nEvery time a user opens a browser tab pointing to your app, the script is\\n   re-executed\\n\\nAs the script executes, Streamlit draws its output live in a browser\\n\\nScripts use the Streamlit cache to avoid recomputing expensive functions, so\\n   updates happen very fast\\n\\nEvery time a user interacts with a widget, your script is re-executed and\\n   the output value of that widget is set to the new value during that run.\\n\\nStreamlit apps can contain multiple pages, which are defined in separate\\n   .py files in a pages folder.\", metadata={'source': 'docs/content/library/get-started/main-concepts.md'})]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 50,\n",
    ")\n",
    "steamlit_doc_splitted = text_splitter.split_documents(steamlit_doc)\n",
    "steamlit_doc_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 22/112 [00:00<00:00, 217.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:00<00:00, 185.52it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    \"docs/\",\n",
    "    glob=\"python/api-examples-source/**/*.py\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON),\n",
    "    show_progress=True\n",
    ")\n",
    "streamlit_code_example_doc = loader.load()\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "streamlit_code_example_doc_splitted = python_splitter.split_documents(streamlit_code_example_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in source code: 163803, max tokens in source code: 582, price: 0.0163803\n",
      "Number of tokens in source code: 23121, max tokens in source code: 161, price: 0.0023121\n"
     ]
    }
   ],
   "source": [
    "token, max_token = num_tokens_from_string(steamlit_doc_splitted, \"cl100k_base\")\n",
    "print(f\"Number of tokens in source code: {token}, max tokens in source code: {max_token}, price: {token * 0.0001/1000}\")\n",
    "token, max_token = num_tokens_from_string(streamlit_code_example_doc_splitted, \"cl100k_base\")\n",
    "print(f\"Number of tokens in source code: {token}, max tokens in source code: {max_token}, price: {token * 0.0001/1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectorstore_code = Chroma.from_documents(documents=steamlit_doc_splitted + streamlit_code_example_doc_splitted,embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit_code_retriever = vectorstore_code.as_retriever()\n",
    "streamlit_code_retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "streamlit_code_retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "streamlit_code_retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "streamlit_code_retriever.search_kwargs[\"k\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "model_name = \"text-davinci-003\"\n",
    "llm = OpenAI(temperature=0, model_name=model_name, openai_api_key=openai_api_key)\n",
    "#llm = ChatOpenAI(temperature=0, model_name=model_name, openai_api_key=openai_api_key)  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "\n",
    "retriever = streamlit_code_retriever\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 5\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 5\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"You're an AI assistant specializing in python development. \n",
    "You will be given a question, the current python code to modify with and several documents.\n",
    "Your goal is to modify the python code to answer the question. You can use the documents to get up to date Streamlit api references.\n",
    "Additionally, offer a brief explanation about how you arrived at the python code and give the shell commands to install additional libraries if needed.\n",
    "If the input is a question, you must explain the code only and additionnaly propose some code. Do not halucinate or make up information. If you do not know the answer, just say \"I don't know\".\n",
    "\n",
    "Documents related to the question:\n",
    "{context}\n",
    "\n",
    "The current python code you must update is the following:\n",
    "```python\n",
    "{python_code}\n",
    "```\n",
    "\n",
    "Write your anwser in the following format:\n",
    "```python\n",
    "the code you generated\n",
    "```\n",
    "the explanation of the code you generated\n",
    "\n",
    "If you did not generated any code (for instance when the user ask a question, not an instruction), this is the format:\n",
    "```python\n",
    "None\n",
    "```\n",
    "the anwser to the question\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"python_code\", \"context\", \"question\"]\n",
    ")\n",
    "combine_docs_chain_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "qa_over_streamlit_code = ConversationalRetrievalCodeChain.from_llm(llm=llm, retriever=retriever, return_source_documents=True, combine_docs_chain_kwargs=combine_docs_chain_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='webcam_demo()\\n\\nshow_code(webcam_demo)', metadata={'source': 'docs/python/api-examples-source/mpa-hello/pages/4_📷_Webcam_Demo.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content=\"title: Enabling camera access in your browser\\nslug: /knowledge-base/using-streamlit/enable-camera\\n\\nEnabling camera access in your browser\\n\\nStreamlit apps may include a widget to upload images directly from your computer's camera. To\\nsafeguard the users' privacy and security, browsers require users to explicitly allow access to the\\ncamera before this widget can work.\\n\\nTo learn how to enable camera access, please check the documentation for your browser:\\n\\nChrome\\n\\nSafari\\n\\nFirefox\", metadata={'source': 'docs/content/kb/using-streamlit/enable-camera.md'}),\n",
       " Document(page_content='title: st.camera_input\\nslug: /library/api-reference/widgets/st.camera_input\\ndescription: st.camera_input displays a widget to upload images from a camera\\n\\nTo read the image file buffer as bytes, you can use getvalue() on the UploadedFile object.\\n\\n```python\\nimport streamlit as st\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as bytes:\\n    bytes_data = img_file_buffer.getvalue()\\n    # Check the type of bytes_data:\\n    # Should output: \\n    st.write(type(bytes_data))\\n```\\n\\nst.camera_input returns an object of the UploadedFile class, which a subclass of BytesIO. Therefore it is a \"file-like\" object. This means you can pass it anywhere where a file is expected, similar to st.file_uploader.\\n\\nImage processing examples\\n\\nPillow,\\n\\nNumPy,\\n\\nOpenCV,\\n\\nTensorFlow,\\n\\ntorchvision, and\\n\\nPyTorch.\\n\\nWhile we provide examples for the most popular use-cases and libraries, you are welcome to adapt these examples to your own needs and favorite libraries.\\n\\nPillow (PIL) and NumPy\\n\\nEnsure you have installed Pillow and NumPy.\\n\\nTo read the image file buffer as a PIL Image and convert it to a NumPy array:\\n\\n```python\\nimport streamlit as st\\nfrom PIL import Image\\nimport numpy as np\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a PIL Image:\\n    img = Image.open(img_file_buffer)\\n\\n```\\n\\nOpenCV (cv2)\\n\\nEnsure you have installed OpenCV and NumPy.\\n\\nTo read the image file buffer with OpenCV:\\n\\n```python\\nimport streamlit as st\\nimport cv2\\nimport numpy as np\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer with OpenCV:\\n    bytes_data = img_file_buffer.getvalue()\\n    cv2_img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)\\n\\n```\\n\\nTensorFlow\\n\\nEnsure you have installed TensorFlow.\\n\\nTo read the image file buffer as a 3 dimensional uint8 tensor with TensorFlow:', metadata={'source': 'docs/content/library/api/widgets/camera_input.md'}),\n",
       " Document(page_content='# Code for: def webcam_demo():\\n\\n\\nst.set_page_config(page_title=\"Webcam Demo\", page_icon=\"📷\")\\nst.markdown(\"# Webcam Demo\")\\nst.sidebar.header(\"Webcam Demo\")\\nst.write(\\n    \"\"\"This demo illustrates the use of the \\n    [`st.camera_input`](https://docs.streamlit.io/library/api-reference/widgets/st.camera_input) \\n    widget — which lets the user take an image through their webcam and upload it to the app — \\n    to apply a filter to the uploaded image. Enjoy!\"\"\"\\n)\\n\\nwebcam_demo()', metadata={'source': 'docs/python/api-examples-source/mpa-hello/pages/4_📷_Webcam_Demo.py', 'content_type': 'simplified_code', 'language': 'python'}),\n",
       " Document(page_content='title: st.chat_input\\nslug: /library/api-reference/chat/st.chat_input\\ndescription: st.chat_input displays a chat input widget.\\n\\nRead the Build conversational apps tutorial to learn how to use st.chat_message and st.chat_input to build chat-based apps.', metadata={'source': 'docs/content/library/api/chat/chat-input.md'})]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    "    Callbacks,\n",
    ")\n",
    "qa_over_streamlit_code._get_docs(question=\"display camera input\", inputs={}, run_manager=CallbackManagerForChainRun.get_noop_manager())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse(output):\n",
    "    python_code = None\n",
    "    explain_code = None\n",
    "    pattern = r\"```python\\n(.*?)\\n```.*\\n(.*?)$\"\n",
    "    python_code_match = re.search(pattern, output, re.DOTALL | re.MULTILINE)\n",
    "    if python_code_match:\n",
    "        python_code = python_code_match.group(1)\n",
    "        explain_code = python_code_match.group(2)\n",
    "        if python_code == \"None\":\n",
    "            python_code = None\n",
    "    return python_code, explain_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_over_streamlit_code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m chat_history \u001b[39m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m questions:\n\u001b[0;32m---> 11\u001b[0m     result \u001b[39m=\u001b[39m qa_over_streamlit_code({\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: question, \u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m: chat_history, \u001b[39m\"\u001b[39m\u001b[39mpython_code\u001b[39m\u001b[39m\"\u001b[39m: last_code})\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(result)\n\u001b[1;32m     13\u001b[0m     display(Markdown(result[\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qa_over_streamlit_code' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "questions = [\n",
    "    \"Add a title to the page\",\n",
    "    \"change the title text to 'Hello world!'\",\n",
    "    \"Get the input from a camera and display it\",\n",
    "]\n",
    "last_code = \"import streamlit as st\\nst.write('Hello world!')\"\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_over_streamlit_code({\"question\": question, \"chat_history\": chat_history, \"python_code\": last_code})\n",
    "    print(result)\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "    code, explain = parse(result[\"answer\"])\n",
    "    last_code = code\n",
    "    chat_history.append((question, explain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_question  \" camera_input Image module error fix\"\n",
      "{'question': \"I have this error 'no module named Image', fix it\", 'chat_history': [('Add a title to the page', 'This code will add a title to the page using the Streamlit built-in st.title() function.'), (\"change the title text to 'Hello world!'\", \"This code will change the title text to 'Hello world!' using the Streamlit built-in st.title() function.\"), ('Get the input from a camera and display it', 'This code will get the input from the camera and display it on the screen using the Streamlit built-in st.camera_input() and st.image() functions.')], 'python_code': 'import streamlit as st\\n\\nimg_file_buffer = st.camera_input(\"Take a picture\")\\n\\nif img_file_buffer is not None:\\n    # To read image file buffer as a PIL Image:\\n    img = Image.open(img_file_buffer)\\n    st.image(img, caption=\\'Image from camera\\', use_column_width=True)', 'answer': '\\n```python\\nNone\\n```\\nThis error occurs when you try to import a module that is not included in your requirements file. To fix this, you need to install the module in your requirements file. For example, if you are trying to import the Image module, you need to add the following line to your requirements file:\\n\\nPillow==7.2.0', 'source_documents': [Document(page_content=\"title: ModuleNotFoundError No module named\\nslug: /knowledge-base/dependencies/module-not-found-error\\n\\nModuleNotFoundError: No module named\\n\\nProblem\\n\\nYou receive the error ModuleNotFoundError: No module named when you deploy an app on Streamlit Community Cloud.\\n\\nSolution\\n\\nThis error occurs when you import a module on Streamlit Community Cloud that isn’t included in your requirements file. Any external Python dependencies that are not distributed with a standard Python installation should be included in your requirements file.\\n\\nE.g. You will see ModuleNotFoundError: No module named 'sklearn' if you don’t include scikit-learn in your requirements file and import sklearn in your app.\\n\\nRelated forum posts:\\n\\nhttps://discuss.streamlit.io/t/getting-error-modulenotfounderror-no-module-named-beautifulsoup/9126\\n\\nhttps://discuss.streamlit.io/t/modulenotfounderror-no-module-named-vega-datasets/16354\", metadata={'source': 'docs/content/kb/dependencies/module-not-found-error.md'}), Document(page_content=\"title: Enabling camera access in your browser\\nslug: /knowledge-base/using-streamlit/enable-camera\\n\\nEnabling camera access in your browser\\n\\nStreamlit apps may include a widget to upload images directly from your computer's camera. To\\nsafeguard the users' privacy and security, browsers require users to explicitly allow access to the\\ncamera before this widget can work.\\n\\nTo learn how to enable camera access, please check the documentation for your browser:\\n\\nChrome\\n\\nSafari\\n\\nFirefox\", metadata={'source': 'docs/content/kb/using-streamlit/enable-camera.md'}), Document(page_content='# Code for: def webcam_demo():\\n\\n\\nst.set_page_config(page_title=\"Webcam Demo\", page_icon=\"📷\")\\nst.markdown(\"# Webcam Demo\")\\nst.sidebar.header(\"Webcam Demo\")\\nst.write(\\n    \"\"\"This demo illustrates the use of the \\n    [`st.camera_input`](https://docs.streamlit.io/library/api-reference/widgets/st.camera_input) \\n    widget — which lets the user take an image through their webcam and upload it to the app — \\n    to apply a filter to the uploaded image. Enjoy!\"\"\"\\n)\\n\\nwebcam_demo()', metadata={'source': 'docs/python/api-examples-source/mpa-hello/pages/4_📷_Webcam_Demo.py', 'content_type': 'simplified_code', 'language': 'python'}), Document(page_content='title: ImportError libGL.so.1 cannot open shared object file No such file or directory\\nslug: /knowledge-base/dependencies/libgl\\n\\nImportError libGL.so.1 cannot open shared object file No such file or directory\\n\\nProblem\\n\\nYou receive the error ImportError libGL.so.1 cannot open shared object file No such file or directory when using OpenCV in your app deployed on Streamlit Community Cloud.\\n\\nSolution\\n\\nIf you use OpenCV in your app, include opencv-python-headless in your requirements file on Streamlit Community Cloud in place of opencv_contrib_python and opencv-python.\\n\\nIf opencv-python is a required (non-optional) dependency of your app or a dependency of a library used in your app, the above solution is not applicable. Instead, you can use the following solution:\\n\\nCreate a packages.txt file in your repo with the following line to install the apt-get dependency libgl:\\n\\nlibgl1', metadata={'source': 'docs/content/kb/dependencies/libgl.md'}), Document(page_content='title: Installing dependencies\\nslug: /knowledge-base/dependencies\\n\\nInstalling dependencies\\n\\nModuleNotFoundError: No module named\\n\\nImportError: libGL.so.1: cannot open shared object file: No such file or directory\\n\\nERROR: No matching distribution found for\\n\\nHow to install a package not on PyPI/Conda but available on GitHub\\n\\nInstall the Snowflake Connector for Python on Streamlit Community Cloud', metadata={'source': 'docs/content/kb/dependencies/index.md'})]}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "None\n",
       "```\n",
       "This error occurs when you try to import a module that is not included in your requirements file. To fix this, you need to install the module in your requirements file. For example, if you are trying to import the Image module, you need to add the following line to your requirements file:\n",
       "\n",
       "Pillow==7.2.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "questions = [\n",
    "    \"But can you modify the app ?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_over_streamlit_code({\"question\": question, \"chat_history\": chat_history, \"python_code\": last_code})\n",
    "    print(result)\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "    code, explain = parse(result[\"answer\"])\n",
    "    last_code = code\n",
    "    chat_history.append((question, explain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAssistant is a large language model trained by OpenAI.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "\n",
      "Human: What is the version of streamlit ?\n",
      "Assistant:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The current version of Streamlit, as of September 2021, is 0.89.0. However, it's important to note that software versions can change over time as new updates and releases are made. It's always a good idea to check the official Streamlit website or documentation for the most up-to-date information on the current version.\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "template = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "\n",
    "chatgpt_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", openai_api_key=openai_api_key),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    ")\n",
    "\n",
    "output = chatgpt_chain.predict(\n",
    "    human_input=\"What is the version of streamlit ?\",\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
